,제목,저자,출판일,기관,초록
0,딥러닝을 이용한 주변 무선단말 파악방안,이웅섭 /Lee， Woong-sup,2018,한국정보통신학회,"최근 단말-대-단말(Device-to-device, D2D) 통신기술이 차세대 무선통신시스템의 핵심기술로 큰 관심을 받고 있다. 이러한 단말간 통신에서는 자신의 주변에 어떠한 단말이 있는지 파악하는 주변단말 탐색(Neighbor discovery)이매우 중요하다. 본 논문에서는 최근 큰 관심을 받고 있는 딥러닝(Deep learning) 기술을 활용하여 단말간 통신에서 주변단말을 파악하는 방안에 대해서 제안한다. 제안 방안은 기존의 방안과 달리 무선채널의 공간적 연관성을 이용하여단말간의 신호 전송 없이 단말이 기지국으로 전송하는 상향링크 파일럿 신호를 기반으로 주변 단말을 찾고 따라서기존의 방식에 비해 신호전송 복잡도(signaling complexity)를 크게 줄일 수 있다. 또한 제안 방안에서는 떨어져 있는거리에 따라서 주변 단말을 분류 가능하여 기존 방안에 비해서 좀 더 세밀한 단말 탐색이 가능하다. 마지막으로 본 논문에서는 tensorflow를 이용한 컴퓨터 시뮬레이션을 통해 제안 방안의 성능을 검증하였다.Recently, the device-to-device (D2D) communication has been conceived as the key technology for the next-generationmobile communication systems. The neighbor discovery in which the nearby users are found, is essential for the properoperation of the D2D communication. In this paper, we propose new neighbor discovery scheme based on deep learningtechnology which has gained a lot of attention recently. In the proposed scheme, the neighboring users can be foundusing the uplink pilot transmission of users only, unlike conventional neighbor discovery schemes in which direct pilotcommunication among users is required, such that the signaling overhead can be greatly reduced in our proposed scheme.
Moreover, the neighbors with different proximity can also be classified accordingly which enables more accurate neighbordiscovery compared to the conventional schemes. The performance of our proposed scheme is verified through thetensorflow-based computer simulations.
키워드 : 딥러닝, 주변단말 탐색, 단말-대-단말 통신Deep learning, Neighbor discovery, Device-to-device communication, Wireless channel correlation"
1,신뢰성있는 딥러닝 기반 분석 모델을 참조하기 위한 딥러닝 기술 언어,문종혁 /Jong Hyeok Mun,2021,한국정보처리학회,"최근 딥러닝은 하드웨어 성능이 향상됨에 따라 자연어 처리, 영상 인식 등의 다양한 기술에 접목되어 활용되고 있다. 이러한 기술들을 활용해 지능형교통 시스템(ITS), 스마트홈, 헬스케어 등의 산업분야에서 데이터를 분석하여 고속도로 속도위반 차량 검출, 에너지 사용량 제어, 응급상황 등과 같은고품질의 서비스를 제공하며, 고품질의 서비스를 제공하기 위해서는 정확도가 향상된 딥러닝 모델이 적용되어야 한다. 이를 위해 서비스 환경의 데이터를분석하기 위한 딥러닝 모델을 개발할 때, 개발자는 신뢰성이 검증된 최신의 딥러닝 모델을 적용할 수 있어야 한다. 이는 개발자가 참조하는 딥러닝 모델에적용된 학습 데이터셋의 정확도를 측정하여 검증할 수 있다. 이러한 검증을 위해서 개발자는 학습 데이터셋, 딥러닝의 계층구조 및 개발 환경 등과 같은내용을 포함하는 딥러닝 모델을 문서화하여 적용하기 위한 구조적인 정보가 필요하다. 본 논문에서는 신뢰성있는 딥러닝 기반 데이터 분석 모델을 참조하기위한 딥러닝 기술 언어를 제안한다. 제안하는 기술 언어는 신뢰성 있는 딥러닝 모델을 개발하는데 필요한 학습데이터셋, 개발 환경 및 설정 등의 정보와더불어 딥러닝 모델의 계층구조를 표현할 수 있다. 제안하는 딥러닝 기술 언어를 이용하여 개발자는 지능형 교통 시스템에서 참조하는 분석 모델의정확도를 검증할 수 있다. 실험에서는 제안하는 언어의 유효성을 검증하기 위해, 번호판 인식 모델을 중심으로 딥러닝 기술 문서의 적용과정을 보인다With the recent advancements of deep learning, companies such as smart home, healthcare, and intelligent transportation systems areutilizing its functionality to provide high-quality services for vehicle detection, emergency situation detection, and controlling energyconsumption. To provide reliable services in such sensitive systems, deep learning models are required to have high accuracy. In orderto develop a deep learning model for analyzing previously mentioned services, developers should utilize the state of the art deep learningmodels that have already been verified for higher accuracy. The developers can verify the accuracy of the referenced model by validatingthe model on the dataset. For this validation, the developer needs structural information to document and apply deep learning models,including metadata such as learning dataset, network architecture, and development environments. In this paper, we propose a descriptionlanguage that represents the network architecture of the deep learning model along with its metadata that are necessary to develop adeep learning model. Through the proposed description language, developers can easily verify the accuracy of the referenced deep learningmodel. Our experiments demonstrate the application scenario of a deep learning description document that focuses on the license platerecognition for the detection of illegally parked vehicles.Trusted Deep Learning, Model Reference, Deep Learning Description Language, Traffic Situation Analysis Model"
2,딥러닝 분석을 이용한 중국 역내·외 위안화 변동성 예측,이우식 /Lee Woosik,2016,한국데이터정보과학회,"2008년 글로벌 금융위기 이후 중국은 위안화 국제화의 점진적 추진을 진행하면서 중국상하이 외환시장과 중국홍콩 외환시장에서 거래되는 통화인 역내위안화와 역외위안화를 형성시켰다. 본 연구는 위안화 국제화와 점진적인 중국 자본계정 개방에 따라 급변하는 외환시장상황의 변동성을 정확하게 파악하기 위해서 GARCH모형 (일반화된 자기회귀 조건부이분산성모형)에 다단계인공신경망을 결합한 MLP-GARCH 모형과 GARCH모형과 기계학습의 일종인 딥러닝 (deep learning)을 통합한 DL-GARCH을 가지고 위안화 변동성예측을 비교 실험과 분석을 하였다. 비교분석 결과 DLGARCH 모형은 MLP-GARCH보다 모형 위안화 역내·외 환율변동성 예측 면에서 더욱 더 개선된예측값을 제공하였다. 그래서 이분산시계열모형을 딥러닝과 결합한 DL-GARCH 모형은 시계열의환율 변동성 예측 문제에 딥러닝을 응용할 수 있음을 확인하였다. 향후 이분산시계열과 결합된 딥러닝 모형은 다른 금융시계열 데이터에 응용하여 그 일반화 가능성을 높일 수 있을 것이다.The People’s Republic of China has vigorously been pursuing the internationalization of the Chinese Yuan or Renminbi after the financial crisis of 2008. In this view, an abrupt increase of use of the Chinese Yuan in the onshore and offshore markets are important milestones to be one of important currencies. One of the most frequently used methods to forecast volatility is GARCH model. Since a prediction error of the GARCH model has been reported quite high, a lot of efforts have been made to improve forecasting capability of the GARCH model. In this paper, we have proposed MLPGARCH and a DL-GARCH by employing Artificial Neural Network to the GARCH. In an application to forecasting Chinese Yuan volatility, we have successfully shown their overall outperformance in forecasting over the GARCH.Deep learning, DL-GARCH, GARCH, The Chinese Yuan, volatility."
3,딥러닝 기법을 이용한 내일강수 예측,하지훈,2016,한국지능시스템학회,"정확한 강수예측을 위해서는 예측인자 선정과 예측방법에 대한 선택이 매우 중요하다. 최근에는 강수예측 방법으로기계학습 기법이 많이 사용되고 있으며, 그 중에서도 특히 인공신경망을 사용한 강수예측 방법은 좋은 성능을 보였다. 본논문에서는 딥러닝 기법 중 하나인 DBN(deep belief network)를 이용한 새로운 강수예측 방법을 제안한다. DBN는 비지도사전 학습을 통해 초기 가중치를 설정하여 기존 인공신경망의 문제점을 보완한다. 예측인자로는 기온, 전일-전주 강수일, 태양과 달 궤도 관련 자료를 선정하였다. 기온과 전일-전주 강수일은 서울에서의 1974년부터 2013년까지 총 40년간의AWS(automatic weather system) 관측 자료를 사용하였고, 태양과 달의 궤도 관련 자료는 서울을 중심으로 계산한 결과를사용하였다. 전체 기간에서 일부는 학습 자료로 사용하여 예측모델을 생성하였고, 나머지를 생성한 모델의 검증 자료로사용하였다. 모델 검증 결과로 나온 예측값들은 확률값을 가지며 임계치를 이용하여 강수유무를 판별하였다. 강수 정확도의척도로 양분예보기법 중 CSI(critical successive index)와 Bias(frequency bias)를 계산하였다. 이를 통해 DBN와 MLP(multilayer perceptron)의 성능을 비교한 결과 DBN의 강수 예측 정확도가 높았고, 수행속도 또한 2배 이상 빨랐다.For accurate precipitation forecasts the choice of weather faoctrs and prediction method is very important. Recently, machinele arning has been widely used for forecasting precipitation, and artificial neural network, one of machine learning techniques, showed good performance. In this paper, we suggest a new method for forecasting precipitation using DBN, one of deep learning techniques.
DBN has an advantage that initial weights are set by unsupervised learning, so this compensates for the defects of artificial neural networks. We used past precipitation, temperature, and the parameters of the sun and moon's motion as features for forecasting precipitation. The dataset consists of observation data which had been measured for 40 years from AWS in Seoul. Experiments were based on 8-fold cross validation. As a result of estimation, we got probabilities of test dataset, so threshold was used for the decision of precipitation. CSI and Bias were used for indicating the precision of precipitation. Our experimental results showed that DBN performed better than MLP.Deep learning, Deep belief network, Precipitation, Forecast"
4,딥러닝 신경망모형을 이용한 실시간 교통정보수집 연구,김대현 /KIM DAE HYON,2016,사단법인 인문사회과학기술융합학회,"비디오 기반의 실시간 교통정보수집 시스템은 자율주행을 포함한 지능형교통체계의 주요 서브시스템중 하나이며, 우수한 예측(추정)력 때문에 핵심 알고리즘으로 인공신경망 모형이 주로 사용되고 있다. 특히, 최근 과학 학술지 네이처에 구글 딥마인드의 알파고가 공개되면서 딥러닝과 인공지능에 대한 전 세계적인 관심이 집중되고 있다. 최근까지 가장 널리 사용된 인공신경망모형은 역전파 모형이었으나 최근 컴퓨터 하드웨어 및 알고리즘의 새로운 개발과 더불어 딥러닝 알고리즘을 이용한 딥인공신경망 모형에 대한 관심이 높아지고 있다. 그러나 아직 딥러닝 인공신경망에 대한 연구는 초기단계에 있는 실정이며 적용분야도 극히 한정되어 있다. 본 연구에서는 실시간 도로교통정보 수집을 위한 영상처리분야에서 딥러닝 알고리즘을 활용하고 실험결과를 통하여 향후 교통정보수집‧분석 분야에서의 딥러닝 인공신경망 모형의 적용가능성을 살펴보고자 한다. 특히, 본 연구에서 도출한 딥러닝 인공신경망모형의 실험결과를 기존 역전파 모형에 의한 실험결과와 비교하여 모형의 우수성을 비교‧평가하고자 한다.Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Normally, artificial neural networks have been used in real-time applications for collecting traffic information because of their remarkable performance in terms of predictive accuracy and real-time computing. Especially, the recent publication of a paper in the journal Nature and the success of Google DeepMind’s world-beating Go program AlphaGo caused significant increases of the public interest in artificial intelligence. Even though various types of learning models have been proposed, the Backpropagation has been the most popular neural network model. However, the Deep learning neural network model has attracted much interest recently. In this study, the Deep leaning neural network will be applied in image processing techniques for the real-time traffic information collection to investigate the performance of Deep learning algorithm in traffic engineering fields. In addition, the experimental results from Deep learing algorithm will be compared with the results from Backpropagation model to evaluate and propose better performing model.Intelligent Transportation Systems (ITS), artificial neural networks, Deep learning, traffic information, image processing"
5,딥러닝의 모형과 응용사례,안성만 /SungMahn Ahn,2016,한국지능정보시스템학회,"딥러닝은 인공신경망(neural network)이라는 인공지능분야의 모형이 발전된 형태로서, 계층구조로 이루어진 인공신경망의 내부계층(hidden layer)이 여러 단계로 이루어진 구조이다. 딥러닝에서의 주요 모형은 합성곱신경망(convolutional neural network), 순환신경망(recurrent neural network), 그리고 심층신뢰신경망(deep belief network)의 세가지라고 할 수있다. 그 중에서 현재 흥미로운 연구가 많이 발표되어서 관심이 집중되고 있는 모형은 지도학습(supervised learning)모형인 처음 두 개의 모형이다. 따라서 본 논문에서는 지도학습모형의 가중치를 최적화하는 기본적인 방법인 오류역전파 알고리즘을 살펴본 뒤에 합성곱신경망과 순환신경망의 구조와 응용사례 등을 살펴보고자 한다. 본문에서 다루지 않은 모형인 심층신뢰신경망은 아직까지는 합성곱신경망 이나 순환신경망보다는 상대적으로 주목을 덜 받고 있다. 그러나 심층신뢰신경망은 CNN이나 RNN과는 달리 비지도학습(unsupervised learning)모형이며, 사람이나 동물은 관찰을 통해서 스스로 학습한다는 점에서 궁극적으로는 비지도학습모형이 더 많이 연구되어야 할 주제가 될 것이다.Deep learning model is a kind of neural networks that allows multiple hidden layers. There are various deep learning architectures such as convolutional neural networks, deep belief networks and recurrent neural networks. Those have been applied to fields like computer vision, automatic speech recognition, natural language processing, audio recognition and bioinformatics where they have been shown to produce state-of-the-art results on various tasks. Among those architectures, convolutional neural networks and recurrent neural networks are classified as the supervised learning model. And in recent years, those supervised learning models have gained more popularity than unsupervised learning models such as deep belief networks, because supervised learning models have shown fashionable applications in such fields mentioned above.
Deep learning models can be trained with backpropagation algorithm. Backpropagation is an abbreviation for “backward propagation of errors” and a common method of training artificial neural networks used in conjunction with an optimization method such as gradient descent. The method calculates the gradient of an error function with respect to all the weights in the network. The gradient is fed to the optimization method which in turn uses it to update the weights, in an attempt to minimize the error function.
Convolutional neural networks use a special architecture which is particularly well-adapted to classify images. Using this architecture makes convolutional networks fast to train. This, in turn, helps us train deep, muti-layer networks, which are very good at classifying images. These days, deep convolutional networks are used in most neural networks for image recognition. Convolutional neural networks use three basic ideas: local receptive fields, shared weights, and pooling. By local receptive fields, we mean that each neuron in the first(or any) hidden layer will be connected to a small region of the input(or previous layer’s) neurons. Shared weights mean that we’re going to use the same weights and bias for each of the local receptive field. This means that all the neurons in the hidden layer detect exactly the same feature, just at different locations in the input image. In addition to the convolutional layers just described, convolutional neural networks also contain pooling layers. Pooling layers are usually used immediately after convolutional layers. What the pooling layers do is to simplify the information in the output from the convolutional layer.
Recent convolutional network architectures have 10 to 20 hidden layers and billions of connections between units. Training deep learning networks has taken weeks several years ago, but thanks to progress in GPU and algorithm enhancement, training time has reduced to several hours.
Neural networks with time-varying behavior are known as recurrent neural networks or RNNs. A recurrent neural network is a class of artificial neural network where connections between units form a directed cycle. This creates an internal state of the network which allows it to exhibit dynamic temporal behavior. Unlike feedforward neural networks, RNNs can use their internal memory to process arbitrary sequences of inputs. Early RNN models turned out to be very difficult to train, harder even than deep feedforward networks. The reason is the unstable gradient problem such as vanishing gradient and exploding gradient. The gradient can get smaller and smaller as it is propagated back through layers. This makes learning in early layers extremely slow. The problem actually gets worse in RNNs, since gradients aren’t just propagated backward through layers, they’re propagated backward through time. If the network runs for a long time, that can make the gradient extremely unstable and hard to learn from. It has been possible to incorporate an idea known as long short-term memory units (LSTMs) into RNNs. LSTMs make it much easier to get good results when training RNNs, and many recent papers make use of LSTMs or related ideas.Deep learning, Convolutional neural networks, Recurrent neural networks, Error backpropagation algorithm"
6,딥러닝분석과 기술적 분석 지표를 이용한 한국 코스피주가지수 방향성 예측,이우식 /Lee Woosik,2017,한국데이터정보과학회,"2016년 3월 구글 (Google)의 바둑인공지능 알파고 (AlphaGo)가 이세돌 9단과의 바둑대결에서승리한 이후 다양한 분야에서 인공지능 사용에 대한 관심이 높아지고 있는 가운데 금융투자 분야에서도 인공지능과 투자자문 전문가의 합성어인 로보어드바이저 (Robo-Advisor)에 대한 관심이 높아지고 있다. 인공지능 (artificial intelligence)기반의 의사결정은 비용 절감은 물론 효과적인 의사결정을가능하게 한다는 점에서 큰 장점이 있다. 본 연구에서는 기술적 분석 (technical analysis) 지표와 딥러닝 (deep learning) 모형을 결합하여 한국 코스피 지수를 예측하는 모형을 개발하고 제시한 모형들의 예측력을 비교, 분석한다. 분석 결과 기술적 분석 지표에 딥러닝 알고리즘을 결합한 모형이 주가지수 방향성 예측 문제에 응용될 수 있음을 확인하였다. 향후 본 연구에서 제안된 기술적 분석 지표와딥러닝모형을 결합한 기법은 로보어드바이저서비스에 응용할 수 있는 일반화 가능성을 보여준다Since Google’s AlphaGo defeated a world champion of Go players in 2016, there have been many interests in the deep learning. In the financial sector, a Robo-Advisor using deep learning gains a significant attention, which builds and manages portfolios of financial instruments for investors.In this paper, we have proposed the a deep learning algorithm geared toward identification and forecast of the KOSPI index direction,and we also have compared the accuracy of the prediction.In an application of forecasting the financial market index direction, we have shown that the Robo-Advisor using deep learning has a significant effect on finance industry. The Robo-Advisor collects a massive data such as earnings statements, news reports and regulatory filings, analyzes those and recommends investors how to view market trends and identify the best time to purchase financial assets. On the other hand, the Robo-Advisor allows businesses to learn more about their customers, develop better marketing strategies, increase sales and decrease costs.Artificial intelligence, deep learning, FinTech, Robo-Advisor, technical analysis."
7,딥러닝의 효율적인 학습을 위한 학습데이터 선별 기법에 관한 연구,주경돈 /Gyoung-don Joo,2017,한국정보과학회,"빅데이터의 증가와 병렬 하드웨어 처리, 학습 알고리즘의 개선은 딥러닝의 발전을 가져왔으며 인공지능이 적용 가능한 영역을 획기적으로 확장시키고 있다. 하지만 이러한 대규모의 학습 데이터에 기반한 딥러닝은 학습시간이 증가되고, 더 나아가 학습 데이터의 구축을 위한 레이블링(labeling) 비용이 데이터 수에 비례하여 증가하게 된다. 따라서 본 논문에서는, 딥러닝의 정확도를 최대한 유지하면서 필요한 학습 데이터의 수를 줄일수 있도록, 전체 데이터에서 제한된 수의 학습 데이터의 후보를 선별하는 기법에 대하여 연구한다. 특히 기존의 능동 학습에 기반한 기법들이 학습 데이터의 수가 매우 적어질 경우 딥러닝의 정확도가 급격히 떨어지는 문제를 해결할 수 있는 새로운 기법을 제안하였으며, 제안된 기법으로 선별된 학습 데이터는 그 수의 변화에 대해 딥러닝이 일관되게 경쟁력있는 정확도를 가짐을 다양한 실험을 통하여 검증하였다.The increase of big data, parallel hardware processing and enhanced learning algorithms have led to the development of deep learning and are dramatically expanding the domain of applications based on artificial intelligence. However, such large-scale training data extends the learning time, and furthermore, the labeling cost for building the training set increases in proportion to the volume of data. Therefore, in this paper, we study how to select a limited number of candidates for training set from given data so as to reduce the number of training set while maintaining the accuracy of deep learning. Especially, we found that, with the training set selected by the previous work, the accuracy of deep learning drastically decreases when the number of training set is very small and so proposed new methods to resolve this fall-off in accuracy. Through experiments, we showed that deep learning consistently has the competitive accuracy with the training set selected by the proposed method regardless of the size of training set.Deep Learning, Train Set Selection, Active Learning, Gamma distribution"
8,딥러닝을 이용한 범용적 스테그아날리시스,김현재 /Hyunjae Kim,2017,한국정보과학회,"스테그아날리시스(Steganalysis)란 이미지 등 일반적인 자료에 암호화된 정보를 은닉하는 스테가노그래피(Steganography)에 대한 검출 및 분석 방법으로, 기계학습 기반 방법론을 포함한다. 기존 기계학습 기반 스테그아날리시스는 영상(Image)의 특징(Feature) 추출 및 모델링에 기반하며, 최근 딥러닝(Deep Learning)의 적용으로 검출 정확도가 큰 폭으로 향상되었다. 하지만 현존하는 스테그아날리시스 모델은 단일 스테가노그래피 기법에 대해 국한되어 있어 학습에 사용되지 않은 스테고(Stego) 이미지의 경우 검출이 불가능한 결정적 한계를 가진다. 본 연구에서는 다양한 스테가노그래피 기법으로 생성된 스테고이미지에 딥러닝을 적용하여 스테그아날리시스를 학습하는 범용적 모델을 제안한다. 다양한 실험을 통해 제안 기법의 효용성 및 가능성을 확인하고, 범용적 스테그아날리시스 모델이 각각에 특화된 검출 기법과 유사한 정확도로 스테고 이미지를 검출할 수 있음을 보인다.Steganalysis is to detect information hidden by steganography inside general data such as images. There are stegoanalysis techniques that use machine learning (ML). Existing ML approaches to steganalysis are based on extracting features from stego images and modeling them.
Recently deep learning-based methodologies have shown significant improvements in detection accuracy. However, all the existing methods, including deep learning-based ones, have a critical limitation in that they can only detect stego images that are created by a specific steganography method. In this paper, we propose a generalized steganalysis method that can model multiple types of stego images using deep learning. Through various experiments, we confirm the effectiveness of our approach and envision directions for future research. In particular, we show that our method can detect each type of steganography with the same level of accuracy as that of a steganalysis method dedicated to that type of steganography, thereby demonstrating the general applicability of our approach to multiple types of stego images.steganalysis, steganography, deep learning, generalized steganalysis model"
9,"딥러닝 프레임워크의 비교: 티아노, 텐서플로, CNTK를 중심으로",정여진 /Yeojin Chung,2017,한국지능정보시스템학회,"딥러닝 프레임워크의 대표적인 기능으로는 ‘자동미분’과 ‘GPU의 활용’ 등을 들 수 있다. 본 논문은 파이썬의 라이브러리 형태로 사용 가능한 프레임워크 중에서 구글의 텐서플로와 마이크로소프트의 CNTK, 그리고 텐서 플로의 원조라고 할 수 있는 티아노를 비교하였다. 본문에서는 자동미분의 개념과 GPU의 활용형태를 간단히 설명하고, 그 다음에 logistic regression을 실행하는 예를 통하여 각 프레임워크의 문법을 알아본 뒤에, 마지막으로 대표적인 딥러닝 응용인 CNN의 예제를 실행시켜보고 코딩의 편의성과 실행속도 등을 확인해 보았다. 그 결과, 편의성의 관점에서 보면 티아노가 가장 코딩 하기가 어렵고, CNTK와 텐서플로는 많은 부분이 비슷하게 추상화 되어 있어서 코딩이 비슷하지만 가중치와 편향을 직접 정의하느냐의 여부에서 차이를 보였다. 그리고 각프레임워크의 실행속도에 대한 평가는 ‘큰 차이는 없다’는 것이다. 텐서플로는 티아노에 비하여 속도가 느리다는 평가가 있어왔는데, 본 연구의 실험에 의하면, 비록 CNN 모형에 국한되었지만, 텐서플로가 아주 조금이지만빠른 것으로 나타났다. CNTK의 경우에도, 비록 실험환경이 달랐지만, 실험환경의 차이에 의한 속도의 차이의 편차범위 이내에 있는 것으로 판단이 되었다. 본 연구에서는 세 종류의 딥러닝 프레임워크만을 살펴보았는데,위키피디아에 따르면 딥러닝 프레임워크의 종류는 12가지가 있으며, 각 프레임워크의 특징을 15가지 속성으로구분하여 차이를 특정하고 있다. 그 많은 속성 중에서 사용자의 입장에서 볼 때 중요한 속성은 어떤 언어(파이썬, C++, Java, 등)로 사용가능한지, 어떤 딥러닝 모형에 대한 라이브러리가 잘 구현되어 있는지 등일 것이다. 그리고 사용자가 대규모의 딥러닝 모형을 구축한다면, 다중 GPU 혹은 다중 서버를 지원하는지의 여부도 중요할 것이다. 또한 딥러닝 모형을 처음 학습하는 경우에는 사용설명서가 많은지 예제 프로그램이 많은지 여부도 중요한 기준이 될 것이다.The deep learning framework is software designed to help develop deep learning models. Some of its important functions include “automatic differentiation” and “utilization of GPU”. The list of popular deep learning framework includes Caffe (BVLC) and Theano (University of Montreal). And recently, Microsoft's deep learning framework, Microsoft Cognitive Toolkit, was released as open-source license, following Google’s Tensorflow a year earlier. The early deep learning frameworks have been developed mainly for research at universities. Beginning with the inception of Tensorflow, however, it seems that companies such as Microsoft and Facebook have started to join the competition of framework development. Given the trend, Google and other companies are expected to continue investing in the deep learning framework to bring forward the initiative in the artificial intelligence business. From this point of view, we think it is a good time to compare some of deep learning frameworks. So we compare three deep learning frameworks which can be used as a Python library. Those are Google's Tensorflow, Microsoft’s CNTK, and Theano which is sort of a predecessor of the preceding two.
The most common and important function of deep learning frameworks is the ability to perform automatic differentiation. Basically all the mathematical expressions of deep learning models can be represented as computational graphs, which consist of nodes and edges. Partial derivatives on each edge of a computational graph can then be obtained. With the partial derivatives, we can let software compute differentiation of any node with respect to any variable by utilizing chain rule of Calculus.
First of all, the convenience of coding is in the order of CNTK, Tensorflow, and Theano. The criterion is simply based on the lengths of the codes and the learning curve and the ease of coding are not the main concern. According to the criteria, Theano was the most difficult to implement with, and CNTK and Tensorflow were somewhat easier. With Tensorflow, we need to define weight variables and biases explicitly. The reason that CNTK and Tensorflow are easier to implement with is that those frameworks provide us with more abstraction than Theano. We, however, need to mention that low-level coding is not always bad. It gives us flexibility of coding. With the low-level coding such as in Theano, we can implement and test any new deep learning models or any new search methods that we can think of.
The assessment of the execution speed of each framework is that there is not meaningful difference. According to the experiment, execution speeds of Theano and Tensorflow are very similar, although the experiment was limited to a CNN model. In the case of CNTK, the experimental environment was not maintained as the same. The code written in CNTK has to be run in PC environment without GPU where codes execute as much as 50 times slower than with GPU. But we concluded that the difference of execution speed was within the range of variation caused by the different hardware setup.
In this study, we compared three types of deep learning framework: Theano, Tensorflow, and CNTK. According to Wikipedia, there are 12 available deep learning frameworks. And 15 different attributes differentiate each framework. Some of the important attributes would include interface language (Python, C ++, Java, etc.) and the availability of libraries on various deep learning models such as CNN, RNN, DBN, and etc. And if a user implements a large scale deep learning model, it will also be important to support multiple GPU or multiple servers. Also, if you are learning the deep learning model, it would also be important if there are enough examples and references.deep learning framework, Theano, TensorFlow, CNTK, computational graph, CIFAR-10"
10,딥러닝을 이용한 주변 무선단말 파악방안,이웅섭 /Lee， Woong-sup,2018,한국정보통신학회,"최근 단말-대-단말(Device-to-device, D2D) 통신기술이 차세대 무선통신시스템의 핵심기술로 큰 관심을 받고 있다. 이러한 단말간 통신에서는 자신의 주변에 어떠한 단말이 있는지 파악하는 주변단말 탐색(Neighbor discovery)이매우 중요하다. 본 논문에서는 최근 큰 관심을 받고 있는 딥러닝(Deep learning) 기술을 활용하여 단말간 통신에서 주변단말을 파악하는 방안에 대해서 제안한다. 제안 방안은 기존의 방안과 달리 무선채널의 공간적 연관성을 이용하여단말간의 신호 전송 없이 단말이 기지국으로 전송하는 상향링크 파일럿 신호를 기반으로 주변 단말을 찾고 따라서기존의 방식에 비해 신호전송 복잡도(signaling complexity)를 크게 줄일 수 있다. 또한 제안 방안에서는 떨어져 있는거리에 따라서 주변 단말을 분류 가능하여 기존 방안에 비해서 좀 더 세밀한 단말 탐색이 가능하다. 마지막으로 본 논문에서는 tensorflow를 이용한 컴퓨터 시뮬레이션을 통해 제안 방안의 성능을 검증하였다.Recently, the device-to-device (D2D) communication has been conceived as the key technology for the next-generationmobile communication systems. The neighbor discovery in which the nearby users are found, is essential for the properoperation of the D2D communication. In this paper, we propose new neighbor discovery scheme based on deep learningtechnology which has gained a lot of attention recently. In the proposed scheme, the neighboring users can be foundusing the uplink pilot transmission of users only, unlike conventional neighbor discovery schemes in which direct pilotcommunication among users is required, such that the signaling overhead can be greatly reduced in our proposed scheme.
Moreover, the neighbors with different proximity can also be classified accordingly which enables more accurate neighbordiscovery compared to the conventional schemes. The performance of our proposed scheme is verified through thetensorflow-based computer simulations.
키워드 : 딥러닝, 주변단말 탐색, 단말-대-단말 통신Deep learning, Neighbor discovery, Device-to-device communication, Wireless channel correlation"
11,신뢰성있는 딥러닝 기반 분석 모델을 참조하기 위한 딥러닝 기술 언어,문종혁 /Jong Hyeok Mun,2021,한국정보처리학회,"최근 딥러닝은 하드웨어 성능이 향상됨에 따라 자연어 처리, 영상 인식 등의 다양한 기술에 접목되어 활용되고 있다. 이러한 기술들을 활용해 지능형교통 시스템(ITS), 스마트홈, 헬스케어 등의 산업분야에서 데이터를 분석하여 고속도로 속도위반 차량 검출, 에너지 사용량 제어, 응급상황 등과 같은고품질의 서비스를 제공하며, 고품질의 서비스를 제공하기 위해서는 정확도가 향상된 딥러닝 모델이 적용되어야 한다. 이를 위해 서비스 환경의 데이터를분석하기 위한 딥러닝 모델을 개발할 때, 개발자는 신뢰성이 검증된 최신의 딥러닝 모델을 적용할 수 있어야 한다. 이는 개발자가 참조하는 딥러닝 모델에적용된 학습 데이터셋의 정확도를 측정하여 검증할 수 있다. 이러한 검증을 위해서 개발자는 학습 데이터셋, 딥러닝의 계층구조 및 개발 환경 등과 같은내용을 포함하는 딥러닝 모델을 문서화하여 적용하기 위한 구조적인 정보가 필요하다. 본 논문에서는 신뢰성있는 딥러닝 기반 데이터 분석 모델을 참조하기위한 딥러닝 기술 언어를 제안한다. 제안하는 기술 언어는 신뢰성 있는 딥러닝 모델을 개발하는데 필요한 학습데이터셋, 개발 환경 및 설정 등의 정보와더불어 딥러닝 모델의 계층구조를 표현할 수 있다. 제안하는 딥러닝 기술 언어를 이용하여 개발자는 지능형 교통 시스템에서 참조하는 분석 모델의정확도를 검증할 수 있다. 실험에서는 제안하는 언어의 유효성을 검증하기 위해, 번호판 인식 모델을 중심으로 딥러닝 기술 문서의 적용과정을 보인다With the recent advancements of deep learning, companies such as smart home, healthcare, and intelligent transportation systems areutilizing its functionality to provide high-quality services for vehicle detection, emergency situation detection, and controlling energyconsumption. To provide reliable services in such sensitive systems, deep learning models are required to have high accuracy. In orderto develop a deep learning model for analyzing previously mentioned services, developers should utilize the state of the art deep learningmodels that have already been verified for higher accuracy. The developers can verify the accuracy of the referenced model by validatingthe model on the dataset. For this validation, the developer needs structural information to document and apply deep learning models,including metadata such as learning dataset, network architecture, and development environments. In this paper, we propose a descriptionlanguage that represents the network architecture of the deep learning model along with its metadata that are necessary to develop adeep learning model. Through the proposed description language, developers can easily verify the accuracy of the referenced deep learningmodel. Our experiments demonstrate the application scenario of a deep learning description document that focuses on the license platerecognition for the detection of illegally parked vehicles.Trusted Deep Learning, Model Reference, Deep Learning Description Language, Traffic Situation Analysis Model"
12,딥러닝 분석을 이용한 중국 역내·외 위안화 변동성 예측,이우식 /Lee Woosik,2016,한국데이터정보과학회,"2008년 글로벌 금융위기 이후 중국은 위안화 국제화의 점진적 추진을 진행하면서 중국상하이 외환시장과 중국홍콩 외환시장에서 거래되는 통화인 역내위안화와 역외위안화를 형성시켰다. 본 연구는 위안화 국제화와 점진적인 중국 자본계정 개방에 따라 급변하는 외환시장상황의 변동성을 정확하게 파악하기 위해서 GARCH모형 (일반화된 자기회귀 조건부이분산성모형)에 다단계인공신경망을 결합한 MLP-GARCH 모형과 GARCH모형과 기계학습의 일종인 딥러닝 (deep learning)을 통합한 DL-GARCH을 가지고 위안화 변동성예측을 비교 실험과 분석을 하였다. 비교분석 결과 DLGARCH 모형은 MLP-GARCH보다 모형 위안화 역내·외 환율변동성 예측 면에서 더욱 더 개선된예측값을 제공하였다. 그래서 이분산시계열모형을 딥러닝과 결합한 DL-GARCH 모형은 시계열의환율 변동성 예측 문제에 딥러닝을 응용할 수 있음을 확인하였다. 향후 이분산시계열과 결합된 딥러닝 모형은 다른 금융시계열 데이터에 응용하여 그 일반화 가능성을 높일 수 있을 것이다.The People’s Republic of China has vigorously been pursuing the internationalization of the Chinese Yuan or Renminbi after the financial crisis of 2008. In this view, an abrupt increase of use of the Chinese Yuan in the onshore and offshore markets are important milestones to be one of important currencies. One of the most frequently used methods to forecast volatility is GARCH model. Since a prediction error of the GARCH model has been reported quite high, a lot of efforts have been made to improve forecasting capability of the GARCH model. In this paper, we have proposed MLPGARCH and a DL-GARCH by employing Artificial Neural Network to the GARCH. In an application to forecasting Chinese Yuan volatility, we have successfully shown their overall outperformance in forecasting over the GARCH.Deep learning, DL-GARCH, GARCH, The Chinese Yuan, volatility."
13,딥러닝 기법을 이용한 내일강수 예측,하지훈,2016,한국지능시스템학회,"정확한 강수예측을 위해서는 예측인자 선정과 예측방법에 대한 선택이 매우 중요하다. 최근에는 강수예측 방법으로기계학습 기법이 많이 사용되고 있으며, 그 중에서도 특히 인공신경망을 사용한 강수예측 방법은 좋은 성능을 보였다. 본논문에서는 딥러닝 기법 중 하나인 DBN(deep belief network)를 이용한 새로운 강수예측 방법을 제안한다. DBN는 비지도사전 학습을 통해 초기 가중치를 설정하여 기존 인공신경망의 문제점을 보완한다. 예측인자로는 기온, 전일-전주 강수일, 태양과 달 궤도 관련 자료를 선정하였다. 기온과 전일-전주 강수일은 서울에서의 1974년부터 2013년까지 총 40년간의AWS(automatic weather system) 관측 자료를 사용하였고, 태양과 달의 궤도 관련 자료는 서울을 중심으로 계산한 결과를사용하였다. 전체 기간에서 일부는 학습 자료로 사용하여 예측모델을 생성하였고, 나머지를 생성한 모델의 검증 자료로사용하였다. 모델 검증 결과로 나온 예측값들은 확률값을 가지며 임계치를 이용하여 강수유무를 판별하였다. 강수 정확도의척도로 양분예보기법 중 CSI(critical successive index)와 Bias(frequency bias)를 계산하였다. 이를 통해 DBN와 MLP(multilayer perceptron)의 성능을 비교한 결과 DBN의 강수 예측 정확도가 높았고, 수행속도 또한 2배 이상 빨랐다.For accurate precipitation forecasts the choice of weather faoctrs and prediction method is very important. Recently, machinele arning has been widely used for forecasting precipitation, and artificial neural network, one of machine learning techniques, showed good performance. In this paper, we suggest a new method for forecasting precipitation using DBN, one of deep learning techniques.
DBN has an advantage that initial weights are set by unsupervised learning, so this compensates for the defects of artificial neural networks. We used past precipitation, temperature, and the parameters of the sun and moon's motion as features for forecasting precipitation. The dataset consists of observation data which had been measured for 40 years from AWS in Seoul. Experiments were based on 8-fold cross validation. As a result of estimation, we got probabilities of test dataset, so threshold was used for the decision of precipitation. CSI and Bias were used for indicating the precision of precipitation. Our experimental results showed that DBN performed better than MLP.Deep learning, Deep belief network, Precipitation, Forecast"
14,딥러닝 신경망모형을 이용한 실시간 교통정보수집 연구,김대현 /KIM DAE HYON,2016,사단법인 인문사회과학기술융합학회,"비디오 기반의 실시간 교통정보수집 시스템은 자율주행을 포함한 지능형교통체계의 주요 서브시스템중 하나이며, 우수한 예측(추정)력 때문에 핵심 알고리즘으로 인공신경망 모형이 주로 사용되고 있다. 특히, 최근 과학 학술지 네이처에 구글 딥마인드의 알파고가 공개되면서 딥러닝과 인공지능에 대한 전 세계적인 관심이 집중되고 있다. 최근까지 가장 널리 사용된 인공신경망모형은 역전파 모형이었으나 최근 컴퓨터 하드웨어 및 알고리즘의 새로운 개발과 더불어 딥러닝 알고리즘을 이용한 딥인공신경망 모형에 대한 관심이 높아지고 있다. 그러나 아직 딥러닝 인공신경망에 대한 연구는 초기단계에 있는 실정이며 적용분야도 극히 한정되어 있다. 본 연구에서는 실시간 도로교통정보 수집을 위한 영상처리분야에서 딥러닝 알고리즘을 활용하고 실험결과를 통하여 향후 교통정보수집‧분석 분야에서의 딥러닝 인공신경망 모형의 적용가능성을 살펴보고자 한다. 특히, 본 연구에서 도출한 딥러닝 인공신경망모형의 실험결과를 기존 역전파 모형에 의한 실험결과와 비교하여 모형의 우수성을 비교‧평가하고자 한다.Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Normally, artificial neural networks have been used in real-time applications for collecting traffic information because of their remarkable performance in terms of predictive accuracy and real-time computing. Especially, the recent publication of a paper in the journal Nature and the success of Google DeepMind’s world-beating Go program AlphaGo caused significant increases of the public interest in artificial intelligence. Even though various types of learning models have been proposed, the Backpropagation has been the most popular neural network model. However, the Deep learning neural network model has attracted much interest recently. In this study, the Deep leaning neural network will be applied in image processing techniques for the real-time traffic information collection to investigate the performance of Deep learning algorithm in traffic engineering fields. In addition, the experimental results from Deep learing algorithm will be compared with the results from Backpropagation model to evaluate and propose better performing model.Intelligent Transportation Systems (ITS), artificial neural networks, Deep learning, traffic information, image processing"
15,딥러닝의 모형과 응용사례,안성만 /SungMahn Ahn,2016,한국지능정보시스템학회,"딥러닝은 인공신경망(neural network)이라는 인공지능분야의 모형이 발전된 형태로서, 계층구조로 이루어진 인공신경망의 내부계층(hidden layer)이 여러 단계로 이루어진 구조이다. 딥러닝에서의 주요 모형은 합성곱신경망(convolutional neural network), 순환신경망(recurrent neural network), 그리고 심층신뢰신경망(deep belief network)의 세가지라고 할 수있다. 그 중에서 현재 흥미로운 연구가 많이 발표되어서 관심이 집중되고 있는 모형은 지도학습(supervised learning)모형인 처음 두 개의 모형이다. 따라서 본 논문에서는 지도학습모형의 가중치를 최적화하는 기본적인 방법인 오류역전파 알고리즘을 살펴본 뒤에 합성곱신경망과 순환신경망의 구조와 응용사례 등을 살펴보고자 한다. 본문에서 다루지 않은 모형인 심층신뢰신경망은 아직까지는 합성곱신경망 이나 순환신경망보다는 상대적으로 주목을 덜 받고 있다. 그러나 심층신뢰신경망은 CNN이나 RNN과는 달리 비지도학습(unsupervised learning)모형이며, 사람이나 동물은 관찰을 통해서 스스로 학습한다는 점에서 궁극적으로는 비지도학습모형이 더 많이 연구되어야 할 주제가 될 것이다.Deep learning model is a kind of neural networks that allows multiple hidden layers. There are various deep learning architectures such as convolutional neural networks, deep belief networks and recurrent neural networks. Those have been applied to fields like computer vision, automatic speech recognition, natural language processing, audio recognition and bioinformatics where they have been shown to produce state-of-the-art results on various tasks. Among those architectures, convolutional neural networks and recurrent neural networks are classified as the supervised learning model. And in recent years, those supervised learning models have gained more popularity than unsupervised learning models such as deep belief networks, because supervised learning models have shown fashionable applications in such fields mentioned above.
Deep learning models can be trained with backpropagation algorithm. Backpropagation is an abbreviation for “backward propagation of errors” and a common method of training artificial neural networks used in conjunction with an optimization method such as gradient descent. The method calculates the gradient of an error function with respect to all the weights in the network. The gradient is fed to the optimization method which in turn uses it to update the weights, in an attempt to minimize the error function.
Convolutional neural networks use a special architecture which is particularly well-adapted to classify images. Using this architecture makes convolutional networks fast to train. This, in turn, helps us train deep, muti-layer networks, which are very good at classifying images. These days, deep convolutional networks are used in most neural networks for image recognition. Convolutional neural networks use three basic ideas: local receptive fields, shared weights, and pooling. By local receptive fields, we mean that each neuron in the first(or any) hidden layer will be connected to a small region of the input(or previous layer’s) neurons. Shared weights mean that we’re going to use the same weights and bias for each of the local receptive field. This means that all the neurons in the hidden layer detect exactly the same feature, just at different locations in the input image. In addition to the convolutional layers just described, convolutional neural networks also contain pooling layers. Pooling layers are usually used immediately after convolutional layers. What the pooling layers do is to simplify the information in the output from the convolutional layer.
Recent convolutional network architectures have 10 to 20 hidden layers and billions of connections between units. Training deep learning networks has taken weeks several years ago, but thanks to progress in GPU and algorithm enhancement, training time has reduced to several hours.
Neural networks with time-varying behavior are known as recurrent neural networks or RNNs. A recurrent neural network is a class of artificial neural network where connections between units form a directed cycle. This creates an internal state of the network which allows it to exhibit dynamic temporal behavior. Unlike feedforward neural networks, RNNs can use their internal memory to process arbitrary sequences of inputs. Early RNN models turned out to be very difficult to train, harder even than deep feedforward networks. The reason is the unstable gradient problem such as vanishing gradient and exploding gradient. The gradient can get smaller and smaller as it is propagated back through layers. This makes learning in early layers extremely slow. The problem actually gets worse in RNNs, since gradients aren’t just propagated backward through layers, they’re propagated backward through time. If the network runs for a long time, that can make the gradient extremely unstable and hard to learn from. It has been possible to incorporate an idea known as long short-term memory units (LSTMs) into RNNs. LSTMs make it much easier to get good results when training RNNs, and many recent papers make use of LSTMs or related ideas.Deep learning, Convolutional neural networks, Recurrent neural networks, Error backpropagation algorithm"
16,딥러닝분석과 기술적 분석 지표를 이용한 한국 코스피주가지수 방향성 예측,이우식 /Lee Woosik,2017,한국데이터정보과학회,"2016년 3월 구글 (Google)의 바둑인공지능 알파고 (AlphaGo)가 이세돌 9단과의 바둑대결에서승리한 이후 다양한 분야에서 인공지능 사용에 대한 관심이 높아지고 있는 가운데 금융투자 분야에서도 인공지능과 투자자문 전문가의 합성어인 로보어드바이저 (Robo-Advisor)에 대한 관심이 높아지고 있다. 인공지능 (artificial intelligence)기반의 의사결정은 비용 절감은 물론 효과적인 의사결정을가능하게 한다는 점에서 큰 장점이 있다. 본 연구에서는 기술적 분석 (technical analysis) 지표와 딥러닝 (deep learning) 모형을 결합하여 한국 코스피 지수를 예측하는 모형을 개발하고 제시한 모형들의 예측력을 비교, 분석한다. 분석 결과 기술적 분석 지표에 딥러닝 알고리즘을 결합한 모형이 주가지수 방향성 예측 문제에 응용될 수 있음을 확인하였다. 향후 본 연구에서 제안된 기술적 분석 지표와딥러닝모형을 결합한 기법은 로보어드바이저서비스에 응용할 수 있는 일반화 가능성을 보여준다Since Google’s AlphaGo defeated a world champion of Go players in 2016, there have been many interests in the deep learning. In the financial sector, a Robo-Advisor using deep learning gains a significant attention, which builds and manages portfolios of financial instruments for investors.In this paper, we have proposed the a deep learning algorithm geared toward identification and forecast of the KOSPI index direction,and we also have compared the accuracy of the prediction.In an application of forecasting the financial market index direction, we have shown that the Robo-Advisor using deep learning has a significant effect on finance industry. The Robo-Advisor collects a massive data such as earnings statements, news reports and regulatory filings, analyzes those and recommends investors how to view market trends and identify the best time to purchase financial assets. On the other hand, the Robo-Advisor allows businesses to learn more about their customers, develop better marketing strategies, increase sales and decrease costs.Artificial intelligence, deep learning, FinTech, Robo-Advisor, technical analysis."
17,딥러닝의 효율적인 학습을 위한 학습데이터 선별 기법에 관한 연구,주경돈 /Gyoung-don Joo,2017,한국정보과학회,"빅데이터의 증가와 병렬 하드웨어 처리, 학습 알고리즘의 개선은 딥러닝의 발전을 가져왔으며 인공지능이 적용 가능한 영역을 획기적으로 확장시키고 있다. 하지만 이러한 대규모의 학습 데이터에 기반한 딥러닝은 학습시간이 증가되고, 더 나아가 학습 데이터의 구축을 위한 레이블링(labeling) 비용이 데이터 수에 비례하여 증가하게 된다. 따라서 본 논문에서는, 딥러닝의 정확도를 최대한 유지하면서 필요한 학습 데이터의 수를 줄일수 있도록, 전체 데이터에서 제한된 수의 학습 데이터의 후보를 선별하는 기법에 대하여 연구한다. 특히 기존의 능동 학습에 기반한 기법들이 학습 데이터의 수가 매우 적어질 경우 딥러닝의 정확도가 급격히 떨어지는 문제를 해결할 수 있는 새로운 기법을 제안하였으며, 제안된 기법으로 선별된 학습 데이터는 그 수의 변화에 대해 딥러닝이 일관되게 경쟁력있는 정확도를 가짐을 다양한 실험을 통하여 검증하였다.The increase of big data, parallel hardware processing and enhanced learning algorithms have led to the development of deep learning and are dramatically expanding the domain of applications based on artificial intelligence. However, such large-scale training data extends the learning time, and furthermore, the labeling cost for building the training set increases in proportion to the volume of data. Therefore, in this paper, we study how to select a limited number of candidates for training set from given data so as to reduce the number of training set while maintaining the accuracy of deep learning. Especially, we found that, with the training set selected by the previous work, the accuracy of deep learning drastically decreases when the number of training set is very small and so proposed new methods to resolve this fall-off in accuracy. Through experiments, we showed that deep learning consistently has the competitive accuracy with the training set selected by the proposed method regardless of the size of training set.Deep Learning, Train Set Selection, Active Learning, Gamma distribution"
18,딥러닝을 이용한 범용적 스테그아날리시스,김현재 /Hyunjae Kim,2017,한국정보과학회,"스테그아날리시스(Steganalysis)란 이미지 등 일반적인 자료에 암호화된 정보를 은닉하는 스테가노그래피(Steganography)에 대한 검출 및 분석 방법으로, 기계학습 기반 방법론을 포함한다. 기존 기계학습 기반 스테그아날리시스는 영상(Image)의 특징(Feature) 추출 및 모델링에 기반하며, 최근 딥러닝(Deep Learning)의 적용으로 검출 정확도가 큰 폭으로 향상되었다. 하지만 현존하는 스테그아날리시스 모델은 단일 스테가노그래피 기법에 대해 국한되어 있어 학습에 사용되지 않은 스테고(Stego) 이미지의 경우 검출이 불가능한 결정적 한계를 가진다. 본 연구에서는 다양한 스테가노그래피 기법으로 생성된 스테고이미지에 딥러닝을 적용하여 스테그아날리시스를 학습하는 범용적 모델을 제안한다. 다양한 실험을 통해 제안 기법의 효용성 및 가능성을 확인하고, 범용적 스테그아날리시스 모델이 각각에 특화된 검출 기법과 유사한 정확도로 스테고 이미지를 검출할 수 있음을 보인다.Steganalysis is to detect information hidden by steganography inside general data such as images. There are stegoanalysis techniques that use machine learning (ML). Existing ML approaches to steganalysis are based on extracting features from stego images and modeling them.
Recently deep learning-based methodologies have shown significant improvements in detection accuracy. However, all the existing methods, including deep learning-based ones, have a critical limitation in that they can only detect stego images that are created by a specific steganography method. In this paper, we propose a generalized steganalysis method that can model multiple types of stego images using deep learning. Through various experiments, we confirm the effectiveness of our approach and envision directions for future research. In particular, we show that our method can detect each type of steganography with the same level of accuracy as that of a steganalysis method dedicated to that type of steganography, thereby demonstrating the general applicability of our approach to multiple types of stego images.steganalysis, steganography, deep learning, generalized steganalysis model"
19,"딥러닝 프레임워크의 비교: 티아노, 텐서플로, CNTK를 중심으로",정여진 /Yeojin Chung,2017,한국지능정보시스템학회,"딥러닝 프레임워크의 대표적인 기능으로는 ‘자동미분’과 ‘GPU의 활용’ 등을 들 수 있다. 본 논문은 파이썬의 라이브러리 형태로 사용 가능한 프레임워크 중에서 구글의 텐서플로와 마이크로소프트의 CNTK, 그리고 텐서 플로의 원조라고 할 수 있는 티아노를 비교하였다. 본문에서는 자동미분의 개념과 GPU의 활용형태를 간단히 설명하고, 그 다음에 logistic regression을 실행하는 예를 통하여 각 프레임워크의 문법을 알아본 뒤에, 마지막으로 대표적인 딥러닝 응용인 CNN의 예제를 실행시켜보고 코딩의 편의성과 실행속도 등을 확인해 보았다. 그 결과, 편의성의 관점에서 보면 티아노가 가장 코딩 하기가 어렵고, CNTK와 텐서플로는 많은 부분이 비슷하게 추상화 되어 있어서 코딩이 비슷하지만 가중치와 편향을 직접 정의하느냐의 여부에서 차이를 보였다. 그리고 각프레임워크의 실행속도에 대한 평가는 ‘큰 차이는 없다’는 것이다. 텐서플로는 티아노에 비하여 속도가 느리다는 평가가 있어왔는데, 본 연구의 실험에 의하면, 비록 CNN 모형에 국한되었지만, 텐서플로가 아주 조금이지만빠른 것으로 나타났다. CNTK의 경우에도, 비록 실험환경이 달랐지만, 실험환경의 차이에 의한 속도의 차이의 편차범위 이내에 있는 것으로 판단이 되었다. 본 연구에서는 세 종류의 딥러닝 프레임워크만을 살펴보았는데,위키피디아에 따르면 딥러닝 프레임워크의 종류는 12가지가 있으며, 각 프레임워크의 특징을 15가지 속성으로구분하여 차이를 특정하고 있다. 그 많은 속성 중에서 사용자의 입장에서 볼 때 중요한 속성은 어떤 언어(파이썬, C++, Java, 등)로 사용가능한지, 어떤 딥러닝 모형에 대한 라이브러리가 잘 구현되어 있는지 등일 것이다. 그리고 사용자가 대규모의 딥러닝 모형을 구축한다면, 다중 GPU 혹은 다중 서버를 지원하는지의 여부도 중요할 것이다. 또한 딥러닝 모형을 처음 학습하는 경우에는 사용설명서가 많은지 예제 프로그램이 많은지 여부도 중요한 기준이 될 것이다.The deep learning framework is software designed to help develop deep learning models. Some of its important functions include “automatic differentiation” and “utilization of GPU”. The list of popular deep learning framework includes Caffe (BVLC) and Theano (University of Montreal). And recently, Microsoft's deep learning framework, Microsoft Cognitive Toolkit, was released as open-source license, following Google’s Tensorflow a year earlier. The early deep learning frameworks have been developed mainly for research at universities. Beginning with the inception of Tensorflow, however, it seems that companies such as Microsoft and Facebook have started to join the competition of framework development. Given the trend, Google and other companies are expected to continue investing in the deep learning framework to bring forward the initiative in the artificial intelligence business. From this point of view, we think it is a good time to compare some of deep learning frameworks. So we compare three deep learning frameworks which can be used as a Python library. Those are Google's Tensorflow, Microsoft’s CNTK, and Theano which is sort of a predecessor of the preceding two.
The most common and important function of deep learning frameworks is the ability to perform automatic differentiation. Basically all the mathematical expressions of deep learning models can be represented as computational graphs, which consist of nodes and edges. Partial derivatives on each edge of a computational graph can then be obtained. With the partial derivatives, we can let software compute differentiation of any node with respect to any variable by utilizing chain rule of Calculus.
First of all, the convenience of coding is in the order of CNTK, Tensorflow, and Theano. The criterion is simply based on the lengths of the codes and the learning curve and the ease of coding are not the main concern. According to the criteria, Theano was the most difficult to implement with, and CNTK and Tensorflow were somewhat easier. With Tensorflow, we need to define weight variables and biases explicitly. The reason that CNTK and Tensorflow are easier to implement with is that those frameworks provide us with more abstraction than Theano. We, however, need to mention that low-level coding is not always bad. It gives us flexibility of coding. With the low-level coding such as in Theano, we can implement and test any new deep learning models or any new search methods that we can think of.
The assessment of the execution speed of each framework is that there is not meaningful difference. According to the experiment, execution speeds of Theano and Tensorflow are very similar, although the experiment was limited to a CNN model. In the case of CNTK, the experimental environment was not maintained as the same. The code written in CNTK has to be run in PC environment without GPU where codes execute as much as 50 times slower than with GPU. But we concluded that the difference of execution speed was within the range of variation caused by the different hardware setup.
In this study, we compared three types of deep learning framework: Theano, Tensorflow, and CNTK. According to Wikipedia, there are 12 available deep learning frameworks. And 15 different attributes differentiate each framework. Some of the important attributes would include interface language (Python, C ++, Java, etc.) and the availability of libraries on various deep learning models such as CNN, RNN, DBN, and etc. And if a user implements a large scale deep learning model, it will also be important to support multiple GPU or multiple servers. Also, if you are learning the deep learning model, it would also be important if there are enough examples and references.deep learning framework, Theano, TensorFlow, CNTK, computational graph, CIFAR-10"
20,딥러닝을 이용한 주변 무선단말 파악방안,이웅섭 /Lee， Woong-sup,2018,한국정보통신학회,"최근 단말-대-단말(Device-to-device, D2D) 통신기술이 차세대 무선통신시스템의 핵심기술로 큰 관심을 받고 있다. 이러한 단말간 통신에서는 자신의 주변에 어떠한 단말이 있는지 파악하는 주변단말 탐색(Neighbor discovery)이매우 중요하다. 본 논문에서는 최근 큰 관심을 받고 있는 딥러닝(Deep learning) 기술을 활용하여 단말간 통신에서 주변단말을 파악하는 방안에 대해서 제안한다. 제안 방안은 기존의 방안과 달리 무선채널의 공간적 연관성을 이용하여단말간의 신호 전송 없이 단말이 기지국으로 전송하는 상향링크 파일럿 신호를 기반으로 주변 단말을 찾고 따라서기존의 방식에 비해 신호전송 복잡도(signaling complexity)를 크게 줄일 수 있다. 또한 제안 방안에서는 떨어져 있는거리에 따라서 주변 단말을 분류 가능하여 기존 방안에 비해서 좀 더 세밀한 단말 탐색이 가능하다. 마지막으로 본 논문에서는 tensorflow를 이용한 컴퓨터 시뮬레이션을 통해 제안 방안의 성능을 검증하였다.Recently, the device-to-device (D2D) communication has been conceived as the key technology for the next-generationmobile communication systems. The neighbor discovery in which the nearby users are found, is essential for the properoperation of the D2D communication. In this paper, we propose new neighbor discovery scheme based on deep learningtechnology which has gained a lot of attention recently. In the proposed scheme, the neighboring users can be foundusing the uplink pilot transmission of users only, unlike conventional neighbor discovery schemes in which direct pilotcommunication among users is required, such that the signaling overhead can be greatly reduced in our proposed scheme.
Moreover, the neighbors with different proximity can also be classified accordingly which enables more accurate neighbordiscovery compared to the conventional schemes. The performance of our proposed scheme is verified through thetensorflow-based computer simulations.
키워드 : 딥러닝, 주변단말 탐색, 단말-대-단말 통신Deep learning, Neighbor discovery, Device-to-device communication, Wireless channel correlation"
21,신뢰성있는 딥러닝 기반 분석 모델을 참조하기 위한 딥러닝 기술 언어,문종혁 /Jong Hyeok Mun,2021,한국정보처리학회,"최근 딥러닝은 하드웨어 성능이 향상됨에 따라 자연어 처리, 영상 인식 등의 다양한 기술에 접목되어 활용되고 있다. 이러한 기술들을 활용해 지능형교통 시스템(ITS), 스마트홈, 헬스케어 등의 산업분야에서 데이터를 분석하여 고속도로 속도위반 차량 검출, 에너지 사용량 제어, 응급상황 등과 같은고품질의 서비스를 제공하며, 고품질의 서비스를 제공하기 위해서는 정확도가 향상된 딥러닝 모델이 적용되어야 한다. 이를 위해 서비스 환경의 데이터를분석하기 위한 딥러닝 모델을 개발할 때, 개발자는 신뢰성이 검증된 최신의 딥러닝 모델을 적용할 수 있어야 한다. 이는 개발자가 참조하는 딥러닝 모델에적용된 학습 데이터셋의 정확도를 측정하여 검증할 수 있다. 이러한 검증을 위해서 개발자는 학습 데이터셋, 딥러닝의 계층구조 및 개발 환경 등과 같은내용을 포함하는 딥러닝 모델을 문서화하여 적용하기 위한 구조적인 정보가 필요하다. 본 논문에서는 신뢰성있는 딥러닝 기반 데이터 분석 모델을 참조하기위한 딥러닝 기술 언어를 제안한다. 제안하는 기술 언어는 신뢰성 있는 딥러닝 모델을 개발하는데 필요한 학습데이터셋, 개발 환경 및 설정 등의 정보와더불어 딥러닝 모델의 계층구조를 표현할 수 있다. 제안하는 딥러닝 기술 언어를 이용하여 개발자는 지능형 교통 시스템에서 참조하는 분석 모델의정확도를 검증할 수 있다. 실험에서는 제안하는 언어의 유효성을 검증하기 위해, 번호판 인식 모델을 중심으로 딥러닝 기술 문서의 적용과정을 보인다With the recent advancements of deep learning, companies such as smart home, healthcare, and intelligent transportation systems areutilizing its functionality to provide high-quality services for vehicle detection, emergency situation detection, and controlling energyconsumption. To provide reliable services in such sensitive systems, deep learning models are required to have high accuracy. In orderto develop a deep learning model for analyzing previously mentioned services, developers should utilize the state of the art deep learningmodels that have already been verified for higher accuracy. The developers can verify the accuracy of the referenced model by validatingthe model on the dataset. For this validation, the developer needs structural information to document and apply deep learning models,including metadata such as learning dataset, network architecture, and development environments. In this paper, we propose a descriptionlanguage that represents the network architecture of the deep learning model along with its metadata that are necessary to develop adeep learning model. Through the proposed description language, developers can easily verify the accuracy of the referenced deep learningmodel. Our experiments demonstrate the application scenario of a deep learning description document that focuses on the license platerecognition for the detection of illegally parked vehicles.Trusted Deep Learning, Model Reference, Deep Learning Description Language, Traffic Situation Analysis Model"
22,딥러닝 분석을 이용한 중국 역내·외 위안화 변동성 예측,이우식 /Lee Woosik,2016,한국데이터정보과학회,"2008년 글로벌 금융위기 이후 중국은 위안화 국제화의 점진적 추진을 진행하면서 중국상하이 외환시장과 중국홍콩 외환시장에서 거래되는 통화인 역내위안화와 역외위안화를 형성시켰다. 본 연구는 위안화 국제화와 점진적인 중국 자본계정 개방에 따라 급변하는 외환시장상황의 변동성을 정확하게 파악하기 위해서 GARCH모형 (일반화된 자기회귀 조건부이분산성모형)에 다단계인공신경망을 결합한 MLP-GARCH 모형과 GARCH모형과 기계학습의 일종인 딥러닝 (deep learning)을 통합한 DL-GARCH을 가지고 위안화 변동성예측을 비교 실험과 분석을 하였다. 비교분석 결과 DLGARCH 모형은 MLP-GARCH보다 모형 위안화 역내·외 환율변동성 예측 면에서 더욱 더 개선된예측값을 제공하였다. 그래서 이분산시계열모형을 딥러닝과 결합한 DL-GARCH 모형은 시계열의환율 변동성 예측 문제에 딥러닝을 응용할 수 있음을 확인하였다. 향후 이분산시계열과 결합된 딥러닝 모형은 다른 금융시계열 데이터에 응용하여 그 일반화 가능성을 높일 수 있을 것이다.The People’s Republic of China has vigorously been pursuing the internationalization of the Chinese Yuan or Renminbi after the financial crisis of 2008. In this view, an abrupt increase of use of the Chinese Yuan in the onshore and offshore markets are important milestones to be one of important currencies. One of the most frequently used methods to forecast volatility is GARCH model. Since a prediction error of the GARCH model has been reported quite high, a lot of efforts have been made to improve forecasting capability of the GARCH model. In this paper, we have proposed MLPGARCH and a DL-GARCH by employing Artificial Neural Network to the GARCH. In an application to forecasting Chinese Yuan volatility, we have successfully shown their overall outperformance in forecasting over the GARCH.Deep learning, DL-GARCH, GARCH, The Chinese Yuan, volatility."
23,딥러닝 기법을 이용한 내일강수 예측,하지훈,2016,한국지능시스템학회,"정확한 강수예측을 위해서는 예측인자 선정과 예측방법에 대한 선택이 매우 중요하다. 최근에는 강수예측 방법으로기계학습 기법이 많이 사용되고 있으며, 그 중에서도 특히 인공신경망을 사용한 강수예측 방법은 좋은 성능을 보였다. 본논문에서는 딥러닝 기법 중 하나인 DBN(deep belief network)를 이용한 새로운 강수예측 방법을 제안한다. DBN는 비지도사전 학습을 통해 초기 가중치를 설정하여 기존 인공신경망의 문제점을 보완한다. 예측인자로는 기온, 전일-전주 강수일, 태양과 달 궤도 관련 자료를 선정하였다. 기온과 전일-전주 강수일은 서울에서의 1974년부터 2013년까지 총 40년간의AWS(automatic weather system) 관측 자료를 사용하였고, 태양과 달의 궤도 관련 자료는 서울을 중심으로 계산한 결과를사용하였다. 전체 기간에서 일부는 학습 자료로 사용하여 예측모델을 생성하였고, 나머지를 생성한 모델의 검증 자료로사용하였다. 모델 검증 결과로 나온 예측값들은 확률값을 가지며 임계치를 이용하여 강수유무를 판별하였다. 강수 정확도의척도로 양분예보기법 중 CSI(critical successive index)와 Bias(frequency bias)를 계산하였다. 이를 통해 DBN와 MLP(multilayer perceptron)의 성능을 비교한 결과 DBN의 강수 예측 정확도가 높았고, 수행속도 또한 2배 이상 빨랐다.For accurate precipitation forecasts the choice of weather faoctrs and prediction method is very important. Recently, machinele arning has been widely used for forecasting precipitation, and artificial neural network, one of machine learning techniques, showed good performance. In this paper, we suggest a new method for forecasting precipitation using DBN, one of deep learning techniques.
DBN has an advantage that initial weights are set by unsupervised learning, so this compensates for the defects of artificial neural networks. We used past precipitation, temperature, and the parameters of the sun and moon's motion as features for forecasting precipitation. The dataset consists of observation data which had been measured for 40 years from AWS in Seoul. Experiments were based on 8-fold cross validation. As a result of estimation, we got probabilities of test dataset, so threshold was used for the decision of precipitation. CSI and Bias were used for indicating the precision of precipitation. Our experimental results showed that DBN performed better than MLP.Deep learning, Deep belief network, Precipitation, Forecast"
24,딥러닝 신경망모형을 이용한 실시간 교통정보수집 연구,김대현 /KIM DAE HYON,2016,사단법인 인문사회과학기술융합학회,"비디오 기반의 실시간 교통정보수집 시스템은 자율주행을 포함한 지능형교통체계의 주요 서브시스템중 하나이며, 우수한 예측(추정)력 때문에 핵심 알고리즘으로 인공신경망 모형이 주로 사용되고 있다. 특히, 최근 과학 학술지 네이처에 구글 딥마인드의 알파고가 공개되면서 딥러닝과 인공지능에 대한 전 세계적인 관심이 집중되고 있다. 최근까지 가장 널리 사용된 인공신경망모형은 역전파 모형이었으나 최근 컴퓨터 하드웨어 및 알고리즘의 새로운 개발과 더불어 딥러닝 알고리즘을 이용한 딥인공신경망 모형에 대한 관심이 높아지고 있다. 그러나 아직 딥러닝 인공신경망에 대한 연구는 초기단계에 있는 실정이며 적용분야도 극히 한정되어 있다. 본 연구에서는 실시간 도로교통정보 수집을 위한 영상처리분야에서 딥러닝 알고리즘을 활용하고 실험결과를 통하여 향후 교통정보수집‧분석 분야에서의 딥러닝 인공신경망 모형의 적용가능성을 살펴보고자 한다. 특히, 본 연구에서 도출한 딥러닝 인공신경망모형의 실험결과를 기존 역전파 모형에 의한 실험결과와 비교하여 모형의 우수성을 비교‧평가하고자 한다.Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Normally, artificial neural networks have been used in real-time applications for collecting traffic information because of their remarkable performance in terms of predictive accuracy and real-time computing. Especially, the recent publication of a paper in the journal Nature and the success of Google DeepMind’s world-beating Go program AlphaGo caused significant increases of the public interest in artificial intelligence. Even though various types of learning models have been proposed, the Backpropagation has been the most popular neural network model. However, the Deep learning neural network model has attracted much interest recently. In this study, the Deep leaning neural network will be applied in image processing techniques for the real-time traffic information collection to investigate the performance of Deep learning algorithm in traffic engineering fields. In addition, the experimental results from Deep learing algorithm will be compared with the results from Backpropagation model to evaluate and propose better performing model.Intelligent Transportation Systems (ITS), artificial neural networks, Deep learning, traffic information, image processing"
25,딥러닝의 모형과 응용사례,안성만 /SungMahn Ahn,2016,한국지능정보시스템학회,"딥러닝은 인공신경망(neural network)이라는 인공지능분야의 모형이 발전된 형태로서, 계층구조로 이루어진 인공신경망의 내부계층(hidden layer)이 여러 단계로 이루어진 구조이다. 딥러닝에서의 주요 모형은 합성곱신경망(convolutional neural network), 순환신경망(recurrent neural network), 그리고 심층신뢰신경망(deep belief network)의 세가지라고 할 수있다. 그 중에서 현재 흥미로운 연구가 많이 발표되어서 관심이 집중되고 있는 모형은 지도학습(supervised learning)모형인 처음 두 개의 모형이다. 따라서 본 논문에서는 지도학습모형의 가중치를 최적화하는 기본적인 방법인 오류역전파 알고리즘을 살펴본 뒤에 합성곱신경망과 순환신경망의 구조와 응용사례 등을 살펴보고자 한다. 본문에서 다루지 않은 모형인 심층신뢰신경망은 아직까지는 합성곱신경망 이나 순환신경망보다는 상대적으로 주목을 덜 받고 있다. 그러나 심층신뢰신경망은 CNN이나 RNN과는 달리 비지도학습(unsupervised learning)모형이며, 사람이나 동물은 관찰을 통해서 스스로 학습한다는 점에서 궁극적으로는 비지도학습모형이 더 많이 연구되어야 할 주제가 될 것이다.Deep learning model is a kind of neural networks that allows multiple hidden layers. There are various deep learning architectures such as convolutional neural networks, deep belief networks and recurrent neural networks. Those have been applied to fields like computer vision, automatic speech recognition, natural language processing, audio recognition and bioinformatics where they have been shown to produce state-of-the-art results on various tasks. Among those architectures, convolutional neural networks and recurrent neural networks are classified as the supervised learning model. And in recent years, those supervised learning models have gained more popularity than unsupervised learning models such as deep belief networks, because supervised learning models have shown fashionable applications in such fields mentioned above.
Deep learning models can be trained with backpropagation algorithm. Backpropagation is an abbreviation for “backward propagation of errors” and a common method of training artificial neural networks used in conjunction with an optimization method such as gradient descent. The method calculates the gradient of an error function with respect to all the weights in the network. The gradient is fed to the optimization method which in turn uses it to update the weights, in an attempt to minimize the error function.
Convolutional neural networks use a special architecture which is particularly well-adapted to classify images. Using this architecture makes convolutional networks fast to train. This, in turn, helps us train deep, muti-layer networks, which are very good at classifying images. These days, deep convolutional networks are used in most neural networks for image recognition. Convolutional neural networks use three basic ideas: local receptive fields, shared weights, and pooling. By local receptive fields, we mean that each neuron in the first(or any) hidden layer will be connected to a small region of the input(or previous layer’s) neurons. Shared weights mean that we’re going to use the same weights and bias for each of the local receptive field. This means that all the neurons in the hidden layer detect exactly the same feature, just at different locations in the input image. In addition to the convolutional layers just described, convolutional neural networks also contain pooling layers. Pooling layers are usually used immediately after convolutional layers. What the pooling layers do is to simplify the information in the output from the convolutional layer.
Recent convolutional network architectures have 10 to 20 hidden layers and billions of connections between units. Training deep learning networks has taken weeks several years ago, but thanks to progress in GPU and algorithm enhancement, training time has reduced to several hours.
Neural networks with time-varying behavior are known as recurrent neural networks or RNNs. A recurrent neural network is a class of artificial neural network where connections between units form a directed cycle. This creates an internal state of the network which allows it to exhibit dynamic temporal behavior. Unlike feedforward neural networks, RNNs can use their internal memory to process arbitrary sequences of inputs. Early RNN models turned out to be very difficult to train, harder even than deep feedforward networks. The reason is the unstable gradient problem such as vanishing gradient and exploding gradient. The gradient can get smaller and smaller as it is propagated back through layers. This makes learning in early layers extremely slow. The problem actually gets worse in RNNs, since gradients aren’t just propagated backward through layers, they’re propagated backward through time. If the network runs for a long time, that can make the gradient extremely unstable and hard to learn from. It has been possible to incorporate an idea known as long short-term memory units (LSTMs) into RNNs. LSTMs make it much easier to get good results when training RNNs, and many recent papers make use of LSTMs or related ideas.Deep learning, Convolutional neural networks, Recurrent neural networks, Error backpropagation algorithm"
26,딥러닝분석과 기술적 분석 지표를 이용한 한국 코스피주가지수 방향성 예측,이우식 /Lee Woosik,2017,한국데이터정보과학회,"2016년 3월 구글 (Google)의 바둑인공지능 알파고 (AlphaGo)가 이세돌 9단과의 바둑대결에서승리한 이후 다양한 분야에서 인공지능 사용에 대한 관심이 높아지고 있는 가운데 금융투자 분야에서도 인공지능과 투자자문 전문가의 합성어인 로보어드바이저 (Robo-Advisor)에 대한 관심이 높아지고 있다. 인공지능 (artificial intelligence)기반의 의사결정은 비용 절감은 물론 효과적인 의사결정을가능하게 한다는 점에서 큰 장점이 있다. 본 연구에서는 기술적 분석 (technical analysis) 지표와 딥러닝 (deep learning) 모형을 결합하여 한국 코스피 지수를 예측하는 모형을 개발하고 제시한 모형들의 예측력을 비교, 분석한다. 분석 결과 기술적 분석 지표에 딥러닝 알고리즘을 결합한 모형이 주가지수 방향성 예측 문제에 응용될 수 있음을 확인하였다. 향후 본 연구에서 제안된 기술적 분석 지표와딥러닝모형을 결합한 기법은 로보어드바이저서비스에 응용할 수 있는 일반화 가능성을 보여준다Since Google’s AlphaGo defeated a world champion of Go players in 2016, there have been many interests in the deep learning. In the financial sector, a Robo-Advisor using deep learning gains a significant attention, which builds and manages portfolios of financial instruments for investors.In this paper, we have proposed the a deep learning algorithm geared toward identification and forecast of the KOSPI index direction,and we also have compared the accuracy of the prediction.In an application of forecasting the financial market index direction, we have shown that the Robo-Advisor using deep learning has a significant effect on finance industry. The Robo-Advisor collects a massive data such as earnings statements, news reports and regulatory filings, analyzes those and recommends investors how to view market trends and identify the best time to purchase financial assets. On the other hand, the Robo-Advisor allows businesses to learn more about their customers, develop better marketing strategies, increase sales and decrease costs.Artificial intelligence, deep learning, FinTech, Robo-Advisor, technical analysis."
27,딥러닝의 효율적인 학습을 위한 학습데이터 선별 기법에 관한 연구,주경돈 /Gyoung-don Joo,2017,한국정보과학회,"빅데이터의 증가와 병렬 하드웨어 처리, 학습 알고리즘의 개선은 딥러닝의 발전을 가져왔으며 인공지능이 적용 가능한 영역을 획기적으로 확장시키고 있다. 하지만 이러한 대규모의 학습 데이터에 기반한 딥러닝은 학습시간이 증가되고, 더 나아가 학습 데이터의 구축을 위한 레이블링(labeling) 비용이 데이터 수에 비례하여 증가하게 된다. 따라서 본 논문에서는, 딥러닝의 정확도를 최대한 유지하면서 필요한 학습 데이터의 수를 줄일수 있도록, 전체 데이터에서 제한된 수의 학습 데이터의 후보를 선별하는 기법에 대하여 연구한다. 특히 기존의 능동 학습에 기반한 기법들이 학습 데이터의 수가 매우 적어질 경우 딥러닝의 정확도가 급격히 떨어지는 문제를 해결할 수 있는 새로운 기법을 제안하였으며, 제안된 기법으로 선별된 학습 데이터는 그 수의 변화에 대해 딥러닝이 일관되게 경쟁력있는 정확도를 가짐을 다양한 실험을 통하여 검증하였다.The increase of big data, parallel hardware processing and enhanced learning algorithms have led to the development of deep learning and are dramatically expanding the domain of applications based on artificial intelligence. However, such large-scale training data extends the learning time, and furthermore, the labeling cost for building the training set increases in proportion to the volume of data. Therefore, in this paper, we study how to select a limited number of candidates for training set from given data so as to reduce the number of training set while maintaining the accuracy of deep learning. Especially, we found that, with the training set selected by the previous work, the accuracy of deep learning drastically decreases when the number of training set is very small and so proposed new methods to resolve this fall-off in accuracy. Through experiments, we showed that deep learning consistently has the competitive accuracy with the training set selected by the proposed method regardless of the size of training set.Deep Learning, Train Set Selection, Active Learning, Gamma distribution"
28,딥러닝을 이용한 범용적 스테그아날리시스,김현재 /Hyunjae Kim,2017,한국정보과학회,"스테그아날리시스(Steganalysis)란 이미지 등 일반적인 자료에 암호화된 정보를 은닉하는 스테가노그래피(Steganography)에 대한 검출 및 분석 방법으로, 기계학습 기반 방법론을 포함한다. 기존 기계학습 기반 스테그아날리시스는 영상(Image)의 특징(Feature) 추출 및 모델링에 기반하며, 최근 딥러닝(Deep Learning)의 적용으로 검출 정확도가 큰 폭으로 향상되었다. 하지만 현존하는 스테그아날리시스 모델은 단일 스테가노그래피 기법에 대해 국한되어 있어 학습에 사용되지 않은 스테고(Stego) 이미지의 경우 검출이 불가능한 결정적 한계를 가진다. 본 연구에서는 다양한 스테가노그래피 기법으로 생성된 스테고이미지에 딥러닝을 적용하여 스테그아날리시스를 학습하는 범용적 모델을 제안한다. 다양한 실험을 통해 제안 기법의 효용성 및 가능성을 확인하고, 범용적 스테그아날리시스 모델이 각각에 특화된 검출 기법과 유사한 정확도로 스테고 이미지를 검출할 수 있음을 보인다.Steganalysis is to detect information hidden by steganography inside general data such as images. There are stegoanalysis techniques that use machine learning (ML). Existing ML approaches to steganalysis are based on extracting features from stego images and modeling them.
Recently deep learning-based methodologies have shown significant improvements in detection accuracy. However, all the existing methods, including deep learning-based ones, have a critical limitation in that they can only detect stego images that are created by a specific steganography method. In this paper, we propose a generalized steganalysis method that can model multiple types of stego images using deep learning. Through various experiments, we confirm the effectiveness of our approach and envision directions for future research. In particular, we show that our method can detect each type of steganography with the same level of accuracy as that of a steganalysis method dedicated to that type of steganography, thereby demonstrating the general applicability of our approach to multiple types of stego images.steganalysis, steganography, deep learning, generalized steganalysis model"
29,"딥러닝 프레임워크의 비교: 티아노, 텐서플로, CNTK를 중심으로",정여진 /Yeojin Chung,2017,한국지능정보시스템학회,"딥러닝 프레임워크의 대표적인 기능으로는 ‘자동미분’과 ‘GPU의 활용’ 등을 들 수 있다. 본 논문은 파이썬의 라이브러리 형태로 사용 가능한 프레임워크 중에서 구글의 텐서플로와 마이크로소프트의 CNTK, 그리고 텐서 플로의 원조라고 할 수 있는 티아노를 비교하였다. 본문에서는 자동미분의 개념과 GPU의 활용형태를 간단히 설명하고, 그 다음에 logistic regression을 실행하는 예를 통하여 각 프레임워크의 문법을 알아본 뒤에, 마지막으로 대표적인 딥러닝 응용인 CNN의 예제를 실행시켜보고 코딩의 편의성과 실행속도 등을 확인해 보았다. 그 결과, 편의성의 관점에서 보면 티아노가 가장 코딩 하기가 어렵고, CNTK와 텐서플로는 많은 부분이 비슷하게 추상화 되어 있어서 코딩이 비슷하지만 가중치와 편향을 직접 정의하느냐의 여부에서 차이를 보였다. 그리고 각프레임워크의 실행속도에 대한 평가는 ‘큰 차이는 없다’는 것이다. 텐서플로는 티아노에 비하여 속도가 느리다는 평가가 있어왔는데, 본 연구의 실험에 의하면, 비록 CNN 모형에 국한되었지만, 텐서플로가 아주 조금이지만빠른 것으로 나타났다. CNTK의 경우에도, 비록 실험환경이 달랐지만, 실험환경의 차이에 의한 속도의 차이의 편차범위 이내에 있는 것으로 판단이 되었다. 본 연구에서는 세 종류의 딥러닝 프레임워크만을 살펴보았는데,위키피디아에 따르면 딥러닝 프레임워크의 종류는 12가지가 있으며, 각 프레임워크의 특징을 15가지 속성으로구분하여 차이를 특정하고 있다. 그 많은 속성 중에서 사용자의 입장에서 볼 때 중요한 속성은 어떤 언어(파이썬, C++, Java, 등)로 사용가능한지, 어떤 딥러닝 모형에 대한 라이브러리가 잘 구현되어 있는지 등일 것이다. 그리고 사용자가 대규모의 딥러닝 모형을 구축한다면, 다중 GPU 혹은 다중 서버를 지원하는지의 여부도 중요할 것이다. 또한 딥러닝 모형을 처음 학습하는 경우에는 사용설명서가 많은지 예제 프로그램이 많은지 여부도 중요한 기준이 될 것이다.The deep learning framework is software designed to help develop deep learning models. Some of its important functions include “automatic differentiation” and “utilization of GPU”. The list of popular deep learning framework includes Caffe (BVLC) and Theano (University of Montreal). And recently, Microsoft's deep learning framework, Microsoft Cognitive Toolkit, was released as open-source license, following Google’s Tensorflow a year earlier. The early deep learning frameworks have been developed mainly for research at universities. Beginning with the inception of Tensorflow, however, it seems that companies such as Microsoft and Facebook have started to join the competition of framework development. Given the trend, Google and other companies are expected to continue investing in the deep learning framework to bring forward the initiative in the artificial intelligence business. From this point of view, we think it is a good time to compare some of deep learning frameworks. So we compare three deep learning frameworks which can be used as a Python library. Those are Google's Tensorflow, Microsoft’s CNTK, and Theano which is sort of a predecessor of the preceding two.
The most common and important function of deep learning frameworks is the ability to perform automatic differentiation. Basically all the mathematical expressions of deep learning models can be represented as computational graphs, which consist of nodes and edges. Partial derivatives on each edge of a computational graph can then be obtained. With the partial derivatives, we can let software compute differentiation of any node with respect to any variable by utilizing chain rule of Calculus.
First of all, the convenience of coding is in the order of CNTK, Tensorflow, and Theano. The criterion is simply based on the lengths of the codes and the learning curve and the ease of coding are not the main concern. According to the criteria, Theano was the most difficult to implement with, and CNTK and Tensorflow were somewhat easier. With Tensorflow, we need to define weight variables and biases explicitly. The reason that CNTK and Tensorflow are easier to implement with is that those frameworks provide us with more abstraction than Theano. We, however, need to mention that low-level coding is not always bad. It gives us flexibility of coding. With the low-level coding such as in Theano, we can implement and test any new deep learning models or any new search methods that we can think of.
The assessment of the execution speed of each framework is that there is not meaningful difference. According to the experiment, execution speeds of Theano and Tensorflow are very similar, although the experiment was limited to a CNN model. In the case of CNTK, the experimental environment was not maintained as the same. The code written in CNTK has to be run in PC environment without GPU where codes execute as much as 50 times slower than with GPU. But we concluded that the difference of execution speed was within the range of variation caused by the different hardware setup.
In this study, we compared three types of deep learning framework: Theano, Tensorflow, and CNTK. According to Wikipedia, there are 12 available deep learning frameworks. And 15 different attributes differentiate each framework. Some of the important attributes would include interface language (Python, C ++, Java, etc.) and the availability of libraries on various deep learning models such as CNN, RNN, DBN, and etc. And if a user implements a large scale deep learning model, it will also be important to support multiple GPU or multiple servers. Also, if you are learning the deep learning model, it would also be important if there are enough examples and references.deep learning framework, Theano, TensorFlow, CNTK, computational graph, CIFAR-10"
30,딥러닝을 이용한 주변 무선단말 파악방안,이웅섭 /Lee， Woong-sup,2018,한국정보통신학회,"최근 단말-대-단말(Device-to-device, D2D) 통신기술이 차세대 무선통신시스템의 핵심기술로 큰 관심을 받고 있다. 이러한 단말간 통신에서는 자신의 주변에 어떠한 단말이 있는지 파악하는 주변단말 탐색(Neighbor discovery)이매우 중요하다. 본 논문에서는 최근 큰 관심을 받고 있는 딥러닝(Deep learning) 기술을 활용하여 단말간 통신에서 주변단말을 파악하는 방안에 대해서 제안한다. 제안 방안은 기존의 방안과 달리 무선채널의 공간적 연관성을 이용하여단말간의 신호 전송 없이 단말이 기지국으로 전송하는 상향링크 파일럿 신호를 기반으로 주변 단말을 찾고 따라서기존의 방식에 비해 신호전송 복잡도(signaling complexity)를 크게 줄일 수 있다. 또한 제안 방안에서는 떨어져 있는거리에 따라서 주변 단말을 분류 가능하여 기존 방안에 비해서 좀 더 세밀한 단말 탐색이 가능하다. 마지막으로 본 논문에서는 tensorflow를 이용한 컴퓨터 시뮬레이션을 통해 제안 방안의 성능을 검증하였다.Recently, the device-to-device (D2D) communication has been conceived as the key technology for the next-generationmobile communication systems. The neighbor discovery in which the nearby users are found, is essential for the properoperation of the D2D communication. In this paper, we propose new neighbor discovery scheme based on deep learningtechnology which has gained a lot of attention recently. In the proposed scheme, the neighboring users can be foundusing the uplink pilot transmission of users only, unlike conventional neighbor discovery schemes in which direct pilotcommunication among users is required, such that the signaling overhead can be greatly reduced in our proposed scheme.
Moreover, the neighbors with different proximity can also be classified accordingly which enables more accurate neighbordiscovery compared to the conventional schemes. The performance of our proposed scheme is verified through thetensorflow-based computer simulations.
키워드 : 딥러닝, 주변단말 탐색, 단말-대-단말 통신Deep learning, Neighbor discovery, Device-to-device communication, Wireless channel correlation"
31,신뢰성있는 딥러닝 기반 분석 모델을 참조하기 위한 딥러닝 기술 언어,문종혁 /Jong Hyeok Mun,2021,한국정보처리학회,"최근 딥러닝은 하드웨어 성능이 향상됨에 따라 자연어 처리, 영상 인식 등의 다양한 기술에 접목되어 활용되고 있다. 이러한 기술들을 활용해 지능형교통 시스템(ITS), 스마트홈, 헬스케어 등의 산업분야에서 데이터를 분석하여 고속도로 속도위반 차량 검출, 에너지 사용량 제어, 응급상황 등과 같은고품질의 서비스를 제공하며, 고품질의 서비스를 제공하기 위해서는 정확도가 향상된 딥러닝 모델이 적용되어야 한다. 이를 위해 서비스 환경의 데이터를분석하기 위한 딥러닝 모델을 개발할 때, 개발자는 신뢰성이 검증된 최신의 딥러닝 모델을 적용할 수 있어야 한다. 이는 개발자가 참조하는 딥러닝 모델에적용된 학습 데이터셋의 정확도를 측정하여 검증할 수 있다. 이러한 검증을 위해서 개발자는 학습 데이터셋, 딥러닝의 계층구조 및 개발 환경 등과 같은내용을 포함하는 딥러닝 모델을 문서화하여 적용하기 위한 구조적인 정보가 필요하다. 본 논문에서는 신뢰성있는 딥러닝 기반 데이터 분석 모델을 참조하기위한 딥러닝 기술 언어를 제안한다. 제안하는 기술 언어는 신뢰성 있는 딥러닝 모델을 개발하는데 필요한 학습데이터셋, 개발 환경 및 설정 등의 정보와더불어 딥러닝 모델의 계층구조를 표현할 수 있다. 제안하는 딥러닝 기술 언어를 이용하여 개발자는 지능형 교통 시스템에서 참조하는 분석 모델의정확도를 검증할 수 있다. 실험에서는 제안하는 언어의 유효성을 검증하기 위해, 번호판 인식 모델을 중심으로 딥러닝 기술 문서의 적용과정을 보인다With the recent advancements of deep learning, companies such as smart home, healthcare, and intelligent transportation systems areutilizing its functionality to provide high-quality services for vehicle detection, emergency situation detection, and controlling energyconsumption. To provide reliable services in such sensitive systems, deep learning models are required to have high accuracy. In orderto develop a deep learning model for analyzing previously mentioned services, developers should utilize the state of the art deep learningmodels that have already been verified for higher accuracy. The developers can verify the accuracy of the referenced model by validatingthe model on the dataset. For this validation, the developer needs structural information to document and apply deep learning models,including metadata such as learning dataset, network architecture, and development environments. In this paper, we propose a descriptionlanguage that represents the network architecture of the deep learning model along with its metadata that are necessary to develop adeep learning model. Through the proposed description language, developers can easily verify the accuracy of the referenced deep learningmodel. Our experiments demonstrate the application scenario of a deep learning description document that focuses on the license platerecognition for the detection of illegally parked vehicles.Trusted Deep Learning, Model Reference, Deep Learning Description Language, Traffic Situation Analysis Model"
32,딥러닝 분석을 이용한 중국 역내·외 위안화 변동성 예측,이우식 /Lee Woosik,2016,한국데이터정보과학회,"2008년 글로벌 금융위기 이후 중국은 위안화 국제화의 점진적 추진을 진행하면서 중국상하이 외환시장과 중국홍콩 외환시장에서 거래되는 통화인 역내위안화와 역외위안화를 형성시켰다. 본 연구는 위안화 국제화와 점진적인 중국 자본계정 개방에 따라 급변하는 외환시장상황의 변동성을 정확하게 파악하기 위해서 GARCH모형 (일반화된 자기회귀 조건부이분산성모형)에 다단계인공신경망을 결합한 MLP-GARCH 모형과 GARCH모형과 기계학습의 일종인 딥러닝 (deep learning)을 통합한 DL-GARCH을 가지고 위안화 변동성예측을 비교 실험과 분석을 하였다. 비교분석 결과 DLGARCH 모형은 MLP-GARCH보다 모형 위안화 역내·외 환율변동성 예측 면에서 더욱 더 개선된예측값을 제공하였다. 그래서 이분산시계열모형을 딥러닝과 결합한 DL-GARCH 모형은 시계열의환율 변동성 예측 문제에 딥러닝을 응용할 수 있음을 확인하였다. 향후 이분산시계열과 결합된 딥러닝 모형은 다른 금융시계열 데이터에 응용하여 그 일반화 가능성을 높일 수 있을 것이다.The People’s Republic of China has vigorously been pursuing the internationalization of the Chinese Yuan or Renminbi after the financial crisis of 2008. In this view, an abrupt increase of use of the Chinese Yuan in the onshore and offshore markets are important milestones to be one of important currencies. One of the most frequently used methods to forecast volatility is GARCH model. Since a prediction error of the GARCH model has been reported quite high, a lot of efforts have been made to improve forecasting capability of the GARCH model. In this paper, we have proposed MLPGARCH and a DL-GARCH by employing Artificial Neural Network to the GARCH. In an application to forecasting Chinese Yuan volatility, we have successfully shown their overall outperformance in forecasting over the GARCH.Deep learning, DL-GARCH, GARCH, The Chinese Yuan, volatility."
33,딥러닝 기법을 이용한 내일강수 예측,하지훈,2016,한국지능시스템학회,"정확한 강수예측을 위해서는 예측인자 선정과 예측방법에 대한 선택이 매우 중요하다. 최근에는 강수예측 방법으로기계학습 기법이 많이 사용되고 있으며, 그 중에서도 특히 인공신경망을 사용한 강수예측 방법은 좋은 성능을 보였다. 본논문에서는 딥러닝 기법 중 하나인 DBN(deep belief network)를 이용한 새로운 강수예측 방법을 제안한다. DBN는 비지도사전 학습을 통해 초기 가중치를 설정하여 기존 인공신경망의 문제점을 보완한다. 예측인자로는 기온, 전일-전주 강수일, 태양과 달 궤도 관련 자료를 선정하였다. 기온과 전일-전주 강수일은 서울에서의 1974년부터 2013년까지 총 40년간의AWS(automatic weather system) 관측 자료를 사용하였고, 태양과 달의 궤도 관련 자료는 서울을 중심으로 계산한 결과를사용하였다. 전체 기간에서 일부는 학습 자료로 사용하여 예측모델을 생성하였고, 나머지를 생성한 모델의 검증 자료로사용하였다. 모델 검증 결과로 나온 예측값들은 확률값을 가지며 임계치를 이용하여 강수유무를 판별하였다. 강수 정확도의척도로 양분예보기법 중 CSI(critical successive index)와 Bias(frequency bias)를 계산하였다. 이를 통해 DBN와 MLP(multilayer perceptron)의 성능을 비교한 결과 DBN의 강수 예측 정확도가 높았고, 수행속도 또한 2배 이상 빨랐다.For accurate precipitation forecasts the choice of weather faoctrs and prediction method is very important. Recently, machinele arning has been widely used for forecasting precipitation, and artificial neural network, one of machine learning techniques, showed good performance. In this paper, we suggest a new method for forecasting precipitation using DBN, one of deep learning techniques.
DBN has an advantage that initial weights are set by unsupervised learning, so this compensates for the defects of artificial neural networks. We used past precipitation, temperature, and the parameters of the sun and moon's motion as features for forecasting precipitation. The dataset consists of observation data which had been measured for 40 years from AWS in Seoul. Experiments were based on 8-fold cross validation. As a result of estimation, we got probabilities of test dataset, so threshold was used for the decision of precipitation. CSI and Bias were used for indicating the precision of precipitation. Our experimental results showed that DBN performed better than MLP.Deep learning, Deep belief network, Precipitation, Forecast"
34,딥러닝 신경망모형을 이용한 실시간 교통정보수집 연구,김대현 /KIM DAE HYON,2016,사단법인 인문사회과학기술융합학회,"비디오 기반의 실시간 교통정보수집 시스템은 자율주행을 포함한 지능형교통체계의 주요 서브시스템중 하나이며, 우수한 예측(추정)력 때문에 핵심 알고리즘으로 인공신경망 모형이 주로 사용되고 있다. 특히, 최근 과학 학술지 네이처에 구글 딥마인드의 알파고가 공개되면서 딥러닝과 인공지능에 대한 전 세계적인 관심이 집중되고 있다. 최근까지 가장 널리 사용된 인공신경망모형은 역전파 모형이었으나 최근 컴퓨터 하드웨어 및 알고리즘의 새로운 개발과 더불어 딥러닝 알고리즘을 이용한 딥인공신경망 모형에 대한 관심이 높아지고 있다. 그러나 아직 딥러닝 인공신경망에 대한 연구는 초기단계에 있는 실정이며 적용분야도 극히 한정되어 있다. 본 연구에서는 실시간 도로교통정보 수집을 위한 영상처리분야에서 딥러닝 알고리즘을 활용하고 실험결과를 통하여 향후 교통정보수집‧분석 분야에서의 딥러닝 인공신경망 모형의 적용가능성을 살펴보고자 한다. 특히, 본 연구에서 도출한 딥러닝 인공신경망모형의 실험결과를 기존 역전파 모형에 의한 실험결과와 비교하여 모형의 우수성을 비교‧평가하고자 한다.Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Normally, artificial neural networks have been used in real-time applications for collecting traffic information because of their remarkable performance in terms of predictive accuracy and real-time computing. Especially, the recent publication of a paper in the journal Nature and the success of Google DeepMind’s world-beating Go program AlphaGo caused significant increases of the public interest in artificial intelligence. Even though various types of learning models have been proposed, the Backpropagation has been the most popular neural network model. However, the Deep learning neural network model has attracted much interest recently. In this study, the Deep leaning neural network will be applied in image processing techniques for the real-time traffic information collection to investigate the performance of Deep learning algorithm in traffic engineering fields. In addition, the experimental results from Deep learing algorithm will be compared with the results from Backpropagation model to evaluate and propose better performing model.Intelligent Transportation Systems (ITS), artificial neural networks, Deep learning, traffic information, image processing"
35,딥러닝의 모형과 응용사례,안성만 /SungMahn Ahn,2016,한국지능정보시스템학회,"딥러닝은 인공신경망(neural network)이라는 인공지능분야의 모형이 발전된 형태로서, 계층구조로 이루어진 인공신경망의 내부계층(hidden layer)이 여러 단계로 이루어진 구조이다. 딥러닝에서의 주요 모형은 합성곱신경망(convolutional neural network), 순환신경망(recurrent neural network), 그리고 심층신뢰신경망(deep belief network)의 세가지라고 할 수있다. 그 중에서 현재 흥미로운 연구가 많이 발표되어서 관심이 집중되고 있는 모형은 지도학습(supervised learning)모형인 처음 두 개의 모형이다. 따라서 본 논문에서는 지도학습모형의 가중치를 최적화하는 기본적인 방법인 오류역전파 알고리즘을 살펴본 뒤에 합성곱신경망과 순환신경망의 구조와 응용사례 등을 살펴보고자 한다. 본문에서 다루지 않은 모형인 심층신뢰신경망은 아직까지는 합성곱신경망 이나 순환신경망보다는 상대적으로 주목을 덜 받고 있다. 그러나 심층신뢰신경망은 CNN이나 RNN과는 달리 비지도학습(unsupervised learning)모형이며, 사람이나 동물은 관찰을 통해서 스스로 학습한다는 점에서 궁극적으로는 비지도학습모형이 더 많이 연구되어야 할 주제가 될 것이다.Deep learning model is a kind of neural networks that allows multiple hidden layers. There are various deep learning architectures such as convolutional neural networks, deep belief networks and recurrent neural networks. Those have been applied to fields like computer vision, automatic speech recognition, natural language processing, audio recognition and bioinformatics where they have been shown to produce state-of-the-art results on various tasks. Among those architectures, convolutional neural networks and recurrent neural networks are classified as the supervised learning model. And in recent years, those supervised learning models have gained more popularity than unsupervised learning models such as deep belief networks, because supervised learning models have shown fashionable applications in such fields mentioned above.
Deep learning models can be trained with backpropagation algorithm. Backpropagation is an abbreviation for “backward propagation of errors” and a common method of training artificial neural networks used in conjunction with an optimization method such as gradient descent. The method calculates the gradient of an error function with respect to all the weights in the network. The gradient is fed to the optimization method which in turn uses it to update the weights, in an attempt to minimize the error function.
Convolutional neural networks use a special architecture which is particularly well-adapted to classify images. Using this architecture makes convolutional networks fast to train. This, in turn, helps us train deep, muti-layer networks, which are very good at classifying images. These days, deep convolutional networks are used in most neural networks for image recognition. Convolutional neural networks use three basic ideas: local receptive fields, shared weights, and pooling. By local receptive fields, we mean that each neuron in the first(or any) hidden layer will be connected to a small region of the input(or previous layer’s) neurons. Shared weights mean that we’re going to use the same weights and bias for each of the local receptive field. This means that all the neurons in the hidden layer detect exactly the same feature, just at different locations in the input image. In addition to the convolutional layers just described, convolutional neural networks also contain pooling layers. Pooling layers are usually used immediately after convolutional layers. What the pooling layers do is to simplify the information in the output from the convolutional layer.
Recent convolutional network architectures have 10 to 20 hidden layers and billions of connections between units. Training deep learning networks has taken weeks several years ago, but thanks to progress in GPU and algorithm enhancement, training time has reduced to several hours.
Neural networks with time-varying behavior are known as recurrent neural networks or RNNs. A recurrent neural network is a class of artificial neural network where connections between units form a directed cycle. This creates an internal state of the network which allows it to exhibit dynamic temporal behavior. Unlike feedforward neural networks, RNNs can use their internal memory to process arbitrary sequences of inputs. Early RNN models turned out to be very difficult to train, harder even than deep feedforward networks. The reason is the unstable gradient problem such as vanishing gradient and exploding gradient. The gradient can get smaller and smaller as it is propagated back through layers. This makes learning in early layers extremely slow. The problem actually gets worse in RNNs, since gradients aren’t just propagated backward through layers, they’re propagated backward through time. If the network runs for a long time, that can make the gradient extremely unstable and hard to learn from. It has been possible to incorporate an idea known as long short-term memory units (LSTMs) into RNNs. LSTMs make it much easier to get good results when training RNNs, and many recent papers make use of LSTMs or related ideas.Deep learning, Convolutional neural networks, Recurrent neural networks, Error backpropagation algorithm"
36,딥러닝분석과 기술적 분석 지표를 이용한 한국 코스피주가지수 방향성 예측,이우식 /Lee Woosik,2017,한국데이터정보과학회,"2016년 3월 구글 (Google)의 바둑인공지능 알파고 (AlphaGo)가 이세돌 9단과의 바둑대결에서승리한 이후 다양한 분야에서 인공지능 사용에 대한 관심이 높아지고 있는 가운데 금융투자 분야에서도 인공지능과 투자자문 전문가의 합성어인 로보어드바이저 (Robo-Advisor)에 대한 관심이 높아지고 있다. 인공지능 (artificial intelligence)기반의 의사결정은 비용 절감은 물론 효과적인 의사결정을가능하게 한다는 점에서 큰 장점이 있다. 본 연구에서는 기술적 분석 (technical analysis) 지표와 딥러닝 (deep learning) 모형을 결합하여 한국 코스피 지수를 예측하는 모형을 개발하고 제시한 모형들의 예측력을 비교, 분석한다. 분석 결과 기술적 분석 지표에 딥러닝 알고리즘을 결합한 모형이 주가지수 방향성 예측 문제에 응용될 수 있음을 확인하였다. 향후 본 연구에서 제안된 기술적 분석 지표와딥러닝모형을 결합한 기법은 로보어드바이저서비스에 응용할 수 있는 일반화 가능성을 보여준다Since Google’s AlphaGo defeated a world champion of Go players in 2016, there have been many interests in the deep learning. In the financial sector, a Robo-Advisor using deep learning gains a significant attention, which builds and manages portfolios of financial instruments for investors.In this paper, we have proposed the a deep learning algorithm geared toward identification and forecast of the KOSPI index direction,and we also have compared the accuracy of the prediction.In an application of forecasting the financial market index direction, we have shown that the Robo-Advisor using deep learning has a significant effect on finance industry. The Robo-Advisor collects a massive data such as earnings statements, news reports and regulatory filings, analyzes those and recommends investors how to view market trends and identify the best time to purchase financial assets. On the other hand, the Robo-Advisor allows businesses to learn more about their customers, develop better marketing strategies, increase sales and decrease costs.Artificial intelligence, deep learning, FinTech, Robo-Advisor, technical analysis."
37,딥러닝의 효율적인 학습을 위한 학습데이터 선별 기법에 관한 연구,주경돈 /Gyoung-don Joo,2017,한국정보과학회,"빅데이터의 증가와 병렬 하드웨어 처리, 학습 알고리즘의 개선은 딥러닝의 발전을 가져왔으며 인공지능이 적용 가능한 영역을 획기적으로 확장시키고 있다. 하지만 이러한 대규모의 학습 데이터에 기반한 딥러닝은 학습시간이 증가되고, 더 나아가 학습 데이터의 구축을 위한 레이블링(labeling) 비용이 데이터 수에 비례하여 증가하게 된다. 따라서 본 논문에서는, 딥러닝의 정확도를 최대한 유지하면서 필요한 학습 데이터의 수를 줄일수 있도록, 전체 데이터에서 제한된 수의 학습 데이터의 후보를 선별하는 기법에 대하여 연구한다. 특히 기존의 능동 학습에 기반한 기법들이 학습 데이터의 수가 매우 적어질 경우 딥러닝의 정확도가 급격히 떨어지는 문제를 해결할 수 있는 새로운 기법을 제안하였으며, 제안된 기법으로 선별된 학습 데이터는 그 수의 변화에 대해 딥러닝이 일관되게 경쟁력있는 정확도를 가짐을 다양한 실험을 통하여 검증하였다.The increase of big data, parallel hardware processing and enhanced learning algorithms have led to the development of deep learning and are dramatically expanding the domain of applications based on artificial intelligence. However, such large-scale training data extends the learning time, and furthermore, the labeling cost for building the training set increases in proportion to the volume of data. Therefore, in this paper, we study how to select a limited number of candidates for training set from given data so as to reduce the number of training set while maintaining the accuracy of deep learning. Especially, we found that, with the training set selected by the previous work, the accuracy of deep learning drastically decreases when the number of training set is very small and so proposed new methods to resolve this fall-off in accuracy. Through experiments, we showed that deep learning consistently has the competitive accuracy with the training set selected by the proposed method regardless of the size of training set.Deep Learning, Train Set Selection, Active Learning, Gamma distribution"
38,딥러닝을 이용한 범용적 스테그아날리시스,김현재 /Hyunjae Kim,2017,한국정보과학회,"스테그아날리시스(Steganalysis)란 이미지 등 일반적인 자료에 암호화된 정보를 은닉하는 스테가노그래피(Steganography)에 대한 검출 및 분석 방법으로, 기계학습 기반 방법론을 포함한다. 기존 기계학습 기반 스테그아날리시스는 영상(Image)의 특징(Feature) 추출 및 모델링에 기반하며, 최근 딥러닝(Deep Learning)의 적용으로 검출 정확도가 큰 폭으로 향상되었다. 하지만 현존하는 스테그아날리시스 모델은 단일 스테가노그래피 기법에 대해 국한되어 있어 학습에 사용되지 않은 스테고(Stego) 이미지의 경우 검출이 불가능한 결정적 한계를 가진다. 본 연구에서는 다양한 스테가노그래피 기법으로 생성된 스테고이미지에 딥러닝을 적용하여 스테그아날리시스를 학습하는 범용적 모델을 제안한다. 다양한 실험을 통해 제안 기법의 효용성 및 가능성을 확인하고, 범용적 스테그아날리시스 모델이 각각에 특화된 검출 기법과 유사한 정확도로 스테고 이미지를 검출할 수 있음을 보인다.Steganalysis is to detect information hidden by steganography inside general data such as images. There are stegoanalysis techniques that use machine learning (ML). Existing ML approaches to steganalysis are based on extracting features from stego images and modeling them.
Recently deep learning-based methodologies have shown significant improvements in detection accuracy. However, all the existing methods, including deep learning-based ones, have a critical limitation in that they can only detect stego images that are created by a specific steganography method. In this paper, we propose a generalized steganalysis method that can model multiple types of stego images using deep learning. Through various experiments, we confirm the effectiveness of our approach and envision directions for future research. In particular, we show that our method can detect each type of steganography with the same level of accuracy as that of a steganalysis method dedicated to that type of steganography, thereby demonstrating the general applicability of our approach to multiple types of stego images.steganalysis, steganography, deep learning, generalized steganalysis model"
39,"딥러닝 프레임워크의 비교: 티아노, 텐서플로, CNTK를 중심으로",정여진 /Yeojin Chung,2017,한국지능정보시스템학회,"딥러닝 프레임워크의 대표적인 기능으로는 ‘자동미분’과 ‘GPU의 활용’ 등을 들 수 있다. 본 논문은 파이썬의 라이브러리 형태로 사용 가능한 프레임워크 중에서 구글의 텐서플로와 마이크로소프트의 CNTK, 그리고 텐서 플로의 원조라고 할 수 있는 티아노를 비교하였다. 본문에서는 자동미분의 개념과 GPU의 활용형태를 간단히 설명하고, 그 다음에 logistic regression을 실행하는 예를 통하여 각 프레임워크의 문법을 알아본 뒤에, 마지막으로 대표적인 딥러닝 응용인 CNN의 예제를 실행시켜보고 코딩의 편의성과 실행속도 등을 확인해 보았다. 그 결과, 편의성의 관점에서 보면 티아노가 가장 코딩 하기가 어렵고, CNTK와 텐서플로는 많은 부분이 비슷하게 추상화 되어 있어서 코딩이 비슷하지만 가중치와 편향을 직접 정의하느냐의 여부에서 차이를 보였다. 그리고 각프레임워크의 실행속도에 대한 평가는 ‘큰 차이는 없다’는 것이다. 텐서플로는 티아노에 비하여 속도가 느리다는 평가가 있어왔는데, 본 연구의 실험에 의하면, 비록 CNN 모형에 국한되었지만, 텐서플로가 아주 조금이지만빠른 것으로 나타났다. CNTK의 경우에도, 비록 실험환경이 달랐지만, 실험환경의 차이에 의한 속도의 차이의 편차범위 이내에 있는 것으로 판단이 되었다. 본 연구에서는 세 종류의 딥러닝 프레임워크만을 살펴보았는데,위키피디아에 따르면 딥러닝 프레임워크의 종류는 12가지가 있으며, 각 프레임워크의 특징을 15가지 속성으로구분하여 차이를 특정하고 있다. 그 많은 속성 중에서 사용자의 입장에서 볼 때 중요한 속성은 어떤 언어(파이썬, C++, Java, 등)로 사용가능한지, 어떤 딥러닝 모형에 대한 라이브러리가 잘 구현되어 있는지 등일 것이다. 그리고 사용자가 대규모의 딥러닝 모형을 구축한다면, 다중 GPU 혹은 다중 서버를 지원하는지의 여부도 중요할 것이다. 또한 딥러닝 모형을 처음 학습하는 경우에는 사용설명서가 많은지 예제 프로그램이 많은지 여부도 중요한 기준이 될 것이다.The deep learning framework is software designed to help develop deep learning models. Some of its important functions include “automatic differentiation” and “utilization of GPU”. The list of popular deep learning framework includes Caffe (BVLC) and Theano (University of Montreal). And recently, Microsoft's deep learning framework, Microsoft Cognitive Toolkit, was released as open-source license, following Google’s Tensorflow a year earlier. The early deep learning frameworks have been developed mainly for research at universities. Beginning with the inception of Tensorflow, however, it seems that companies such as Microsoft and Facebook have started to join the competition of framework development. Given the trend, Google and other companies are expected to continue investing in the deep learning framework to bring forward the initiative in the artificial intelligence business. From this point of view, we think it is a good time to compare some of deep learning frameworks. So we compare three deep learning frameworks which can be used as a Python library. Those are Google's Tensorflow, Microsoft’s CNTK, and Theano which is sort of a predecessor of the preceding two.
The most common and important function of deep learning frameworks is the ability to perform automatic differentiation. Basically all the mathematical expressions of deep learning models can be represented as computational graphs, which consist of nodes and edges. Partial derivatives on each edge of a computational graph can then be obtained. With the partial derivatives, we can let software compute differentiation of any node with respect to any variable by utilizing chain rule of Calculus.
First of all, the convenience of coding is in the order of CNTK, Tensorflow, and Theano. The criterion is simply based on the lengths of the codes and the learning curve and the ease of coding are not the main concern. According to the criteria, Theano was the most difficult to implement with, and CNTK and Tensorflow were somewhat easier. With Tensorflow, we need to define weight variables and biases explicitly. The reason that CNTK and Tensorflow are easier to implement with is that those frameworks provide us with more abstraction than Theano. We, however, need to mention that low-level coding is not always bad. It gives us flexibility of coding. With the low-level coding such as in Theano, we can implement and test any new deep learning models or any new search methods that we can think of.
The assessment of the execution speed of each framework is that there is not meaningful difference. According to the experiment, execution speeds of Theano and Tensorflow are very similar, although the experiment was limited to a CNN model. In the case of CNTK, the experimental environment was not maintained as the same. The code written in CNTK has to be run in PC environment without GPU where codes execute as much as 50 times slower than with GPU. But we concluded that the difference of execution speed was within the range of variation caused by the different hardware setup.
In this study, we compared three types of deep learning framework: Theano, Tensorflow, and CNTK. According to Wikipedia, there are 12 available deep learning frameworks. And 15 different attributes differentiate each framework. Some of the important attributes would include interface language (Python, C ++, Java, etc.) and the availability of libraries on various deep learning models such as CNN, RNN, DBN, and etc. And if a user implements a large scale deep learning model, it will also be important to support multiple GPU or multiple servers. Also, if you are learning the deep learning model, it would also be important if there are enough examples and references.deep learning framework, Theano, TensorFlow, CNTK, computational graph, CIFAR-10"
40,딥러닝을 이용한 주변 무선단말 파악방안,이웅섭 /Lee， Woong-sup,2018,한국정보통신학회,"최근 단말-대-단말(Device-to-device, D2D) 통신기술이 차세대 무선통신시스템의 핵심기술로 큰 관심을 받고 있다. 이러한 단말간 통신에서는 자신의 주변에 어떠한 단말이 있는지 파악하는 주변단말 탐색(Neighbor discovery)이매우 중요하다. 본 논문에서는 최근 큰 관심을 받고 있는 딥러닝(Deep learning) 기술을 활용하여 단말간 통신에서 주변단말을 파악하는 방안에 대해서 제안한다. 제안 방안은 기존의 방안과 달리 무선채널의 공간적 연관성을 이용하여단말간의 신호 전송 없이 단말이 기지국으로 전송하는 상향링크 파일럿 신호를 기반으로 주변 단말을 찾고 따라서기존의 방식에 비해 신호전송 복잡도(signaling complexity)를 크게 줄일 수 있다. 또한 제안 방안에서는 떨어져 있는거리에 따라서 주변 단말을 분류 가능하여 기존 방안에 비해서 좀 더 세밀한 단말 탐색이 가능하다. 마지막으로 본 논문에서는 tensorflow를 이용한 컴퓨터 시뮬레이션을 통해 제안 방안의 성능을 검증하였다.Recently, the device-to-device (D2D) communication has been conceived as the key technology for the next-generationmobile communication systems. The neighbor discovery in which the nearby users are found, is essential for the properoperation of the D2D communication. In this paper, we propose new neighbor discovery scheme based on deep learningtechnology which has gained a lot of attention recently. In the proposed scheme, the neighboring users can be foundusing the uplink pilot transmission of users only, unlike conventional neighbor discovery schemes in which direct pilotcommunication among users is required, such that the signaling overhead can be greatly reduced in our proposed scheme.
Moreover, the neighbors with different proximity can also be classified accordingly which enables more accurate neighbordiscovery compared to the conventional schemes. The performance of our proposed scheme is verified through thetensorflow-based computer simulations.
키워드 : 딥러닝, 주변단말 탐색, 단말-대-단말 통신Deep learning, Neighbor discovery, Device-to-device communication, Wireless channel correlation"
41,신뢰성있는 딥러닝 기반 분석 모델을 참조하기 위한 딥러닝 기술 언어,문종혁 /Jong Hyeok Mun,2021,한국정보처리학회,"최근 딥러닝은 하드웨어 성능이 향상됨에 따라 자연어 처리, 영상 인식 등의 다양한 기술에 접목되어 활용되고 있다. 이러한 기술들을 활용해 지능형교통 시스템(ITS), 스마트홈, 헬스케어 등의 산업분야에서 데이터를 분석하여 고속도로 속도위반 차량 검출, 에너지 사용량 제어, 응급상황 등과 같은고품질의 서비스를 제공하며, 고품질의 서비스를 제공하기 위해서는 정확도가 향상된 딥러닝 모델이 적용되어야 한다. 이를 위해 서비스 환경의 데이터를분석하기 위한 딥러닝 모델을 개발할 때, 개발자는 신뢰성이 검증된 최신의 딥러닝 모델을 적용할 수 있어야 한다. 이는 개발자가 참조하는 딥러닝 모델에적용된 학습 데이터셋의 정확도를 측정하여 검증할 수 있다. 이러한 검증을 위해서 개발자는 학습 데이터셋, 딥러닝의 계층구조 및 개발 환경 등과 같은내용을 포함하는 딥러닝 모델을 문서화하여 적용하기 위한 구조적인 정보가 필요하다. 본 논문에서는 신뢰성있는 딥러닝 기반 데이터 분석 모델을 참조하기위한 딥러닝 기술 언어를 제안한다. 제안하는 기술 언어는 신뢰성 있는 딥러닝 모델을 개발하는데 필요한 학습데이터셋, 개발 환경 및 설정 등의 정보와더불어 딥러닝 모델의 계층구조를 표현할 수 있다. 제안하는 딥러닝 기술 언어를 이용하여 개발자는 지능형 교통 시스템에서 참조하는 분석 모델의정확도를 검증할 수 있다. 실험에서는 제안하는 언어의 유효성을 검증하기 위해, 번호판 인식 모델을 중심으로 딥러닝 기술 문서의 적용과정을 보인다With the recent advancements of deep learning, companies such as smart home, healthcare, and intelligent transportation systems areutilizing its functionality to provide high-quality services for vehicle detection, emergency situation detection, and controlling energyconsumption. To provide reliable services in such sensitive systems, deep learning models are required to have high accuracy. In orderto develop a deep learning model for analyzing previously mentioned services, developers should utilize the state of the art deep learningmodels that have already been verified for higher accuracy. The developers can verify the accuracy of the referenced model by validatingthe model on the dataset. For this validation, the developer needs structural information to document and apply deep learning models,including metadata such as learning dataset, network architecture, and development environments. In this paper, we propose a descriptionlanguage that represents the network architecture of the deep learning model along with its metadata that are necessary to develop adeep learning model. Through the proposed description language, developers can easily verify the accuracy of the referenced deep learningmodel. Our experiments demonstrate the application scenario of a deep learning description document that focuses on the license platerecognition for the detection of illegally parked vehicles.Trusted Deep Learning, Model Reference, Deep Learning Description Language, Traffic Situation Analysis Model"
42,딥러닝 분석을 이용한 중국 역내·외 위안화 변동성 예측,이우식 /Lee Woosik,2016,한국데이터정보과학회,"2008년 글로벌 금융위기 이후 중국은 위안화 국제화의 점진적 추진을 진행하면서 중국상하이 외환시장과 중국홍콩 외환시장에서 거래되는 통화인 역내위안화와 역외위안화를 형성시켰다. 본 연구는 위안화 국제화와 점진적인 중국 자본계정 개방에 따라 급변하는 외환시장상황의 변동성을 정확하게 파악하기 위해서 GARCH모형 (일반화된 자기회귀 조건부이분산성모형)에 다단계인공신경망을 결합한 MLP-GARCH 모형과 GARCH모형과 기계학습의 일종인 딥러닝 (deep learning)을 통합한 DL-GARCH을 가지고 위안화 변동성예측을 비교 실험과 분석을 하였다. 비교분석 결과 DLGARCH 모형은 MLP-GARCH보다 모형 위안화 역내·외 환율변동성 예측 면에서 더욱 더 개선된예측값을 제공하였다. 그래서 이분산시계열모형을 딥러닝과 결합한 DL-GARCH 모형은 시계열의환율 변동성 예측 문제에 딥러닝을 응용할 수 있음을 확인하였다. 향후 이분산시계열과 결합된 딥러닝 모형은 다른 금융시계열 데이터에 응용하여 그 일반화 가능성을 높일 수 있을 것이다.The People’s Republic of China has vigorously been pursuing the internationalization of the Chinese Yuan or Renminbi after the financial crisis of 2008. In this view, an abrupt increase of use of the Chinese Yuan in the onshore and offshore markets are important milestones to be one of important currencies. One of the most frequently used methods to forecast volatility is GARCH model. Since a prediction error of the GARCH model has been reported quite high, a lot of efforts have been made to improve forecasting capability of the GARCH model. In this paper, we have proposed MLPGARCH and a DL-GARCH by employing Artificial Neural Network to the GARCH. In an application to forecasting Chinese Yuan volatility, we have successfully shown their overall outperformance in forecasting over the GARCH.Deep learning, DL-GARCH, GARCH, The Chinese Yuan, volatility."
43,딥러닝 기법을 이용한 내일강수 예측,하지훈,2016,한국지능시스템학회,"정확한 강수예측을 위해서는 예측인자 선정과 예측방법에 대한 선택이 매우 중요하다. 최근에는 강수예측 방법으로기계학습 기법이 많이 사용되고 있으며, 그 중에서도 특히 인공신경망을 사용한 강수예측 방법은 좋은 성능을 보였다. 본논문에서는 딥러닝 기법 중 하나인 DBN(deep belief network)를 이용한 새로운 강수예측 방법을 제안한다. DBN는 비지도사전 학습을 통해 초기 가중치를 설정하여 기존 인공신경망의 문제점을 보완한다. 예측인자로는 기온, 전일-전주 강수일, 태양과 달 궤도 관련 자료를 선정하였다. 기온과 전일-전주 강수일은 서울에서의 1974년부터 2013년까지 총 40년간의AWS(automatic weather system) 관측 자료를 사용하였고, 태양과 달의 궤도 관련 자료는 서울을 중심으로 계산한 결과를사용하였다. 전체 기간에서 일부는 학습 자료로 사용하여 예측모델을 생성하였고, 나머지를 생성한 모델의 검증 자료로사용하였다. 모델 검증 결과로 나온 예측값들은 확률값을 가지며 임계치를 이용하여 강수유무를 판별하였다. 강수 정확도의척도로 양분예보기법 중 CSI(critical successive index)와 Bias(frequency bias)를 계산하였다. 이를 통해 DBN와 MLP(multilayer perceptron)의 성능을 비교한 결과 DBN의 강수 예측 정확도가 높았고, 수행속도 또한 2배 이상 빨랐다.For accurate precipitation forecasts the choice of weather faoctrs and prediction method is very important. Recently, machinele arning has been widely used for forecasting precipitation, and artificial neural network, one of machine learning techniques, showed good performance. In this paper, we suggest a new method for forecasting precipitation using DBN, one of deep learning techniques.
DBN has an advantage that initial weights are set by unsupervised learning, so this compensates for the defects of artificial neural networks. We used past precipitation, temperature, and the parameters of the sun and moon's motion as features for forecasting precipitation. The dataset consists of observation data which had been measured for 40 years from AWS in Seoul. Experiments were based on 8-fold cross validation. As a result of estimation, we got probabilities of test dataset, so threshold was used for the decision of precipitation. CSI and Bias were used for indicating the precision of precipitation. Our experimental results showed that DBN performed better than MLP.Deep learning, Deep belief network, Precipitation, Forecast"
44,딥러닝 신경망모형을 이용한 실시간 교통정보수집 연구,김대현 /KIM DAE HYON,2016,사단법인 인문사회과학기술융합학회,"비디오 기반의 실시간 교통정보수집 시스템은 자율주행을 포함한 지능형교통체계의 주요 서브시스템중 하나이며, 우수한 예측(추정)력 때문에 핵심 알고리즘으로 인공신경망 모형이 주로 사용되고 있다. 특히, 최근 과학 학술지 네이처에 구글 딥마인드의 알파고가 공개되면서 딥러닝과 인공지능에 대한 전 세계적인 관심이 집중되고 있다. 최근까지 가장 널리 사용된 인공신경망모형은 역전파 모형이었으나 최근 컴퓨터 하드웨어 및 알고리즘의 새로운 개발과 더불어 딥러닝 알고리즘을 이용한 딥인공신경망 모형에 대한 관심이 높아지고 있다. 그러나 아직 딥러닝 인공신경망에 대한 연구는 초기단계에 있는 실정이며 적용분야도 극히 한정되어 있다. 본 연구에서는 실시간 도로교통정보 수집을 위한 영상처리분야에서 딥러닝 알고리즘을 활용하고 실험결과를 통하여 향후 교통정보수집‧분석 분야에서의 딥러닝 인공신경망 모형의 적용가능성을 살펴보고자 한다. 특히, 본 연구에서 도출한 딥러닝 인공신경망모형의 실험결과를 기존 역전파 모형에 의한 실험결과와 비교하여 모형의 우수성을 비교‧평가하고자 한다.Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Normally, artificial neural networks have been used in real-time applications for collecting traffic information because of their remarkable performance in terms of predictive accuracy and real-time computing. Especially, the recent publication of a paper in the journal Nature and the success of Google DeepMind’s world-beating Go program AlphaGo caused significant increases of the public interest in artificial intelligence. Even though various types of learning models have been proposed, the Backpropagation has been the most popular neural network model. However, the Deep learning neural network model has attracted much interest recently. In this study, the Deep leaning neural network will be applied in image processing techniques for the real-time traffic information collection to investigate the performance of Deep learning algorithm in traffic engineering fields. In addition, the experimental results from Deep learing algorithm will be compared with the results from Backpropagation model to evaluate and propose better performing model.Intelligent Transportation Systems (ITS), artificial neural networks, Deep learning, traffic information, image processing"
45,딥러닝의 모형과 응용사례,안성만 /SungMahn Ahn,2016,한국지능정보시스템학회,"딥러닝은 인공신경망(neural network)이라는 인공지능분야의 모형이 발전된 형태로서, 계층구조로 이루어진 인공신경망의 내부계층(hidden layer)이 여러 단계로 이루어진 구조이다. 딥러닝에서의 주요 모형은 합성곱신경망(convolutional neural network), 순환신경망(recurrent neural network), 그리고 심층신뢰신경망(deep belief network)의 세가지라고 할 수있다. 그 중에서 현재 흥미로운 연구가 많이 발표되어서 관심이 집중되고 있는 모형은 지도학습(supervised learning)모형인 처음 두 개의 모형이다. 따라서 본 논문에서는 지도학습모형의 가중치를 최적화하는 기본적인 방법인 오류역전파 알고리즘을 살펴본 뒤에 합성곱신경망과 순환신경망의 구조와 응용사례 등을 살펴보고자 한다. 본문에서 다루지 않은 모형인 심층신뢰신경망은 아직까지는 합성곱신경망 이나 순환신경망보다는 상대적으로 주목을 덜 받고 있다. 그러나 심층신뢰신경망은 CNN이나 RNN과는 달리 비지도학습(unsupervised learning)모형이며, 사람이나 동물은 관찰을 통해서 스스로 학습한다는 점에서 궁극적으로는 비지도학습모형이 더 많이 연구되어야 할 주제가 될 것이다.Deep learning model is a kind of neural networks that allows multiple hidden layers. There are various deep learning architectures such as convolutional neural networks, deep belief networks and recurrent neural networks. Those have been applied to fields like computer vision, automatic speech recognition, natural language processing, audio recognition and bioinformatics where they have been shown to produce state-of-the-art results on various tasks. Among those architectures, convolutional neural networks and recurrent neural networks are classified as the supervised learning model. And in recent years, those supervised learning models have gained more popularity than unsupervised learning models such as deep belief networks, because supervised learning models have shown fashionable applications in such fields mentioned above.
Deep learning models can be trained with backpropagation algorithm. Backpropagation is an abbreviation for “backward propagation of errors” and a common method of training artificial neural networks used in conjunction with an optimization method such as gradient descent. The method calculates the gradient of an error function with respect to all the weights in the network. The gradient is fed to the optimization method which in turn uses it to update the weights, in an attempt to minimize the error function.
Convolutional neural networks use a special architecture which is particularly well-adapted to classify images. Using this architecture makes convolutional networks fast to train. This, in turn, helps us train deep, muti-layer networks, which are very good at classifying images. These days, deep convolutional networks are used in most neural networks for image recognition. Convolutional neural networks use three basic ideas: local receptive fields, shared weights, and pooling. By local receptive fields, we mean that each neuron in the first(or any) hidden layer will be connected to a small region of the input(or previous layer’s) neurons. Shared weights mean that we’re going to use the same weights and bias for each of the local receptive field. This means that all the neurons in the hidden layer detect exactly the same feature, just at different locations in the input image. In addition to the convolutional layers just described, convolutional neural networks also contain pooling layers. Pooling layers are usually used immediately after convolutional layers. What the pooling layers do is to simplify the information in the output from the convolutional layer.
Recent convolutional network architectures have 10 to 20 hidden layers and billions of connections between units. Training deep learning networks has taken weeks several years ago, but thanks to progress in GPU and algorithm enhancement, training time has reduced to several hours.
Neural networks with time-varying behavior are known as recurrent neural networks or RNNs. A recurrent neural network is a class of artificial neural network where connections between units form a directed cycle. This creates an internal state of the network which allows it to exhibit dynamic temporal behavior. Unlike feedforward neural networks, RNNs can use their internal memory to process arbitrary sequences of inputs. Early RNN models turned out to be very difficult to train, harder even than deep feedforward networks. The reason is the unstable gradient problem such as vanishing gradient and exploding gradient. The gradient can get smaller and smaller as it is propagated back through layers. This makes learning in early layers extremely slow. The problem actually gets worse in RNNs, since gradients aren’t just propagated backward through layers, they’re propagated backward through time. If the network runs for a long time, that can make the gradient extremely unstable and hard to learn from. It has been possible to incorporate an idea known as long short-term memory units (LSTMs) into RNNs. LSTMs make it much easier to get good results when training RNNs, and many recent papers make use of LSTMs or related ideas.Deep learning, Convolutional neural networks, Recurrent neural networks, Error backpropagation algorithm"
46,딥러닝분석과 기술적 분석 지표를 이용한 한국 코스피주가지수 방향성 예측,이우식 /Lee Woosik,2017,한국데이터정보과학회,"2016년 3월 구글 (Google)의 바둑인공지능 알파고 (AlphaGo)가 이세돌 9단과의 바둑대결에서승리한 이후 다양한 분야에서 인공지능 사용에 대한 관심이 높아지고 있는 가운데 금융투자 분야에서도 인공지능과 투자자문 전문가의 합성어인 로보어드바이저 (Robo-Advisor)에 대한 관심이 높아지고 있다. 인공지능 (artificial intelligence)기반의 의사결정은 비용 절감은 물론 효과적인 의사결정을가능하게 한다는 점에서 큰 장점이 있다. 본 연구에서는 기술적 분석 (technical analysis) 지표와 딥러닝 (deep learning) 모형을 결합하여 한국 코스피 지수를 예측하는 모형을 개발하고 제시한 모형들의 예측력을 비교, 분석한다. 분석 결과 기술적 분석 지표에 딥러닝 알고리즘을 결합한 모형이 주가지수 방향성 예측 문제에 응용될 수 있음을 확인하였다. 향후 본 연구에서 제안된 기술적 분석 지표와딥러닝모형을 결합한 기법은 로보어드바이저서비스에 응용할 수 있는 일반화 가능성을 보여준다Since Google’s AlphaGo defeated a world champion of Go players in 2016, there have been many interests in the deep learning. In the financial sector, a Robo-Advisor using deep learning gains a significant attention, which builds and manages portfolios of financial instruments for investors.In this paper, we have proposed the a deep learning algorithm geared toward identification and forecast of the KOSPI index direction,and we also have compared the accuracy of the prediction.In an application of forecasting the financial market index direction, we have shown that the Robo-Advisor using deep learning has a significant effect on finance industry. The Robo-Advisor collects a massive data such as earnings statements, news reports and regulatory filings, analyzes those and recommends investors how to view market trends and identify the best time to purchase financial assets. On the other hand, the Robo-Advisor allows businesses to learn more about their customers, develop better marketing strategies, increase sales and decrease costs.Artificial intelligence, deep learning, FinTech, Robo-Advisor, technical analysis."
47,딥러닝의 효율적인 학습을 위한 학습데이터 선별 기법에 관한 연구,주경돈 /Gyoung-don Joo,2017,한국정보과학회,"빅데이터의 증가와 병렬 하드웨어 처리, 학습 알고리즘의 개선은 딥러닝의 발전을 가져왔으며 인공지능이 적용 가능한 영역을 획기적으로 확장시키고 있다. 하지만 이러한 대규모의 학습 데이터에 기반한 딥러닝은 학습시간이 증가되고, 더 나아가 학습 데이터의 구축을 위한 레이블링(labeling) 비용이 데이터 수에 비례하여 증가하게 된다. 따라서 본 논문에서는, 딥러닝의 정확도를 최대한 유지하면서 필요한 학습 데이터의 수를 줄일수 있도록, 전체 데이터에서 제한된 수의 학습 데이터의 후보를 선별하는 기법에 대하여 연구한다. 특히 기존의 능동 학습에 기반한 기법들이 학습 데이터의 수가 매우 적어질 경우 딥러닝의 정확도가 급격히 떨어지는 문제를 해결할 수 있는 새로운 기법을 제안하였으며, 제안된 기법으로 선별된 학습 데이터는 그 수의 변화에 대해 딥러닝이 일관되게 경쟁력있는 정확도를 가짐을 다양한 실험을 통하여 검증하였다.The increase of big data, parallel hardware processing and enhanced learning algorithms have led to the development of deep learning and are dramatically expanding the domain of applications based on artificial intelligence. However, such large-scale training data extends the learning time, and furthermore, the labeling cost for building the training set increases in proportion to the volume of data. Therefore, in this paper, we study how to select a limited number of candidates for training set from given data so as to reduce the number of training set while maintaining the accuracy of deep learning. Especially, we found that, with the training set selected by the previous work, the accuracy of deep learning drastically decreases when the number of training set is very small and so proposed new methods to resolve this fall-off in accuracy. Through experiments, we showed that deep learning consistently has the competitive accuracy with the training set selected by the proposed method regardless of the size of training set.Deep Learning, Train Set Selection, Active Learning, Gamma distribution"
48,딥러닝을 이용한 범용적 스테그아날리시스,김현재 /Hyunjae Kim,2017,한국정보과학회,"스테그아날리시스(Steganalysis)란 이미지 등 일반적인 자료에 암호화된 정보를 은닉하는 스테가노그래피(Steganography)에 대한 검출 및 분석 방법으로, 기계학습 기반 방법론을 포함한다. 기존 기계학습 기반 스테그아날리시스는 영상(Image)의 특징(Feature) 추출 및 모델링에 기반하며, 최근 딥러닝(Deep Learning)의 적용으로 검출 정확도가 큰 폭으로 향상되었다. 하지만 현존하는 스테그아날리시스 모델은 단일 스테가노그래피 기법에 대해 국한되어 있어 학습에 사용되지 않은 스테고(Stego) 이미지의 경우 검출이 불가능한 결정적 한계를 가진다. 본 연구에서는 다양한 스테가노그래피 기법으로 생성된 스테고이미지에 딥러닝을 적용하여 스테그아날리시스를 학습하는 범용적 모델을 제안한다. 다양한 실험을 통해 제안 기법의 효용성 및 가능성을 확인하고, 범용적 스테그아날리시스 모델이 각각에 특화된 검출 기법과 유사한 정확도로 스테고 이미지를 검출할 수 있음을 보인다.Steganalysis is to detect information hidden by steganography inside general data such as images. There are stegoanalysis techniques that use machine learning (ML). Existing ML approaches to steganalysis are based on extracting features from stego images and modeling them.
Recently deep learning-based methodologies have shown significant improvements in detection accuracy. However, all the existing methods, including deep learning-based ones, have a critical limitation in that they can only detect stego images that are created by a specific steganography method. In this paper, we propose a generalized steganalysis method that can model multiple types of stego images using deep learning. Through various experiments, we confirm the effectiveness of our approach and envision directions for future research. In particular, we show that our method can detect each type of steganography with the same level of accuracy as that of a steganalysis method dedicated to that type of steganography, thereby demonstrating the general applicability of our approach to multiple types of stego images.steganalysis, steganography, deep learning, generalized steganalysis model"
49,"딥러닝 프레임워크의 비교: 티아노, 텐서플로, CNTK를 중심으로",정여진 /Yeojin Chung,2017,한국지능정보시스템학회,"딥러닝 프레임워크의 대표적인 기능으로는 ‘자동미분’과 ‘GPU의 활용’ 등을 들 수 있다. 본 논문은 파이썬의 라이브러리 형태로 사용 가능한 프레임워크 중에서 구글의 텐서플로와 마이크로소프트의 CNTK, 그리고 텐서 플로의 원조라고 할 수 있는 티아노를 비교하였다. 본문에서는 자동미분의 개념과 GPU의 활용형태를 간단히 설명하고, 그 다음에 logistic regression을 실행하는 예를 통하여 각 프레임워크의 문법을 알아본 뒤에, 마지막으로 대표적인 딥러닝 응용인 CNN의 예제를 실행시켜보고 코딩의 편의성과 실행속도 등을 확인해 보았다. 그 결과, 편의성의 관점에서 보면 티아노가 가장 코딩 하기가 어렵고, CNTK와 텐서플로는 많은 부분이 비슷하게 추상화 되어 있어서 코딩이 비슷하지만 가중치와 편향을 직접 정의하느냐의 여부에서 차이를 보였다. 그리고 각프레임워크의 실행속도에 대한 평가는 ‘큰 차이는 없다’는 것이다. 텐서플로는 티아노에 비하여 속도가 느리다는 평가가 있어왔는데, 본 연구의 실험에 의하면, 비록 CNN 모형에 국한되었지만, 텐서플로가 아주 조금이지만빠른 것으로 나타났다. CNTK의 경우에도, 비록 실험환경이 달랐지만, 실험환경의 차이에 의한 속도의 차이의 편차범위 이내에 있는 것으로 판단이 되었다. 본 연구에서는 세 종류의 딥러닝 프레임워크만을 살펴보았는데,위키피디아에 따르면 딥러닝 프레임워크의 종류는 12가지가 있으며, 각 프레임워크의 특징을 15가지 속성으로구분하여 차이를 특정하고 있다. 그 많은 속성 중에서 사용자의 입장에서 볼 때 중요한 속성은 어떤 언어(파이썬, C++, Java, 등)로 사용가능한지, 어떤 딥러닝 모형에 대한 라이브러리가 잘 구현되어 있는지 등일 것이다. 그리고 사용자가 대규모의 딥러닝 모형을 구축한다면, 다중 GPU 혹은 다중 서버를 지원하는지의 여부도 중요할 것이다. 또한 딥러닝 모형을 처음 학습하는 경우에는 사용설명서가 많은지 예제 프로그램이 많은지 여부도 중요한 기준이 될 것이다.The deep learning framework is software designed to help develop deep learning models. Some of its important functions include “automatic differentiation” and “utilization of GPU”. The list of popular deep learning framework includes Caffe (BVLC) and Theano (University of Montreal). And recently, Microsoft's deep learning framework, Microsoft Cognitive Toolkit, was released as open-source license, following Google’s Tensorflow a year earlier. The early deep learning frameworks have been developed mainly for research at universities. Beginning with the inception of Tensorflow, however, it seems that companies such as Microsoft and Facebook have started to join the competition of framework development. Given the trend, Google and other companies are expected to continue investing in the deep learning framework to bring forward the initiative in the artificial intelligence business. From this point of view, we think it is a good time to compare some of deep learning frameworks. So we compare three deep learning frameworks which can be used as a Python library. Those are Google's Tensorflow, Microsoft’s CNTK, and Theano which is sort of a predecessor of the preceding two.
The most common and important function of deep learning frameworks is the ability to perform automatic differentiation. Basically all the mathematical expressions of deep learning models can be represented as computational graphs, which consist of nodes and edges. Partial derivatives on each edge of a computational graph can then be obtained. With the partial derivatives, we can let software compute differentiation of any node with respect to any variable by utilizing chain rule of Calculus.
First of all, the convenience of coding is in the order of CNTK, Tensorflow, and Theano. The criterion is simply based on the lengths of the codes and the learning curve and the ease of coding are not the main concern. According to the criteria, Theano was the most difficult to implement with, and CNTK and Tensorflow were somewhat easier. With Tensorflow, we need to define weight variables and biases explicitly. The reason that CNTK and Tensorflow are easier to implement with is that those frameworks provide us with more abstraction than Theano. We, however, need to mention that low-level coding is not always bad. It gives us flexibility of coding. With the low-level coding such as in Theano, we can implement and test any new deep learning models or any new search methods that we can think of.
The assessment of the execution speed of each framework is that there is not meaningful difference. According to the experiment, execution speeds of Theano and Tensorflow are very similar, although the experiment was limited to a CNN model. In the case of CNTK, the experimental environment was not maintained as the same. The code written in CNTK has to be run in PC environment without GPU where codes execute as much as 50 times slower than with GPU. But we concluded that the difference of execution speed was within the range of variation caused by the different hardware setup.
In this study, we compared three types of deep learning framework: Theano, Tensorflow, and CNTK. According to Wikipedia, there are 12 available deep learning frameworks. And 15 different attributes differentiate each framework. Some of the important attributes would include interface language (Python, C ++, Java, etc.) and the availability of libraries on various deep learning models such as CNN, RNN, DBN, and etc. And if a user implements a large scale deep learning model, it will also be important to support multiple GPU or multiple servers. Also, if you are learning the deep learning model, it would also be important if there are enough examples and references.deep learning framework, Theano, TensorFlow, CNTK, computational graph, CIFAR-10"
50,딥러닝을 이용한 주변 무선단말 파악방안,이웅섭 /Lee， Woong-sup,2018,한국정보통신학회,"최근 단말-대-단말(Device-to-device, D2D) 통신기술이 차세대 무선통신시스템의 핵심기술로 큰 관심을 받고 있다. 이러한 단말간 통신에서는 자신의 주변에 어떠한 단말이 있는지 파악하는 주변단말 탐색(Neighbor discovery)이매우 중요하다. 본 논문에서는 최근 큰 관심을 받고 있는 딥러닝(Deep learning) 기술을 활용하여 단말간 통신에서 주변단말을 파악하는 방안에 대해서 제안한다. 제안 방안은 기존의 방안과 달리 무선채널의 공간적 연관성을 이용하여단말간의 신호 전송 없이 단말이 기지국으로 전송하는 상향링크 파일럿 신호를 기반으로 주변 단말을 찾고 따라서기존의 방식에 비해 신호전송 복잡도(signaling complexity)를 크게 줄일 수 있다. 또한 제안 방안에서는 떨어져 있는거리에 따라서 주변 단말을 분류 가능하여 기존 방안에 비해서 좀 더 세밀한 단말 탐색이 가능하다. 마지막으로 본 논문에서는 tensorflow를 이용한 컴퓨터 시뮬레이션을 통해 제안 방안의 성능을 검증하였다.Recently, the device-to-device (D2D) communication has been conceived as the key technology for the next-generationmobile communication systems. The neighbor discovery in which the nearby users are found, is essential for the properoperation of the D2D communication. In this paper, we propose new neighbor discovery scheme based on deep learningtechnology which has gained a lot of attention recently. In the proposed scheme, the neighboring users can be foundusing the uplink pilot transmission of users only, unlike conventional neighbor discovery schemes in which direct pilotcommunication among users is required, such that the signaling overhead can be greatly reduced in our proposed scheme.
Moreover, the neighbors with different proximity can also be classified accordingly which enables more accurate neighbordiscovery compared to the conventional schemes. The performance of our proposed scheme is verified through thetensorflow-based computer simulations.
키워드 : 딥러닝, 주변단말 탐색, 단말-대-단말 통신Deep learning, Neighbor discovery, Device-to-device communication, Wireless channel correlation"
51,신뢰성있는 딥러닝 기반 분석 모델을 참조하기 위한 딥러닝 기술 언어,문종혁 /Jong Hyeok Mun,2021,한국정보처리학회,"최근 딥러닝은 하드웨어 성능이 향상됨에 따라 자연어 처리, 영상 인식 등의 다양한 기술에 접목되어 활용되고 있다. 이러한 기술들을 활용해 지능형교통 시스템(ITS), 스마트홈, 헬스케어 등의 산업분야에서 데이터를 분석하여 고속도로 속도위반 차량 검출, 에너지 사용량 제어, 응급상황 등과 같은고품질의 서비스를 제공하며, 고품질의 서비스를 제공하기 위해서는 정확도가 향상된 딥러닝 모델이 적용되어야 한다. 이를 위해 서비스 환경의 데이터를분석하기 위한 딥러닝 모델을 개발할 때, 개발자는 신뢰성이 검증된 최신의 딥러닝 모델을 적용할 수 있어야 한다. 이는 개발자가 참조하는 딥러닝 모델에적용된 학습 데이터셋의 정확도를 측정하여 검증할 수 있다. 이러한 검증을 위해서 개발자는 학습 데이터셋, 딥러닝의 계층구조 및 개발 환경 등과 같은내용을 포함하는 딥러닝 모델을 문서화하여 적용하기 위한 구조적인 정보가 필요하다. 본 논문에서는 신뢰성있는 딥러닝 기반 데이터 분석 모델을 참조하기위한 딥러닝 기술 언어를 제안한다. 제안하는 기술 언어는 신뢰성 있는 딥러닝 모델을 개발하는데 필요한 학습데이터셋, 개발 환경 및 설정 등의 정보와더불어 딥러닝 모델의 계층구조를 표현할 수 있다. 제안하는 딥러닝 기술 언어를 이용하여 개발자는 지능형 교통 시스템에서 참조하는 분석 모델의정확도를 검증할 수 있다. 실험에서는 제안하는 언어의 유효성을 검증하기 위해, 번호판 인식 모델을 중심으로 딥러닝 기술 문서의 적용과정을 보인다With the recent advancements of deep learning, companies such as smart home, healthcare, and intelligent transportation systems areutilizing its functionality to provide high-quality services for vehicle detection, emergency situation detection, and controlling energyconsumption. To provide reliable services in such sensitive systems, deep learning models are required to have high accuracy. In orderto develop a deep learning model for analyzing previously mentioned services, developers should utilize the state of the art deep learningmodels that have already been verified for higher accuracy. The developers can verify the accuracy of the referenced model by validatingthe model on the dataset. For this validation, the developer needs structural information to document and apply deep learning models,including metadata such as learning dataset, network architecture, and development environments. In this paper, we propose a descriptionlanguage that represents the network architecture of the deep learning model along with its metadata that are necessary to develop adeep learning model. Through the proposed description language, developers can easily verify the accuracy of the referenced deep learningmodel. Our experiments demonstrate the application scenario of a deep learning description document that focuses on the license platerecognition for the detection of illegally parked vehicles.Trusted Deep Learning, Model Reference, Deep Learning Description Language, Traffic Situation Analysis Model"
52,딥러닝 분석을 이용한 중국 역내·외 위안화 변동성 예측,이우식 /Lee Woosik,2016,한국데이터정보과학회,"2008년 글로벌 금융위기 이후 중국은 위안화 국제화의 점진적 추진을 진행하면서 중국상하이 외환시장과 중국홍콩 외환시장에서 거래되는 통화인 역내위안화와 역외위안화를 형성시켰다. 본 연구는 위안화 국제화와 점진적인 중국 자본계정 개방에 따라 급변하는 외환시장상황의 변동성을 정확하게 파악하기 위해서 GARCH모형 (일반화된 자기회귀 조건부이분산성모형)에 다단계인공신경망을 결합한 MLP-GARCH 모형과 GARCH모형과 기계학습의 일종인 딥러닝 (deep learning)을 통합한 DL-GARCH을 가지고 위안화 변동성예측을 비교 실험과 분석을 하였다. 비교분석 결과 DLGARCH 모형은 MLP-GARCH보다 모형 위안화 역내·외 환율변동성 예측 면에서 더욱 더 개선된예측값을 제공하였다. 그래서 이분산시계열모형을 딥러닝과 결합한 DL-GARCH 모형은 시계열의환율 변동성 예측 문제에 딥러닝을 응용할 수 있음을 확인하였다. 향후 이분산시계열과 결합된 딥러닝 모형은 다른 금융시계열 데이터에 응용하여 그 일반화 가능성을 높일 수 있을 것이다.The People’s Republic of China has vigorously been pursuing the internationalization of the Chinese Yuan or Renminbi after the financial crisis of 2008. In this view, an abrupt increase of use of the Chinese Yuan in the onshore and offshore markets are important milestones to be one of important currencies. One of the most frequently used methods to forecast volatility is GARCH model. Since a prediction error of the GARCH model has been reported quite high, a lot of efforts have been made to improve forecasting capability of the GARCH model. In this paper, we have proposed MLPGARCH and a DL-GARCH by employing Artificial Neural Network to the GARCH. In an application to forecasting Chinese Yuan volatility, we have successfully shown their overall outperformance in forecasting over the GARCH.Deep learning, DL-GARCH, GARCH, The Chinese Yuan, volatility."
53,딥러닝 기법을 이용한 내일강수 예측,하지훈,2016,한국지능시스템학회,"정확한 강수예측을 위해서는 예측인자 선정과 예측방법에 대한 선택이 매우 중요하다. 최근에는 강수예측 방법으로기계학습 기법이 많이 사용되고 있으며, 그 중에서도 특히 인공신경망을 사용한 강수예측 방법은 좋은 성능을 보였다. 본논문에서는 딥러닝 기법 중 하나인 DBN(deep belief network)를 이용한 새로운 강수예측 방법을 제안한다. DBN는 비지도사전 학습을 통해 초기 가중치를 설정하여 기존 인공신경망의 문제점을 보완한다. 예측인자로는 기온, 전일-전주 강수일, 태양과 달 궤도 관련 자료를 선정하였다. 기온과 전일-전주 강수일은 서울에서의 1974년부터 2013년까지 총 40년간의AWS(automatic weather system) 관측 자료를 사용하였고, 태양과 달의 궤도 관련 자료는 서울을 중심으로 계산한 결과를사용하였다. 전체 기간에서 일부는 학습 자료로 사용하여 예측모델을 생성하였고, 나머지를 생성한 모델의 검증 자료로사용하였다. 모델 검증 결과로 나온 예측값들은 확률값을 가지며 임계치를 이용하여 강수유무를 판별하였다. 강수 정확도의척도로 양분예보기법 중 CSI(critical successive index)와 Bias(frequency bias)를 계산하였다. 이를 통해 DBN와 MLP(multilayer perceptron)의 성능을 비교한 결과 DBN의 강수 예측 정확도가 높았고, 수행속도 또한 2배 이상 빨랐다.For accurate precipitation forecasts the choice of weather faoctrs and prediction method is very important. Recently, machinele arning has been widely used for forecasting precipitation, and artificial neural network, one of machine learning techniques, showed good performance. In this paper, we suggest a new method for forecasting precipitation using DBN, one of deep learning techniques.
DBN has an advantage that initial weights are set by unsupervised learning, so this compensates for the defects of artificial neural networks. We used past precipitation, temperature, and the parameters of the sun and moon's motion as features for forecasting precipitation. The dataset consists of observation data which had been measured for 40 years from AWS in Seoul. Experiments were based on 8-fold cross validation. As a result of estimation, we got probabilities of test dataset, so threshold was used for the decision of precipitation. CSI and Bias were used for indicating the precision of precipitation. Our experimental results showed that DBN performed better than MLP.Deep learning, Deep belief network, Precipitation, Forecast"
54,딥러닝 신경망모형을 이용한 실시간 교통정보수집 연구,김대현 /KIM DAE HYON,2016,사단법인 인문사회과학기술융합학회,"비디오 기반의 실시간 교통정보수집 시스템은 자율주행을 포함한 지능형교통체계의 주요 서브시스템중 하나이며, 우수한 예측(추정)력 때문에 핵심 알고리즘으로 인공신경망 모형이 주로 사용되고 있다. 특히, 최근 과학 학술지 네이처에 구글 딥마인드의 알파고가 공개되면서 딥러닝과 인공지능에 대한 전 세계적인 관심이 집중되고 있다. 최근까지 가장 널리 사용된 인공신경망모형은 역전파 모형이었으나 최근 컴퓨터 하드웨어 및 알고리즘의 새로운 개발과 더불어 딥러닝 알고리즘을 이용한 딥인공신경망 모형에 대한 관심이 높아지고 있다. 그러나 아직 딥러닝 인공신경망에 대한 연구는 초기단계에 있는 실정이며 적용분야도 극히 한정되어 있다. 본 연구에서는 실시간 도로교통정보 수집을 위한 영상처리분야에서 딥러닝 알고리즘을 활용하고 실험결과를 통하여 향후 교통정보수집‧분석 분야에서의 딥러닝 인공신경망 모형의 적용가능성을 살펴보고자 한다. 특히, 본 연구에서 도출한 딥러닝 인공신경망모형의 실험결과를 기존 역전파 모형에 의한 실험결과와 비교하여 모형의 우수성을 비교‧평가하고자 한다.Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Normally, artificial neural networks have been used in real-time applications for collecting traffic information because of their remarkable performance in terms of predictive accuracy and real-time computing. Especially, the recent publication of a paper in the journal Nature and the success of Google DeepMind’s world-beating Go program AlphaGo caused significant increases of the public interest in artificial intelligence. Even though various types of learning models have been proposed, the Backpropagation has been the most popular neural network model. However, the Deep learning neural network model has attracted much interest recently. In this study, the Deep leaning neural network will be applied in image processing techniques for the real-time traffic information collection to investigate the performance of Deep learning algorithm in traffic engineering fields. In addition, the experimental results from Deep learing algorithm will be compared with the results from Backpropagation model to evaluate and propose better performing model.Intelligent Transportation Systems (ITS), artificial neural networks, Deep learning, traffic information, image processing"
55,딥러닝의 모형과 응용사례,안성만 /SungMahn Ahn,2016,한국지능정보시스템학회,"딥러닝은 인공신경망(neural network)이라는 인공지능분야의 모형이 발전된 형태로서, 계층구조로 이루어진 인공신경망의 내부계층(hidden layer)이 여러 단계로 이루어진 구조이다. 딥러닝에서의 주요 모형은 합성곱신경망(convolutional neural network), 순환신경망(recurrent neural network), 그리고 심층신뢰신경망(deep belief network)의 세가지라고 할 수있다. 그 중에서 현재 흥미로운 연구가 많이 발표되어서 관심이 집중되고 있는 모형은 지도학습(supervised learning)모형인 처음 두 개의 모형이다. 따라서 본 논문에서는 지도학습모형의 가중치를 최적화하는 기본적인 방법인 오류역전파 알고리즘을 살펴본 뒤에 합성곱신경망과 순환신경망의 구조와 응용사례 등을 살펴보고자 한다. 본문에서 다루지 않은 모형인 심층신뢰신경망은 아직까지는 합성곱신경망 이나 순환신경망보다는 상대적으로 주목을 덜 받고 있다. 그러나 심층신뢰신경망은 CNN이나 RNN과는 달리 비지도학습(unsupervised learning)모형이며, 사람이나 동물은 관찰을 통해서 스스로 학습한다는 점에서 궁극적으로는 비지도학습모형이 더 많이 연구되어야 할 주제가 될 것이다.Deep learning model is a kind of neural networks that allows multiple hidden layers. There are various deep learning architectures such as convolutional neural networks, deep belief networks and recurrent neural networks. Those have been applied to fields like computer vision, automatic speech recognition, natural language processing, audio recognition and bioinformatics where they have been shown to produce state-of-the-art results on various tasks. Among those architectures, convolutional neural networks and recurrent neural networks are classified as the supervised learning model. And in recent years, those supervised learning models have gained more popularity than unsupervised learning models such as deep belief networks, because supervised learning models have shown fashionable applications in such fields mentioned above.
Deep learning models can be trained with backpropagation algorithm. Backpropagation is an abbreviation for “backward propagation of errors” and a common method of training artificial neural networks used in conjunction with an optimization method such as gradient descent. The method calculates the gradient of an error function with respect to all the weights in the network. The gradient is fed to the optimization method which in turn uses it to update the weights, in an attempt to minimize the error function.
Convolutional neural networks use a special architecture which is particularly well-adapted to classify images. Using this architecture makes convolutional networks fast to train. This, in turn, helps us train deep, muti-layer networks, which are very good at classifying images. These days, deep convolutional networks are used in most neural networks for image recognition. Convolutional neural networks use three basic ideas: local receptive fields, shared weights, and pooling. By local receptive fields, we mean that each neuron in the first(or any) hidden layer will be connected to a small region of the input(or previous layer’s) neurons. Shared weights mean that we’re going to use the same weights and bias for each of the local receptive field. This means that all the neurons in the hidden layer detect exactly the same feature, just at different locations in the input image. In addition to the convolutional layers just described, convolutional neural networks also contain pooling layers. Pooling layers are usually used immediately after convolutional layers. What the pooling layers do is to simplify the information in the output from the convolutional layer.
Recent convolutional network architectures have 10 to 20 hidden layers and billions of connections between units. Training deep learning networks has taken weeks several years ago, but thanks to progress in GPU and algorithm enhancement, training time has reduced to several hours.
Neural networks with time-varying behavior are known as recurrent neural networks or RNNs. A recurrent neural network is a class of artificial neural network where connections between units form a directed cycle. This creates an internal state of the network which allows it to exhibit dynamic temporal behavior. Unlike feedforward neural networks, RNNs can use their internal memory to process arbitrary sequences of inputs. Early RNN models turned out to be very difficult to train, harder even than deep feedforward networks. The reason is the unstable gradient problem such as vanishing gradient and exploding gradient. The gradient can get smaller and smaller as it is propagated back through layers. This makes learning in early layers extremely slow. The problem actually gets worse in RNNs, since gradients aren’t just propagated backward through layers, they’re propagated backward through time. If the network runs for a long time, that can make the gradient extremely unstable and hard to learn from. It has been possible to incorporate an idea known as long short-term memory units (LSTMs) into RNNs. LSTMs make it much easier to get good results when training RNNs, and many recent papers make use of LSTMs or related ideas.Deep learning, Convolutional neural networks, Recurrent neural networks, Error backpropagation algorithm"
56,딥러닝분석과 기술적 분석 지표를 이용한 한국 코스피주가지수 방향성 예측,이우식 /Lee Woosik,2017,한국데이터정보과학회,"2016년 3월 구글 (Google)의 바둑인공지능 알파고 (AlphaGo)가 이세돌 9단과의 바둑대결에서승리한 이후 다양한 분야에서 인공지능 사용에 대한 관심이 높아지고 있는 가운데 금융투자 분야에서도 인공지능과 투자자문 전문가의 합성어인 로보어드바이저 (Robo-Advisor)에 대한 관심이 높아지고 있다. 인공지능 (artificial intelligence)기반의 의사결정은 비용 절감은 물론 효과적인 의사결정을가능하게 한다는 점에서 큰 장점이 있다. 본 연구에서는 기술적 분석 (technical analysis) 지표와 딥러닝 (deep learning) 모형을 결합하여 한국 코스피 지수를 예측하는 모형을 개발하고 제시한 모형들의 예측력을 비교, 분석한다. 분석 결과 기술적 분석 지표에 딥러닝 알고리즘을 결합한 모형이 주가지수 방향성 예측 문제에 응용될 수 있음을 확인하였다. 향후 본 연구에서 제안된 기술적 분석 지표와딥러닝모형을 결합한 기법은 로보어드바이저서비스에 응용할 수 있는 일반화 가능성을 보여준다Since Google’s AlphaGo defeated a world champion of Go players in 2016, there have been many interests in the deep learning. In the financial sector, a Robo-Advisor using deep learning gains a significant attention, which builds and manages portfolios of financial instruments for investors.In this paper, we have proposed the a deep learning algorithm geared toward identification and forecast of the KOSPI index direction,and we also have compared the accuracy of the prediction.In an application of forecasting the financial market index direction, we have shown that the Robo-Advisor using deep learning has a significant effect on finance industry. The Robo-Advisor collects a massive data such as earnings statements, news reports and regulatory filings, analyzes those and recommends investors how to view market trends and identify the best time to purchase financial assets. On the other hand, the Robo-Advisor allows businesses to learn more about their customers, develop better marketing strategies, increase sales and decrease costs.Artificial intelligence, deep learning, FinTech, Robo-Advisor, technical analysis."
57,딥러닝의 효율적인 학습을 위한 학습데이터 선별 기법에 관한 연구,주경돈 /Gyoung-don Joo,2017,한국정보과학회,"빅데이터의 증가와 병렬 하드웨어 처리, 학습 알고리즘의 개선은 딥러닝의 발전을 가져왔으며 인공지능이 적용 가능한 영역을 획기적으로 확장시키고 있다. 하지만 이러한 대규모의 학습 데이터에 기반한 딥러닝은 학습시간이 증가되고, 더 나아가 학습 데이터의 구축을 위한 레이블링(labeling) 비용이 데이터 수에 비례하여 증가하게 된다. 따라서 본 논문에서는, 딥러닝의 정확도를 최대한 유지하면서 필요한 학습 데이터의 수를 줄일수 있도록, 전체 데이터에서 제한된 수의 학습 데이터의 후보를 선별하는 기법에 대하여 연구한다. 특히 기존의 능동 학습에 기반한 기법들이 학습 데이터의 수가 매우 적어질 경우 딥러닝의 정확도가 급격히 떨어지는 문제를 해결할 수 있는 새로운 기법을 제안하였으며, 제안된 기법으로 선별된 학습 데이터는 그 수의 변화에 대해 딥러닝이 일관되게 경쟁력있는 정확도를 가짐을 다양한 실험을 통하여 검증하였다.The increase of big data, parallel hardware processing and enhanced learning algorithms have led to the development of deep learning and are dramatically expanding the domain of applications based on artificial intelligence. However, such large-scale training data extends the learning time, and furthermore, the labeling cost for building the training set increases in proportion to the volume of data. Therefore, in this paper, we study how to select a limited number of candidates for training set from given data so as to reduce the number of training set while maintaining the accuracy of deep learning. Especially, we found that, with the training set selected by the previous work, the accuracy of deep learning drastically decreases when the number of training set is very small and so proposed new methods to resolve this fall-off in accuracy. Through experiments, we showed that deep learning consistently has the competitive accuracy with the training set selected by the proposed method regardless of the size of training set.Deep Learning, Train Set Selection, Active Learning, Gamma distribution"
58,딥러닝을 이용한 범용적 스테그아날리시스,김현재 /Hyunjae Kim,2017,한국정보과학회,"스테그아날리시스(Steganalysis)란 이미지 등 일반적인 자료에 암호화된 정보를 은닉하는 스테가노그래피(Steganography)에 대한 검출 및 분석 방법으로, 기계학습 기반 방법론을 포함한다. 기존 기계학습 기반 스테그아날리시스는 영상(Image)의 특징(Feature) 추출 및 모델링에 기반하며, 최근 딥러닝(Deep Learning)의 적용으로 검출 정확도가 큰 폭으로 향상되었다. 하지만 현존하는 스테그아날리시스 모델은 단일 스테가노그래피 기법에 대해 국한되어 있어 학습에 사용되지 않은 스테고(Stego) 이미지의 경우 검출이 불가능한 결정적 한계를 가진다. 본 연구에서는 다양한 스테가노그래피 기법으로 생성된 스테고이미지에 딥러닝을 적용하여 스테그아날리시스를 학습하는 범용적 모델을 제안한다. 다양한 실험을 통해 제안 기법의 효용성 및 가능성을 확인하고, 범용적 스테그아날리시스 모델이 각각에 특화된 검출 기법과 유사한 정확도로 스테고 이미지를 검출할 수 있음을 보인다.Steganalysis is to detect information hidden by steganography inside general data such as images. There are stegoanalysis techniques that use machine learning (ML). Existing ML approaches to steganalysis are based on extracting features from stego images and modeling them.
Recently deep learning-based methodologies have shown significant improvements in detection accuracy. However, all the existing methods, including deep learning-based ones, have a critical limitation in that they can only detect stego images that are created by a specific steganography method. In this paper, we propose a generalized steganalysis method that can model multiple types of stego images using deep learning. Through various experiments, we confirm the effectiveness of our approach and envision directions for future research. In particular, we show that our method can detect each type of steganography with the same level of accuracy as that of a steganalysis method dedicated to that type of steganography, thereby demonstrating the general applicability of our approach to multiple types of stego images.steganalysis, steganography, deep learning, generalized steganalysis model"
59,"딥러닝 프레임워크의 비교: 티아노, 텐서플로, CNTK를 중심으로",정여진 /Yeojin Chung,2017,한국지능정보시스템학회,"딥러닝 프레임워크의 대표적인 기능으로는 ‘자동미분’과 ‘GPU의 활용’ 등을 들 수 있다. 본 논문은 파이썬의 라이브러리 형태로 사용 가능한 프레임워크 중에서 구글의 텐서플로와 마이크로소프트의 CNTK, 그리고 텐서 플로의 원조라고 할 수 있는 티아노를 비교하였다. 본문에서는 자동미분의 개념과 GPU의 활용형태를 간단히 설명하고, 그 다음에 logistic regression을 실행하는 예를 통하여 각 프레임워크의 문법을 알아본 뒤에, 마지막으로 대표적인 딥러닝 응용인 CNN의 예제를 실행시켜보고 코딩의 편의성과 실행속도 등을 확인해 보았다. 그 결과, 편의성의 관점에서 보면 티아노가 가장 코딩 하기가 어렵고, CNTK와 텐서플로는 많은 부분이 비슷하게 추상화 되어 있어서 코딩이 비슷하지만 가중치와 편향을 직접 정의하느냐의 여부에서 차이를 보였다. 그리고 각프레임워크의 실행속도에 대한 평가는 ‘큰 차이는 없다’는 것이다. 텐서플로는 티아노에 비하여 속도가 느리다는 평가가 있어왔는데, 본 연구의 실험에 의하면, 비록 CNN 모형에 국한되었지만, 텐서플로가 아주 조금이지만빠른 것으로 나타났다. CNTK의 경우에도, 비록 실험환경이 달랐지만, 실험환경의 차이에 의한 속도의 차이의 편차범위 이내에 있는 것으로 판단이 되었다. 본 연구에서는 세 종류의 딥러닝 프레임워크만을 살펴보았는데,위키피디아에 따르면 딥러닝 프레임워크의 종류는 12가지가 있으며, 각 프레임워크의 특징을 15가지 속성으로구분하여 차이를 특정하고 있다. 그 많은 속성 중에서 사용자의 입장에서 볼 때 중요한 속성은 어떤 언어(파이썬, C++, Java, 등)로 사용가능한지, 어떤 딥러닝 모형에 대한 라이브러리가 잘 구현되어 있는지 등일 것이다. 그리고 사용자가 대규모의 딥러닝 모형을 구축한다면, 다중 GPU 혹은 다중 서버를 지원하는지의 여부도 중요할 것이다. 또한 딥러닝 모형을 처음 학습하는 경우에는 사용설명서가 많은지 예제 프로그램이 많은지 여부도 중요한 기준이 될 것이다.The deep learning framework is software designed to help develop deep learning models. Some of its important functions include “automatic differentiation” and “utilization of GPU”. The list of popular deep learning framework includes Caffe (BVLC) and Theano (University of Montreal). And recently, Microsoft's deep learning framework, Microsoft Cognitive Toolkit, was released as open-source license, following Google’s Tensorflow a year earlier. The early deep learning frameworks have been developed mainly for research at universities. Beginning with the inception of Tensorflow, however, it seems that companies such as Microsoft and Facebook have started to join the competition of framework development. Given the trend, Google and other companies are expected to continue investing in the deep learning framework to bring forward the initiative in the artificial intelligence business. From this point of view, we think it is a good time to compare some of deep learning frameworks. So we compare three deep learning frameworks which can be used as a Python library. Those are Google's Tensorflow, Microsoft’s CNTK, and Theano which is sort of a predecessor of the preceding two.
The most common and important function of deep learning frameworks is the ability to perform automatic differentiation. Basically all the mathematical expressions of deep learning models can be represented as computational graphs, which consist of nodes and edges. Partial derivatives on each edge of a computational graph can then be obtained. With the partial derivatives, we can let software compute differentiation of any node with respect to any variable by utilizing chain rule of Calculus.
First of all, the convenience of coding is in the order of CNTK, Tensorflow, and Theano. The criterion is simply based on the lengths of the codes and the learning curve and the ease of coding are not the main concern. According to the criteria, Theano was the most difficult to implement with, and CNTK and Tensorflow were somewhat easier. With Tensorflow, we need to define weight variables and biases explicitly. The reason that CNTK and Tensorflow are easier to implement with is that those frameworks provide us with more abstraction than Theano. We, however, need to mention that low-level coding is not always bad. It gives us flexibility of coding. With the low-level coding such as in Theano, we can implement and test any new deep learning models or any new search methods that we can think of.
The assessment of the execution speed of each framework is that there is not meaningful difference. According to the experiment, execution speeds of Theano and Tensorflow are very similar, although the experiment was limited to a CNN model. In the case of CNTK, the experimental environment was not maintained as the same. The code written in CNTK has to be run in PC environment without GPU where codes execute as much as 50 times slower than with GPU. But we concluded that the difference of execution speed was within the range of variation caused by the different hardware setup.
In this study, we compared three types of deep learning framework: Theano, Tensorflow, and CNTK. According to Wikipedia, there are 12 available deep learning frameworks. And 15 different attributes differentiate each framework. Some of the important attributes would include interface language (Python, C ++, Java, etc.) and the availability of libraries on various deep learning models such as CNN, RNN, DBN, and etc. And if a user implements a large scale deep learning model, it will also be important to support multiple GPU or multiple servers. Also, if you are learning the deep learning model, it would also be important if there are enough examples and references.deep learning framework, Theano, TensorFlow, CNTK, computational graph, CIFAR-10"
60,딥러닝을 이용한 주변 무선단말 파악방안,이웅섭 /Lee， Woong-sup,2018,한국정보통신학회,"최근 단말-대-단말(Device-to-device, D2D) 통신기술이 차세대 무선통신시스템의 핵심기술로 큰 관심을 받고 있다. 이러한 단말간 통신에서는 자신의 주변에 어떠한 단말이 있는지 파악하는 주변단말 탐색(Neighbor discovery)이매우 중요하다. 본 논문에서는 최근 큰 관심을 받고 있는 딥러닝(Deep learning) 기술을 활용하여 단말간 통신에서 주변단말을 파악하는 방안에 대해서 제안한다. 제안 방안은 기존의 방안과 달리 무선채널의 공간적 연관성을 이용하여단말간의 신호 전송 없이 단말이 기지국으로 전송하는 상향링크 파일럿 신호를 기반으로 주변 단말을 찾고 따라서기존의 방식에 비해 신호전송 복잡도(signaling complexity)를 크게 줄일 수 있다. 또한 제안 방안에서는 떨어져 있는거리에 따라서 주변 단말을 분류 가능하여 기존 방안에 비해서 좀 더 세밀한 단말 탐색이 가능하다. 마지막으로 본 논문에서는 tensorflow를 이용한 컴퓨터 시뮬레이션을 통해 제안 방안의 성능을 검증하였다.Recently, the device-to-device (D2D) communication has been conceived as the key technology for the next-generationmobile communication systems. The neighbor discovery in which the nearby users are found, is essential for the properoperation of the D2D communication. In this paper, we propose new neighbor discovery scheme based on deep learningtechnology which has gained a lot of attention recently. In the proposed scheme, the neighboring users can be foundusing the uplink pilot transmission of users only, unlike conventional neighbor discovery schemes in which direct pilotcommunication among users is required, such that the signaling overhead can be greatly reduced in our proposed scheme.
Moreover, the neighbors with different proximity can also be classified accordingly which enables more accurate neighbordiscovery compared to the conventional schemes. The performance of our proposed scheme is verified through thetensorflow-based computer simulations.
키워드 : 딥러닝, 주변단말 탐색, 단말-대-단말 통신Deep learning, Neighbor discovery, Device-to-device communication, Wireless channel correlation"
61,신뢰성있는 딥러닝 기반 분석 모델을 참조하기 위한 딥러닝 기술 언어,문종혁 /Jong Hyeok Mun,2021,한국정보처리학회,"최근 딥러닝은 하드웨어 성능이 향상됨에 따라 자연어 처리, 영상 인식 등의 다양한 기술에 접목되어 활용되고 있다. 이러한 기술들을 활용해 지능형교통 시스템(ITS), 스마트홈, 헬스케어 등의 산업분야에서 데이터를 분석하여 고속도로 속도위반 차량 검출, 에너지 사용량 제어, 응급상황 등과 같은고품질의 서비스를 제공하며, 고품질의 서비스를 제공하기 위해서는 정확도가 향상된 딥러닝 모델이 적용되어야 한다. 이를 위해 서비스 환경의 데이터를분석하기 위한 딥러닝 모델을 개발할 때, 개발자는 신뢰성이 검증된 최신의 딥러닝 모델을 적용할 수 있어야 한다. 이는 개발자가 참조하는 딥러닝 모델에적용된 학습 데이터셋의 정확도를 측정하여 검증할 수 있다. 이러한 검증을 위해서 개발자는 학습 데이터셋, 딥러닝의 계층구조 및 개발 환경 등과 같은내용을 포함하는 딥러닝 모델을 문서화하여 적용하기 위한 구조적인 정보가 필요하다. 본 논문에서는 신뢰성있는 딥러닝 기반 데이터 분석 모델을 참조하기위한 딥러닝 기술 언어를 제안한다. 제안하는 기술 언어는 신뢰성 있는 딥러닝 모델을 개발하는데 필요한 학습데이터셋, 개발 환경 및 설정 등의 정보와더불어 딥러닝 모델의 계층구조를 표현할 수 있다. 제안하는 딥러닝 기술 언어를 이용하여 개발자는 지능형 교통 시스템에서 참조하는 분석 모델의정확도를 검증할 수 있다. 실험에서는 제안하는 언어의 유효성을 검증하기 위해, 번호판 인식 모델을 중심으로 딥러닝 기술 문서의 적용과정을 보인다With the recent advancements of deep learning, companies such as smart home, healthcare, and intelligent transportation systems areutilizing its functionality to provide high-quality services for vehicle detection, emergency situation detection, and controlling energyconsumption. To provide reliable services in such sensitive systems, deep learning models are required to have high accuracy. In orderto develop a deep learning model for analyzing previously mentioned services, developers should utilize the state of the art deep learningmodels that have already been verified for higher accuracy. The developers can verify the accuracy of the referenced model by validatingthe model on the dataset. For this validation, the developer needs structural information to document and apply deep learning models,including metadata such as learning dataset, network architecture, and development environments. In this paper, we propose a descriptionlanguage that represents the network architecture of the deep learning model along with its metadata that are necessary to develop adeep learning model. Through the proposed description language, developers can easily verify the accuracy of the referenced deep learningmodel. Our experiments demonstrate the application scenario of a deep learning description document that focuses on the license platerecognition for the detection of illegally parked vehicles.Trusted Deep Learning, Model Reference, Deep Learning Description Language, Traffic Situation Analysis Model"
62,딥러닝 분석을 이용한 중국 역내·외 위안화 변동성 예측,이우식 /Lee Woosik,2016,한국데이터정보과학회,"2008년 글로벌 금융위기 이후 중국은 위안화 국제화의 점진적 추진을 진행하면서 중국상하이 외환시장과 중국홍콩 외환시장에서 거래되는 통화인 역내위안화와 역외위안화를 형성시켰다. 본 연구는 위안화 국제화와 점진적인 중국 자본계정 개방에 따라 급변하는 외환시장상황의 변동성을 정확하게 파악하기 위해서 GARCH모형 (일반화된 자기회귀 조건부이분산성모형)에 다단계인공신경망을 결합한 MLP-GARCH 모형과 GARCH모형과 기계학습의 일종인 딥러닝 (deep learning)을 통합한 DL-GARCH을 가지고 위안화 변동성예측을 비교 실험과 분석을 하였다. 비교분석 결과 DLGARCH 모형은 MLP-GARCH보다 모형 위안화 역내·외 환율변동성 예측 면에서 더욱 더 개선된예측값을 제공하였다. 그래서 이분산시계열모형을 딥러닝과 결합한 DL-GARCH 모형은 시계열의환율 변동성 예측 문제에 딥러닝을 응용할 수 있음을 확인하였다. 향후 이분산시계열과 결합된 딥러닝 모형은 다른 금융시계열 데이터에 응용하여 그 일반화 가능성을 높일 수 있을 것이다.The People’s Republic of China has vigorously been pursuing the internationalization of the Chinese Yuan or Renminbi after the financial crisis of 2008. In this view, an abrupt increase of use of the Chinese Yuan in the onshore and offshore markets are important milestones to be one of important currencies. One of the most frequently used methods to forecast volatility is GARCH model. Since a prediction error of the GARCH model has been reported quite high, a lot of efforts have been made to improve forecasting capability of the GARCH model. In this paper, we have proposed MLPGARCH and a DL-GARCH by employing Artificial Neural Network to the GARCH. In an application to forecasting Chinese Yuan volatility, we have successfully shown their overall outperformance in forecasting over the GARCH.Deep learning, DL-GARCH, GARCH, The Chinese Yuan, volatility."
63,딥러닝 기법을 이용한 내일강수 예측,하지훈,2016,한국지능시스템학회,"정확한 강수예측을 위해서는 예측인자 선정과 예측방법에 대한 선택이 매우 중요하다. 최근에는 강수예측 방법으로기계학습 기법이 많이 사용되고 있으며, 그 중에서도 특히 인공신경망을 사용한 강수예측 방법은 좋은 성능을 보였다. 본논문에서는 딥러닝 기법 중 하나인 DBN(deep belief network)를 이용한 새로운 강수예측 방법을 제안한다. DBN는 비지도사전 학습을 통해 초기 가중치를 설정하여 기존 인공신경망의 문제점을 보완한다. 예측인자로는 기온, 전일-전주 강수일, 태양과 달 궤도 관련 자료를 선정하였다. 기온과 전일-전주 강수일은 서울에서의 1974년부터 2013년까지 총 40년간의AWS(automatic weather system) 관측 자료를 사용하였고, 태양과 달의 궤도 관련 자료는 서울을 중심으로 계산한 결과를사용하였다. 전체 기간에서 일부는 학습 자료로 사용하여 예측모델을 생성하였고, 나머지를 생성한 모델의 검증 자료로사용하였다. 모델 검증 결과로 나온 예측값들은 확률값을 가지며 임계치를 이용하여 강수유무를 판별하였다. 강수 정확도의척도로 양분예보기법 중 CSI(critical successive index)와 Bias(frequency bias)를 계산하였다. 이를 통해 DBN와 MLP(multilayer perceptron)의 성능을 비교한 결과 DBN의 강수 예측 정확도가 높았고, 수행속도 또한 2배 이상 빨랐다.For accurate precipitation forecasts the choice of weather faoctrs and prediction method is very important. Recently, machinele arning has been widely used for forecasting precipitation, and artificial neural network, one of machine learning techniques, showed good performance. In this paper, we suggest a new method for forecasting precipitation using DBN, one of deep learning techniques.
DBN has an advantage that initial weights are set by unsupervised learning, so this compensates for the defects of artificial neural networks. We used past precipitation, temperature, and the parameters of the sun and moon's motion as features for forecasting precipitation. The dataset consists of observation data which had been measured for 40 years from AWS in Seoul. Experiments were based on 8-fold cross validation. As a result of estimation, we got probabilities of test dataset, so threshold was used for the decision of precipitation. CSI and Bias were used for indicating the precision of precipitation. Our experimental results showed that DBN performed better than MLP.Deep learning, Deep belief network, Precipitation, Forecast"
64,딥러닝 신경망모형을 이용한 실시간 교통정보수집 연구,김대현 /KIM DAE HYON,2016,사단법인 인문사회과학기술융합학회,"비디오 기반의 실시간 교통정보수집 시스템은 자율주행을 포함한 지능형교통체계의 주요 서브시스템중 하나이며, 우수한 예측(추정)력 때문에 핵심 알고리즘으로 인공신경망 모형이 주로 사용되고 있다. 특히, 최근 과학 학술지 네이처에 구글 딥마인드의 알파고가 공개되면서 딥러닝과 인공지능에 대한 전 세계적인 관심이 집중되고 있다. 최근까지 가장 널리 사용된 인공신경망모형은 역전파 모형이었으나 최근 컴퓨터 하드웨어 및 알고리즘의 새로운 개발과 더불어 딥러닝 알고리즘을 이용한 딥인공신경망 모형에 대한 관심이 높아지고 있다. 그러나 아직 딥러닝 인공신경망에 대한 연구는 초기단계에 있는 실정이며 적용분야도 극히 한정되어 있다. 본 연구에서는 실시간 도로교통정보 수집을 위한 영상처리분야에서 딥러닝 알고리즘을 활용하고 실험결과를 통하여 향후 교통정보수집‧분석 분야에서의 딥러닝 인공신경망 모형의 적용가능성을 살펴보고자 한다. 특히, 본 연구에서 도출한 딥러닝 인공신경망모형의 실험결과를 기존 역전파 모형에 의한 실험결과와 비교하여 모형의 우수성을 비교‧평가하고자 한다.Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Normally, artificial neural networks have been used in real-time applications for collecting traffic information because of their remarkable performance in terms of predictive accuracy and real-time computing. Especially, the recent publication of a paper in the journal Nature and the success of Google DeepMind’s world-beating Go program AlphaGo caused significant increases of the public interest in artificial intelligence. Even though various types of learning models have been proposed, the Backpropagation has been the most popular neural network model. However, the Deep learning neural network model has attracted much interest recently. In this study, the Deep leaning neural network will be applied in image processing techniques for the real-time traffic information collection to investigate the performance of Deep learning algorithm in traffic engineering fields. In addition, the experimental results from Deep learing algorithm will be compared with the results from Backpropagation model to evaluate and propose better performing model.Intelligent Transportation Systems (ITS), artificial neural networks, Deep learning, traffic information, image processing"
65,딥러닝의 모형과 응용사례,안성만 /SungMahn Ahn,2016,한국지능정보시스템학회,"딥러닝은 인공신경망(neural network)이라는 인공지능분야의 모형이 발전된 형태로서, 계층구조로 이루어진 인공신경망의 내부계층(hidden layer)이 여러 단계로 이루어진 구조이다. 딥러닝에서의 주요 모형은 합성곱신경망(convolutional neural network), 순환신경망(recurrent neural network), 그리고 심층신뢰신경망(deep belief network)의 세가지라고 할 수있다. 그 중에서 현재 흥미로운 연구가 많이 발표되어서 관심이 집중되고 있는 모형은 지도학습(supervised learning)모형인 처음 두 개의 모형이다. 따라서 본 논문에서는 지도학습모형의 가중치를 최적화하는 기본적인 방법인 오류역전파 알고리즘을 살펴본 뒤에 합성곱신경망과 순환신경망의 구조와 응용사례 등을 살펴보고자 한다. 본문에서 다루지 않은 모형인 심층신뢰신경망은 아직까지는 합성곱신경망 이나 순환신경망보다는 상대적으로 주목을 덜 받고 있다. 그러나 심층신뢰신경망은 CNN이나 RNN과는 달리 비지도학습(unsupervised learning)모형이며, 사람이나 동물은 관찰을 통해서 스스로 학습한다는 점에서 궁극적으로는 비지도학습모형이 더 많이 연구되어야 할 주제가 될 것이다.Deep learning model is a kind of neural networks that allows multiple hidden layers. There are various deep learning architectures such as convolutional neural networks, deep belief networks and recurrent neural networks. Those have been applied to fields like computer vision, automatic speech recognition, natural language processing, audio recognition and bioinformatics where they have been shown to produce state-of-the-art results on various tasks. Among those architectures, convolutional neural networks and recurrent neural networks are classified as the supervised learning model. And in recent years, those supervised learning models have gained more popularity than unsupervised learning models such as deep belief networks, because supervised learning models have shown fashionable applications in such fields mentioned above.
Deep learning models can be trained with backpropagation algorithm. Backpropagation is an abbreviation for “backward propagation of errors” and a common method of training artificial neural networks used in conjunction with an optimization method such as gradient descent. The method calculates the gradient of an error function with respect to all the weights in the network. The gradient is fed to the optimization method which in turn uses it to update the weights, in an attempt to minimize the error function.
Convolutional neural networks use a special architecture which is particularly well-adapted to classify images. Using this architecture makes convolutional networks fast to train. This, in turn, helps us train deep, muti-layer networks, which are very good at classifying images. These days, deep convolutional networks are used in most neural networks for image recognition. Convolutional neural networks use three basic ideas: local receptive fields, shared weights, and pooling. By local receptive fields, we mean that each neuron in the first(or any) hidden layer will be connected to a small region of the input(or previous layer’s) neurons. Shared weights mean that we’re going to use the same weights and bias for each of the local receptive field. This means that all the neurons in the hidden layer detect exactly the same feature, just at different locations in the input image. In addition to the convolutional layers just described, convolutional neural networks also contain pooling layers. Pooling layers are usually used immediately after convolutional layers. What the pooling layers do is to simplify the information in the output from the convolutional layer.
Recent convolutional network architectures have 10 to 20 hidden layers and billions of connections between units. Training deep learning networks has taken weeks several years ago, but thanks to progress in GPU and algorithm enhancement, training time has reduced to several hours.
Neural networks with time-varying behavior are known as recurrent neural networks or RNNs. A recurrent neural network is a class of artificial neural network where connections between units form a directed cycle. This creates an internal state of the network which allows it to exhibit dynamic temporal behavior. Unlike feedforward neural networks, RNNs can use their internal memory to process arbitrary sequences of inputs. Early RNN models turned out to be very difficult to train, harder even than deep feedforward networks. The reason is the unstable gradient problem such as vanishing gradient and exploding gradient. The gradient can get smaller and smaller as it is propagated back through layers. This makes learning in early layers extremely slow. The problem actually gets worse in RNNs, since gradients aren’t just propagated backward through layers, they’re propagated backward through time. If the network runs for a long time, that can make the gradient extremely unstable and hard to learn from. It has been possible to incorporate an idea known as long short-term memory units (LSTMs) into RNNs. LSTMs make it much easier to get good results when training RNNs, and many recent papers make use of LSTMs or related ideas.Deep learning, Convolutional neural networks, Recurrent neural networks, Error backpropagation algorithm"
66,딥러닝분석과 기술적 분석 지표를 이용한 한국 코스피주가지수 방향성 예측,이우식 /Lee Woosik,2017,한국데이터정보과학회,"2016년 3월 구글 (Google)의 바둑인공지능 알파고 (AlphaGo)가 이세돌 9단과의 바둑대결에서승리한 이후 다양한 분야에서 인공지능 사용에 대한 관심이 높아지고 있는 가운데 금융투자 분야에서도 인공지능과 투자자문 전문가의 합성어인 로보어드바이저 (Robo-Advisor)에 대한 관심이 높아지고 있다. 인공지능 (artificial intelligence)기반의 의사결정은 비용 절감은 물론 효과적인 의사결정을가능하게 한다는 점에서 큰 장점이 있다. 본 연구에서는 기술적 분석 (technical analysis) 지표와 딥러닝 (deep learning) 모형을 결합하여 한국 코스피 지수를 예측하는 모형을 개발하고 제시한 모형들의 예측력을 비교, 분석한다. 분석 결과 기술적 분석 지표에 딥러닝 알고리즘을 결합한 모형이 주가지수 방향성 예측 문제에 응용될 수 있음을 확인하였다. 향후 본 연구에서 제안된 기술적 분석 지표와딥러닝모형을 결합한 기법은 로보어드바이저서비스에 응용할 수 있는 일반화 가능성을 보여준다Since Google’s AlphaGo defeated a world champion of Go players in 2016, there have been many interests in the deep learning. In the financial sector, a Robo-Advisor using deep learning gains a significant attention, which builds and manages portfolios of financial instruments for investors.In this paper, we have proposed the a deep learning algorithm geared toward identification and forecast of the KOSPI index direction,and we also have compared the accuracy of the prediction.In an application of forecasting the financial market index direction, we have shown that the Robo-Advisor using deep learning has a significant effect on finance industry. The Robo-Advisor collects a massive data such as earnings statements, news reports and regulatory filings, analyzes those and recommends investors how to view market trends and identify the best time to purchase financial assets. On the other hand, the Robo-Advisor allows businesses to learn more about their customers, develop better marketing strategies, increase sales and decrease costs.Artificial intelligence, deep learning, FinTech, Robo-Advisor, technical analysis."
67,딥러닝의 효율적인 학습을 위한 학습데이터 선별 기법에 관한 연구,주경돈 /Gyoung-don Joo,2017,한국정보과학회,"빅데이터의 증가와 병렬 하드웨어 처리, 학습 알고리즘의 개선은 딥러닝의 발전을 가져왔으며 인공지능이 적용 가능한 영역을 획기적으로 확장시키고 있다. 하지만 이러한 대규모의 학습 데이터에 기반한 딥러닝은 학습시간이 증가되고, 더 나아가 학습 데이터의 구축을 위한 레이블링(labeling) 비용이 데이터 수에 비례하여 증가하게 된다. 따라서 본 논문에서는, 딥러닝의 정확도를 최대한 유지하면서 필요한 학습 데이터의 수를 줄일수 있도록, 전체 데이터에서 제한된 수의 학습 데이터의 후보를 선별하는 기법에 대하여 연구한다. 특히 기존의 능동 학습에 기반한 기법들이 학습 데이터의 수가 매우 적어질 경우 딥러닝의 정확도가 급격히 떨어지는 문제를 해결할 수 있는 새로운 기법을 제안하였으며, 제안된 기법으로 선별된 학습 데이터는 그 수의 변화에 대해 딥러닝이 일관되게 경쟁력있는 정확도를 가짐을 다양한 실험을 통하여 검증하였다.The increase of big data, parallel hardware processing and enhanced learning algorithms have led to the development of deep learning and are dramatically expanding the domain of applications based on artificial intelligence. However, such large-scale training data extends the learning time, and furthermore, the labeling cost for building the training set increases in proportion to the volume of data. Therefore, in this paper, we study how to select a limited number of candidates for training set from given data so as to reduce the number of training set while maintaining the accuracy of deep learning. Especially, we found that, with the training set selected by the previous work, the accuracy of deep learning drastically decreases when the number of training set is very small and so proposed new methods to resolve this fall-off in accuracy. Through experiments, we showed that deep learning consistently has the competitive accuracy with the training set selected by the proposed method regardless of the size of training set.Deep Learning, Train Set Selection, Active Learning, Gamma distribution"
68,딥러닝을 이용한 범용적 스테그아날리시스,김현재 /Hyunjae Kim,2017,한국정보과학회,"스테그아날리시스(Steganalysis)란 이미지 등 일반적인 자료에 암호화된 정보를 은닉하는 스테가노그래피(Steganography)에 대한 검출 및 분석 방법으로, 기계학습 기반 방법론을 포함한다. 기존 기계학습 기반 스테그아날리시스는 영상(Image)의 특징(Feature) 추출 및 모델링에 기반하며, 최근 딥러닝(Deep Learning)의 적용으로 검출 정확도가 큰 폭으로 향상되었다. 하지만 현존하는 스테그아날리시스 모델은 단일 스테가노그래피 기법에 대해 국한되어 있어 학습에 사용되지 않은 스테고(Stego) 이미지의 경우 검출이 불가능한 결정적 한계를 가진다. 본 연구에서는 다양한 스테가노그래피 기법으로 생성된 스테고이미지에 딥러닝을 적용하여 스테그아날리시스를 학습하는 범용적 모델을 제안한다. 다양한 실험을 통해 제안 기법의 효용성 및 가능성을 확인하고, 범용적 스테그아날리시스 모델이 각각에 특화된 검출 기법과 유사한 정확도로 스테고 이미지를 검출할 수 있음을 보인다.Steganalysis is to detect information hidden by steganography inside general data such as images. There are stegoanalysis techniques that use machine learning (ML). Existing ML approaches to steganalysis are based on extracting features from stego images and modeling them.
Recently deep learning-based methodologies have shown significant improvements in detection accuracy. However, all the existing methods, including deep learning-based ones, have a critical limitation in that they can only detect stego images that are created by a specific steganography method. In this paper, we propose a generalized steganalysis method that can model multiple types of stego images using deep learning. Through various experiments, we confirm the effectiveness of our approach and envision directions for future research. In particular, we show that our method can detect each type of steganography with the same level of accuracy as that of a steganalysis method dedicated to that type of steganography, thereby demonstrating the general applicability of our approach to multiple types of stego images.steganalysis, steganography, deep learning, generalized steganalysis model"
69,"딥러닝 프레임워크의 비교: 티아노, 텐서플로, CNTK를 중심으로",정여진 /Yeojin Chung,2017,한국지능정보시스템학회,"딥러닝 프레임워크의 대표적인 기능으로는 ‘자동미분’과 ‘GPU의 활용’ 등을 들 수 있다. 본 논문은 파이썬의 라이브러리 형태로 사용 가능한 프레임워크 중에서 구글의 텐서플로와 마이크로소프트의 CNTK, 그리고 텐서 플로의 원조라고 할 수 있는 티아노를 비교하였다. 본문에서는 자동미분의 개념과 GPU의 활용형태를 간단히 설명하고, 그 다음에 logistic regression을 실행하는 예를 통하여 각 프레임워크의 문법을 알아본 뒤에, 마지막으로 대표적인 딥러닝 응용인 CNN의 예제를 실행시켜보고 코딩의 편의성과 실행속도 등을 확인해 보았다. 그 결과, 편의성의 관점에서 보면 티아노가 가장 코딩 하기가 어렵고, CNTK와 텐서플로는 많은 부분이 비슷하게 추상화 되어 있어서 코딩이 비슷하지만 가중치와 편향을 직접 정의하느냐의 여부에서 차이를 보였다. 그리고 각프레임워크의 실행속도에 대한 평가는 ‘큰 차이는 없다’는 것이다. 텐서플로는 티아노에 비하여 속도가 느리다는 평가가 있어왔는데, 본 연구의 실험에 의하면, 비록 CNN 모형에 국한되었지만, 텐서플로가 아주 조금이지만빠른 것으로 나타났다. CNTK의 경우에도, 비록 실험환경이 달랐지만, 실험환경의 차이에 의한 속도의 차이의 편차범위 이내에 있는 것으로 판단이 되었다. 본 연구에서는 세 종류의 딥러닝 프레임워크만을 살펴보았는데,위키피디아에 따르면 딥러닝 프레임워크의 종류는 12가지가 있으며, 각 프레임워크의 특징을 15가지 속성으로구분하여 차이를 특정하고 있다. 그 많은 속성 중에서 사용자의 입장에서 볼 때 중요한 속성은 어떤 언어(파이썬, C++, Java, 등)로 사용가능한지, 어떤 딥러닝 모형에 대한 라이브러리가 잘 구현되어 있는지 등일 것이다. 그리고 사용자가 대규모의 딥러닝 모형을 구축한다면, 다중 GPU 혹은 다중 서버를 지원하는지의 여부도 중요할 것이다. 또한 딥러닝 모형을 처음 학습하는 경우에는 사용설명서가 많은지 예제 프로그램이 많은지 여부도 중요한 기준이 될 것이다.The deep learning framework is software designed to help develop deep learning models. Some of its important functions include “automatic differentiation” and “utilization of GPU”. The list of popular deep learning framework includes Caffe (BVLC) and Theano (University of Montreal). And recently, Microsoft's deep learning framework, Microsoft Cognitive Toolkit, was released as open-source license, following Google’s Tensorflow a year earlier. The early deep learning frameworks have been developed mainly for research at universities. Beginning with the inception of Tensorflow, however, it seems that companies such as Microsoft and Facebook have started to join the competition of framework development. Given the trend, Google and other companies are expected to continue investing in the deep learning framework to bring forward the initiative in the artificial intelligence business. From this point of view, we think it is a good time to compare some of deep learning frameworks. So we compare three deep learning frameworks which can be used as a Python library. Those are Google's Tensorflow, Microsoft’s CNTK, and Theano which is sort of a predecessor of the preceding two.
The most common and important function of deep learning frameworks is the ability to perform automatic differentiation. Basically all the mathematical expressions of deep learning models can be represented as computational graphs, which consist of nodes and edges. Partial derivatives on each edge of a computational graph can then be obtained. With the partial derivatives, we can let software compute differentiation of any node with respect to any variable by utilizing chain rule of Calculus.
First of all, the convenience of coding is in the order of CNTK, Tensorflow, and Theano. The criterion is simply based on the lengths of the codes and the learning curve and the ease of coding are not the main concern. According to the criteria, Theano was the most difficult to implement with, and CNTK and Tensorflow were somewhat easier. With Tensorflow, we need to define weight variables and biases explicitly. The reason that CNTK and Tensorflow are easier to implement with is that those frameworks provide us with more abstraction than Theano. We, however, need to mention that low-level coding is not always bad. It gives us flexibility of coding. With the low-level coding such as in Theano, we can implement and test any new deep learning models or any new search methods that we can think of.
The assessment of the execution speed of each framework is that there is not meaningful difference. According to the experiment, execution speeds of Theano and Tensorflow are very similar, although the experiment was limited to a CNN model. In the case of CNTK, the experimental environment was not maintained as the same. The code written in CNTK has to be run in PC environment without GPU where codes execute as much as 50 times slower than with GPU. But we concluded that the difference of execution speed was within the range of variation caused by the different hardware setup.
In this study, we compared three types of deep learning framework: Theano, Tensorflow, and CNTK. According to Wikipedia, there are 12 available deep learning frameworks. And 15 different attributes differentiate each framework. Some of the important attributes would include interface language (Python, C ++, Java, etc.) and the availability of libraries on various deep learning models such as CNN, RNN, DBN, and etc. And if a user implements a large scale deep learning model, it will also be important to support multiple GPU or multiple servers. Also, if you are learning the deep learning model, it would also be important if there are enough examples and references.deep learning framework, Theano, TensorFlow, CNTK, computational graph, CIFAR-10"
70,딥러닝을 이용한 주변 무선단말 파악방안,이웅섭 /Lee， Woong-sup,2018,한국정보통신학회,"최근 단말-대-단말(Device-to-device, D2D) 통신기술이 차세대 무선통신시스템의 핵심기술로 큰 관심을 받고 있다. 이러한 단말간 통신에서는 자신의 주변에 어떠한 단말이 있는지 파악하는 주변단말 탐색(Neighbor discovery)이매우 중요하다. 본 논문에서는 최근 큰 관심을 받고 있는 딥러닝(Deep learning) 기술을 활용하여 단말간 통신에서 주변단말을 파악하는 방안에 대해서 제안한다. 제안 방안은 기존의 방안과 달리 무선채널의 공간적 연관성을 이용하여단말간의 신호 전송 없이 단말이 기지국으로 전송하는 상향링크 파일럿 신호를 기반으로 주변 단말을 찾고 따라서기존의 방식에 비해 신호전송 복잡도(signaling complexity)를 크게 줄일 수 있다. 또한 제안 방안에서는 떨어져 있는거리에 따라서 주변 단말을 분류 가능하여 기존 방안에 비해서 좀 더 세밀한 단말 탐색이 가능하다. 마지막으로 본 논문에서는 tensorflow를 이용한 컴퓨터 시뮬레이션을 통해 제안 방안의 성능을 검증하였다.Recently, the device-to-device (D2D) communication has been conceived as the key technology for the next-generationmobile communication systems. The neighbor discovery in which the nearby users are found, is essential for the properoperation of the D2D communication. In this paper, we propose new neighbor discovery scheme based on deep learningtechnology which has gained a lot of attention recently. In the proposed scheme, the neighboring users can be foundusing the uplink pilot transmission of users only, unlike conventional neighbor discovery schemes in which direct pilotcommunication among users is required, such that the signaling overhead can be greatly reduced in our proposed scheme.
Moreover, the neighbors with different proximity can also be classified accordingly which enables more accurate neighbordiscovery compared to the conventional schemes. The performance of our proposed scheme is verified through thetensorflow-based computer simulations.
키워드 : 딥러닝, 주변단말 탐색, 단말-대-단말 통신Deep learning, Neighbor discovery, Device-to-device communication, Wireless channel correlation"
71,신뢰성있는 딥러닝 기반 분석 모델을 참조하기 위한 딥러닝 기술 언어,문종혁 /Jong Hyeok Mun,2021,한국정보처리학회,"최근 딥러닝은 하드웨어 성능이 향상됨에 따라 자연어 처리, 영상 인식 등의 다양한 기술에 접목되어 활용되고 있다. 이러한 기술들을 활용해 지능형교통 시스템(ITS), 스마트홈, 헬스케어 등의 산업분야에서 데이터를 분석하여 고속도로 속도위반 차량 검출, 에너지 사용량 제어, 응급상황 등과 같은고품질의 서비스를 제공하며, 고품질의 서비스를 제공하기 위해서는 정확도가 향상된 딥러닝 모델이 적용되어야 한다. 이를 위해 서비스 환경의 데이터를분석하기 위한 딥러닝 모델을 개발할 때, 개발자는 신뢰성이 검증된 최신의 딥러닝 모델을 적용할 수 있어야 한다. 이는 개발자가 참조하는 딥러닝 모델에적용된 학습 데이터셋의 정확도를 측정하여 검증할 수 있다. 이러한 검증을 위해서 개발자는 학습 데이터셋, 딥러닝의 계층구조 및 개발 환경 등과 같은내용을 포함하는 딥러닝 모델을 문서화하여 적용하기 위한 구조적인 정보가 필요하다. 본 논문에서는 신뢰성있는 딥러닝 기반 데이터 분석 모델을 참조하기위한 딥러닝 기술 언어를 제안한다. 제안하는 기술 언어는 신뢰성 있는 딥러닝 모델을 개발하는데 필요한 학습데이터셋, 개발 환경 및 설정 등의 정보와더불어 딥러닝 모델의 계층구조를 표현할 수 있다. 제안하는 딥러닝 기술 언어를 이용하여 개발자는 지능형 교통 시스템에서 참조하는 분석 모델의정확도를 검증할 수 있다. 실험에서는 제안하는 언어의 유효성을 검증하기 위해, 번호판 인식 모델을 중심으로 딥러닝 기술 문서의 적용과정을 보인다With the recent advancements of deep learning, companies such as smart home, healthcare, and intelligent transportation systems areutilizing its functionality to provide high-quality services for vehicle detection, emergency situation detection, and controlling energyconsumption. To provide reliable services in such sensitive systems, deep learning models are required to have high accuracy. In orderto develop a deep learning model for analyzing previously mentioned services, developers should utilize the state of the art deep learningmodels that have already been verified for higher accuracy. The developers can verify the accuracy of the referenced model by validatingthe model on the dataset. For this validation, the developer needs structural information to document and apply deep learning models,including metadata such as learning dataset, network architecture, and development environments. In this paper, we propose a descriptionlanguage that represents the network architecture of the deep learning model along with its metadata that are necessary to develop adeep learning model. Through the proposed description language, developers can easily verify the accuracy of the referenced deep learningmodel. Our experiments demonstrate the application scenario of a deep learning description document that focuses on the license platerecognition for the detection of illegally parked vehicles.Trusted Deep Learning, Model Reference, Deep Learning Description Language, Traffic Situation Analysis Model"
72,딥러닝 분석을 이용한 중국 역내·외 위안화 변동성 예측,이우식 /Lee Woosik,2016,한국데이터정보과학회,"2008년 글로벌 금융위기 이후 중국은 위안화 국제화의 점진적 추진을 진행하면서 중국상하이 외환시장과 중국홍콩 외환시장에서 거래되는 통화인 역내위안화와 역외위안화를 형성시켰다. 본 연구는 위안화 국제화와 점진적인 중국 자본계정 개방에 따라 급변하는 외환시장상황의 변동성을 정확하게 파악하기 위해서 GARCH모형 (일반화된 자기회귀 조건부이분산성모형)에 다단계인공신경망을 결합한 MLP-GARCH 모형과 GARCH모형과 기계학습의 일종인 딥러닝 (deep learning)을 통합한 DL-GARCH을 가지고 위안화 변동성예측을 비교 실험과 분석을 하였다. 비교분석 결과 DLGARCH 모형은 MLP-GARCH보다 모형 위안화 역내·외 환율변동성 예측 면에서 더욱 더 개선된예측값을 제공하였다. 그래서 이분산시계열모형을 딥러닝과 결합한 DL-GARCH 모형은 시계열의환율 변동성 예측 문제에 딥러닝을 응용할 수 있음을 확인하였다. 향후 이분산시계열과 결합된 딥러닝 모형은 다른 금융시계열 데이터에 응용하여 그 일반화 가능성을 높일 수 있을 것이다.The People’s Republic of China has vigorously been pursuing the internationalization of the Chinese Yuan or Renminbi after the financial crisis of 2008. In this view, an abrupt increase of use of the Chinese Yuan in the onshore and offshore markets are important milestones to be one of important currencies. One of the most frequently used methods to forecast volatility is GARCH model. Since a prediction error of the GARCH model has been reported quite high, a lot of efforts have been made to improve forecasting capability of the GARCH model. In this paper, we have proposed MLPGARCH and a DL-GARCH by employing Artificial Neural Network to the GARCH. In an application to forecasting Chinese Yuan volatility, we have successfully shown their overall outperformance in forecasting over the GARCH.Deep learning, DL-GARCH, GARCH, The Chinese Yuan, volatility."
73,딥러닝 기법을 이용한 내일강수 예측,하지훈,2016,한국지능시스템학회,"정확한 강수예측을 위해서는 예측인자 선정과 예측방법에 대한 선택이 매우 중요하다. 최근에는 강수예측 방법으로기계학습 기법이 많이 사용되고 있으며, 그 중에서도 특히 인공신경망을 사용한 강수예측 방법은 좋은 성능을 보였다. 본논문에서는 딥러닝 기법 중 하나인 DBN(deep belief network)를 이용한 새로운 강수예측 방법을 제안한다. DBN는 비지도사전 학습을 통해 초기 가중치를 설정하여 기존 인공신경망의 문제점을 보완한다. 예측인자로는 기온, 전일-전주 강수일, 태양과 달 궤도 관련 자료를 선정하였다. 기온과 전일-전주 강수일은 서울에서의 1974년부터 2013년까지 총 40년간의AWS(automatic weather system) 관측 자료를 사용하였고, 태양과 달의 궤도 관련 자료는 서울을 중심으로 계산한 결과를사용하였다. 전체 기간에서 일부는 학습 자료로 사용하여 예측모델을 생성하였고, 나머지를 생성한 모델의 검증 자료로사용하였다. 모델 검증 결과로 나온 예측값들은 확률값을 가지며 임계치를 이용하여 강수유무를 판별하였다. 강수 정확도의척도로 양분예보기법 중 CSI(critical successive index)와 Bias(frequency bias)를 계산하였다. 이를 통해 DBN와 MLP(multilayer perceptron)의 성능을 비교한 결과 DBN의 강수 예측 정확도가 높았고, 수행속도 또한 2배 이상 빨랐다.For accurate precipitation forecasts the choice of weather faoctrs and prediction method is very important. Recently, machinele arning has been widely used for forecasting precipitation, and artificial neural network, one of machine learning techniques, showed good performance. In this paper, we suggest a new method for forecasting precipitation using DBN, one of deep learning techniques.
DBN has an advantage that initial weights are set by unsupervised learning, so this compensates for the defects of artificial neural networks. We used past precipitation, temperature, and the parameters of the sun and moon's motion as features for forecasting precipitation. The dataset consists of observation data which had been measured for 40 years from AWS in Seoul. Experiments were based on 8-fold cross validation. As a result of estimation, we got probabilities of test dataset, so threshold was used for the decision of precipitation. CSI and Bias were used for indicating the precision of precipitation. Our experimental results showed that DBN performed better than MLP.Deep learning, Deep belief network, Precipitation, Forecast"
74,딥러닝 신경망모형을 이용한 실시간 교통정보수집 연구,김대현 /KIM DAE HYON,2016,사단법인 인문사회과학기술융합학회,"비디오 기반의 실시간 교통정보수집 시스템은 자율주행을 포함한 지능형교통체계의 주요 서브시스템중 하나이며, 우수한 예측(추정)력 때문에 핵심 알고리즘으로 인공신경망 모형이 주로 사용되고 있다. 특히, 최근 과학 학술지 네이처에 구글 딥마인드의 알파고가 공개되면서 딥러닝과 인공지능에 대한 전 세계적인 관심이 집중되고 있다. 최근까지 가장 널리 사용된 인공신경망모형은 역전파 모형이었으나 최근 컴퓨터 하드웨어 및 알고리즘의 새로운 개발과 더불어 딥러닝 알고리즘을 이용한 딥인공신경망 모형에 대한 관심이 높아지고 있다. 그러나 아직 딥러닝 인공신경망에 대한 연구는 초기단계에 있는 실정이며 적용분야도 극히 한정되어 있다. 본 연구에서는 실시간 도로교통정보 수집을 위한 영상처리분야에서 딥러닝 알고리즘을 활용하고 실험결과를 통하여 향후 교통정보수집‧분석 분야에서의 딥러닝 인공신경망 모형의 적용가능성을 살펴보고자 한다. 특히, 본 연구에서 도출한 딥러닝 인공신경망모형의 실험결과를 기존 역전파 모형에 의한 실험결과와 비교하여 모형의 우수성을 비교‧평가하고자 한다.Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Normally, artificial neural networks have been used in real-time applications for collecting traffic information because of their remarkable performance in terms of predictive accuracy and real-time computing. Especially, the recent publication of a paper in the journal Nature and the success of Google DeepMind’s world-beating Go program AlphaGo caused significant increases of the public interest in artificial intelligence. Even though various types of learning models have been proposed, the Backpropagation has been the most popular neural network model. However, the Deep learning neural network model has attracted much interest recently. In this study, the Deep leaning neural network will be applied in image processing techniques for the real-time traffic information collection to investigate the performance of Deep learning algorithm in traffic engineering fields. In addition, the experimental results from Deep learing algorithm will be compared with the results from Backpropagation model to evaluate and propose better performing model.Intelligent Transportation Systems (ITS), artificial neural networks, Deep learning, traffic information, image processing"
75,딥러닝의 모형과 응용사례,안성만 /SungMahn Ahn,2016,한국지능정보시스템학회,"딥러닝은 인공신경망(neural network)이라는 인공지능분야의 모형이 발전된 형태로서, 계층구조로 이루어진 인공신경망의 내부계층(hidden layer)이 여러 단계로 이루어진 구조이다. 딥러닝에서의 주요 모형은 합성곱신경망(convolutional neural network), 순환신경망(recurrent neural network), 그리고 심층신뢰신경망(deep belief network)의 세가지라고 할 수있다. 그 중에서 현재 흥미로운 연구가 많이 발표되어서 관심이 집중되고 있는 모형은 지도학습(supervised learning)모형인 처음 두 개의 모형이다. 따라서 본 논문에서는 지도학습모형의 가중치를 최적화하는 기본적인 방법인 오류역전파 알고리즘을 살펴본 뒤에 합성곱신경망과 순환신경망의 구조와 응용사례 등을 살펴보고자 한다. 본문에서 다루지 않은 모형인 심층신뢰신경망은 아직까지는 합성곱신경망 이나 순환신경망보다는 상대적으로 주목을 덜 받고 있다. 그러나 심층신뢰신경망은 CNN이나 RNN과는 달리 비지도학습(unsupervised learning)모형이며, 사람이나 동물은 관찰을 통해서 스스로 학습한다는 점에서 궁극적으로는 비지도학습모형이 더 많이 연구되어야 할 주제가 될 것이다.Deep learning model is a kind of neural networks that allows multiple hidden layers. There are various deep learning architectures such as convolutional neural networks, deep belief networks and recurrent neural networks. Those have been applied to fields like computer vision, automatic speech recognition, natural language processing, audio recognition and bioinformatics where they have been shown to produce state-of-the-art results on various tasks. Among those architectures, convolutional neural networks and recurrent neural networks are classified as the supervised learning model. And in recent years, those supervised learning models have gained more popularity than unsupervised learning models such as deep belief networks, because supervised learning models have shown fashionable applications in such fields mentioned above.
Deep learning models can be trained with backpropagation algorithm. Backpropagation is an abbreviation for “backward propagation of errors” and a common method of training artificial neural networks used in conjunction with an optimization method such as gradient descent. The method calculates the gradient of an error function with respect to all the weights in the network. The gradient is fed to the optimization method which in turn uses it to update the weights, in an attempt to minimize the error function.
Convolutional neural networks use a special architecture which is particularly well-adapted to classify images. Using this architecture makes convolutional networks fast to train. This, in turn, helps us train deep, muti-layer networks, which are very good at classifying images. These days, deep convolutional networks are used in most neural networks for image recognition. Convolutional neural networks use three basic ideas: local receptive fields, shared weights, and pooling. By local receptive fields, we mean that each neuron in the first(or any) hidden layer will be connected to a small region of the input(or previous layer’s) neurons. Shared weights mean that we’re going to use the same weights and bias for each of the local receptive field. This means that all the neurons in the hidden layer detect exactly the same feature, just at different locations in the input image. In addition to the convolutional layers just described, convolutional neural networks also contain pooling layers. Pooling layers are usually used immediately after convolutional layers. What the pooling layers do is to simplify the information in the output from the convolutional layer.
Recent convolutional network architectures have 10 to 20 hidden layers and billions of connections between units. Training deep learning networks has taken weeks several years ago, but thanks to progress in GPU and algorithm enhancement, training time has reduced to several hours.
Neural networks with time-varying behavior are known as recurrent neural networks or RNNs. A recurrent neural network is a class of artificial neural network where connections between units form a directed cycle. This creates an internal state of the network which allows it to exhibit dynamic temporal behavior. Unlike feedforward neural networks, RNNs can use their internal memory to process arbitrary sequences of inputs. Early RNN models turned out to be very difficult to train, harder even than deep feedforward networks. The reason is the unstable gradient problem such as vanishing gradient and exploding gradient. The gradient can get smaller and smaller as it is propagated back through layers. This makes learning in early layers extremely slow. The problem actually gets worse in RNNs, since gradients aren’t just propagated backward through layers, they’re propagated backward through time. If the network runs for a long time, that can make the gradient extremely unstable and hard to learn from. It has been possible to incorporate an idea known as long short-term memory units (LSTMs) into RNNs. LSTMs make it much easier to get good results when training RNNs, and many recent papers make use of LSTMs or related ideas.Deep learning, Convolutional neural networks, Recurrent neural networks, Error backpropagation algorithm"
76,딥러닝분석과 기술적 분석 지표를 이용한 한국 코스피주가지수 방향성 예측,이우식 /Lee Woosik,2017,한국데이터정보과학회,"2016년 3월 구글 (Google)의 바둑인공지능 알파고 (AlphaGo)가 이세돌 9단과의 바둑대결에서승리한 이후 다양한 분야에서 인공지능 사용에 대한 관심이 높아지고 있는 가운데 금융투자 분야에서도 인공지능과 투자자문 전문가의 합성어인 로보어드바이저 (Robo-Advisor)에 대한 관심이 높아지고 있다. 인공지능 (artificial intelligence)기반의 의사결정은 비용 절감은 물론 효과적인 의사결정을가능하게 한다는 점에서 큰 장점이 있다. 본 연구에서는 기술적 분석 (technical analysis) 지표와 딥러닝 (deep learning) 모형을 결합하여 한국 코스피 지수를 예측하는 모형을 개발하고 제시한 모형들의 예측력을 비교, 분석한다. 분석 결과 기술적 분석 지표에 딥러닝 알고리즘을 결합한 모형이 주가지수 방향성 예측 문제에 응용될 수 있음을 확인하였다. 향후 본 연구에서 제안된 기술적 분석 지표와딥러닝모형을 결합한 기법은 로보어드바이저서비스에 응용할 수 있는 일반화 가능성을 보여준다Since Google’s AlphaGo defeated a world champion of Go players in 2016, there have been many interests in the deep learning. In the financial sector, a Robo-Advisor using deep learning gains a significant attention, which builds and manages portfolios of financial instruments for investors.In this paper, we have proposed the a deep learning algorithm geared toward identification and forecast of the KOSPI index direction,and we also have compared the accuracy of the prediction.In an application of forecasting the financial market index direction, we have shown that the Robo-Advisor using deep learning has a significant effect on finance industry. The Robo-Advisor collects a massive data such as earnings statements, news reports and regulatory filings, analyzes those and recommends investors how to view market trends and identify the best time to purchase financial assets. On the other hand, the Robo-Advisor allows businesses to learn more about their customers, develop better marketing strategies, increase sales and decrease costs.Artificial intelligence, deep learning, FinTech, Robo-Advisor, technical analysis."
77,딥러닝의 효율적인 학습을 위한 학습데이터 선별 기법에 관한 연구,주경돈 /Gyoung-don Joo,2017,한국정보과학회,"빅데이터의 증가와 병렬 하드웨어 처리, 학습 알고리즘의 개선은 딥러닝의 발전을 가져왔으며 인공지능이 적용 가능한 영역을 획기적으로 확장시키고 있다. 하지만 이러한 대규모의 학습 데이터에 기반한 딥러닝은 학습시간이 증가되고, 더 나아가 학습 데이터의 구축을 위한 레이블링(labeling) 비용이 데이터 수에 비례하여 증가하게 된다. 따라서 본 논문에서는, 딥러닝의 정확도를 최대한 유지하면서 필요한 학습 데이터의 수를 줄일수 있도록, 전체 데이터에서 제한된 수의 학습 데이터의 후보를 선별하는 기법에 대하여 연구한다. 특히 기존의 능동 학습에 기반한 기법들이 학습 데이터의 수가 매우 적어질 경우 딥러닝의 정확도가 급격히 떨어지는 문제를 해결할 수 있는 새로운 기법을 제안하였으며, 제안된 기법으로 선별된 학습 데이터는 그 수의 변화에 대해 딥러닝이 일관되게 경쟁력있는 정확도를 가짐을 다양한 실험을 통하여 검증하였다.The increase of big data, parallel hardware processing and enhanced learning algorithms have led to the development of deep learning and are dramatically expanding the domain of applications based on artificial intelligence. However, such large-scale training data extends the learning time, and furthermore, the labeling cost for building the training set increases in proportion to the volume of data. Therefore, in this paper, we study how to select a limited number of candidates for training set from given data so as to reduce the number of training set while maintaining the accuracy of deep learning. Especially, we found that, with the training set selected by the previous work, the accuracy of deep learning drastically decreases when the number of training set is very small and so proposed new methods to resolve this fall-off in accuracy. Through experiments, we showed that deep learning consistently has the competitive accuracy with the training set selected by the proposed method regardless of the size of training set.Deep Learning, Train Set Selection, Active Learning, Gamma distribution"
78,딥러닝을 이용한 범용적 스테그아날리시스,김현재 /Hyunjae Kim,2017,한국정보과학회,"스테그아날리시스(Steganalysis)란 이미지 등 일반적인 자료에 암호화된 정보를 은닉하는 스테가노그래피(Steganography)에 대한 검출 및 분석 방법으로, 기계학습 기반 방법론을 포함한다. 기존 기계학습 기반 스테그아날리시스는 영상(Image)의 특징(Feature) 추출 및 모델링에 기반하며, 최근 딥러닝(Deep Learning)의 적용으로 검출 정확도가 큰 폭으로 향상되었다. 하지만 현존하는 스테그아날리시스 모델은 단일 스테가노그래피 기법에 대해 국한되어 있어 학습에 사용되지 않은 스테고(Stego) 이미지의 경우 검출이 불가능한 결정적 한계를 가진다. 본 연구에서는 다양한 스테가노그래피 기법으로 생성된 스테고이미지에 딥러닝을 적용하여 스테그아날리시스를 학습하는 범용적 모델을 제안한다. 다양한 실험을 통해 제안 기법의 효용성 및 가능성을 확인하고, 범용적 스테그아날리시스 모델이 각각에 특화된 검출 기법과 유사한 정확도로 스테고 이미지를 검출할 수 있음을 보인다.Steganalysis is to detect information hidden by steganography inside general data such as images. There are stegoanalysis techniques that use machine learning (ML). Existing ML approaches to steganalysis are based on extracting features from stego images and modeling them.
Recently deep learning-based methodologies have shown significant improvements in detection accuracy. However, all the existing methods, including deep learning-based ones, have a critical limitation in that they can only detect stego images that are created by a specific steganography method. In this paper, we propose a generalized steganalysis method that can model multiple types of stego images using deep learning. Through various experiments, we confirm the effectiveness of our approach and envision directions for future research. In particular, we show that our method can detect each type of steganography with the same level of accuracy as that of a steganalysis method dedicated to that type of steganography, thereby demonstrating the general applicability of our approach to multiple types of stego images.steganalysis, steganography, deep learning, generalized steganalysis model"
79,"딥러닝 프레임워크의 비교: 티아노, 텐서플로, CNTK를 중심으로",정여진 /Yeojin Chung,2017,한국지능정보시스템학회,"딥러닝 프레임워크의 대표적인 기능으로는 ‘자동미분’과 ‘GPU의 활용’ 등을 들 수 있다. 본 논문은 파이썬의 라이브러리 형태로 사용 가능한 프레임워크 중에서 구글의 텐서플로와 마이크로소프트의 CNTK, 그리고 텐서 플로의 원조라고 할 수 있는 티아노를 비교하였다. 본문에서는 자동미분의 개념과 GPU의 활용형태를 간단히 설명하고, 그 다음에 logistic regression을 실행하는 예를 통하여 각 프레임워크의 문법을 알아본 뒤에, 마지막으로 대표적인 딥러닝 응용인 CNN의 예제를 실행시켜보고 코딩의 편의성과 실행속도 등을 확인해 보았다. 그 결과, 편의성의 관점에서 보면 티아노가 가장 코딩 하기가 어렵고, CNTK와 텐서플로는 많은 부분이 비슷하게 추상화 되어 있어서 코딩이 비슷하지만 가중치와 편향을 직접 정의하느냐의 여부에서 차이를 보였다. 그리고 각프레임워크의 실행속도에 대한 평가는 ‘큰 차이는 없다’는 것이다. 텐서플로는 티아노에 비하여 속도가 느리다는 평가가 있어왔는데, 본 연구의 실험에 의하면, 비록 CNN 모형에 국한되었지만, 텐서플로가 아주 조금이지만빠른 것으로 나타났다. CNTK의 경우에도, 비록 실험환경이 달랐지만, 실험환경의 차이에 의한 속도의 차이의 편차범위 이내에 있는 것으로 판단이 되었다. 본 연구에서는 세 종류의 딥러닝 프레임워크만을 살펴보았는데,위키피디아에 따르면 딥러닝 프레임워크의 종류는 12가지가 있으며, 각 프레임워크의 특징을 15가지 속성으로구분하여 차이를 특정하고 있다. 그 많은 속성 중에서 사용자의 입장에서 볼 때 중요한 속성은 어떤 언어(파이썬, C++, Java, 등)로 사용가능한지, 어떤 딥러닝 모형에 대한 라이브러리가 잘 구현되어 있는지 등일 것이다. 그리고 사용자가 대규모의 딥러닝 모형을 구축한다면, 다중 GPU 혹은 다중 서버를 지원하는지의 여부도 중요할 것이다. 또한 딥러닝 모형을 처음 학습하는 경우에는 사용설명서가 많은지 예제 프로그램이 많은지 여부도 중요한 기준이 될 것이다.The deep learning framework is software designed to help develop deep learning models. Some of its important functions include “automatic differentiation” and “utilization of GPU”. The list of popular deep learning framework includes Caffe (BVLC) and Theano (University of Montreal). And recently, Microsoft's deep learning framework, Microsoft Cognitive Toolkit, was released as open-source license, following Google’s Tensorflow a year earlier. The early deep learning frameworks have been developed mainly for research at universities. Beginning with the inception of Tensorflow, however, it seems that companies such as Microsoft and Facebook have started to join the competition of framework development. Given the trend, Google and other companies are expected to continue investing in the deep learning framework to bring forward the initiative in the artificial intelligence business. From this point of view, we think it is a good time to compare some of deep learning frameworks. So we compare three deep learning frameworks which can be used as a Python library. Those are Google's Tensorflow, Microsoft’s CNTK, and Theano which is sort of a predecessor of the preceding two.
The most common and important function of deep learning frameworks is the ability to perform automatic differentiation. Basically all the mathematical expressions of deep learning models can be represented as computational graphs, which consist of nodes and edges. Partial derivatives on each edge of a computational graph can then be obtained. With the partial derivatives, we can let software compute differentiation of any node with respect to any variable by utilizing chain rule of Calculus.
First of all, the convenience of coding is in the order of CNTK, Tensorflow, and Theano. The criterion is simply based on the lengths of the codes and the learning curve and the ease of coding are not the main concern. According to the criteria, Theano was the most difficult to implement with, and CNTK and Tensorflow were somewhat easier. With Tensorflow, we need to define weight variables and biases explicitly. The reason that CNTK and Tensorflow are easier to implement with is that those frameworks provide us with more abstraction than Theano. We, however, need to mention that low-level coding is not always bad. It gives us flexibility of coding. With the low-level coding such as in Theano, we can implement and test any new deep learning models or any new search methods that we can think of.
The assessment of the execution speed of each framework is that there is not meaningful difference. According to the experiment, execution speeds of Theano and Tensorflow are very similar, although the experiment was limited to a CNN model. In the case of CNTK, the experimental environment was not maintained as the same. The code written in CNTK has to be run in PC environment without GPU where codes execute as much as 50 times slower than with GPU. But we concluded that the difference of execution speed was within the range of variation caused by the different hardware setup.
In this study, we compared three types of deep learning framework: Theano, Tensorflow, and CNTK. According to Wikipedia, there are 12 available deep learning frameworks. And 15 different attributes differentiate each framework. Some of the important attributes would include interface language (Python, C ++, Java, etc.) and the availability of libraries on various deep learning models such as CNN, RNN, DBN, and etc. And if a user implements a large scale deep learning model, it will also be important to support multiple GPU or multiple servers. Also, if you are learning the deep learning model, it would also be important if there are enough examples and references.deep learning framework, Theano, TensorFlow, CNTK, computational graph, CIFAR-10"
80,딥러닝을 이용한 주변 무선단말 파악방안,이웅섭 /Lee， Woong-sup,2018,한국정보통신학회,"최근 단말-대-단말(Device-to-device, D2D) 통신기술이 차세대 무선통신시스템의 핵심기술로 큰 관심을 받고 있다. 이러한 단말간 통신에서는 자신의 주변에 어떠한 단말이 있는지 파악하는 주변단말 탐색(Neighbor discovery)이매우 중요하다. 본 논문에서는 최근 큰 관심을 받고 있는 딥러닝(Deep learning) 기술을 활용하여 단말간 통신에서 주변단말을 파악하는 방안에 대해서 제안한다. 제안 방안은 기존의 방안과 달리 무선채널의 공간적 연관성을 이용하여단말간의 신호 전송 없이 단말이 기지국으로 전송하는 상향링크 파일럿 신호를 기반으로 주변 단말을 찾고 따라서기존의 방식에 비해 신호전송 복잡도(signaling complexity)를 크게 줄일 수 있다. 또한 제안 방안에서는 떨어져 있는거리에 따라서 주변 단말을 분류 가능하여 기존 방안에 비해서 좀 더 세밀한 단말 탐색이 가능하다. 마지막으로 본 논문에서는 tensorflow를 이용한 컴퓨터 시뮬레이션을 통해 제안 방안의 성능을 검증하였다.Recently, the device-to-device (D2D) communication has been conceived as the key technology for the next-generationmobile communication systems. The neighbor discovery in which the nearby users are found, is essential for the properoperation of the D2D communication. In this paper, we propose new neighbor discovery scheme based on deep learningtechnology which has gained a lot of attention recently. In the proposed scheme, the neighboring users can be foundusing the uplink pilot transmission of users only, unlike conventional neighbor discovery schemes in which direct pilotcommunication among users is required, such that the signaling overhead can be greatly reduced in our proposed scheme.
Moreover, the neighbors with different proximity can also be classified accordingly which enables more accurate neighbordiscovery compared to the conventional schemes. The performance of our proposed scheme is verified through thetensorflow-based computer simulations.
키워드 : 딥러닝, 주변단말 탐색, 단말-대-단말 통신Deep learning, Neighbor discovery, Device-to-device communication, Wireless channel correlation"
81,신뢰성있는 딥러닝 기반 분석 모델을 참조하기 위한 딥러닝 기술 언어,문종혁 /Jong Hyeok Mun,2021,한국정보처리학회,"최근 딥러닝은 하드웨어 성능이 향상됨에 따라 자연어 처리, 영상 인식 등의 다양한 기술에 접목되어 활용되고 있다. 이러한 기술들을 활용해 지능형교통 시스템(ITS), 스마트홈, 헬스케어 등의 산업분야에서 데이터를 분석하여 고속도로 속도위반 차량 검출, 에너지 사용량 제어, 응급상황 등과 같은고품질의 서비스를 제공하며, 고품질의 서비스를 제공하기 위해서는 정확도가 향상된 딥러닝 모델이 적용되어야 한다. 이를 위해 서비스 환경의 데이터를분석하기 위한 딥러닝 모델을 개발할 때, 개발자는 신뢰성이 검증된 최신의 딥러닝 모델을 적용할 수 있어야 한다. 이는 개발자가 참조하는 딥러닝 모델에적용된 학습 데이터셋의 정확도를 측정하여 검증할 수 있다. 이러한 검증을 위해서 개발자는 학습 데이터셋, 딥러닝의 계층구조 및 개발 환경 등과 같은내용을 포함하는 딥러닝 모델을 문서화하여 적용하기 위한 구조적인 정보가 필요하다. 본 논문에서는 신뢰성있는 딥러닝 기반 데이터 분석 모델을 참조하기위한 딥러닝 기술 언어를 제안한다. 제안하는 기술 언어는 신뢰성 있는 딥러닝 모델을 개발하는데 필요한 학습데이터셋, 개발 환경 및 설정 등의 정보와더불어 딥러닝 모델의 계층구조를 표현할 수 있다. 제안하는 딥러닝 기술 언어를 이용하여 개발자는 지능형 교통 시스템에서 참조하는 분석 모델의정확도를 검증할 수 있다. 실험에서는 제안하는 언어의 유효성을 검증하기 위해, 번호판 인식 모델을 중심으로 딥러닝 기술 문서의 적용과정을 보인다With the recent advancements of deep learning, companies such as smart home, healthcare, and intelligent transportation systems areutilizing its functionality to provide high-quality services for vehicle detection, emergency situation detection, and controlling energyconsumption. To provide reliable services in such sensitive systems, deep learning models are required to have high accuracy. In orderto develop a deep learning model for analyzing previously mentioned services, developers should utilize the state of the art deep learningmodels that have already been verified for higher accuracy. The developers can verify the accuracy of the referenced model by validatingthe model on the dataset. For this validation, the developer needs structural information to document and apply deep learning models,including metadata such as learning dataset, network architecture, and development environments. In this paper, we propose a descriptionlanguage that represents the network architecture of the deep learning model along with its metadata that are necessary to develop adeep learning model. Through the proposed description language, developers can easily verify the accuracy of the referenced deep learningmodel. Our experiments demonstrate the application scenario of a deep learning description document that focuses on the license platerecognition for the detection of illegally parked vehicles.Trusted Deep Learning, Model Reference, Deep Learning Description Language, Traffic Situation Analysis Model"
82,딥러닝 분석을 이용한 중국 역내·외 위안화 변동성 예측,이우식 /Lee Woosik,2016,한국데이터정보과학회,"2008년 글로벌 금융위기 이후 중국은 위안화 국제화의 점진적 추진을 진행하면서 중국상하이 외환시장과 중국홍콩 외환시장에서 거래되는 통화인 역내위안화와 역외위안화를 형성시켰다. 본 연구는 위안화 국제화와 점진적인 중국 자본계정 개방에 따라 급변하는 외환시장상황의 변동성을 정확하게 파악하기 위해서 GARCH모형 (일반화된 자기회귀 조건부이분산성모형)에 다단계인공신경망을 결합한 MLP-GARCH 모형과 GARCH모형과 기계학습의 일종인 딥러닝 (deep learning)을 통합한 DL-GARCH을 가지고 위안화 변동성예측을 비교 실험과 분석을 하였다. 비교분석 결과 DLGARCH 모형은 MLP-GARCH보다 모형 위안화 역내·외 환율변동성 예측 면에서 더욱 더 개선된예측값을 제공하였다. 그래서 이분산시계열모형을 딥러닝과 결합한 DL-GARCH 모형은 시계열의환율 변동성 예측 문제에 딥러닝을 응용할 수 있음을 확인하였다. 향후 이분산시계열과 결합된 딥러닝 모형은 다른 금융시계열 데이터에 응용하여 그 일반화 가능성을 높일 수 있을 것이다.The People’s Republic of China has vigorously been pursuing the internationalization of the Chinese Yuan or Renminbi after the financial crisis of 2008. In this view, an abrupt increase of use of the Chinese Yuan in the onshore and offshore markets are important milestones to be one of important currencies. One of the most frequently used methods to forecast volatility is GARCH model. Since a prediction error of the GARCH model has been reported quite high, a lot of efforts have been made to improve forecasting capability of the GARCH model. In this paper, we have proposed MLPGARCH and a DL-GARCH by employing Artificial Neural Network to the GARCH. In an application to forecasting Chinese Yuan volatility, we have successfully shown their overall outperformance in forecasting over the GARCH.Deep learning, DL-GARCH, GARCH, The Chinese Yuan, volatility."
83,딥러닝 기법을 이용한 내일강수 예측,하지훈,2016,한국지능시스템학회,"정확한 강수예측을 위해서는 예측인자 선정과 예측방법에 대한 선택이 매우 중요하다. 최근에는 강수예측 방법으로기계학습 기법이 많이 사용되고 있으며, 그 중에서도 특히 인공신경망을 사용한 강수예측 방법은 좋은 성능을 보였다. 본논문에서는 딥러닝 기법 중 하나인 DBN(deep belief network)를 이용한 새로운 강수예측 방법을 제안한다. DBN는 비지도사전 학습을 통해 초기 가중치를 설정하여 기존 인공신경망의 문제점을 보완한다. 예측인자로는 기온, 전일-전주 강수일, 태양과 달 궤도 관련 자료를 선정하였다. 기온과 전일-전주 강수일은 서울에서의 1974년부터 2013년까지 총 40년간의AWS(automatic weather system) 관측 자료를 사용하였고, 태양과 달의 궤도 관련 자료는 서울을 중심으로 계산한 결과를사용하였다. 전체 기간에서 일부는 학습 자료로 사용하여 예측모델을 생성하였고, 나머지를 생성한 모델의 검증 자료로사용하였다. 모델 검증 결과로 나온 예측값들은 확률값을 가지며 임계치를 이용하여 강수유무를 판별하였다. 강수 정확도의척도로 양분예보기법 중 CSI(critical successive index)와 Bias(frequency bias)를 계산하였다. 이를 통해 DBN와 MLP(multilayer perceptron)의 성능을 비교한 결과 DBN의 강수 예측 정확도가 높았고, 수행속도 또한 2배 이상 빨랐다.For accurate precipitation forecasts the choice of weather faoctrs and prediction method is very important. Recently, machinele arning has been widely used for forecasting precipitation, and artificial neural network, one of machine learning techniques, showed good performance. In this paper, we suggest a new method for forecasting precipitation using DBN, one of deep learning techniques.
DBN has an advantage that initial weights are set by unsupervised learning, so this compensates for the defects of artificial neural networks. We used past precipitation, temperature, and the parameters of the sun and moon's motion as features for forecasting precipitation. The dataset consists of observation data which had been measured for 40 years from AWS in Seoul. Experiments were based on 8-fold cross validation. As a result of estimation, we got probabilities of test dataset, so threshold was used for the decision of precipitation. CSI and Bias were used for indicating the precision of precipitation. Our experimental results showed that DBN performed better than MLP.Deep learning, Deep belief network, Precipitation, Forecast"
84,딥러닝 신경망모형을 이용한 실시간 교통정보수집 연구,김대현 /KIM DAE HYON,2016,사단법인 인문사회과학기술융합학회,"비디오 기반의 실시간 교통정보수집 시스템은 자율주행을 포함한 지능형교통체계의 주요 서브시스템중 하나이며, 우수한 예측(추정)력 때문에 핵심 알고리즘으로 인공신경망 모형이 주로 사용되고 있다. 특히, 최근 과학 학술지 네이처에 구글 딥마인드의 알파고가 공개되면서 딥러닝과 인공지능에 대한 전 세계적인 관심이 집중되고 있다. 최근까지 가장 널리 사용된 인공신경망모형은 역전파 모형이었으나 최근 컴퓨터 하드웨어 및 알고리즘의 새로운 개발과 더불어 딥러닝 알고리즘을 이용한 딥인공신경망 모형에 대한 관심이 높아지고 있다. 그러나 아직 딥러닝 인공신경망에 대한 연구는 초기단계에 있는 실정이며 적용분야도 극히 한정되어 있다. 본 연구에서는 실시간 도로교통정보 수집을 위한 영상처리분야에서 딥러닝 알고리즘을 활용하고 실험결과를 통하여 향후 교통정보수집‧분석 분야에서의 딥러닝 인공신경망 모형의 적용가능성을 살펴보고자 한다. 특히, 본 연구에서 도출한 딥러닝 인공신경망모형의 실험결과를 기존 역전파 모형에 의한 실험결과와 비교하여 모형의 우수성을 비교‧평가하고자 한다.Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Normally, artificial neural networks have been used in real-time applications for collecting traffic information because of their remarkable performance in terms of predictive accuracy and real-time computing. Especially, the recent publication of a paper in the journal Nature and the success of Google DeepMind’s world-beating Go program AlphaGo caused significant increases of the public interest in artificial intelligence. Even though various types of learning models have been proposed, the Backpropagation has been the most popular neural network model. However, the Deep learning neural network model has attracted much interest recently. In this study, the Deep leaning neural network will be applied in image processing techniques for the real-time traffic information collection to investigate the performance of Deep learning algorithm in traffic engineering fields. In addition, the experimental results from Deep learing algorithm will be compared with the results from Backpropagation model to evaluate and propose better performing model.Intelligent Transportation Systems (ITS), artificial neural networks, Deep learning, traffic information, image processing"
85,딥러닝의 모형과 응용사례,안성만 /SungMahn Ahn,2016,한국지능정보시스템학회,"딥러닝은 인공신경망(neural network)이라는 인공지능분야의 모형이 발전된 형태로서, 계층구조로 이루어진 인공신경망의 내부계층(hidden layer)이 여러 단계로 이루어진 구조이다. 딥러닝에서의 주요 모형은 합성곱신경망(convolutional neural network), 순환신경망(recurrent neural network), 그리고 심층신뢰신경망(deep belief network)의 세가지라고 할 수있다. 그 중에서 현재 흥미로운 연구가 많이 발표되어서 관심이 집중되고 있는 모형은 지도학습(supervised learning)모형인 처음 두 개의 모형이다. 따라서 본 논문에서는 지도학습모형의 가중치를 최적화하는 기본적인 방법인 오류역전파 알고리즘을 살펴본 뒤에 합성곱신경망과 순환신경망의 구조와 응용사례 등을 살펴보고자 한다. 본문에서 다루지 않은 모형인 심층신뢰신경망은 아직까지는 합성곱신경망 이나 순환신경망보다는 상대적으로 주목을 덜 받고 있다. 그러나 심층신뢰신경망은 CNN이나 RNN과는 달리 비지도학습(unsupervised learning)모형이며, 사람이나 동물은 관찰을 통해서 스스로 학습한다는 점에서 궁극적으로는 비지도학습모형이 더 많이 연구되어야 할 주제가 될 것이다.Deep learning model is a kind of neural networks that allows multiple hidden layers. There are various deep learning architectures such as convolutional neural networks, deep belief networks and recurrent neural networks. Those have been applied to fields like computer vision, automatic speech recognition, natural language processing, audio recognition and bioinformatics where they have been shown to produce state-of-the-art results on various tasks. Among those architectures, convolutional neural networks and recurrent neural networks are classified as the supervised learning model. And in recent years, those supervised learning models have gained more popularity than unsupervised learning models such as deep belief networks, because supervised learning models have shown fashionable applications in such fields mentioned above.
Deep learning models can be trained with backpropagation algorithm. Backpropagation is an abbreviation for “backward propagation of errors” and a common method of training artificial neural networks used in conjunction with an optimization method such as gradient descent. The method calculates the gradient of an error function with respect to all the weights in the network. The gradient is fed to the optimization method which in turn uses it to update the weights, in an attempt to minimize the error function.
Convolutional neural networks use a special architecture which is particularly well-adapted to classify images. Using this architecture makes convolutional networks fast to train. This, in turn, helps us train deep, muti-layer networks, which are very good at classifying images. These days, deep convolutional networks are used in most neural networks for image recognition. Convolutional neural networks use three basic ideas: local receptive fields, shared weights, and pooling. By local receptive fields, we mean that each neuron in the first(or any) hidden layer will be connected to a small region of the input(or previous layer’s) neurons. Shared weights mean that we’re going to use the same weights and bias for each of the local receptive field. This means that all the neurons in the hidden layer detect exactly the same feature, just at different locations in the input image. In addition to the convolutional layers just described, convolutional neural networks also contain pooling layers. Pooling layers are usually used immediately after convolutional layers. What the pooling layers do is to simplify the information in the output from the convolutional layer.
Recent convolutional network architectures have 10 to 20 hidden layers and billions of connections between units. Training deep learning networks has taken weeks several years ago, but thanks to progress in GPU and algorithm enhancement, training time has reduced to several hours.
Neural networks with time-varying behavior are known as recurrent neural networks or RNNs. A recurrent neural network is a class of artificial neural network where connections between units form a directed cycle. This creates an internal state of the network which allows it to exhibit dynamic temporal behavior. Unlike feedforward neural networks, RNNs can use their internal memory to process arbitrary sequences of inputs. Early RNN models turned out to be very difficult to train, harder even than deep feedforward networks. The reason is the unstable gradient problem such as vanishing gradient and exploding gradient. The gradient can get smaller and smaller as it is propagated back through layers. This makes learning in early layers extremely slow. The problem actually gets worse in RNNs, since gradients aren’t just propagated backward through layers, they’re propagated backward through time. If the network runs for a long time, that can make the gradient extremely unstable and hard to learn from. It has been possible to incorporate an idea known as long short-term memory units (LSTMs) into RNNs. LSTMs make it much easier to get good results when training RNNs, and many recent papers make use of LSTMs or related ideas.Deep learning, Convolutional neural networks, Recurrent neural networks, Error backpropagation algorithm"
86,딥러닝분석과 기술적 분석 지표를 이용한 한국 코스피주가지수 방향성 예측,이우식 /Lee Woosik,2017,한국데이터정보과학회,"2016년 3월 구글 (Google)의 바둑인공지능 알파고 (AlphaGo)가 이세돌 9단과의 바둑대결에서승리한 이후 다양한 분야에서 인공지능 사용에 대한 관심이 높아지고 있는 가운데 금융투자 분야에서도 인공지능과 투자자문 전문가의 합성어인 로보어드바이저 (Robo-Advisor)에 대한 관심이 높아지고 있다. 인공지능 (artificial intelligence)기반의 의사결정은 비용 절감은 물론 효과적인 의사결정을가능하게 한다는 점에서 큰 장점이 있다. 본 연구에서는 기술적 분석 (technical analysis) 지표와 딥러닝 (deep learning) 모형을 결합하여 한국 코스피 지수를 예측하는 모형을 개발하고 제시한 모형들의 예측력을 비교, 분석한다. 분석 결과 기술적 분석 지표에 딥러닝 알고리즘을 결합한 모형이 주가지수 방향성 예측 문제에 응용될 수 있음을 확인하였다. 향후 본 연구에서 제안된 기술적 분석 지표와딥러닝모형을 결합한 기법은 로보어드바이저서비스에 응용할 수 있는 일반화 가능성을 보여준다Since Google’s AlphaGo defeated a world champion of Go players in 2016, there have been many interests in the deep learning. In the financial sector, a Robo-Advisor using deep learning gains a significant attention, which builds and manages portfolios of financial instruments for investors.In this paper, we have proposed the a deep learning algorithm geared toward identification and forecast of the KOSPI index direction,and we also have compared the accuracy of the prediction.In an application of forecasting the financial market index direction, we have shown that the Robo-Advisor using deep learning has a significant effect on finance industry. The Robo-Advisor collects a massive data such as earnings statements, news reports and regulatory filings, analyzes those and recommends investors how to view market trends and identify the best time to purchase financial assets. On the other hand, the Robo-Advisor allows businesses to learn more about their customers, develop better marketing strategies, increase sales and decrease costs.Artificial intelligence, deep learning, FinTech, Robo-Advisor, technical analysis."
87,딥러닝의 효율적인 학습을 위한 학습데이터 선별 기법에 관한 연구,주경돈 /Gyoung-don Joo,2017,한국정보과학회,"빅데이터의 증가와 병렬 하드웨어 처리, 학습 알고리즘의 개선은 딥러닝의 발전을 가져왔으며 인공지능이 적용 가능한 영역을 획기적으로 확장시키고 있다. 하지만 이러한 대규모의 학습 데이터에 기반한 딥러닝은 학습시간이 증가되고, 더 나아가 학습 데이터의 구축을 위한 레이블링(labeling) 비용이 데이터 수에 비례하여 증가하게 된다. 따라서 본 논문에서는, 딥러닝의 정확도를 최대한 유지하면서 필요한 학습 데이터의 수를 줄일수 있도록, 전체 데이터에서 제한된 수의 학습 데이터의 후보를 선별하는 기법에 대하여 연구한다. 특히 기존의 능동 학습에 기반한 기법들이 학습 데이터의 수가 매우 적어질 경우 딥러닝의 정확도가 급격히 떨어지는 문제를 해결할 수 있는 새로운 기법을 제안하였으며, 제안된 기법으로 선별된 학습 데이터는 그 수의 변화에 대해 딥러닝이 일관되게 경쟁력있는 정확도를 가짐을 다양한 실험을 통하여 검증하였다.The increase of big data, parallel hardware processing and enhanced learning algorithms have led to the development of deep learning and are dramatically expanding the domain of applications based on artificial intelligence. However, such large-scale training data extends the learning time, and furthermore, the labeling cost for building the training set increases in proportion to the volume of data. Therefore, in this paper, we study how to select a limited number of candidates for training set from given data so as to reduce the number of training set while maintaining the accuracy of deep learning. Especially, we found that, with the training set selected by the previous work, the accuracy of deep learning drastically decreases when the number of training set is very small and so proposed new methods to resolve this fall-off in accuracy. Through experiments, we showed that deep learning consistently has the competitive accuracy with the training set selected by the proposed method regardless of the size of training set.Deep Learning, Train Set Selection, Active Learning, Gamma distribution"
88,딥러닝을 이용한 범용적 스테그아날리시스,김현재 /Hyunjae Kim,2017,한국정보과학회,"스테그아날리시스(Steganalysis)란 이미지 등 일반적인 자료에 암호화된 정보를 은닉하는 스테가노그래피(Steganography)에 대한 검출 및 분석 방법으로, 기계학습 기반 방법론을 포함한다. 기존 기계학습 기반 스테그아날리시스는 영상(Image)의 특징(Feature) 추출 및 모델링에 기반하며, 최근 딥러닝(Deep Learning)의 적용으로 검출 정확도가 큰 폭으로 향상되었다. 하지만 현존하는 스테그아날리시스 모델은 단일 스테가노그래피 기법에 대해 국한되어 있어 학습에 사용되지 않은 스테고(Stego) 이미지의 경우 검출이 불가능한 결정적 한계를 가진다. 본 연구에서는 다양한 스테가노그래피 기법으로 생성된 스테고이미지에 딥러닝을 적용하여 스테그아날리시스를 학습하는 범용적 모델을 제안한다. 다양한 실험을 통해 제안 기법의 효용성 및 가능성을 확인하고, 범용적 스테그아날리시스 모델이 각각에 특화된 검출 기법과 유사한 정확도로 스테고 이미지를 검출할 수 있음을 보인다.Steganalysis is to detect information hidden by steganography inside general data such as images. There are stegoanalysis techniques that use machine learning (ML). Existing ML approaches to steganalysis are based on extracting features from stego images and modeling them.
Recently deep learning-based methodologies have shown significant improvements in detection accuracy. However, all the existing methods, including deep learning-based ones, have a critical limitation in that they can only detect stego images that are created by a specific steganography method. In this paper, we propose a generalized steganalysis method that can model multiple types of stego images using deep learning. Through various experiments, we confirm the effectiveness of our approach and envision directions for future research. In particular, we show that our method can detect each type of steganography with the same level of accuracy as that of a steganalysis method dedicated to that type of steganography, thereby demonstrating the general applicability of our approach to multiple types of stego images.steganalysis, steganography, deep learning, generalized steganalysis model"
89,"딥러닝 프레임워크의 비교: 티아노, 텐서플로, CNTK를 중심으로",정여진 /Yeojin Chung,2017,한국지능정보시스템학회,"딥러닝 프레임워크의 대표적인 기능으로는 ‘자동미분’과 ‘GPU의 활용’ 등을 들 수 있다. 본 논문은 파이썬의 라이브러리 형태로 사용 가능한 프레임워크 중에서 구글의 텐서플로와 마이크로소프트의 CNTK, 그리고 텐서 플로의 원조라고 할 수 있는 티아노를 비교하였다. 본문에서는 자동미분의 개념과 GPU의 활용형태를 간단히 설명하고, 그 다음에 logistic regression을 실행하는 예를 통하여 각 프레임워크의 문법을 알아본 뒤에, 마지막으로 대표적인 딥러닝 응용인 CNN의 예제를 실행시켜보고 코딩의 편의성과 실행속도 등을 확인해 보았다. 그 결과, 편의성의 관점에서 보면 티아노가 가장 코딩 하기가 어렵고, CNTK와 텐서플로는 많은 부분이 비슷하게 추상화 되어 있어서 코딩이 비슷하지만 가중치와 편향을 직접 정의하느냐의 여부에서 차이를 보였다. 그리고 각프레임워크의 실행속도에 대한 평가는 ‘큰 차이는 없다’는 것이다. 텐서플로는 티아노에 비하여 속도가 느리다는 평가가 있어왔는데, 본 연구의 실험에 의하면, 비록 CNN 모형에 국한되었지만, 텐서플로가 아주 조금이지만빠른 것으로 나타났다. CNTK의 경우에도, 비록 실험환경이 달랐지만, 실험환경의 차이에 의한 속도의 차이의 편차범위 이내에 있는 것으로 판단이 되었다. 본 연구에서는 세 종류의 딥러닝 프레임워크만을 살펴보았는데,위키피디아에 따르면 딥러닝 프레임워크의 종류는 12가지가 있으며, 각 프레임워크의 특징을 15가지 속성으로구분하여 차이를 특정하고 있다. 그 많은 속성 중에서 사용자의 입장에서 볼 때 중요한 속성은 어떤 언어(파이썬, C++, Java, 등)로 사용가능한지, 어떤 딥러닝 모형에 대한 라이브러리가 잘 구현되어 있는지 등일 것이다. 그리고 사용자가 대규모의 딥러닝 모형을 구축한다면, 다중 GPU 혹은 다중 서버를 지원하는지의 여부도 중요할 것이다. 또한 딥러닝 모형을 처음 학습하는 경우에는 사용설명서가 많은지 예제 프로그램이 많은지 여부도 중요한 기준이 될 것이다.The deep learning framework is software designed to help develop deep learning models. Some of its important functions include “automatic differentiation” and “utilization of GPU”. The list of popular deep learning framework includes Caffe (BVLC) and Theano (University of Montreal). And recently, Microsoft's deep learning framework, Microsoft Cognitive Toolkit, was released as open-source license, following Google’s Tensorflow a year earlier. The early deep learning frameworks have been developed mainly for research at universities. Beginning with the inception of Tensorflow, however, it seems that companies such as Microsoft and Facebook have started to join the competition of framework development. Given the trend, Google and other companies are expected to continue investing in the deep learning framework to bring forward the initiative in the artificial intelligence business. From this point of view, we think it is a good time to compare some of deep learning frameworks. So we compare three deep learning frameworks which can be used as a Python library. Those are Google's Tensorflow, Microsoft’s CNTK, and Theano which is sort of a predecessor of the preceding two.
The most common and important function of deep learning frameworks is the ability to perform automatic differentiation. Basically all the mathematical expressions of deep learning models can be represented as computational graphs, which consist of nodes and edges. Partial derivatives on each edge of a computational graph can then be obtained. With the partial derivatives, we can let software compute differentiation of any node with respect to any variable by utilizing chain rule of Calculus.
First of all, the convenience of coding is in the order of CNTK, Tensorflow, and Theano. The criterion is simply based on the lengths of the codes and the learning curve and the ease of coding are not the main concern. According to the criteria, Theano was the most difficult to implement with, and CNTK and Tensorflow were somewhat easier. With Tensorflow, we need to define weight variables and biases explicitly. The reason that CNTK and Tensorflow are easier to implement with is that those frameworks provide us with more abstraction than Theano. We, however, need to mention that low-level coding is not always bad. It gives us flexibility of coding. With the low-level coding such as in Theano, we can implement and test any new deep learning models or any new search methods that we can think of.
The assessment of the execution speed of each framework is that there is not meaningful difference. According to the experiment, execution speeds of Theano and Tensorflow are very similar, although the experiment was limited to a CNN model. In the case of CNTK, the experimental environment was not maintained as the same. The code written in CNTK has to be run in PC environment without GPU where codes execute as much as 50 times slower than with GPU. But we concluded that the difference of execution speed was within the range of variation caused by the different hardware setup.
In this study, we compared three types of deep learning framework: Theano, Tensorflow, and CNTK. According to Wikipedia, there are 12 available deep learning frameworks. And 15 different attributes differentiate each framework. Some of the important attributes would include interface language (Python, C ++, Java, etc.) and the availability of libraries on various deep learning models such as CNN, RNN, DBN, and etc. And if a user implements a large scale deep learning model, it will also be important to support multiple GPU or multiple servers. Also, if you are learning the deep learning model, it would also be important if there are enough examples and references.deep learning framework, Theano, TensorFlow, CNTK, computational graph, CIFAR-10"
90,딥러닝을 이용한 주변 무선단말 파악방안,이웅섭 /Lee， Woong-sup,2018,한국정보통신학회,"최근 단말-대-단말(Device-to-device, D2D) 통신기술이 차세대 무선통신시스템의 핵심기술로 큰 관심을 받고 있다. 이러한 단말간 통신에서는 자신의 주변에 어떠한 단말이 있는지 파악하는 주변단말 탐색(Neighbor discovery)이매우 중요하다. 본 논문에서는 최근 큰 관심을 받고 있는 딥러닝(Deep learning) 기술을 활용하여 단말간 통신에서 주변단말을 파악하는 방안에 대해서 제안한다. 제안 방안은 기존의 방안과 달리 무선채널의 공간적 연관성을 이용하여단말간의 신호 전송 없이 단말이 기지국으로 전송하는 상향링크 파일럿 신호를 기반으로 주변 단말을 찾고 따라서기존의 방식에 비해 신호전송 복잡도(signaling complexity)를 크게 줄일 수 있다. 또한 제안 방안에서는 떨어져 있는거리에 따라서 주변 단말을 분류 가능하여 기존 방안에 비해서 좀 더 세밀한 단말 탐색이 가능하다. 마지막으로 본 논문에서는 tensorflow를 이용한 컴퓨터 시뮬레이션을 통해 제안 방안의 성능을 검증하였다.Recently, the device-to-device (D2D) communication has been conceived as the key technology for the next-generationmobile communication systems. The neighbor discovery in which the nearby users are found, is essential for the properoperation of the D2D communication. In this paper, we propose new neighbor discovery scheme based on deep learningtechnology which has gained a lot of attention recently. In the proposed scheme, the neighboring users can be foundusing the uplink pilot transmission of users only, unlike conventional neighbor discovery schemes in which direct pilotcommunication among users is required, such that the signaling overhead can be greatly reduced in our proposed scheme.
Moreover, the neighbors with different proximity can also be classified accordingly which enables more accurate neighbordiscovery compared to the conventional schemes. The performance of our proposed scheme is verified through thetensorflow-based computer simulations.
키워드 : 딥러닝, 주변단말 탐색, 단말-대-단말 통신Deep learning, Neighbor discovery, Device-to-device communication, Wireless channel correlation"
91,신뢰성있는 딥러닝 기반 분석 모델을 참조하기 위한 딥러닝 기술 언어,문종혁 /Jong Hyeok Mun,2021,한국정보처리학회,"최근 딥러닝은 하드웨어 성능이 향상됨에 따라 자연어 처리, 영상 인식 등의 다양한 기술에 접목되어 활용되고 있다. 이러한 기술들을 활용해 지능형교통 시스템(ITS), 스마트홈, 헬스케어 등의 산업분야에서 데이터를 분석하여 고속도로 속도위반 차량 검출, 에너지 사용량 제어, 응급상황 등과 같은고품질의 서비스를 제공하며, 고품질의 서비스를 제공하기 위해서는 정확도가 향상된 딥러닝 모델이 적용되어야 한다. 이를 위해 서비스 환경의 데이터를분석하기 위한 딥러닝 모델을 개발할 때, 개발자는 신뢰성이 검증된 최신의 딥러닝 모델을 적용할 수 있어야 한다. 이는 개발자가 참조하는 딥러닝 모델에적용된 학습 데이터셋의 정확도를 측정하여 검증할 수 있다. 이러한 검증을 위해서 개발자는 학습 데이터셋, 딥러닝의 계층구조 및 개발 환경 등과 같은내용을 포함하는 딥러닝 모델을 문서화하여 적용하기 위한 구조적인 정보가 필요하다. 본 논문에서는 신뢰성있는 딥러닝 기반 데이터 분석 모델을 참조하기위한 딥러닝 기술 언어를 제안한다. 제안하는 기술 언어는 신뢰성 있는 딥러닝 모델을 개발하는데 필요한 학습데이터셋, 개발 환경 및 설정 등의 정보와더불어 딥러닝 모델의 계층구조를 표현할 수 있다. 제안하는 딥러닝 기술 언어를 이용하여 개발자는 지능형 교통 시스템에서 참조하는 분석 모델의정확도를 검증할 수 있다. 실험에서는 제안하는 언어의 유효성을 검증하기 위해, 번호판 인식 모델을 중심으로 딥러닝 기술 문서의 적용과정을 보인다With the recent advancements of deep learning, companies such as smart home, healthcare, and intelligent transportation systems areutilizing its functionality to provide high-quality services for vehicle detection, emergency situation detection, and controlling energyconsumption. To provide reliable services in such sensitive systems, deep learning models are required to have high accuracy. In orderto develop a deep learning model for analyzing previously mentioned services, developers should utilize the state of the art deep learningmodels that have already been verified for higher accuracy. The developers can verify the accuracy of the referenced model by validatingthe model on the dataset. For this validation, the developer needs structural information to document and apply deep learning models,including metadata such as learning dataset, network architecture, and development environments. In this paper, we propose a descriptionlanguage that represents the network architecture of the deep learning model along with its metadata that are necessary to develop adeep learning model. Through the proposed description language, developers can easily verify the accuracy of the referenced deep learningmodel. Our experiments demonstrate the application scenario of a deep learning description document that focuses on the license platerecognition for the detection of illegally parked vehicles.Trusted Deep Learning, Model Reference, Deep Learning Description Language, Traffic Situation Analysis Model"
92,딥러닝 분석을 이용한 중국 역내·외 위안화 변동성 예측,이우식 /Lee Woosik,2016,한국데이터정보과학회,"2008년 글로벌 금융위기 이후 중국은 위안화 국제화의 점진적 추진을 진행하면서 중국상하이 외환시장과 중국홍콩 외환시장에서 거래되는 통화인 역내위안화와 역외위안화를 형성시켰다. 본 연구는 위안화 국제화와 점진적인 중국 자본계정 개방에 따라 급변하는 외환시장상황의 변동성을 정확하게 파악하기 위해서 GARCH모형 (일반화된 자기회귀 조건부이분산성모형)에 다단계인공신경망을 결합한 MLP-GARCH 모형과 GARCH모형과 기계학습의 일종인 딥러닝 (deep learning)을 통합한 DL-GARCH을 가지고 위안화 변동성예측을 비교 실험과 분석을 하였다. 비교분석 결과 DLGARCH 모형은 MLP-GARCH보다 모형 위안화 역내·외 환율변동성 예측 면에서 더욱 더 개선된예측값을 제공하였다. 그래서 이분산시계열모형을 딥러닝과 결합한 DL-GARCH 모형은 시계열의환율 변동성 예측 문제에 딥러닝을 응용할 수 있음을 확인하였다. 향후 이분산시계열과 결합된 딥러닝 모형은 다른 금융시계열 데이터에 응용하여 그 일반화 가능성을 높일 수 있을 것이다.The People’s Republic of China has vigorously been pursuing the internationalization of the Chinese Yuan or Renminbi after the financial crisis of 2008. In this view, an abrupt increase of use of the Chinese Yuan in the onshore and offshore markets are important milestones to be one of important currencies. One of the most frequently used methods to forecast volatility is GARCH model. Since a prediction error of the GARCH model has been reported quite high, a lot of efforts have been made to improve forecasting capability of the GARCH model. In this paper, we have proposed MLPGARCH and a DL-GARCH by employing Artificial Neural Network to the GARCH. In an application to forecasting Chinese Yuan volatility, we have successfully shown their overall outperformance in forecasting over the GARCH.Deep learning, DL-GARCH, GARCH, The Chinese Yuan, volatility."
93,딥러닝 기법을 이용한 내일강수 예측,하지훈,2016,한국지능시스템학회,"정확한 강수예측을 위해서는 예측인자 선정과 예측방법에 대한 선택이 매우 중요하다. 최근에는 강수예측 방법으로기계학습 기법이 많이 사용되고 있으며, 그 중에서도 특히 인공신경망을 사용한 강수예측 방법은 좋은 성능을 보였다. 본논문에서는 딥러닝 기법 중 하나인 DBN(deep belief network)를 이용한 새로운 강수예측 방법을 제안한다. DBN는 비지도사전 학습을 통해 초기 가중치를 설정하여 기존 인공신경망의 문제점을 보완한다. 예측인자로는 기온, 전일-전주 강수일, 태양과 달 궤도 관련 자료를 선정하였다. 기온과 전일-전주 강수일은 서울에서의 1974년부터 2013년까지 총 40년간의AWS(automatic weather system) 관측 자료를 사용하였고, 태양과 달의 궤도 관련 자료는 서울을 중심으로 계산한 결과를사용하였다. 전체 기간에서 일부는 학습 자료로 사용하여 예측모델을 생성하였고, 나머지를 생성한 모델의 검증 자료로사용하였다. 모델 검증 결과로 나온 예측값들은 확률값을 가지며 임계치를 이용하여 강수유무를 판별하였다. 강수 정확도의척도로 양분예보기법 중 CSI(critical successive index)와 Bias(frequency bias)를 계산하였다. 이를 통해 DBN와 MLP(multilayer perceptron)의 성능을 비교한 결과 DBN의 강수 예측 정확도가 높았고, 수행속도 또한 2배 이상 빨랐다.For accurate precipitation forecasts the choice of weather faoctrs and prediction method is very important. Recently, machinele arning has been widely used for forecasting precipitation, and artificial neural network, one of machine learning techniques, showed good performance. In this paper, we suggest a new method for forecasting precipitation using DBN, one of deep learning techniques.
DBN has an advantage that initial weights are set by unsupervised learning, so this compensates for the defects of artificial neural networks. We used past precipitation, temperature, and the parameters of the sun and moon's motion as features for forecasting precipitation. The dataset consists of observation data which had been measured for 40 years from AWS in Seoul. Experiments were based on 8-fold cross validation. As a result of estimation, we got probabilities of test dataset, so threshold was used for the decision of precipitation. CSI and Bias were used for indicating the precision of precipitation. Our experimental results showed that DBN performed better than MLP.Deep learning, Deep belief network, Precipitation, Forecast"
94,딥러닝 신경망모형을 이용한 실시간 교통정보수집 연구,김대현 /KIM DAE HYON,2016,사단법인 인문사회과학기술융합학회,"비디오 기반의 실시간 교통정보수집 시스템은 자율주행을 포함한 지능형교통체계의 주요 서브시스템중 하나이며, 우수한 예측(추정)력 때문에 핵심 알고리즘으로 인공신경망 모형이 주로 사용되고 있다. 특히, 최근 과학 학술지 네이처에 구글 딥마인드의 알파고가 공개되면서 딥러닝과 인공지능에 대한 전 세계적인 관심이 집중되고 있다. 최근까지 가장 널리 사용된 인공신경망모형은 역전파 모형이었으나 최근 컴퓨터 하드웨어 및 알고리즘의 새로운 개발과 더불어 딥러닝 알고리즘을 이용한 딥인공신경망 모형에 대한 관심이 높아지고 있다. 그러나 아직 딥러닝 인공신경망에 대한 연구는 초기단계에 있는 실정이며 적용분야도 극히 한정되어 있다. 본 연구에서는 실시간 도로교통정보 수집을 위한 영상처리분야에서 딥러닝 알고리즘을 활용하고 실험결과를 통하여 향후 교통정보수집‧분석 분야에서의 딥러닝 인공신경망 모형의 적용가능성을 살펴보고자 한다. 특히, 본 연구에서 도출한 딥러닝 인공신경망모형의 실험결과를 기존 역전파 모형에 의한 실험결과와 비교하여 모형의 우수성을 비교‧평가하고자 한다.Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Normally, artificial neural networks have been used in real-time applications for collecting traffic information because of their remarkable performance in terms of predictive accuracy and real-time computing. Especially, the recent publication of a paper in the journal Nature and the success of Google DeepMind’s world-beating Go program AlphaGo caused significant increases of the public interest in artificial intelligence. Even though various types of learning models have been proposed, the Backpropagation has been the most popular neural network model. However, the Deep learning neural network model has attracted much interest recently. In this study, the Deep leaning neural network will be applied in image processing techniques for the real-time traffic information collection to investigate the performance of Deep learning algorithm in traffic engineering fields. In addition, the experimental results from Deep learing algorithm will be compared with the results from Backpropagation model to evaluate and propose better performing model.Intelligent Transportation Systems (ITS), artificial neural networks, Deep learning, traffic information, image processing"
95,딥러닝의 모형과 응용사례,안성만 /SungMahn Ahn,2016,한국지능정보시스템학회,"딥러닝은 인공신경망(neural network)이라는 인공지능분야의 모형이 발전된 형태로서, 계층구조로 이루어진 인공신경망의 내부계층(hidden layer)이 여러 단계로 이루어진 구조이다. 딥러닝에서의 주요 모형은 합성곱신경망(convolutional neural network), 순환신경망(recurrent neural network), 그리고 심층신뢰신경망(deep belief network)의 세가지라고 할 수있다. 그 중에서 현재 흥미로운 연구가 많이 발표되어서 관심이 집중되고 있는 모형은 지도학습(supervised learning)모형인 처음 두 개의 모형이다. 따라서 본 논문에서는 지도학습모형의 가중치를 최적화하는 기본적인 방법인 오류역전파 알고리즘을 살펴본 뒤에 합성곱신경망과 순환신경망의 구조와 응용사례 등을 살펴보고자 한다. 본문에서 다루지 않은 모형인 심층신뢰신경망은 아직까지는 합성곱신경망 이나 순환신경망보다는 상대적으로 주목을 덜 받고 있다. 그러나 심층신뢰신경망은 CNN이나 RNN과는 달리 비지도학습(unsupervised learning)모형이며, 사람이나 동물은 관찰을 통해서 스스로 학습한다는 점에서 궁극적으로는 비지도학습모형이 더 많이 연구되어야 할 주제가 될 것이다.Deep learning model is a kind of neural networks that allows multiple hidden layers. There are various deep learning architectures such as convolutional neural networks, deep belief networks and recurrent neural networks. Those have been applied to fields like computer vision, automatic speech recognition, natural language processing, audio recognition and bioinformatics where they have been shown to produce state-of-the-art results on various tasks. Among those architectures, convolutional neural networks and recurrent neural networks are classified as the supervised learning model. And in recent years, those supervised learning models have gained more popularity than unsupervised learning models such as deep belief networks, because supervised learning models have shown fashionable applications in such fields mentioned above.
Deep learning models can be trained with backpropagation algorithm. Backpropagation is an abbreviation for “backward propagation of errors” and a common method of training artificial neural networks used in conjunction with an optimization method such as gradient descent. The method calculates the gradient of an error function with respect to all the weights in the network. The gradient is fed to the optimization method which in turn uses it to update the weights, in an attempt to minimize the error function.
Convolutional neural networks use a special architecture which is particularly well-adapted to classify images. Using this architecture makes convolutional networks fast to train. This, in turn, helps us train deep, muti-layer networks, which are very good at classifying images. These days, deep convolutional networks are used in most neural networks for image recognition. Convolutional neural networks use three basic ideas: local receptive fields, shared weights, and pooling. By local receptive fields, we mean that each neuron in the first(or any) hidden layer will be connected to a small region of the input(or previous layer’s) neurons. Shared weights mean that we’re going to use the same weights and bias for each of the local receptive field. This means that all the neurons in the hidden layer detect exactly the same feature, just at different locations in the input image. In addition to the convolutional layers just described, convolutional neural networks also contain pooling layers. Pooling layers are usually used immediately after convolutional layers. What the pooling layers do is to simplify the information in the output from the convolutional layer.
Recent convolutional network architectures have 10 to 20 hidden layers and billions of connections between units. Training deep learning networks has taken weeks several years ago, but thanks to progress in GPU and algorithm enhancement, training time has reduced to several hours.
Neural networks with time-varying behavior are known as recurrent neural networks or RNNs. A recurrent neural network is a class of artificial neural network where connections between units form a directed cycle. This creates an internal state of the network which allows it to exhibit dynamic temporal behavior. Unlike feedforward neural networks, RNNs can use their internal memory to process arbitrary sequences of inputs. Early RNN models turned out to be very difficult to train, harder even than deep feedforward networks. The reason is the unstable gradient problem such as vanishing gradient and exploding gradient. The gradient can get smaller and smaller as it is propagated back through layers. This makes learning in early layers extremely slow. The problem actually gets worse in RNNs, since gradients aren’t just propagated backward through layers, they’re propagated backward through time. If the network runs for a long time, that can make the gradient extremely unstable and hard to learn from. It has been possible to incorporate an idea known as long short-term memory units (LSTMs) into RNNs. LSTMs make it much easier to get good results when training RNNs, and many recent papers make use of LSTMs or related ideas.Deep learning, Convolutional neural networks, Recurrent neural networks, Error backpropagation algorithm"
96,딥러닝분석과 기술적 분석 지표를 이용한 한국 코스피주가지수 방향성 예측,이우식 /Lee Woosik,2017,한국데이터정보과학회,"2016년 3월 구글 (Google)의 바둑인공지능 알파고 (AlphaGo)가 이세돌 9단과의 바둑대결에서승리한 이후 다양한 분야에서 인공지능 사용에 대한 관심이 높아지고 있는 가운데 금융투자 분야에서도 인공지능과 투자자문 전문가의 합성어인 로보어드바이저 (Robo-Advisor)에 대한 관심이 높아지고 있다. 인공지능 (artificial intelligence)기반의 의사결정은 비용 절감은 물론 효과적인 의사결정을가능하게 한다는 점에서 큰 장점이 있다. 본 연구에서는 기술적 분석 (technical analysis) 지표와 딥러닝 (deep learning) 모형을 결합하여 한국 코스피 지수를 예측하는 모형을 개발하고 제시한 모형들의 예측력을 비교, 분석한다. 분석 결과 기술적 분석 지표에 딥러닝 알고리즘을 결합한 모형이 주가지수 방향성 예측 문제에 응용될 수 있음을 확인하였다. 향후 본 연구에서 제안된 기술적 분석 지표와딥러닝모형을 결합한 기법은 로보어드바이저서비스에 응용할 수 있는 일반화 가능성을 보여준다Since Google’s AlphaGo defeated a world champion of Go players in 2016, there have been many interests in the deep learning. In the financial sector, a Robo-Advisor using deep learning gains a significant attention, which builds and manages portfolios of financial instruments for investors.In this paper, we have proposed the a deep learning algorithm geared toward identification and forecast of the KOSPI index direction,and we also have compared the accuracy of the prediction.In an application of forecasting the financial market index direction, we have shown that the Robo-Advisor using deep learning has a significant effect on finance industry. The Robo-Advisor collects a massive data such as earnings statements, news reports and regulatory filings, analyzes those and recommends investors how to view market trends and identify the best time to purchase financial assets. On the other hand, the Robo-Advisor allows businesses to learn more about their customers, develop better marketing strategies, increase sales and decrease costs.Artificial intelligence, deep learning, FinTech, Robo-Advisor, technical analysis."
97,딥러닝의 효율적인 학습을 위한 학습데이터 선별 기법에 관한 연구,주경돈 /Gyoung-don Joo,2017,한국정보과학회,"빅데이터의 증가와 병렬 하드웨어 처리, 학습 알고리즘의 개선은 딥러닝의 발전을 가져왔으며 인공지능이 적용 가능한 영역을 획기적으로 확장시키고 있다. 하지만 이러한 대규모의 학습 데이터에 기반한 딥러닝은 학습시간이 증가되고, 더 나아가 학습 데이터의 구축을 위한 레이블링(labeling) 비용이 데이터 수에 비례하여 증가하게 된다. 따라서 본 논문에서는, 딥러닝의 정확도를 최대한 유지하면서 필요한 학습 데이터의 수를 줄일수 있도록, 전체 데이터에서 제한된 수의 학습 데이터의 후보를 선별하는 기법에 대하여 연구한다. 특히 기존의 능동 학습에 기반한 기법들이 학습 데이터의 수가 매우 적어질 경우 딥러닝의 정확도가 급격히 떨어지는 문제를 해결할 수 있는 새로운 기법을 제안하였으며, 제안된 기법으로 선별된 학습 데이터는 그 수의 변화에 대해 딥러닝이 일관되게 경쟁력있는 정확도를 가짐을 다양한 실험을 통하여 검증하였다.The increase of big data, parallel hardware processing and enhanced learning algorithms have led to the development of deep learning and are dramatically expanding the domain of applications based on artificial intelligence. However, such large-scale training data extends the learning time, and furthermore, the labeling cost for building the training set increases in proportion to the volume of data. Therefore, in this paper, we study how to select a limited number of candidates for training set from given data so as to reduce the number of training set while maintaining the accuracy of deep learning. Especially, we found that, with the training set selected by the previous work, the accuracy of deep learning drastically decreases when the number of training set is very small and so proposed new methods to resolve this fall-off in accuracy. Through experiments, we showed that deep learning consistently has the competitive accuracy with the training set selected by the proposed method regardless of the size of training set.Deep Learning, Train Set Selection, Active Learning, Gamma distribution"
98,딥러닝을 이용한 범용적 스테그아날리시스,김현재 /Hyunjae Kim,2017,한국정보과학회,"스테그아날리시스(Steganalysis)란 이미지 등 일반적인 자료에 암호화된 정보를 은닉하는 스테가노그래피(Steganography)에 대한 검출 및 분석 방법으로, 기계학습 기반 방법론을 포함한다. 기존 기계학습 기반 스테그아날리시스는 영상(Image)의 특징(Feature) 추출 및 모델링에 기반하며, 최근 딥러닝(Deep Learning)의 적용으로 검출 정확도가 큰 폭으로 향상되었다. 하지만 현존하는 스테그아날리시스 모델은 단일 스테가노그래피 기법에 대해 국한되어 있어 학습에 사용되지 않은 스테고(Stego) 이미지의 경우 검출이 불가능한 결정적 한계를 가진다. 본 연구에서는 다양한 스테가노그래피 기법으로 생성된 스테고이미지에 딥러닝을 적용하여 스테그아날리시스를 학습하는 범용적 모델을 제안한다. 다양한 실험을 통해 제안 기법의 효용성 및 가능성을 확인하고, 범용적 스테그아날리시스 모델이 각각에 특화된 검출 기법과 유사한 정확도로 스테고 이미지를 검출할 수 있음을 보인다.Steganalysis is to detect information hidden by steganography inside general data such as images. There are stegoanalysis techniques that use machine learning (ML). Existing ML approaches to steganalysis are based on extracting features from stego images and modeling them.
Recently deep learning-based methodologies have shown significant improvements in detection accuracy. However, all the existing methods, including deep learning-based ones, have a critical limitation in that they can only detect stego images that are created by a specific steganography method. In this paper, we propose a generalized steganalysis method that can model multiple types of stego images using deep learning. Through various experiments, we confirm the effectiveness of our approach and envision directions for future research. In particular, we show that our method can detect each type of steganography with the same level of accuracy as that of a steganalysis method dedicated to that type of steganography, thereby demonstrating the general applicability of our approach to multiple types of stego images.steganalysis, steganography, deep learning, generalized steganalysis model"
99,"딥러닝 프레임워크의 비교: 티아노, 텐서플로, CNTK를 중심으로",정여진 /Yeojin Chung,2017,한국지능정보시스템학회,"딥러닝 프레임워크의 대표적인 기능으로는 ‘자동미분’과 ‘GPU의 활용’ 등을 들 수 있다. 본 논문은 파이썬의 라이브러리 형태로 사용 가능한 프레임워크 중에서 구글의 텐서플로와 마이크로소프트의 CNTK, 그리고 텐서 플로의 원조라고 할 수 있는 티아노를 비교하였다. 본문에서는 자동미분의 개념과 GPU의 활용형태를 간단히 설명하고, 그 다음에 logistic regression을 실행하는 예를 통하여 각 프레임워크의 문법을 알아본 뒤에, 마지막으로 대표적인 딥러닝 응용인 CNN의 예제를 실행시켜보고 코딩의 편의성과 실행속도 등을 확인해 보았다. 그 결과, 편의성의 관점에서 보면 티아노가 가장 코딩 하기가 어렵고, CNTK와 텐서플로는 많은 부분이 비슷하게 추상화 되어 있어서 코딩이 비슷하지만 가중치와 편향을 직접 정의하느냐의 여부에서 차이를 보였다. 그리고 각프레임워크의 실행속도에 대한 평가는 ‘큰 차이는 없다’는 것이다. 텐서플로는 티아노에 비하여 속도가 느리다는 평가가 있어왔는데, 본 연구의 실험에 의하면, 비록 CNN 모형에 국한되었지만, 텐서플로가 아주 조금이지만빠른 것으로 나타났다. CNTK의 경우에도, 비록 실험환경이 달랐지만, 실험환경의 차이에 의한 속도의 차이의 편차범위 이내에 있는 것으로 판단이 되었다. 본 연구에서는 세 종류의 딥러닝 프레임워크만을 살펴보았는데,위키피디아에 따르면 딥러닝 프레임워크의 종류는 12가지가 있으며, 각 프레임워크의 특징을 15가지 속성으로구분하여 차이를 특정하고 있다. 그 많은 속성 중에서 사용자의 입장에서 볼 때 중요한 속성은 어떤 언어(파이썬, C++, Java, 등)로 사용가능한지, 어떤 딥러닝 모형에 대한 라이브러리가 잘 구현되어 있는지 등일 것이다. 그리고 사용자가 대규모의 딥러닝 모형을 구축한다면, 다중 GPU 혹은 다중 서버를 지원하는지의 여부도 중요할 것이다. 또한 딥러닝 모형을 처음 학습하는 경우에는 사용설명서가 많은지 예제 프로그램이 많은지 여부도 중요한 기준이 될 것이다.The deep learning framework is software designed to help develop deep learning models. Some of its important functions include “automatic differentiation” and “utilization of GPU”. The list of popular deep learning framework includes Caffe (BVLC) and Theano (University of Montreal). And recently, Microsoft's deep learning framework, Microsoft Cognitive Toolkit, was released as open-source license, following Google’s Tensorflow a year earlier. The early deep learning frameworks have been developed mainly for research at universities. Beginning with the inception of Tensorflow, however, it seems that companies such as Microsoft and Facebook have started to join the competition of framework development. Given the trend, Google and other companies are expected to continue investing in the deep learning framework to bring forward the initiative in the artificial intelligence business. From this point of view, we think it is a good time to compare some of deep learning frameworks. So we compare three deep learning frameworks which can be used as a Python library. Those are Google's Tensorflow, Microsoft’s CNTK, and Theano which is sort of a predecessor of the preceding two.
The most common and important function of deep learning frameworks is the ability to perform automatic differentiation. Basically all the mathematical expressions of deep learning models can be represented as computational graphs, which consist of nodes and edges. Partial derivatives on each edge of a computational graph can then be obtained. With the partial derivatives, we can let software compute differentiation of any node with respect to any variable by utilizing chain rule of Calculus.
First of all, the convenience of coding is in the order of CNTK, Tensorflow, and Theano. The criterion is simply based on the lengths of the codes and the learning curve and the ease of coding are not the main concern. According to the criteria, Theano was the most difficult to implement with, and CNTK and Tensorflow were somewhat easier. With Tensorflow, we need to define weight variables and biases explicitly. The reason that CNTK and Tensorflow are easier to implement with is that those frameworks provide us with more abstraction than Theano. We, however, need to mention that low-level coding is not always bad. It gives us flexibility of coding. With the low-level coding such as in Theano, we can implement and test any new deep learning models or any new search methods that we can think of.
The assessment of the execution speed of each framework is that there is not meaningful difference. According to the experiment, execution speeds of Theano and Tensorflow are very similar, although the experiment was limited to a CNN model. In the case of CNTK, the experimental environment was not maintained as the same. The code written in CNTK has to be run in PC environment without GPU where codes execute as much as 50 times slower than with GPU. But we concluded that the difference of execution speed was within the range of variation caused by the different hardware setup.
In this study, we compared three types of deep learning framework: Theano, Tensorflow, and CNTK. According to Wikipedia, there are 12 available deep learning frameworks. And 15 different attributes differentiate each framework. Some of the important attributes would include interface language (Python, C ++, Java, etc.) and the availability of libraries on various deep learning models such as CNN, RNN, DBN, and etc. And if a user implements a large scale deep learning model, it will also be important to support multiple GPU or multiple servers. Also, if you are learning the deep learning model, it would also be important if there are enough examples and references.deep learning framework, Theano, TensorFlow, CNTK, computational graph, CIFAR-10"
100,딥러닝을 이용한 주변 무선단말 파악방안,이웅섭 /Lee， Woong-sup,2018,한국정보통신학회,"최근 단말-대-단말(Device-to-device, D2D) 통신기술이 차세대 무선통신시스템의 핵심기술로 큰 관심을 받고 있다. 이러한 단말간 통신에서는 자신의 주변에 어떠한 단말이 있는지 파악하는 주변단말 탐색(Neighbor discovery)이매우 중요하다. 본 논문에서는 최근 큰 관심을 받고 있는 딥러닝(Deep learning) 기술을 활용하여 단말간 통신에서 주변단말을 파악하는 방안에 대해서 제안한다. 제안 방안은 기존의 방안과 달리 무선채널의 공간적 연관성을 이용하여단말간의 신호 전송 없이 단말이 기지국으로 전송하는 상향링크 파일럿 신호를 기반으로 주변 단말을 찾고 따라서기존의 방식에 비해 신호전송 복잡도(signaling complexity)를 크게 줄일 수 있다. 또한 제안 방안에서는 떨어져 있는거리에 따라서 주변 단말을 분류 가능하여 기존 방안에 비해서 좀 더 세밀한 단말 탐색이 가능하다. 마지막으로 본 논문에서는 tensorflow를 이용한 컴퓨터 시뮬레이션을 통해 제안 방안의 성능을 검증하였다.Recently, the device-to-device (D2D) communication has been conceived as the key technology for the next-generationmobile communication systems. The neighbor discovery in which the nearby users are found, is essential for the properoperation of the D2D communication. In this paper, we propose new neighbor discovery scheme based on deep learningtechnology which has gained a lot of attention recently. In the proposed scheme, the neighboring users can be foundusing the uplink pilot transmission of users only, unlike conventional neighbor discovery schemes in which direct pilotcommunication among users is required, such that the signaling overhead can be greatly reduced in our proposed scheme.
Moreover, the neighbors with different proximity can also be classified accordingly which enables more accurate neighbordiscovery compared to the conventional schemes. The performance of our proposed scheme is verified through thetensorflow-based computer simulations.
키워드 : 딥러닝, 주변단말 탐색, 단말-대-단말 통신Deep learning, Neighbor discovery, Device-to-device communication, Wireless channel correlation"
101,신뢰성있는 딥러닝 기반 분석 모델을 참조하기 위한 딥러닝 기술 언어,문종혁 /Jong Hyeok Mun,2021,한국정보처리학회,"최근 딥러닝은 하드웨어 성능이 향상됨에 따라 자연어 처리, 영상 인식 등의 다양한 기술에 접목되어 활용되고 있다. 이러한 기술들을 활용해 지능형교통 시스템(ITS), 스마트홈, 헬스케어 등의 산업분야에서 데이터를 분석하여 고속도로 속도위반 차량 검출, 에너지 사용량 제어, 응급상황 등과 같은고품질의 서비스를 제공하며, 고품질의 서비스를 제공하기 위해서는 정확도가 향상된 딥러닝 모델이 적용되어야 한다. 이를 위해 서비스 환경의 데이터를분석하기 위한 딥러닝 모델을 개발할 때, 개발자는 신뢰성이 검증된 최신의 딥러닝 모델을 적용할 수 있어야 한다. 이는 개발자가 참조하는 딥러닝 모델에적용된 학습 데이터셋의 정확도를 측정하여 검증할 수 있다. 이러한 검증을 위해서 개발자는 학습 데이터셋, 딥러닝의 계층구조 및 개발 환경 등과 같은내용을 포함하는 딥러닝 모델을 문서화하여 적용하기 위한 구조적인 정보가 필요하다. 본 논문에서는 신뢰성있는 딥러닝 기반 데이터 분석 모델을 참조하기위한 딥러닝 기술 언어를 제안한다. 제안하는 기술 언어는 신뢰성 있는 딥러닝 모델을 개발하는데 필요한 학습데이터셋, 개발 환경 및 설정 등의 정보와더불어 딥러닝 모델의 계층구조를 표현할 수 있다. 제안하는 딥러닝 기술 언어를 이용하여 개발자는 지능형 교통 시스템에서 참조하는 분석 모델의정확도를 검증할 수 있다. 실험에서는 제안하는 언어의 유효성을 검증하기 위해, 번호판 인식 모델을 중심으로 딥러닝 기술 문서의 적용과정을 보인다With the recent advancements of deep learning, companies such as smart home, healthcare, and intelligent transportation systems areutilizing its functionality to provide high-quality services for vehicle detection, emergency situation detection, and controlling energyconsumption. To provide reliable services in such sensitive systems, deep learning models are required to have high accuracy. In orderto develop a deep learning model for analyzing previously mentioned services, developers should utilize the state of the art deep learningmodels that have already been verified for higher accuracy. The developers can verify the accuracy of the referenced model by validatingthe model on the dataset. For this validation, the developer needs structural information to document and apply deep learning models,including metadata such as learning dataset, network architecture, and development environments. In this paper, we propose a descriptionlanguage that represents the network architecture of the deep learning model along with its metadata that are necessary to develop adeep learning model. Through the proposed description language, developers can easily verify the accuracy of the referenced deep learningmodel. Our experiments demonstrate the application scenario of a deep learning description document that focuses on the license platerecognition for the detection of illegally parked vehicles.Trusted Deep Learning, Model Reference, Deep Learning Description Language, Traffic Situation Analysis Model"
102,딥러닝 분석을 이용한 중국 역내·외 위안화 변동성 예측,이우식 /Lee Woosik,2016,한국데이터정보과학회,"2008년 글로벌 금융위기 이후 중국은 위안화 국제화의 점진적 추진을 진행하면서 중국상하이 외환시장과 중국홍콩 외환시장에서 거래되는 통화인 역내위안화와 역외위안화를 형성시켰다. 본 연구는 위안화 국제화와 점진적인 중국 자본계정 개방에 따라 급변하는 외환시장상황의 변동성을 정확하게 파악하기 위해서 GARCH모형 (일반화된 자기회귀 조건부이분산성모형)에 다단계인공신경망을 결합한 MLP-GARCH 모형과 GARCH모형과 기계학습의 일종인 딥러닝 (deep learning)을 통합한 DL-GARCH을 가지고 위안화 변동성예측을 비교 실험과 분석을 하였다. 비교분석 결과 DLGARCH 모형은 MLP-GARCH보다 모형 위안화 역내·외 환율변동성 예측 면에서 더욱 더 개선된예측값을 제공하였다. 그래서 이분산시계열모형을 딥러닝과 결합한 DL-GARCH 모형은 시계열의환율 변동성 예측 문제에 딥러닝을 응용할 수 있음을 확인하였다. 향후 이분산시계열과 결합된 딥러닝 모형은 다른 금융시계열 데이터에 응용하여 그 일반화 가능성을 높일 수 있을 것이다.The People’s Republic of China has vigorously been pursuing the internationalization of the Chinese Yuan or Renminbi after the financial crisis of 2008. In this view, an abrupt increase of use of the Chinese Yuan in the onshore and offshore markets are important milestones to be one of important currencies. One of the most frequently used methods to forecast volatility is GARCH model. Since a prediction error of the GARCH model has been reported quite high, a lot of efforts have been made to improve forecasting capability of the GARCH model. In this paper, we have proposed MLPGARCH and a DL-GARCH by employing Artificial Neural Network to the GARCH. In an application to forecasting Chinese Yuan volatility, we have successfully shown their overall outperformance in forecasting over the GARCH.Deep learning, DL-GARCH, GARCH, The Chinese Yuan, volatility."
103,딥러닝 기법을 이용한 내일강수 예측,하지훈,2016,한국지능시스템학회,"정확한 강수예측을 위해서는 예측인자 선정과 예측방법에 대한 선택이 매우 중요하다. 최근에는 강수예측 방법으로기계학습 기법이 많이 사용되고 있으며, 그 중에서도 특히 인공신경망을 사용한 강수예측 방법은 좋은 성능을 보였다. 본논문에서는 딥러닝 기법 중 하나인 DBN(deep belief network)를 이용한 새로운 강수예측 방법을 제안한다. DBN는 비지도사전 학습을 통해 초기 가중치를 설정하여 기존 인공신경망의 문제점을 보완한다. 예측인자로는 기온, 전일-전주 강수일, 태양과 달 궤도 관련 자료를 선정하였다. 기온과 전일-전주 강수일은 서울에서의 1974년부터 2013년까지 총 40년간의AWS(automatic weather system) 관측 자료를 사용하였고, 태양과 달의 궤도 관련 자료는 서울을 중심으로 계산한 결과를사용하였다. 전체 기간에서 일부는 학습 자료로 사용하여 예측모델을 생성하였고, 나머지를 생성한 모델의 검증 자료로사용하였다. 모델 검증 결과로 나온 예측값들은 확률값을 가지며 임계치를 이용하여 강수유무를 판별하였다. 강수 정확도의척도로 양분예보기법 중 CSI(critical successive index)와 Bias(frequency bias)를 계산하였다. 이를 통해 DBN와 MLP(multilayer perceptron)의 성능을 비교한 결과 DBN의 강수 예측 정확도가 높았고, 수행속도 또한 2배 이상 빨랐다.For accurate precipitation forecasts the choice of weather faoctrs and prediction method is very important. Recently, machinele arning has been widely used for forecasting precipitation, and artificial neural network, one of machine learning techniques, showed good performance. In this paper, we suggest a new method for forecasting precipitation using DBN, one of deep learning techniques.
DBN has an advantage that initial weights are set by unsupervised learning, so this compensates for the defects of artificial neural networks. We used past precipitation, temperature, and the parameters of the sun and moon's motion as features for forecasting precipitation. The dataset consists of observation data which had been measured for 40 years from AWS in Seoul. Experiments were based on 8-fold cross validation. As a result of estimation, we got probabilities of test dataset, so threshold was used for the decision of precipitation. CSI and Bias were used for indicating the precision of precipitation. Our experimental results showed that DBN performed better than MLP.Deep learning, Deep belief network, Precipitation, Forecast"
104,딥러닝 신경망모형을 이용한 실시간 교통정보수집 연구,김대현 /KIM DAE HYON,2016,사단법인 인문사회과학기술융합학회,"비디오 기반의 실시간 교통정보수집 시스템은 자율주행을 포함한 지능형교통체계의 주요 서브시스템중 하나이며, 우수한 예측(추정)력 때문에 핵심 알고리즘으로 인공신경망 모형이 주로 사용되고 있다. 특히, 최근 과학 학술지 네이처에 구글 딥마인드의 알파고가 공개되면서 딥러닝과 인공지능에 대한 전 세계적인 관심이 집중되고 있다. 최근까지 가장 널리 사용된 인공신경망모형은 역전파 모형이었으나 최근 컴퓨터 하드웨어 및 알고리즘의 새로운 개발과 더불어 딥러닝 알고리즘을 이용한 딥인공신경망 모형에 대한 관심이 높아지고 있다. 그러나 아직 딥러닝 인공신경망에 대한 연구는 초기단계에 있는 실정이며 적용분야도 극히 한정되어 있다. 본 연구에서는 실시간 도로교통정보 수집을 위한 영상처리분야에서 딥러닝 알고리즘을 활용하고 실험결과를 통하여 향후 교통정보수집‧분석 분야에서의 딥러닝 인공신경망 모형의 적용가능성을 살펴보고자 한다. 특히, 본 연구에서 도출한 딥러닝 인공신경망모형의 실험결과를 기존 역전파 모형에 의한 실험결과와 비교하여 모형의 우수성을 비교‧평가하고자 한다.Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Normally, artificial neural networks have been used in real-time applications for collecting traffic information because of their remarkable performance in terms of predictive accuracy and real-time computing. Especially, the recent publication of a paper in the journal Nature and the success of Google DeepMind’s world-beating Go program AlphaGo caused significant increases of the public interest in artificial intelligence. Even though various types of learning models have been proposed, the Backpropagation has been the most popular neural network model. However, the Deep learning neural network model has attracted much interest recently. In this study, the Deep leaning neural network will be applied in image processing techniques for the real-time traffic information collection to investigate the performance of Deep learning algorithm in traffic engineering fields. In addition, the experimental results from Deep learing algorithm will be compared with the results from Backpropagation model to evaluate and propose better performing model.Intelligent Transportation Systems (ITS), artificial neural networks, Deep learning, traffic information, image processing"
105,딥러닝의 모형과 응용사례,안성만 /SungMahn Ahn,2016,한국지능정보시스템학회,"딥러닝은 인공신경망(neural network)이라는 인공지능분야의 모형이 발전된 형태로서, 계층구조로 이루어진 인공신경망의 내부계층(hidden layer)이 여러 단계로 이루어진 구조이다. 딥러닝에서의 주요 모형은 합성곱신경망(convolutional neural network), 순환신경망(recurrent neural network), 그리고 심층신뢰신경망(deep belief network)의 세가지라고 할 수있다. 그 중에서 현재 흥미로운 연구가 많이 발표되어서 관심이 집중되고 있는 모형은 지도학습(supervised learning)모형인 처음 두 개의 모형이다. 따라서 본 논문에서는 지도학습모형의 가중치를 최적화하는 기본적인 방법인 오류역전파 알고리즘을 살펴본 뒤에 합성곱신경망과 순환신경망의 구조와 응용사례 등을 살펴보고자 한다. 본문에서 다루지 않은 모형인 심층신뢰신경망은 아직까지는 합성곱신경망 이나 순환신경망보다는 상대적으로 주목을 덜 받고 있다. 그러나 심층신뢰신경망은 CNN이나 RNN과는 달리 비지도학습(unsupervised learning)모형이며, 사람이나 동물은 관찰을 통해서 스스로 학습한다는 점에서 궁극적으로는 비지도학습모형이 더 많이 연구되어야 할 주제가 될 것이다.Deep learning model is a kind of neural networks that allows multiple hidden layers. There are various deep learning architectures such as convolutional neural networks, deep belief networks and recurrent neural networks. Those have been applied to fields like computer vision, automatic speech recognition, natural language processing, audio recognition and bioinformatics where they have been shown to produce state-of-the-art results on various tasks. Among those architectures, convolutional neural networks and recurrent neural networks are classified as the supervised learning model. And in recent years, those supervised learning models have gained more popularity than unsupervised learning models such as deep belief networks, because supervised learning models have shown fashionable applications in such fields mentioned above.
Deep learning models can be trained with backpropagation algorithm. Backpropagation is an abbreviation for “backward propagation of errors” and a common method of training artificial neural networks used in conjunction with an optimization method such as gradient descent. The method calculates the gradient of an error function with respect to all the weights in the network. The gradient is fed to the optimization method which in turn uses it to update the weights, in an attempt to minimize the error function.
Convolutional neural networks use a special architecture which is particularly well-adapted to classify images. Using this architecture makes convolutional networks fast to train. This, in turn, helps us train deep, muti-layer networks, which are very good at classifying images. These days, deep convolutional networks are used in most neural networks for image recognition. Convolutional neural networks use three basic ideas: local receptive fields, shared weights, and pooling. By local receptive fields, we mean that each neuron in the first(or any) hidden layer will be connected to a small region of the input(or previous layer’s) neurons. Shared weights mean that we’re going to use the same weights and bias for each of the local receptive field. This means that all the neurons in the hidden layer detect exactly the same feature, just at different locations in the input image. In addition to the convolutional layers just described, convolutional neural networks also contain pooling layers. Pooling layers are usually used immediately after convolutional layers. What the pooling layers do is to simplify the information in the output from the convolutional layer.
Recent convolutional network architectures have 10 to 20 hidden layers and billions of connections between units. Training deep learning networks has taken weeks several years ago, but thanks to progress in GPU and algorithm enhancement, training time has reduced to several hours.
Neural networks with time-varying behavior are known as recurrent neural networks or RNNs. A recurrent neural network is a class of artificial neural network where connections between units form a directed cycle. This creates an internal state of the network which allows it to exhibit dynamic temporal behavior. Unlike feedforward neural networks, RNNs can use their internal memory to process arbitrary sequences of inputs. Early RNN models turned out to be very difficult to train, harder even than deep feedforward networks. The reason is the unstable gradient problem such as vanishing gradient and exploding gradient. The gradient can get smaller and smaller as it is propagated back through layers. This makes learning in early layers extremely slow. The problem actually gets worse in RNNs, since gradients aren’t just propagated backward through layers, they’re propagated backward through time. If the network runs for a long time, that can make the gradient extremely unstable and hard to learn from. It has been possible to incorporate an idea known as long short-term memory units (LSTMs) into RNNs. LSTMs make it much easier to get good results when training RNNs, and many recent papers make use of LSTMs or related ideas.Deep learning, Convolutional neural networks, Recurrent neural networks, Error backpropagation algorithm"
106,딥러닝분석과 기술적 분석 지표를 이용한 한국 코스피주가지수 방향성 예측,이우식 /Lee Woosik,2017,한국데이터정보과학회,"2016년 3월 구글 (Google)의 바둑인공지능 알파고 (AlphaGo)가 이세돌 9단과의 바둑대결에서승리한 이후 다양한 분야에서 인공지능 사용에 대한 관심이 높아지고 있는 가운데 금융투자 분야에서도 인공지능과 투자자문 전문가의 합성어인 로보어드바이저 (Robo-Advisor)에 대한 관심이 높아지고 있다. 인공지능 (artificial intelligence)기반의 의사결정은 비용 절감은 물론 효과적인 의사결정을가능하게 한다는 점에서 큰 장점이 있다. 본 연구에서는 기술적 분석 (technical analysis) 지표와 딥러닝 (deep learning) 모형을 결합하여 한국 코스피 지수를 예측하는 모형을 개발하고 제시한 모형들의 예측력을 비교, 분석한다. 분석 결과 기술적 분석 지표에 딥러닝 알고리즘을 결합한 모형이 주가지수 방향성 예측 문제에 응용될 수 있음을 확인하였다. 향후 본 연구에서 제안된 기술적 분석 지표와딥러닝모형을 결합한 기법은 로보어드바이저서비스에 응용할 수 있는 일반화 가능성을 보여준다Since Google’s AlphaGo defeated a world champion of Go players in 2016, there have been many interests in the deep learning. In the financial sector, a Robo-Advisor using deep learning gains a significant attention, which builds and manages portfolios of financial instruments for investors.In this paper, we have proposed the a deep learning algorithm geared toward identification and forecast of the KOSPI index direction,and we also have compared the accuracy of the prediction.In an application of forecasting the financial market index direction, we have shown that the Robo-Advisor using deep learning has a significant effect on finance industry. The Robo-Advisor collects a massive data such as earnings statements, news reports and regulatory filings, analyzes those and recommends investors how to view market trends and identify the best time to purchase financial assets. On the other hand, the Robo-Advisor allows businesses to learn more about their customers, develop better marketing strategies, increase sales and decrease costs.Artificial intelligence, deep learning, FinTech, Robo-Advisor, technical analysis."
107,딥러닝의 효율적인 학습을 위한 학습데이터 선별 기법에 관한 연구,주경돈 /Gyoung-don Joo,2017,한국정보과학회,"빅데이터의 증가와 병렬 하드웨어 처리, 학습 알고리즘의 개선은 딥러닝의 발전을 가져왔으며 인공지능이 적용 가능한 영역을 획기적으로 확장시키고 있다. 하지만 이러한 대규모의 학습 데이터에 기반한 딥러닝은 학습시간이 증가되고, 더 나아가 학습 데이터의 구축을 위한 레이블링(labeling) 비용이 데이터 수에 비례하여 증가하게 된다. 따라서 본 논문에서는, 딥러닝의 정확도를 최대한 유지하면서 필요한 학습 데이터의 수를 줄일수 있도록, 전체 데이터에서 제한된 수의 학습 데이터의 후보를 선별하는 기법에 대하여 연구한다. 특히 기존의 능동 학습에 기반한 기법들이 학습 데이터의 수가 매우 적어질 경우 딥러닝의 정확도가 급격히 떨어지는 문제를 해결할 수 있는 새로운 기법을 제안하였으며, 제안된 기법으로 선별된 학습 데이터는 그 수의 변화에 대해 딥러닝이 일관되게 경쟁력있는 정확도를 가짐을 다양한 실험을 통하여 검증하였다.The increase of big data, parallel hardware processing and enhanced learning algorithms have led to the development of deep learning and are dramatically expanding the domain of applications based on artificial intelligence. However, such large-scale training data extends the learning time, and furthermore, the labeling cost for building the training set increases in proportion to the volume of data. Therefore, in this paper, we study how to select a limited number of candidates for training set from given data so as to reduce the number of training set while maintaining the accuracy of deep learning. Especially, we found that, with the training set selected by the previous work, the accuracy of deep learning drastically decreases when the number of training set is very small and so proposed new methods to resolve this fall-off in accuracy. Through experiments, we showed that deep learning consistently has the competitive accuracy with the training set selected by the proposed method regardless of the size of training set.Deep Learning, Train Set Selection, Active Learning, Gamma distribution"
108,딥러닝을 이용한 범용적 스테그아날리시스,김현재 /Hyunjae Kim,2017,한국정보과학회,"스테그아날리시스(Steganalysis)란 이미지 등 일반적인 자료에 암호화된 정보를 은닉하는 스테가노그래피(Steganography)에 대한 검출 및 분석 방법으로, 기계학습 기반 방법론을 포함한다. 기존 기계학습 기반 스테그아날리시스는 영상(Image)의 특징(Feature) 추출 및 모델링에 기반하며, 최근 딥러닝(Deep Learning)의 적용으로 검출 정확도가 큰 폭으로 향상되었다. 하지만 현존하는 스테그아날리시스 모델은 단일 스테가노그래피 기법에 대해 국한되어 있어 학습에 사용되지 않은 스테고(Stego) 이미지의 경우 검출이 불가능한 결정적 한계를 가진다. 본 연구에서는 다양한 스테가노그래피 기법으로 생성된 스테고이미지에 딥러닝을 적용하여 스테그아날리시스를 학습하는 범용적 모델을 제안한다. 다양한 실험을 통해 제안 기법의 효용성 및 가능성을 확인하고, 범용적 스테그아날리시스 모델이 각각에 특화된 검출 기법과 유사한 정확도로 스테고 이미지를 검출할 수 있음을 보인다.Steganalysis is to detect information hidden by steganography inside general data such as images. There are stegoanalysis techniques that use machine learning (ML). Existing ML approaches to steganalysis are based on extracting features from stego images and modeling them.
Recently deep learning-based methodologies have shown significant improvements in detection accuracy. However, all the existing methods, including deep learning-based ones, have a critical limitation in that they can only detect stego images that are created by a specific steganography method. In this paper, we propose a generalized steganalysis method that can model multiple types of stego images using deep learning. Through various experiments, we confirm the effectiveness of our approach and envision directions for future research. In particular, we show that our method can detect each type of steganography with the same level of accuracy as that of a steganalysis method dedicated to that type of steganography, thereby demonstrating the general applicability of our approach to multiple types of stego images.steganalysis, steganography, deep learning, generalized steganalysis model"
109,"딥러닝 프레임워크의 비교: 티아노, 텐서플로, CNTK를 중심으로",정여진 /Yeojin Chung,2017,한국지능정보시스템학회,"딥러닝 프레임워크의 대표적인 기능으로는 ‘자동미분’과 ‘GPU의 활용’ 등을 들 수 있다. 본 논문은 파이썬의 라이브러리 형태로 사용 가능한 프레임워크 중에서 구글의 텐서플로와 마이크로소프트의 CNTK, 그리고 텐서 플로의 원조라고 할 수 있는 티아노를 비교하였다. 본문에서는 자동미분의 개념과 GPU의 활용형태를 간단히 설명하고, 그 다음에 logistic regression을 실행하는 예를 통하여 각 프레임워크의 문법을 알아본 뒤에, 마지막으로 대표적인 딥러닝 응용인 CNN의 예제를 실행시켜보고 코딩의 편의성과 실행속도 등을 확인해 보았다. 그 결과, 편의성의 관점에서 보면 티아노가 가장 코딩 하기가 어렵고, CNTK와 텐서플로는 많은 부분이 비슷하게 추상화 되어 있어서 코딩이 비슷하지만 가중치와 편향을 직접 정의하느냐의 여부에서 차이를 보였다. 그리고 각프레임워크의 실행속도에 대한 평가는 ‘큰 차이는 없다’는 것이다. 텐서플로는 티아노에 비하여 속도가 느리다는 평가가 있어왔는데, 본 연구의 실험에 의하면, 비록 CNN 모형에 국한되었지만, 텐서플로가 아주 조금이지만빠른 것으로 나타났다. CNTK의 경우에도, 비록 실험환경이 달랐지만, 실험환경의 차이에 의한 속도의 차이의 편차범위 이내에 있는 것으로 판단이 되었다. 본 연구에서는 세 종류의 딥러닝 프레임워크만을 살펴보았는데,위키피디아에 따르면 딥러닝 프레임워크의 종류는 12가지가 있으며, 각 프레임워크의 특징을 15가지 속성으로구분하여 차이를 특정하고 있다. 그 많은 속성 중에서 사용자의 입장에서 볼 때 중요한 속성은 어떤 언어(파이썬, C++, Java, 등)로 사용가능한지, 어떤 딥러닝 모형에 대한 라이브러리가 잘 구현되어 있는지 등일 것이다. 그리고 사용자가 대규모의 딥러닝 모형을 구축한다면, 다중 GPU 혹은 다중 서버를 지원하는지의 여부도 중요할 것이다. 또한 딥러닝 모형을 처음 학습하는 경우에는 사용설명서가 많은지 예제 프로그램이 많은지 여부도 중요한 기준이 될 것이다.The deep learning framework is software designed to help develop deep learning models. Some of its important functions include “automatic differentiation” and “utilization of GPU”. The list of popular deep learning framework includes Caffe (BVLC) and Theano (University of Montreal). And recently, Microsoft's deep learning framework, Microsoft Cognitive Toolkit, was released as open-source license, following Google’s Tensorflow a year earlier. The early deep learning frameworks have been developed mainly for research at universities. Beginning with the inception of Tensorflow, however, it seems that companies such as Microsoft and Facebook have started to join the competition of framework development. Given the trend, Google and other companies are expected to continue investing in the deep learning framework to bring forward the initiative in the artificial intelligence business. From this point of view, we think it is a good time to compare some of deep learning frameworks. So we compare three deep learning frameworks which can be used as a Python library. Those are Google's Tensorflow, Microsoft’s CNTK, and Theano which is sort of a predecessor of the preceding two.
The most common and important function of deep learning frameworks is the ability to perform automatic differentiation. Basically all the mathematical expressions of deep learning models can be represented as computational graphs, which consist of nodes and edges. Partial derivatives on each edge of a computational graph can then be obtained. With the partial derivatives, we can let software compute differentiation of any node with respect to any variable by utilizing chain rule of Calculus.
First of all, the convenience of coding is in the order of CNTK, Tensorflow, and Theano. The criterion is simply based on the lengths of the codes and the learning curve and the ease of coding are not the main concern. According to the criteria, Theano was the most difficult to implement with, and CNTK and Tensorflow were somewhat easier. With Tensorflow, we need to define weight variables and biases explicitly. The reason that CNTK and Tensorflow are easier to implement with is that those frameworks provide us with more abstraction than Theano. We, however, need to mention that low-level coding is not always bad. It gives us flexibility of coding. With the low-level coding such as in Theano, we can implement and test any new deep learning models or any new search methods that we can think of.
The assessment of the execution speed of each framework is that there is not meaningful difference. According to the experiment, execution speeds of Theano and Tensorflow are very similar, although the experiment was limited to a CNN model. In the case of CNTK, the experimental environment was not maintained as the same. The code written in CNTK has to be run in PC environment without GPU where codes execute as much as 50 times slower than with GPU. But we concluded that the difference of execution speed was within the range of variation caused by the different hardware setup.
In this study, we compared three types of deep learning framework: Theano, Tensorflow, and CNTK. According to Wikipedia, there are 12 available deep learning frameworks. And 15 different attributes differentiate each framework. Some of the important attributes would include interface language (Python, C ++, Java, etc.) and the availability of libraries on various deep learning models such as CNN, RNN, DBN, and etc. And if a user implements a large scale deep learning model, it will also be important to support multiple GPU or multiple servers. Also, if you are learning the deep learning model, it would also be important if there are enough examples and references.deep learning framework, Theano, TensorFlow, CNTK, computational graph, CIFAR-10"
110,딥러닝을 이용한 주변 무선단말 파악방안,이웅섭 /Lee， Woong-sup,2018,한국정보통신학회,"최근 단말-대-단말(Device-to-device, D2D) 통신기술이 차세대 무선통신시스템의 핵심기술로 큰 관심을 받고 있다. 이러한 단말간 통신에서는 자신의 주변에 어떠한 단말이 있는지 파악하는 주변단말 탐색(Neighbor discovery)이매우 중요하다. 본 논문에서는 최근 큰 관심을 받고 있는 딥러닝(Deep learning) 기술을 활용하여 단말간 통신에서 주변단말을 파악하는 방안에 대해서 제안한다. 제안 방안은 기존의 방안과 달리 무선채널의 공간적 연관성을 이용하여단말간의 신호 전송 없이 단말이 기지국으로 전송하는 상향링크 파일럿 신호를 기반으로 주변 단말을 찾고 따라서기존의 방식에 비해 신호전송 복잡도(signaling complexity)를 크게 줄일 수 있다. 또한 제안 방안에서는 떨어져 있는거리에 따라서 주변 단말을 분류 가능하여 기존 방안에 비해서 좀 더 세밀한 단말 탐색이 가능하다. 마지막으로 본 논문에서는 tensorflow를 이용한 컴퓨터 시뮬레이션을 통해 제안 방안의 성능을 검증하였다.Recently, the device-to-device (D2D) communication has been conceived as the key technology for the next-generationmobile communication systems. The neighbor discovery in which the nearby users are found, is essential for the properoperation of the D2D communication. In this paper, we propose new neighbor discovery scheme based on deep learningtechnology which has gained a lot of attention recently. In the proposed scheme, the neighboring users can be foundusing the uplink pilot transmission of users only, unlike conventional neighbor discovery schemes in which direct pilotcommunication among users is required, such that the signaling overhead can be greatly reduced in our proposed scheme.
Moreover, the neighbors with different proximity can also be classified accordingly which enables more accurate neighbordiscovery compared to the conventional schemes. The performance of our proposed scheme is verified through thetensorflow-based computer simulations.
키워드 : 딥러닝, 주변단말 탐색, 단말-대-단말 통신Deep learning, Neighbor discovery, Device-to-device communication, Wireless channel correlation"
111,신뢰성있는 딥러닝 기반 분석 모델을 참조하기 위한 딥러닝 기술 언어,문종혁 /Jong Hyeok Mun,2021,한국정보처리학회,"최근 딥러닝은 하드웨어 성능이 향상됨에 따라 자연어 처리, 영상 인식 등의 다양한 기술에 접목되어 활용되고 있다. 이러한 기술들을 활용해 지능형교통 시스템(ITS), 스마트홈, 헬스케어 등의 산업분야에서 데이터를 분석하여 고속도로 속도위반 차량 검출, 에너지 사용량 제어, 응급상황 등과 같은고품질의 서비스를 제공하며, 고품질의 서비스를 제공하기 위해서는 정확도가 향상된 딥러닝 모델이 적용되어야 한다. 이를 위해 서비스 환경의 데이터를분석하기 위한 딥러닝 모델을 개발할 때, 개발자는 신뢰성이 검증된 최신의 딥러닝 모델을 적용할 수 있어야 한다. 이는 개발자가 참조하는 딥러닝 모델에적용된 학습 데이터셋의 정확도를 측정하여 검증할 수 있다. 이러한 검증을 위해서 개발자는 학습 데이터셋, 딥러닝의 계층구조 및 개발 환경 등과 같은내용을 포함하는 딥러닝 모델을 문서화하여 적용하기 위한 구조적인 정보가 필요하다. 본 논문에서는 신뢰성있는 딥러닝 기반 데이터 분석 모델을 참조하기위한 딥러닝 기술 언어를 제안한다. 제안하는 기술 언어는 신뢰성 있는 딥러닝 모델을 개발하는데 필요한 학습데이터셋, 개발 환경 및 설정 등의 정보와더불어 딥러닝 모델의 계층구조를 표현할 수 있다. 제안하는 딥러닝 기술 언어를 이용하여 개발자는 지능형 교통 시스템에서 참조하는 분석 모델의정확도를 검증할 수 있다. 실험에서는 제안하는 언어의 유효성을 검증하기 위해, 번호판 인식 모델을 중심으로 딥러닝 기술 문서의 적용과정을 보인다With the recent advancements of deep learning, companies such as smart home, healthcare, and intelligent transportation systems areutilizing its functionality to provide high-quality services for vehicle detection, emergency situation detection, and controlling energyconsumption. To provide reliable services in such sensitive systems, deep learning models are required to have high accuracy. In orderto develop a deep learning model for analyzing previously mentioned services, developers should utilize the state of the art deep learningmodels that have already been verified for higher accuracy. The developers can verify the accuracy of the referenced model by validatingthe model on the dataset. For this validation, the developer needs structural information to document and apply deep learning models,including metadata such as learning dataset, network architecture, and development environments. In this paper, we propose a descriptionlanguage that represents the network architecture of the deep learning model along with its metadata that are necessary to develop adeep learning model. Through the proposed description language, developers can easily verify the accuracy of the referenced deep learningmodel. Our experiments demonstrate the application scenario of a deep learning description document that focuses on the license platerecognition for the detection of illegally parked vehicles.Trusted Deep Learning, Model Reference, Deep Learning Description Language, Traffic Situation Analysis Model"
112,딥러닝 분석을 이용한 중국 역내·외 위안화 변동성 예측,이우식 /Lee Woosik,2016,한국데이터정보과학회,"2008년 글로벌 금융위기 이후 중국은 위안화 국제화의 점진적 추진을 진행하면서 중국상하이 외환시장과 중국홍콩 외환시장에서 거래되는 통화인 역내위안화와 역외위안화를 형성시켰다. 본 연구는 위안화 국제화와 점진적인 중국 자본계정 개방에 따라 급변하는 외환시장상황의 변동성을 정확하게 파악하기 위해서 GARCH모형 (일반화된 자기회귀 조건부이분산성모형)에 다단계인공신경망을 결합한 MLP-GARCH 모형과 GARCH모형과 기계학습의 일종인 딥러닝 (deep learning)을 통합한 DL-GARCH을 가지고 위안화 변동성예측을 비교 실험과 분석을 하였다. 비교분석 결과 DLGARCH 모형은 MLP-GARCH보다 모형 위안화 역내·외 환율변동성 예측 면에서 더욱 더 개선된예측값을 제공하였다. 그래서 이분산시계열모형을 딥러닝과 결합한 DL-GARCH 모형은 시계열의환율 변동성 예측 문제에 딥러닝을 응용할 수 있음을 확인하였다. 향후 이분산시계열과 결합된 딥러닝 모형은 다른 금융시계열 데이터에 응용하여 그 일반화 가능성을 높일 수 있을 것이다.The People’s Republic of China has vigorously been pursuing the internationalization of the Chinese Yuan or Renminbi after the financial crisis of 2008. In this view, an abrupt increase of use of the Chinese Yuan in the onshore and offshore markets are important milestones to be one of important currencies. One of the most frequently used methods to forecast volatility is GARCH model. Since a prediction error of the GARCH model has been reported quite high, a lot of efforts have been made to improve forecasting capability of the GARCH model. In this paper, we have proposed MLPGARCH and a DL-GARCH by employing Artificial Neural Network to the GARCH. In an application to forecasting Chinese Yuan volatility, we have successfully shown their overall outperformance in forecasting over the GARCH.Deep learning, DL-GARCH, GARCH, The Chinese Yuan, volatility."
113,딥러닝 기법을 이용한 내일강수 예측,하지훈,2016,한국지능시스템학회,"정확한 강수예측을 위해서는 예측인자 선정과 예측방법에 대한 선택이 매우 중요하다. 최근에는 강수예측 방법으로기계학습 기법이 많이 사용되고 있으며, 그 중에서도 특히 인공신경망을 사용한 강수예측 방법은 좋은 성능을 보였다. 본논문에서는 딥러닝 기법 중 하나인 DBN(deep belief network)를 이용한 새로운 강수예측 방법을 제안한다. DBN는 비지도사전 학습을 통해 초기 가중치를 설정하여 기존 인공신경망의 문제점을 보완한다. 예측인자로는 기온, 전일-전주 강수일, 태양과 달 궤도 관련 자료를 선정하였다. 기온과 전일-전주 강수일은 서울에서의 1974년부터 2013년까지 총 40년간의AWS(automatic weather system) 관측 자료를 사용하였고, 태양과 달의 궤도 관련 자료는 서울을 중심으로 계산한 결과를사용하였다. 전체 기간에서 일부는 학습 자료로 사용하여 예측모델을 생성하였고, 나머지를 생성한 모델의 검증 자료로사용하였다. 모델 검증 결과로 나온 예측값들은 확률값을 가지며 임계치를 이용하여 강수유무를 판별하였다. 강수 정확도의척도로 양분예보기법 중 CSI(critical successive index)와 Bias(frequency bias)를 계산하였다. 이를 통해 DBN와 MLP(multilayer perceptron)의 성능을 비교한 결과 DBN의 강수 예측 정확도가 높았고, 수행속도 또한 2배 이상 빨랐다.For accurate precipitation forecasts the choice of weather faoctrs and prediction method is very important. Recently, machinele arning has been widely used for forecasting precipitation, and artificial neural network, one of machine learning techniques, showed good performance. In this paper, we suggest a new method for forecasting precipitation using DBN, one of deep learning techniques.
DBN has an advantage that initial weights are set by unsupervised learning, so this compensates for the defects of artificial neural networks. We used past precipitation, temperature, and the parameters of the sun and moon's motion as features for forecasting precipitation. The dataset consists of observation data which had been measured for 40 years from AWS in Seoul. Experiments were based on 8-fold cross validation. As a result of estimation, we got probabilities of test dataset, so threshold was used for the decision of precipitation. CSI and Bias were used for indicating the precision of precipitation. Our experimental results showed that DBN performed better than MLP.Deep learning, Deep belief network, Precipitation, Forecast"
114,딥러닝 신경망모형을 이용한 실시간 교통정보수집 연구,김대현 /KIM DAE HYON,2016,사단법인 인문사회과학기술융합학회,"비디오 기반의 실시간 교통정보수집 시스템은 자율주행을 포함한 지능형교통체계의 주요 서브시스템중 하나이며, 우수한 예측(추정)력 때문에 핵심 알고리즘으로 인공신경망 모형이 주로 사용되고 있다. 특히, 최근 과학 학술지 네이처에 구글 딥마인드의 알파고가 공개되면서 딥러닝과 인공지능에 대한 전 세계적인 관심이 집중되고 있다. 최근까지 가장 널리 사용된 인공신경망모형은 역전파 모형이었으나 최근 컴퓨터 하드웨어 및 알고리즘의 새로운 개발과 더불어 딥러닝 알고리즘을 이용한 딥인공신경망 모형에 대한 관심이 높아지고 있다. 그러나 아직 딥러닝 인공신경망에 대한 연구는 초기단계에 있는 실정이며 적용분야도 극히 한정되어 있다. 본 연구에서는 실시간 도로교통정보 수집을 위한 영상처리분야에서 딥러닝 알고리즘을 활용하고 실험결과를 통하여 향후 교통정보수집‧분석 분야에서의 딥러닝 인공신경망 모형의 적용가능성을 살펴보고자 한다. 특히, 본 연구에서 도출한 딥러닝 인공신경망모형의 실험결과를 기존 역전파 모형에 의한 실험결과와 비교하여 모형의 우수성을 비교‧평가하고자 한다.Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Normally, artificial neural networks have been used in real-time applications for collecting traffic information because of their remarkable performance in terms of predictive accuracy and real-time computing. Especially, the recent publication of a paper in the journal Nature and the success of Google DeepMind’s world-beating Go program AlphaGo caused significant increases of the public interest in artificial intelligence. Even though various types of learning models have been proposed, the Backpropagation has been the most popular neural network model. However, the Deep learning neural network model has attracted much interest recently. In this study, the Deep leaning neural network will be applied in image processing techniques for the real-time traffic information collection to investigate the performance of Deep learning algorithm in traffic engineering fields. In addition, the experimental results from Deep learing algorithm will be compared with the results from Backpropagation model to evaluate and propose better performing model.Intelligent Transportation Systems (ITS), artificial neural networks, Deep learning, traffic information, image processing"
115,딥러닝의 모형과 응용사례,안성만 /SungMahn Ahn,2016,한국지능정보시스템학회,"딥러닝은 인공신경망(neural network)이라는 인공지능분야의 모형이 발전된 형태로서, 계층구조로 이루어진 인공신경망의 내부계층(hidden layer)이 여러 단계로 이루어진 구조이다. 딥러닝에서의 주요 모형은 합성곱신경망(convolutional neural network), 순환신경망(recurrent neural network), 그리고 심층신뢰신경망(deep belief network)의 세가지라고 할 수있다. 그 중에서 현재 흥미로운 연구가 많이 발표되어서 관심이 집중되고 있는 모형은 지도학습(supervised learning)모형인 처음 두 개의 모형이다. 따라서 본 논문에서는 지도학습모형의 가중치를 최적화하는 기본적인 방법인 오류역전파 알고리즘을 살펴본 뒤에 합성곱신경망과 순환신경망의 구조와 응용사례 등을 살펴보고자 한다. 본문에서 다루지 않은 모형인 심층신뢰신경망은 아직까지는 합성곱신경망 이나 순환신경망보다는 상대적으로 주목을 덜 받고 있다. 그러나 심층신뢰신경망은 CNN이나 RNN과는 달리 비지도학습(unsupervised learning)모형이며, 사람이나 동물은 관찰을 통해서 스스로 학습한다는 점에서 궁극적으로는 비지도학습모형이 더 많이 연구되어야 할 주제가 될 것이다.Deep learning model is a kind of neural networks that allows multiple hidden layers. There are various deep learning architectures such as convolutional neural networks, deep belief networks and recurrent neural networks. Those have been applied to fields like computer vision, automatic speech recognition, natural language processing, audio recognition and bioinformatics where they have been shown to produce state-of-the-art results on various tasks. Among those architectures, convolutional neural networks and recurrent neural networks are classified as the supervised learning model. And in recent years, those supervised learning models have gained more popularity than unsupervised learning models such as deep belief networks, because supervised learning models have shown fashionable applications in such fields mentioned above.
Deep learning models can be trained with backpropagation algorithm. Backpropagation is an abbreviation for “backward propagation of errors” and a common method of training artificial neural networks used in conjunction with an optimization method such as gradient descent. The method calculates the gradient of an error function with respect to all the weights in the network. The gradient is fed to the optimization method which in turn uses it to update the weights, in an attempt to minimize the error function.
Convolutional neural networks use a special architecture which is particularly well-adapted to classify images. Using this architecture makes convolutional networks fast to train. This, in turn, helps us train deep, muti-layer networks, which are very good at classifying images. These days, deep convolutional networks are used in most neural networks for image recognition. Convolutional neural networks use three basic ideas: local receptive fields, shared weights, and pooling. By local receptive fields, we mean that each neuron in the first(or any) hidden layer will be connected to a small region of the input(or previous layer’s) neurons. Shared weights mean that we’re going to use the same weights and bias for each of the local receptive field. This means that all the neurons in the hidden layer detect exactly the same feature, just at different locations in the input image. In addition to the convolutional layers just described, convolutional neural networks also contain pooling layers. Pooling layers are usually used immediately after convolutional layers. What the pooling layers do is to simplify the information in the output from the convolutional layer.
Recent convolutional network architectures have 10 to 20 hidden layers and billions of connections between units. Training deep learning networks has taken weeks several years ago, but thanks to progress in GPU and algorithm enhancement, training time has reduced to several hours.
Neural networks with time-varying behavior are known as recurrent neural networks or RNNs. A recurrent neural network is a class of artificial neural network where connections between units form a directed cycle. This creates an internal state of the network which allows it to exhibit dynamic temporal behavior. Unlike feedforward neural networks, RNNs can use their internal memory to process arbitrary sequences of inputs. Early RNN models turned out to be very difficult to train, harder even than deep feedforward networks. The reason is the unstable gradient problem such as vanishing gradient and exploding gradient. The gradient can get smaller and smaller as it is propagated back through layers. This makes learning in early layers extremely slow. The problem actually gets worse in RNNs, since gradients aren’t just propagated backward through layers, they’re propagated backward through time. If the network runs for a long time, that can make the gradient extremely unstable and hard to learn from. It has been possible to incorporate an idea known as long short-term memory units (LSTMs) into RNNs. LSTMs make it much easier to get good results when training RNNs, and many recent papers make use of LSTMs or related ideas.Deep learning, Convolutional neural networks, Recurrent neural networks, Error backpropagation algorithm"
116,딥러닝분석과 기술적 분석 지표를 이용한 한국 코스피주가지수 방향성 예측,이우식 /Lee Woosik,2017,한국데이터정보과학회,"2016년 3월 구글 (Google)의 바둑인공지능 알파고 (AlphaGo)가 이세돌 9단과의 바둑대결에서승리한 이후 다양한 분야에서 인공지능 사용에 대한 관심이 높아지고 있는 가운데 금융투자 분야에서도 인공지능과 투자자문 전문가의 합성어인 로보어드바이저 (Robo-Advisor)에 대한 관심이 높아지고 있다. 인공지능 (artificial intelligence)기반의 의사결정은 비용 절감은 물론 효과적인 의사결정을가능하게 한다는 점에서 큰 장점이 있다. 본 연구에서는 기술적 분석 (technical analysis) 지표와 딥러닝 (deep learning) 모형을 결합하여 한국 코스피 지수를 예측하는 모형을 개발하고 제시한 모형들의 예측력을 비교, 분석한다. 분석 결과 기술적 분석 지표에 딥러닝 알고리즘을 결합한 모형이 주가지수 방향성 예측 문제에 응용될 수 있음을 확인하였다. 향후 본 연구에서 제안된 기술적 분석 지표와딥러닝모형을 결합한 기법은 로보어드바이저서비스에 응용할 수 있는 일반화 가능성을 보여준다Since Google’s AlphaGo defeated a world champion of Go players in 2016, there have been many interests in the deep learning. In the financial sector, a Robo-Advisor using deep learning gains a significant attention, which builds and manages portfolios of financial instruments for investors.In this paper, we have proposed the a deep learning algorithm geared toward identification and forecast of the KOSPI index direction,and we also have compared the accuracy of the prediction.In an application of forecasting the financial market index direction, we have shown that the Robo-Advisor using deep learning has a significant effect on finance industry. The Robo-Advisor collects a massive data such as earnings statements, news reports and regulatory filings, analyzes those and recommends investors how to view market trends and identify the best time to purchase financial assets. On the other hand, the Robo-Advisor allows businesses to learn more about their customers, develop better marketing strategies, increase sales and decrease costs.Artificial intelligence, deep learning, FinTech, Robo-Advisor, technical analysis."
117,딥러닝의 효율적인 학습을 위한 학습데이터 선별 기법에 관한 연구,주경돈 /Gyoung-don Joo,2017,한국정보과학회,"빅데이터의 증가와 병렬 하드웨어 처리, 학습 알고리즘의 개선은 딥러닝의 발전을 가져왔으며 인공지능이 적용 가능한 영역을 획기적으로 확장시키고 있다. 하지만 이러한 대규모의 학습 데이터에 기반한 딥러닝은 학습시간이 증가되고, 더 나아가 학습 데이터의 구축을 위한 레이블링(labeling) 비용이 데이터 수에 비례하여 증가하게 된다. 따라서 본 논문에서는, 딥러닝의 정확도를 최대한 유지하면서 필요한 학습 데이터의 수를 줄일수 있도록, 전체 데이터에서 제한된 수의 학습 데이터의 후보를 선별하는 기법에 대하여 연구한다. 특히 기존의 능동 학습에 기반한 기법들이 학습 데이터의 수가 매우 적어질 경우 딥러닝의 정확도가 급격히 떨어지는 문제를 해결할 수 있는 새로운 기법을 제안하였으며, 제안된 기법으로 선별된 학습 데이터는 그 수의 변화에 대해 딥러닝이 일관되게 경쟁력있는 정확도를 가짐을 다양한 실험을 통하여 검증하였다.The increase of big data, parallel hardware processing and enhanced learning algorithms have led to the development of deep learning and are dramatically expanding the domain of applications based on artificial intelligence. However, such large-scale training data extends the learning time, and furthermore, the labeling cost for building the training set increases in proportion to the volume of data. Therefore, in this paper, we study how to select a limited number of candidates for training set from given data so as to reduce the number of training set while maintaining the accuracy of deep learning. Especially, we found that, with the training set selected by the previous work, the accuracy of deep learning drastically decreases when the number of training set is very small and so proposed new methods to resolve this fall-off in accuracy. Through experiments, we showed that deep learning consistently has the competitive accuracy with the training set selected by the proposed method regardless of the size of training set.Deep Learning, Train Set Selection, Active Learning, Gamma distribution"
118,딥러닝을 이용한 범용적 스테그아날리시스,김현재 /Hyunjae Kim,2017,한국정보과학회,"스테그아날리시스(Steganalysis)란 이미지 등 일반적인 자료에 암호화된 정보를 은닉하는 스테가노그래피(Steganography)에 대한 검출 및 분석 방법으로, 기계학습 기반 방법론을 포함한다. 기존 기계학습 기반 스테그아날리시스는 영상(Image)의 특징(Feature) 추출 및 모델링에 기반하며, 최근 딥러닝(Deep Learning)의 적용으로 검출 정확도가 큰 폭으로 향상되었다. 하지만 현존하는 스테그아날리시스 모델은 단일 스테가노그래피 기법에 대해 국한되어 있어 학습에 사용되지 않은 스테고(Stego) 이미지의 경우 검출이 불가능한 결정적 한계를 가진다. 본 연구에서는 다양한 스테가노그래피 기법으로 생성된 스테고이미지에 딥러닝을 적용하여 스테그아날리시스를 학습하는 범용적 모델을 제안한다. 다양한 실험을 통해 제안 기법의 효용성 및 가능성을 확인하고, 범용적 스테그아날리시스 모델이 각각에 특화된 검출 기법과 유사한 정확도로 스테고 이미지를 검출할 수 있음을 보인다.Steganalysis is to detect information hidden by steganography inside general data such as images. There are stegoanalysis techniques that use machine learning (ML). Existing ML approaches to steganalysis are based on extracting features from stego images and modeling them.
Recently deep learning-based methodologies have shown significant improvements in detection accuracy. However, all the existing methods, including deep learning-based ones, have a critical limitation in that they can only detect stego images that are created by a specific steganography method. In this paper, we propose a generalized steganalysis method that can model multiple types of stego images using deep learning. Through various experiments, we confirm the effectiveness of our approach and envision directions for future research. In particular, we show that our method can detect each type of steganography with the same level of accuracy as that of a steganalysis method dedicated to that type of steganography, thereby demonstrating the general applicability of our approach to multiple types of stego images.steganalysis, steganography, deep learning, generalized steganalysis model"
119,"딥러닝 프레임워크의 비교: 티아노, 텐서플로, CNTK를 중심으로",정여진 /Yeojin Chung,2017,한국지능정보시스템학회,"딥러닝 프레임워크의 대표적인 기능으로는 ‘자동미분’과 ‘GPU의 활용’ 등을 들 수 있다. 본 논문은 파이썬의 라이브러리 형태로 사용 가능한 프레임워크 중에서 구글의 텐서플로와 마이크로소프트의 CNTK, 그리고 텐서 플로의 원조라고 할 수 있는 티아노를 비교하였다. 본문에서는 자동미분의 개념과 GPU의 활용형태를 간단히 설명하고, 그 다음에 logistic regression을 실행하는 예를 통하여 각 프레임워크의 문법을 알아본 뒤에, 마지막으로 대표적인 딥러닝 응용인 CNN의 예제를 실행시켜보고 코딩의 편의성과 실행속도 등을 확인해 보았다. 그 결과, 편의성의 관점에서 보면 티아노가 가장 코딩 하기가 어렵고, CNTK와 텐서플로는 많은 부분이 비슷하게 추상화 되어 있어서 코딩이 비슷하지만 가중치와 편향을 직접 정의하느냐의 여부에서 차이를 보였다. 그리고 각프레임워크의 실행속도에 대한 평가는 ‘큰 차이는 없다’는 것이다. 텐서플로는 티아노에 비하여 속도가 느리다는 평가가 있어왔는데, 본 연구의 실험에 의하면, 비록 CNN 모형에 국한되었지만, 텐서플로가 아주 조금이지만빠른 것으로 나타났다. CNTK의 경우에도, 비록 실험환경이 달랐지만, 실험환경의 차이에 의한 속도의 차이의 편차범위 이내에 있는 것으로 판단이 되었다. 본 연구에서는 세 종류의 딥러닝 프레임워크만을 살펴보았는데,위키피디아에 따르면 딥러닝 프레임워크의 종류는 12가지가 있으며, 각 프레임워크의 특징을 15가지 속성으로구분하여 차이를 특정하고 있다. 그 많은 속성 중에서 사용자의 입장에서 볼 때 중요한 속성은 어떤 언어(파이썬, C++, Java, 등)로 사용가능한지, 어떤 딥러닝 모형에 대한 라이브러리가 잘 구현되어 있는지 등일 것이다. 그리고 사용자가 대규모의 딥러닝 모형을 구축한다면, 다중 GPU 혹은 다중 서버를 지원하는지의 여부도 중요할 것이다. 또한 딥러닝 모형을 처음 학습하는 경우에는 사용설명서가 많은지 예제 프로그램이 많은지 여부도 중요한 기준이 될 것이다.The deep learning framework is software designed to help develop deep learning models. Some of its important functions include “automatic differentiation” and “utilization of GPU”. The list of popular deep learning framework includes Caffe (BVLC) and Theano (University of Montreal). And recently, Microsoft's deep learning framework, Microsoft Cognitive Toolkit, was released as open-source license, following Google’s Tensorflow a year earlier. The early deep learning frameworks have been developed mainly for research at universities. Beginning with the inception of Tensorflow, however, it seems that companies such as Microsoft and Facebook have started to join the competition of framework development. Given the trend, Google and other companies are expected to continue investing in the deep learning framework to bring forward the initiative in the artificial intelligence business. From this point of view, we think it is a good time to compare some of deep learning frameworks. So we compare three deep learning frameworks which can be used as a Python library. Those are Google's Tensorflow, Microsoft’s CNTK, and Theano which is sort of a predecessor of the preceding two.
The most common and important function of deep learning frameworks is the ability to perform automatic differentiation. Basically all the mathematical expressions of deep learning models can be represented as computational graphs, which consist of nodes and edges. Partial derivatives on each edge of a computational graph can then be obtained. With the partial derivatives, we can let software compute differentiation of any node with respect to any variable by utilizing chain rule of Calculus.
First of all, the convenience of coding is in the order of CNTK, Tensorflow, and Theano. The criterion is simply based on the lengths of the codes and the learning curve and the ease of coding are not the main concern. According to the criteria, Theano was the most difficult to implement with, and CNTK and Tensorflow were somewhat easier. With Tensorflow, we need to define weight variables and biases explicitly. The reason that CNTK and Tensorflow are easier to implement with is that those frameworks provide us with more abstraction than Theano. We, however, need to mention that low-level coding is not always bad. It gives us flexibility of coding. With the low-level coding such as in Theano, we can implement and test any new deep learning models or any new search methods that we can think of.
The assessment of the execution speed of each framework is that there is not meaningful difference. According to the experiment, execution speeds of Theano and Tensorflow are very similar, although the experiment was limited to a CNN model. In the case of CNTK, the experimental environment was not maintained as the same. The code written in CNTK has to be run in PC environment without GPU where codes execute as much as 50 times slower than with GPU. But we concluded that the difference of execution speed was within the range of variation caused by the different hardware setup.
In this study, we compared three types of deep learning framework: Theano, Tensorflow, and CNTK. According to Wikipedia, there are 12 available deep learning frameworks. And 15 different attributes differentiate each framework. Some of the important attributes would include interface language (Python, C ++, Java, etc.) and the availability of libraries on various deep learning models such as CNN, RNN, DBN, and etc. And if a user implements a large scale deep learning model, it will also be important to support multiple GPU or multiple servers. Also, if you are learning the deep learning model, it would also be important if there are enough examples and references.deep learning framework, Theano, TensorFlow, CNTK, computational graph, CIFAR-10"
120,딥러닝을 이용한 주변 무선단말 파악방안,이웅섭 /Lee， Woong-sup,2018,한국정보통신학회,"최근 단말-대-단말(Device-to-device, D2D) 통신기술이 차세대 무선통신시스템의 핵심기술로 큰 관심을 받고 있다. 이러한 단말간 통신에서는 자신의 주변에 어떠한 단말이 있는지 파악하는 주변단말 탐색(Neighbor discovery)이매우 중요하다. 본 논문에서는 최근 큰 관심을 받고 있는 딥러닝(Deep learning) 기술을 활용하여 단말간 통신에서 주변단말을 파악하는 방안에 대해서 제안한다. 제안 방안은 기존의 방안과 달리 무선채널의 공간적 연관성을 이용하여단말간의 신호 전송 없이 단말이 기지국으로 전송하는 상향링크 파일럿 신호를 기반으로 주변 단말을 찾고 따라서기존의 방식에 비해 신호전송 복잡도(signaling complexity)를 크게 줄일 수 있다. 또한 제안 방안에서는 떨어져 있는거리에 따라서 주변 단말을 분류 가능하여 기존 방안에 비해서 좀 더 세밀한 단말 탐색이 가능하다. 마지막으로 본 논문에서는 tensorflow를 이용한 컴퓨터 시뮬레이션을 통해 제안 방안의 성능을 검증하였다.Recently, the device-to-device (D2D) communication has been conceived as the key technology for the next-generationmobile communication systems. The neighbor discovery in which the nearby users are found, is essential for the properoperation of the D2D communication. In this paper, we propose new neighbor discovery scheme based on deep learningtechnology which has gained a lot of attention recently. In the proposed scheme, the neighboring users can be foundusing the uplink pilot transmission of users only, unlike conventional neighbor discovery schemes in which direct pilotcommunication among users is required, such that the signaling overhead can be greatly reduced in our proposed scheme.
Moreover, the neighbors with different proximity can also be classified accordingly which enables more accurate neighbordiscovery compared to the conventional schemes. The performance of our proposed scheme is verified through thetensorflow-based computer simulations.
키워드 : 딥러닝, 주변단말 탐색, 단말-대-단말 통신Deep learning, Neighbor discovery, Device-to-device communication, Wireless channel correlation"
121,신뢰성있는 딥러닝 기반 분석 모델을 참조하기 위한 딥러닝 기술 언어,문종혁 /Jong Hyeok Mun,2021,한국정보처리학회,"최근 딥러닝은 하드웨어 성능이 향상됨에 따라 자연어 처리, 영상 인식 등의 다양한 기술에 접목되어 활용되고 있다. 이러한 기술들을 활용해 지능형교통 시스템(ITS), 스마트홈, 헬스케어 등의 산업분야에서 데이터를 분석하여 고속도로 속도위반 차량 검출, 에너지 사용량 제어, 응급상황 등과 같은고품질의 서비스를 제공하며, 고품질의 서비스를 제공하기 위해서는 정확도가 향상된 딥러닝 모델이 적용되어야 한다. 이를 위해 서비스 환경의 데이터를분석하기 위한 딥러닝 모델을 개발할 때, 개발자는 신뢰성이 검증된 최신의 딥러닝 모델을 적용할 수 있어야 한다. 이는 개발자가 참조하는 딥러닝 모델에적용된 학습 데이터셋의 정확도를 측정하여 검증할 수 있다. 이러한 검증을 위해서 개발자는 학습 데이터셋, 딥러닝의 계층구조 및 개발 환경 등과 같은내용을 포함하는 딥러닝 모델을 문서화하여 적용하기 위한 구조적인 정보가 필요하다. 본 논문에서는 신뢰성있는 딥러닝 기반 데이터 분석 모델을 참조하기위한 딥러닝 기술 언어를 제안한다. 제안하는 기술 언어는 신뢰성 있는 딥러닝 모델을 개발하는데 필요한 학습데이터셋, 개발 환경 및 설정 등의 정보와더불어 딥러닝 모델의 계층구조를 표현할 수 있다. 제안하는 딥러닝 기술 언어를 이용하여 개발자는 지능형 교통 시스템에서 참조하는 분석 모델의정확도를 검증할 수 있다. 실험에서는 제안하는 언어의 유효성을 검증하기 위해, 번호판 인식 모델을 중심으로 딥러닝 기술 문서의 적용과정을 보인다With the recent advancements of deep learning, companies such as smart home, healthcare, and intelligent transportation systems areutilizing its functionality to provide high-quality services for vehicle detection, emergency situation detection, and controlling energyconsumption. To provide reliable services in such sensitive systems, deep learning models are required to have high accuracy. In orderto develop a deep learning model for analyzing previously mentioned services, developers should utilize the state of the art deep learningmodels that have already been verified for higher accuracy. The developers can verify the accuracy of the referenced model by validatingthe model on the dataset. For this validation, the developer needs structural information to document and apply deep learning models,including metadata such as learning dataset, network architecture, and development environments. In this paper, we propose a descriptionlanguage that represents the network architecture of the deep learning model along with its metadata that are necessary to develop adeep learning model. Through the proposed description language, developers can easily verify the accuracy of the referenced deep learningmodel. Our experiments demonstrate the application scenario of a deep learning description document that focuses on the license platerecognition for the detection of illegally parked vehicles.Trusted Deep Learning, Model Reference, Deep Learning Description Language, Traffic Situation Analysis Model"
122,딥러닝 분석을 이용한 중국 역내·외 위안화 변동성 예측,이우식 /Lee Woosik,2016,한국데이터정보과학회,"2008년 글로벌 금융위기 이후 중국은 위안화 국제화의 점진적 추진을 진행하면서 중국상하이 외환시장과 중국홍콩 외환시장에서 거래되는 통화인 역내위안화와 역외위안화를 형성시켰다. 본 연구는 위안화 국제화와 점진적인 중국 자본계정 개방에 따라 급변하는 외환시장상황의 변동성을 정확하게 파악하기 위해서 GARCH모형 (일반화된 자기회귀 조건부이분산성모형)에 다단계인공신경망을 결합한 MLP-GARCH 모형과 GARCH모형과 기계학습의 일종인 딥러닝 (deep learning)을 통합한 DL-GARCH을 가지고 위안화 변동성예측을 비교 실험과 분석을 하였다. 비교분석 결과 DLGARCH 모형은 MLP-GARCH보다 모형 위안화 역내·외 환율변동성 예측 면에서 더욱 더 개선된예측값을 제공하였다. 그래서 이분산시계열모형을 딥러닝과 결합한 DL-GARCH 모형은 시계열의환율 변동성 예측 문제에 딥러닝을 응용할 수 있음을 확인하였다. 향후 이분산시계열과 결합된 딥러닝 모형은 다른 금융시계열 데이터에 응용하여 그 일반화 가능성을 높일 수 있을 것이다.The People’s Republic of China has vigorously been pursuing the internationalization of the Chinese Yuan or Renminbi after the financial crisis of 2008. In this view, an abrupt increase of use of the Chinese Yuan in the onshore and offshore markets are important milestones to be one of important currencies. One of the most frequently used methods to forecast volatility is GARCH model. Since a prediction error of the GARCH model has been reported quite high, a lot of efforts have been made to improve forecasting capability of the GARCH model. In this paper, we have proposed MLPGARCH and a DL-GARCH by employing Artificial Neural Network to the GARCH. In an application to forecasting Chinese Yuan volatility, we have successfully shown their overall outperformance in forecasting over the GARCH.Deep learning, DL-GARCH, GARCH, The Chinese Yuan, volatility."
123,딥러닝 기법을 이용한 내일강수 예측,하지훈,2016,한국지능시스템학회,"정확한 강수예측을 위해서는 예측인자 선정과 예측방법에 대한 선택이 매우 중요하다. 최근에는 강수예측 방법으로기계학습 기법이 많이 사용되고 있으며, 그 중에서도 특히 인공신경망을 사용한 강수예측 방법은 좋은 성능을 보였다. 본논문에서는 딥러닝 기법 중 하나인 DBN(deep belief network)를 이용한 새로운 강수예측 방법을 제안한다. DBN는 비지도사전 학습을 통해 초기 가중치를 설정하여 기존 인공신경망의 문제점을 보완한다. 예측인자로는 기온, 전일-전주 강수일, 태양과 달 궤도 관련 자료를 선정하였다. 기온과 전일-전주 강수일은 서울에서의 1974년부터 2013년까지 총 40년간의AWS(automatic weather system) 관측 자료를 사용하였고, 태양과 달의 궤도 관련 자료는 서울을 중심으로 계산한 결과를사용하였다. 전체 기간에서 일부는 학습 자료로 사용하여 예측모델을 생성하였고, 나머지를 생성한 모델의 검증 자료로사용하였다. 모델 검증 결과로 나온 예측값들은 확률값을 가지며 임계치를 이용하여 강수유무를 판별하였다. 강수 정확도의척도로 양분예보기법 중 CSI(critical successive index)와 Bias(frequency bias)를 계산하였다. 이를 통해 DBN와 MLP(multilayer perceptron)의 성능을 비교한 결과 DBN의 강수 예측 정확도가 높았고, 수행속도 또한 2배 이상 빨랐다.For accurate precipitation forecasts the choice of weather faoctrs and prediction method is very important. Recently, machinele arning has been widely used for forecasting precipitation, and artificial neural network, one of machine learning techniques, showed good performance. In this paper, we suggest a new method for forecasting precipitation using DBN, one of deep learning techniques.
DBN has an advantage that initial weights are set by unsupervised learning, so this compensates for the defects of artificial neural networks. We used past precipitation, temperature, and the parameters of the sun and moon's motion as features for forecasting precipitation. The dataset consists of observation data which had been measured for 40 years from AWS in Seoul. Experiments were based on 8-fold cross validation. As a result of estimation, we got probabilities of test dataset, so threshold was used for the decision of precipitation. CSI and Bias were used for indicating the precision of precipitation. Our experimental results showed that DBN performed better than MLP.Deep learning, Deep belief network, Precipitation, Forecast"
124,딥러닝 신경망모형을 이용한 실시간 교통정보수집 연구,김대현 /KIM DAE HYON,2016,사단법인 인문사회과학기술융합학회,"비디오 기반의 실시간 교통정보수집 시스템은 자율주행을 포함한 지능형교통체계의 주요 서브시스템중 하나이며, 우수한 예측(추정)력 때문에 핵심 알고리즘으로 인공신경망 모형이 주로 사용되고 있다. 특히, 최근 과학 학술지 네이처에 구글 딥마인드의 알파고가 공개되면서 딥러닝과 인공지능에 대한 전 세계적인 관심이 집중되고 있다. 최근까지 가장 널리 사용된 인공신경망모형은 역전파 모형이었으나 최근 컴퓨터 하드웨어 및 알고리즘의 새로운 개발과 더불어 딥러닝 알고리즘을 이용한 딥인공신경망 모형에 대한 관심이 높아지고 있다. 그러나 아직 딥러닝 인공신경망에 대한 연구는 초기단계에 있는 실정이며 적용분야도 극히 한정되어 있다. 본 연구에서는 실시간 도로교통정보 수집을 위한 영상처리분야에서 딥러닝 알고리즘을 활용하고 실험결과를 통하여 향후 교통정보수집‧분석 분야에서의 딥러닝 인공신경망 모형의 적용가능성을 살펴보고자 한다. 특히, 본 연구에서 도출한 딥러닝 인공신경망모형의 실험결과를 기존 역전파 모형에 의한 실험결과와 비교하여 모형의 우수성을 비교‧평가하고자 한다.Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Normally, artificial neural networks have been used in real-time applications for collecting traffic information because of their remarkable performance in terms of predictive accuracy and real-time computing. Especially, the recent publication of a paper in the journal Nature and the success of Google DeepMind’s world-beating Go program AlphaGo caused significant increases of the public interest in artificial intelligence. Even though various types of learning models have been proposed, the Backpropagation has been the most popular neural network model. However, the Deep learning neural network model has attracted much interest recently. In this study, the Deep leaning neural network will be applied in image processing techniques for the real-time traffic information collection to investigate the performance of Deep learning algorithm in traffic engineering fields. In addition, the experimental results from Deep learing algorithm will be compared with the results from Backpropagation model to evaluate and propose better performing model.Intelligent Transportation Systems (ITS), artificial neural networks, Deep learning, traffic information, image processing"
125,딥러닝의 모형과 응용사례,안성만 /SungMahn Ahn,2016,한국지능정보시스템학회,"딥러닝은 인공신경망(neural network)이라는 인공지능분야의 모형이 발전된 형태로서, 계층구조로 이루어진 인공신경망의 내부계층(hidden layer)이 여러 단계로 이루어진 구조이다. 딥러닝에서의 주요 모형은 합성곱신경망(convolutional neural network), 순환신경망(recurrent neural network), 그리고 심층신뢰신경망(deep belief network)의 세가지라고 할 수있다. 그 중에서 현재 흥미로운 연구가 많이 발표되어서 관심이 집중되고 있는 모형은 지도학습(supervised learning)모형인 처음 두 개의 모형이다. 따라서 본 논문에서는 지도학습모형의 가중치를 최적화하는 기본적인 방법인 오류역전파 알고리즘을 살펴본 뒤에 합성곱신경망과 순환신경망의 구조와 응용사례 등을 살펴보고자 한다. 본문에서 다루지 않은 모형인 심층신뢰신경망은 아직까지는 합성곱신경망 이나 순환신경망보다는 상대적으로 주목을 덜 받고 있다. 그러나 심층신뢰신경망은 CNN이나 RNN과는 달리 비지도학습(unsupervised learning)모형이며, 사람이나 동물은 관찰을 통해서 스스로 학습한다는 점에서 궁극적으로는 비지도학습모형이 더 많이 연구되어야 할 주제가 될 것이다.Deep learning model is a kind of neural networks that allows multiple hidden layers. There are various deep learning architectures such as convolutional neural networks, deep belief networks and recurrent neural networks. Those have been applied to fields like computer vision, automatic speech recognition, natural language processing, audio recognition and bioinformatics where they have been shown to produce state-of-the-art results on various tasks. Among those architectures, convolutional neural networks and recurrent neural networks are classified as the supervised learning model. And in recent years, those supervised learning models have gained more popularity than unsupervised learning models such as deep belief networks, because supervised learning models have shown fashionable applications in such fields mentioned above.
Deep learning models can be trained with backpropagation algorithm. Backpropagation is an abbreviation for “backward propagation of errors” and a common method of training artificial neural networks used in conjunction with an optimization method such as gradient descent. The method calculates the gradient of an error function with respect to all the weights in the network. The gradient is fed to the optimization method which in turn uses it to update the weights, in an attempt to minimize the error function.
Convolutional neural networks use a special architecture which is particularly well-adapted to classify images. Using this architecture makes convolutional networks fast to train. This, in turn, helps us train deep, muti-layer networks, which are very good at classifying images. These days, deep convolutional networks are used in most neural networks for image recognition. Convolutional neural networks use three basic ideas: local receptive fields, shared weights, and pooling. By local receptive fields, we mean that each neuron in the first(or any) hidden layer will be connected to a small region of the input(or previous layer’s) neurons. Shared weights mean that we’re going to use the same weights and bias for each of the local receptive field. This means that all the neurons in the hidden layer detect exactly the same feature, just at different locations in the input image. In addition to the convolutional layers just described, convolutional neural networks also contain pooling layers. Pooling layers are usually used immediately after convolutional layers. What the pooling layers do is to simplify the information in the output from the convolutional layer.
Recent convolutional network architectures have 10 to 20 hidden layers and billions of connections between units. Training deep learning networks has taken weeks several years ago, but thanks to progress in GPU and algorithm enhancement, training time has reduced to several hours.
Neural networks with time-varying behavior are known as recurrent neural networks or RNNs. A recurrent neural network is a class of artificial neural network where connections between units form a directed cycle. This creates an internal state of the network which allows it to exhibit dynamic temporal behavior. Unlike feedforward neural networks, RNNs can use their internal memory to process arbitrary sequences of inputs. Early RNN models turned out to be very difficult to train, harder even than deep feedforward networks. The reason is the unstable gradient problem such as vanishing gradient and exploding gradient. The gradient can get smaller and smaller as it is propagated back through layers. This makes learning in early layers extremely slow. The problem actually gets worse in RNNs, since gradients aren’t just propagated backward through layers, they’re propagated backward through time. If the network runs for a long time, that can make the gradient extremely unstable and hard to learn from. It has been possible to incorporate an idea known as long short-term memory units (LSTMs) into RNNs. LSTMs make it much easier to get good results when training RNNs, and many recent papers make use of LSTMs or related ideas.Deep learning, Convolutional neural networks, Recurrent neural networks, Error backpropagation algorithm"
126,딥러닝분석과 기술적 분석 지표를 이용한 한국 코스피주가지수 방향성 예측,이우식 /Lee Woosik,2017,한국데이터정보과학회,"2016년 3월 구글 (Google)의 바둑인공지능 알파고 (AlphaGo)가 이세돌 9단과의 바둑대결에서승리한 이후 다양한 분야에서 인공지능 사용에 대한 관심이 높아지고 있는 가운데 금융투자 분야에서도 인공지능과 투자자문 전문가의 합성어인 로보어드바이저 (Robo-Advisor)에 대한 관심이 높아지고 있다. 인공지능 (artificial intelligence)기반의 의사결정은 비용 절감은 물론 효과적인 의사결정을가능하게 한다는 점에서 큰 장점이 있다. 본 연구에서는 기술적 분석 (technical analysis) 지표와 딥러닝 (deep learning) 모형을 결합하여 한국 코스피 지수를 예측하는 모형을 개발하고 제시한 모형들의 예측력을 비교, 분석한다. 분석 결과 기술적 분석 지표에 딥러닝 알고리즘을 결합한 모형이 주가지수 방향성 예측 문제에 응용될 수 있음을 확인하였다. 향후 본 연구에서 제안된 기술적 분석 지표와딥러닝모형을 결합한 기법은 로보어드바이저서비스에 응용할 수 있는 일반화 가능성을 보여준다Since Google’s AlphaGo defeated a world champion of Go players in 2016, there have been many interests in the deep learning. In the financial sector, a Robo-Advisor using deep learning gains a significant attention, which builds and manages portfolios of financial instruments for investors.In this paper, we have proposed the a deep learning algorithm geared toward identification and forecast of the KOSPI index direction,and we also have compared the accuracy of the prediction.In an application of forecasting the financial market index direction, we have shown that the Robo-Advisor using deep learning has a significant effect on finance industry. The Robo-Advisor collects a massive data such as earnings statements, news reports and regulatory filings, analyzes those and recommends investors how to view market trends and identify the best time to purchase financial assets. On the other hand, the Robo-Advisor allows businesses to learn more about their customers, develop better marketing strategies, increase sales and decrease costs.Artificial intelligence, deep learning, FinTech, Robo-Advisor, technical analysis."
127,딥러닝의 효율적인 학습을 위한 학습데이터 선별 기법에 관한 연구,주경돈 /Gyoung-don Joo,2017,한국정보과학회,"빅데이터의 증가와 병렬 하드웨어 처리, 학습 알고리즘의 개선은 딥러닝의 발전을 가져왔으며 인공지능이 적용 가능한 영역을 획기적으로 확장시키고 있다. 하지만 이러한 대규모의 학습 데이터에 기반한 딥러닝은 학습시간이 증가되고, 더 나아가 학습 데이터의 구축을 위한 레이블링(labeling) 비용이 데이터 수에 비례하여 증가하게 된다. 따라서 본 논문에서는, 딥러닝의 정확도를 최대한 유지하면서 필요한 학습 데이터의 수를 줄일수 있도록, 전체 데이터에서 제한된 수의 학습 데이터의 후보를 선별하는 기법에 대하여 연구한다. 특히 기존의 능동 학습에 기반한 기법들이 학습 데이터의 수가 매우 적어질 경우 딥러닝의 정확도가 급격히 떨어지는 문제를 해결할 수 있는 새로운 기법을 제안하였으며, 제안된 기법으로 선별된 학습 데이터는 그 수의 변화에 대해 딥러닝이 일관되게 경쟁력있는 정확도를 가짐을 다양한 실험을 통하여 검증하였다.The increase of big data, parallel hardware processing and enhanced learning algorithms have led to the development of deep learning and are dramatically expanding the domain of applications based on artificial intelligence. However, such large-scale training data extends the learning time, and furthermore, the labeling cost for building the training set increases in proportion to the volume of data. Therefore, in this paper, we study how to select a limited number of candidates for training set from given data so as to reduce the number of training set while maintaining the accuracy of deep learning. Especially, we found that, with the training set selected by the previous work, the accuracy of deep learning drastically decreases when the number of training set is very small and so proposed new methods to resolve this fall-off in accuracy. Through experiments, we showed that deep learning consistently has the competitive accuracy with the training set selected by the proposed method regardless of the size of training set.Deep Learning, Train Set Selection, Active Learning, Gamma distribution"
128,딥러닝을 이용한 범용적 스테그아날리시스,김현재 /Hyunjae Kim,2017,한국정보과학회,"스테그아날리시스(Steganalysis)란 이미지 등 일반적인 자료에 암호화된 정보를 은닉하는 스테가노그래피(Steganography)에 대한 검출 및 분석 방법으로, 기계학습 기반 방법론을 포함한다. 기존 기계학습 기반 스테그아날리시스는 영상(Image)의 특징(Feature) 추출 및 모델링에 기반하며, 최근 딥러닝(Deep Learning)의 적용으로 검출 정확도가 큰 폭으로 향상되었다. 하지만 현존하는 스테그아날리시스 모델은 단일 스테가노그래피 기법에 대해 국한되어 있어 학습에 사용되지 않은 스테고(Stego) 이미지의 경우 검출이 불가능한 결정적 한계를 가진다. 본 연구에서는 다양한 스테가노그래피 기법으로 생성된 스테고이미지에 딥러닝을 적용하여 스테그아날리시스를 학습하는 범용적 모델을 제안한다. 다양한 실험을 통해 제안 기법의 효용성 및 가능성을 확인하고, 범용적 스테그아날리시스 모델이 각각에 특화된 검출 기법과 유사한 정확도로 스테고 이미지를 검출할 수 있음을 보인다.Steganalysis is to detect information hidden by steganography inside general data such as images. There are stegoanalysis techniques that use machine learning (ML). Existing ML approaches to steganalysis are based on extracting features from stego images and modeling them.
Recently deep learning-based methodologies have shown significant improvements in detection accuracy. However, all the existing methods, including deep learning-based ones, have a critical limitation in that they can only detect stego images that are created by a specific steganography method. In this paper, we propose a generalized steganalysis method that can model multiple types of stego images using deep learning. Through various experiments, we confirm the effectiveness of our approach and envision directions for future research. In particular, we show that our method can detect each type of steganography with the same level of accuracy as that of a steganalysis method dedicated to that type of steganography, thereby demonstrating the general applicability of our approach to multiple types of stego images.steganalysis, steganography, deep learning, generalized steganalysis model"
129,"딥러닝 프레임워크의 비교: 티아노, 텐서플로, CNTK를 중심으로",정여진 /Yeojin Chung,2017,한국지능정보시스템학회,"딥러닝 프레임워크의 대표적인 기능으로는 ‘자동미분’과 ‘GPU의 활용’ 등을 들 수 있다. 본 논문은 파이썬의 라이브러리 형태로 사용 가능한 프레임워크 중에서 구글의 텐서플로와 마이크로소프트의 CNTK, 그리고 텐서 플로의 원조라고 할 수 있는 티아노를 비교하였다. 본문에서는 자동미분의 개념과 GPU의 활용형태를 간단히 설명하고, 그 다음에 logistic regression을 실행하는 예를 통하여 각 프레임워크의 문법을 알아본 뒤에, 마지막으로 대표적인 딥러닝 응용인 CNN의 예제를 실행시켜보고 코딩의 편의성과 실행속도 등을 확인해 보았다. 그 결과, 편의성의 관점에서 보면 티아노가 가장 코딩 하기가 어렵고, CNTK와 텐서플로는 많은 부분이 비슷하게 추상화 되어 있어서 코딩이 비슷하지만 가중치와 편향을 직접 정의하느냐의 여부에서 차이를 보였다. 그리고 각프레임워크의 실행속도에 대한 평가는 ‘큰 차이는 없다’는 것이다. 텐서플로는 티아노에 비하여 속도가 느리다는 평가가 있어왔는데, 본 연구의 실험에 의하면, 비록 CNN 모형에 국한되었지만, 텐서플로가 아주 조금이지만빠른 것으로 나타났다. CNTK의 경우에도, 비록 실험환경이 달랐지만, 실험환경의 차이에 의한 속도의 차이의 편차범위 이내에 있는 것으로 판단이 되었다. 본 연구에서는 세 종류의 딥러닝 프레임워크만을 살펴보았는데,위키피디아에 따르면 딥러닝 프레임워크의 종류는 12가지가 있으며, 각 프레임워크의 특징을 15가지 속성으로구분하여 차이를 특정하고 있다. 그 많은 속성 중에서 사용자의 입장에서 볼 때 중요한 속성은 어떤 언어(파이썬, C++, Java, 등)로 사용가능한지, 어떤 딥러닝 모형에 대한 라이브러리가 잘 구현되어 있는지 등일 것이다. 그리고 사용자가 대규모의 딥러닝 모형을 구축한다면, 다중 GPU 혹은 다중 서버를 지원하는지의 여부도 중요할 것이다. 또한 딥러닝 모형을 처음 학습하는 경우에는 사용설명서가 많은지 예제 프로그램이 많은지 여부도 중요한 기준이 될 것이다.The deep learning framework is software designed to help develop deep learning models. Some of its important functions include “automatic differentiation” and “utilization of GPU”. The list of popular deep learning framework includes Caffe (BVLC) and Theano (University of Montreal). And recently, Microsoft's deep learning framework, Microsoft Cognitive Toolkit, was released as open-source license, following Google’s Tensorflow a year earlier. The early deep learning frameworks have been developed mainly for research at universities. Beginning with the inception of Tensorflow, however, it seems that companies such as Microsoft and Facebook have started to join the competition of framework development. Given the trend, Google and other companies are expected to continue investing in the deep learning framework to bring forward the initiative in the artificial intelligence business. From this point of view, we think it is a good time to compare some of deep learning frameworks. So we compare three deep learning frameworks which can be used as a Python library. Those are Google's Tensorflow, Microsoft’s CNTK, and Theano which is sort of a predecessor of the preceding two.
The most common and important function of deep learning frameworks is the ability to perform automatic differentiation. Basically all the mathematical expressions of deep learning models can be represented as computational graphs, which consist of nodes and edges. Partial derivatives on each edge of a computational graph can then be obtained. With the partial derivatives, we can let software compute differentiation of any node with respect to any variable by utilizing chain rule of Calculus.
First of all, the convenience of coding is in the order of CNTK, Tensorflow, and Theano. The criterion is simply based on the lengths of the codes and the learning curve and the ease of coding are not the main concern. According to the criteria, Theano was the most difficult to implement with, and CNTK and Tensorflow were somewhat easier. With Tensorflow, we need to define weight variables and biases explicitly. The reason that CNTK and Tensorflow are easier to implement with is that those frameworks provide us with more abstraction than Theano. We, however, need to mention that low-level coding is not always bad. It gives us flexibility of coding. With the low-level coding such as in Theano, we can implement and test any new deep learning models or any new search methods that we can think of.
The assessment of the execution speed of each framework is that there is not meaningful difference. According to the experiment, execution speeds of Theano and Tensorflow are very similar, although the experiment was limited to a CNN model. In the case of CNTK, the experimental environment was not maintained as the same. The code written in CNTK has to be run in PC environment without GPU where codes execute as much as 50 times slower than with GPU. But we concluded that the difference of execution speed was within the range of variation caused by the different hardware setup.
In this study, we compared three types of deep learning framework: Theano, Tensorflow, and CNTK. According to Wikipedia, there are 12 available deep learning frameworks. And 15 different attributes differentiate each framework. Some of the important attributes would include interface language (Python, C ++, Java, etc.) and the availability of libraries on various deep learning models such as CNN, RNN, DBN, and etc. And if a user implements a large scale deep learning model, it will also be important to support multiple GPU or multiple servers. Also, if you are learning the deep learning model, it would also be important if there are enough examples and references.deep learning framework, Theano, TensorFlow, CNTK, computational graph, CIFAR-10"
130,딥러닝을 이용한 주변 무선단말 파악방안,이웅섭 /Lee， Woong-sup,2018,한국정보통신학회,"최근 단말-대-단말(Device-to-device, D2D) 통신기술이 차세대 무선통신시스템의 핵심기술로 큰 관심을 받고 있다. 이러한 단말간 통신에서는 자신의 주변에 어떠한 단말이 있는지 파악하는 주변단말 탐색(Neighbor discovery)이매우 중요하다. 본 논문에서는 최근 큰 관심을 받고 있는 딥러닝(Deep learning) 기술을 활용하여 단말간 통신에서 주변단말을 파악하는 방안에 대해서 제안한다. 제안 방안은 기존의 방안과 달리 무선채널의 공간적 연관성을 이용하여단말간의 신호 전송 없이 단말이 기지국으로 전송하는 상향링크 파일럿 신호를 기반으로 주변 단말을 찾고 따라서기존의 방식에 비해 신호전송 복잡도(signaling complexity)를 크게 줄일 수 있다. 또한 제안 방안에서는 떨어져 있는거리에 따라서 주변 단말을 분류 가능하여 기존 방안에 비해서 좀 더 세밀한 단말 탐색이 가능하다. 마지막으로 본 논문에서는 tensorflow를 이용한 컴퓨터 시뮬레이션을 통해 제안 방안의 성능을 검증하였다.Recently, the device-to-device (D2D) communication has been conceived as the key technology for the next-generationmobile communication systems. The neighbor discovery in which the nearby users are found, is essential for the properoperation of the D2D communication. In this paper, we propose new neighbor discovery scheme based on deep learningtechnology which has gained a lot of attention recently. In the proposed scheme, the neighboring users can be foundusing the uplink pilot transmission of users only, unlike conventional neighbor discovery schemes in which direct pilotcommunication among users is required, such that the signaling overhead can be greatly reduced in our proposed scheme.
Moreover, the neighbors with different proximity can also be classified accordingly which enables more accurate neighbordiscovery compared to the conventional schemes. The performance of our proposed scheme is verified through thetensorflow-based computer simulations.
키워드 : 딥러닝, 주변단말 탐색, 단말-대-단말 통신Deep learning, Neighbor discovery, Device-to-device communication, Wireless channel correlation"
131,신뢰성있는 딥러닝 기반 분석 모델을 참조하기 위한 딥러닝 기술 언어,문종혁 /Jong Hyeok Mun,2021,한국정보처리학회,"최근 딥러닝은 하드웨어 성능이 향상됨에 따라 자연어 처리, 영상 인식 등의 다양한 기술에 접목되어 활용되고 있다. 이러한 기술들을 활용해 지능형교통 시스템(ITS), 스마트홈, 헬스케어 등의 산업분야에서 데이터를 분석하여 고속도로 속도위반 차량 검출, 에너지 사용량 제어, 응급상황 등과 같은고품질의 서비스를 제공하며, 고품질의 서비스를 제공하기 위해서는 정확도가 향상된 딥러닝 모델이 적용되어야 한다. 이를 위해 서비스 환경의 데이터를분석하기 위한 딥러닝 모델을 개발할 때, 개발자는 신뢰성이 검증된 최신의 딥러닝 모델을 적용할 수 있어야 한다. 이는 개발자가 참조하는 딥러닝 모델에적용된 학습 데이터셋의 정확도를 측정하여 검증할 수 있다. 이러한 검증을 위해서 개발자는 학습 데이터셋, 딥러닝의 계층구조 및 개발 환경 등과 같은내용을 포함하는 딥러닝 모델을 문서화하여 적용하기 위한 구조적인 정보가 필요하다. 본 논문에서는 신뢰성있는 딥러닝 기반 데이터 분석 모델을 참조하기위한 딥러닝 기술 언어를 제안한다. 제안하는 기술 언어는 신뢰성 있는 딥러닝 모델을 개발하는데 필요한 학습데이터셋, 개발 환경 및 설정 등의 정보와더불어 딥러닝 모델의 계층구조를 표현할 수 있다. 제안하는 딥러닝 기술 언어를 이용하여 개발자는 지능형 교통 시스템에서 참조하는 분석 모델의정확도를 검증할 수 있다. 실험에서는 제안하는 언어의 유효성을 검증하기 위해, 번호판 인식 모델을 중심으로 딥러닝 기술 문서의 적용과정을 보인다With the recent advancements of deep learning, companies such as smart home, healthcare, and intelligent transportation systems areutilizing its functionality to provide high-quality services for vehicle detection, emergency situation detection, and controlling energyconsumption. To provide reliable services in such sensitive systems, deep learning models are required to have high accuracy. In orderto develop a deep learning model for analyzing previously mentioned services, developers should utilize the state of the art deep learningmodels that have already been verified for higher accuracy. The developers can verify the accuracy of the referenced model by validatingthe model on the dataset. For this validation, the developer needs structural information to document and apply deep learning models,including metadata such as learning dataset, network architecture, and development environments. In this paper, we propose a descriptionlanguage that represents the network architecture of the deep learning model along with its metadata that are necessary to develop adeep learning model. Through the proposed description language, developers can easily verify the accuracy of the referenced deep learningmodel. Our experiments demonstrate the application scenario of a deep learning description document that focuses on the license platerecognition for the detection of illegally parked vehicles.Trusted Deep Learning, Model Reference, Deep Learning Description Language, Traffic Situation Analysis Model"
132,딥러닝 분석을 이용한 중국 역내·외 위안화 변동성 예측,이우식 /Lee Woosik,2016,한국데이터정보과학회,"2008년 글로벌 금융위기 이후 중국은 위안화 국제화의 점진적 추진을 진행하면서 중국상하이 외환시장과 중국홍콩 외환시장에서 거래되는 통화인 역내위안화와 역외위안화를 형성시켰다. 본 연구는 위안화 국제화와 점진적인 중국 자본계정 개방에 따라 급변하는 외환시장상황의 변동성을 정확하게 파악하기 위해서 GARCH모형 (일반화된 자기회귀 조건부이분산성모형)에 다단계인공신경망을 결합한 MLP-GARCH 모형과 GARCH모형과 기계학습의 일종인 딥러닝 (deep learning)을 통합한 DL-GARCH을 가지고 위안화 변동성예측을 비교 실험과 분석을 하였다. 비교분석 결과 DLGARCH 모형은 MLP-GARCH보다 모형 위안화 역내·외 환율변동성 예측 면에서 더욱 더 개선된예측값을 제공하였다. 그래서 이분산시계열모형을 딥러닝과 결합한 DL-GARCH 모형은 시계열의환율 변동성 예측 문제에 딥러닝을 응용할 수 있음을 확인하였다. 향후 이분산시계열과 결합된 딥러닝 모형은 다른 금융시계열 데이터에 응용하여 그 일반화 가능성을 높일 수 있을 것이다.The People’s Republic of China has vigorously been pursuing the internationalization of the Chinese Yuan or Renminbi after the financial crisis of 2008. In this view, an abrupt increase of use of the Chinese Yuan in the onshore and offshore markets are important milestones to be one of important currencies. One of the most frequently used methods to forecast volatility is GARCH model. Since a prediction error of the GARCH model has been reported quite high, a lot of efforts have been made to improve forecasting capability of the GARCH model. In this paper, we have proposed MLPGARCH and a DL-GARCH by employing Artificial Neural Network to the GARCH. In an application to forecasting Chinese Yuan volatility, we have successfully shown their overall outperformance in forecasting over the GARCH.Deep learning, DL-GARCH, GARCH, The Chinese Yuan, volatility."
133,딥러닝 기법을 이용한 내일강수 예측,하지훈,2016,한국지능시스템학회,"정확한 강수예측을 위해서는 예측인자 선정과 예측방법에 대한 선택이 매우 중요하다. 최근에는 강수예측 방법으로기계학습 기법이 많이 사용되고 있으며, 그 중에서도 특히 인공신경망을 사용한 강수예측 방법은 좋은 성능을 보였다. 본논문에서는 딥러닝 기법 중 하나인 DBN(deep belief network)를 이용한 새로운 강수예측 방법을 제안한다. DBN는 비지도사전 학습을 통해 초기 가중치를 설정하여 기존 인공신경망의 문제점을 보완한다. 예측인자로는 기온, 전일-전주 강수일, 태양과 달 궤도 관련 자료를 선정하였다. 기온과 전일-전주 강수일은 서울에서의 1974년부터 2013년까지 총 40년간의AWS(automatic weather system) 관측 자료를 사용하였고, 태양과 달의 궤도 관련 자료는 서울을 중심으로 계산한 결과를사용하였다. 전체 기간에서 일부는 학습 자료로 사용하여 예측모델을 생성하였고, 나머지를 생성한 모델의 검증 자료로사용하였다. 모델 검증 결과로 나온 예측값들은 확률값을 가지며 임계치를 이용하여 강수유무를 판별하였다. 강수 정확도의척도로 양분예보기법 중 CSI(critical successive index)와 Bias(frequency bias)를 계산하였다. 이를 통해 DBN와 MLP(multilayer perceptron)의 성능을 비교한 결과 DBN의 강수 예측 정확도가 높았고, 수행속도 또한 2배 이상 빨랐다.For accurate precipitation forecasts the choice of weather faoctrs and prediction method is very important. Recently, machinele arning has been widely used for forecasting precipitation, and artificial neural network, one of machine learning techniques, showed good performance. In this paper, we suggest a new method for forecasting precipitation using DBN, one of deep learning techniques.
DBN has an advantage that initial weights are set by unsupervised learning, so this compensates for the defects of artificial neural networks. We used past precipitation, temperature, and the parameters of the sun and moon's motion as features for forecasting precipitation. The dataset consists of observation data which had been measured for 40 years from AWS in Seoul. Experiments were based on 8-fold cross validation. As a result of estimation, we got probabilities of test dataset, so threshold was used for the decision of precipitation. CSI and Bias were used for indicating the precision of precipitation. Our experimental results showed that DBN performed better than MLP.Deep learning, Deep belief network, Precipitation, Forecast"
134,딥러닝 신경망모형을 이용한 실시간 교통정보수집 연구,김대현 /KIM DAE HYON,2016,사단법인 인문사회과학기술융합학회,"비디오 기반의 실시간 교통정보수집 시스템은 자율주행을 포함한 지능형교통체계의 주요 서브시스템중 하나이며, 우수한 예측(추정)력 때문에 핵심 알고리즘으로 인공신경망 모형이 주로 사용되고 있다. 특히, 최근 과학 학술지 네이처에 구글 딥마인드의 알파고가 공개되면서 딥러닝과 인공지능에 대한 전 세계적인 관심이 집중되고 있다. 최근까지 가장 널리 사용된 인공신경망모형은 역전파 모형이었으나 최근 컴퓨터 하드웨어 및 알고리즘의 새로운 개발과 더불어 딥러닝 알고리즘을 이용한 딥인공신경망 모형에 대한 관심이 높아지고 있다. 그러나 아직 딥러닝 인공신경망에 대한 연구는 초기단계에 있는 실정이며 적용분야도 극히 한정되어 있다. 본 연구에서는 실시간 도로교통정보 수집을 위한 영상처리분야에서 딥러닝 알고리즘을 활용하고 실험결과를 통하여 향후 교통정보수집‧분석 분야에서의 딥러닝 인공신경망 모형의 적용가능성을 살펴보고자 한다. 특히, 본 연구에서 도출한 딥러닝 인공신경망모형의 실험결과를 기존 역전파 모형에 의한 실험결과와 비교하여 모형의 우수성을 비교‧평가하고자 한다.Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Normally, artificial neural networks have been used in real-time applications for collecting traffic information because of their remarkable performance in terms of predictive accuracy and real-time computing. Especially, the recent publication of a paper in the journal Nature and the success of Google DeepMind’s world-beating Go program AlphaGo caused significant increases of the public interest in artificial intelligence. Even though various types of learning models have been proposed, the Backpropagation has been the most popular neural network model. However, the Deep learning neural network model has attracted much interest recently. In this study, the Deep leaning neural network will be applied in image processing techniques for the real-time traffic information collection to investigate the performance of Deep learning algorithm in traffic engineering fields. In addition, the experimental results from Deep learing algorithm will be compared with the results from Backpropagation model to evaluate and propose better performing model.Intelligent Transportation Systems (ITS), artificial neural networks, Deep learning, traffic information, image processing"
135,딥러닝의 모형과 응용사례,안성만 /SungMahn Ahn,2016,한국지능정보시스템학회,"딥러닝은 인공신경망(neural network)이라는 인공지능분야의 모형이 발전된 형태로서, 계층구조로 이루어진 인공신경망의 내부계층(hidden layer)이 여러 단계로 이루어진 구조이다. 딥러닝에서의 주요 모형은 합성곱신경망(convolutional neural network), 순환신경망(recurrent neural network), 그리고 심층신뢰신경망(deep belief network)의 세가지라고 할 수있다. 그 중에서 현재 흥미로운 연구가 많이 발표되어서 관심이 집중되고 있는 모형은 지도학습(supervised learning)모형인 처음 두 개의 모형이다. 따라서 본 논문에서는 지도학습모형의 가중치를 최적화하는 기본적인 방법인 오류역전파 알고리즘을 살펴본 뒤에 합성곱신경망과 순환신경망의 구조와 응용사례 등을 살펴보고자 한다. 본문에서 다루지 않은 모형인 심층신뢰신경망은 아직까지는 합성곱신경망 이나 순환신경망보다는 상대적으로 주목을 덜 받고 있다. 그러나 심층신뢰신경망은 CNN이나 RNN과는 달리 비지도학습(unsupervised learning)모형이며, 사람이나 동물은 관찰을 통해서 스스로 학습한다는 점에서 궁극적으로는 비지도학습모형이 더 많이 연구되어야 할 주제가 될 것이다.Deep learning model is a kind of neural networks that allows multiple hidden layers. There are various deep learning architectures such as convolutional neural networks, deep belief networks and recurrent neural networks. Those have been applied to fields like computer vision, automatic speech recognition, natural language processing, audio recognition and bioinformatics where they have been shown to produce state-of-the-art results on various tasks. Among those architectures, convolutional neural networks and recurrent neural networks are classified as the supervised learning model. And in recent years, those supervised learning models have gained more popularity than unsupervised learning models such as deep belief networks, because supervised learning models have shown fashionable applications in such fields mentioned above.
Deep learning models can be trained with backpropagation algorithm. Backpropagation is an abbreviation for “backward propagation of errors” and a common method of training artificial neural networks used in conjunction with an optimization method such as gradient descent. The method calculates the gradient of an error function with respect to all the weights in the network. The gradient is fed to the optimization method which in turn uses it to update the weights, in an attempt to minimize the error function.
Convolutional neural networks use a special architecture which is particularly well-adapted to classify images. Using this architecture makes convolutional networks fast to train. This, in turn, helps us train deep, muti-layer networks, which are very good at classifying images. These days, deep convolutional networks are used in most neural networks for image recognition. Convolutional neural networks use three basic ideas: local receptive fields, shared weights, and pooling. By local receptive fields, we mean that each neuron in the first(or any) hidden layer will be connected to a small region of the input(or previous layer’s) neurons. Shared weights mean that we’re going to use the same weights and bias for each of the local receptive field. This means that all the neurons in the hidden layer detect exactly the same feature, just at different locations in the input image. In addition to the convolutional layers just described, convolutional neural networks also contain pooling layers. Pooling layers are usually used immediately after convolutional layers. What the pooling layers do is to simplify the information in the output from the convolutional layer.
Recent convolutional network architectures have 10 to 20 hidden layers and billions of connections between units. Training deep learning networks has taken weeks several years ago, but thanks to progress in GPU and algorithm enhancement, training time has reduced to several hours.
Neural networks with time-varying behavior are known as recurrent neural networks or RNNs. A recurrent neural network is a class of artificial neural network where connections between units form a directed cycle. This creates an internal state of the network which allows it to exhibit dynamic temporal behavior. Unlike feedforward neural networks, RNNs can use their internal memory to process arbitrary sequences of inputs. Early RNN models turned out to be very difficult to train, harder even than deep feedforward networks. The reason is the unstable gradient problem such as vanishing gradient and exploding gradient. The gradient can get smaller and smaller as it is propagated back through layers. This makes learning in early layers extremely slow. The problem actually gets worse in RNNs, since gradients aren’t just propagated backward through layers, they’re propagated backward through time. If the network runs for a long time, that can make the gradient extremely unstable and hard to learn from. It has been possible to incorporate an idea known as long short-term memory units (LSTMs) into RNNs. LSTMs make it much easier to get good results when training RNNs, and many recent papers make use of LSTMs or related ideas.Deep learning, Convolutional neural networks, Recurrent neural networks, Error backpropagation algorithm"
136,딥러닝분석과 기술적 분석 지표를 이용한 한국 코스피주가지수 방향성 예측,이우식 /Lee Woosik,2017,한국데이터정보과학회,"2016년 3월 구글 (Google)의 바둑인공지능 알파고 (AlphaGo)가 이세돌 9단과의 바둑대결에서승리한 이후 다양한 분야에서 인공지능 사용에 대한 관심이 높아지고 있는 가운데 금융투자 분야에서도 인공지능과 투자자문 전문가의 합성어인 로보어드바이저 (Robo-Advisor)에 대한 관심이 높아지고 있다. 인공지능 (artificial intelligence)기반의 의사결정은 비용 절감은 물론 효과적인 의사결정을가능하게 한다는 점에서 큰 장점이 있다. 본 연구에서는 기술적 분석 (technical analysis) 지표와 딥러닝 (deep learning) 모형을 결합하여 한국 코스피 지수를 예측하는 모형을 개발하고 제시한 모형들의 예측력을 비교, 분석한다. 분석 결과 기술적 분석 지표에 딥러닝 알고리즘을 결합한 모형이 주가지수 방향성 예측 문제에 응용될 수 있음을 확인하였다. 향후 본 연구에서 제안된 기술적 분석 지표와딥러닝모형을 결합한 기법은 로보어드바이저서비스에 응용할 수 있는 일반화 가능성을 보여준다Since Google’s AlphaGo defeated a world champion of Go players in 2016, there have been many interests in the deep learning. In the financial sector, a Robo-Advisor using deep learning gains a significant attention, which builds and manages portfolios of financial instruments for investors.In this paper, we have proposed the a deep learning algorithm geared toward identification and forecast of the KOSPI index direction,and we also have compared the accuracy of the prediction.In an application of forecasting the financial market index direction, we have shown that the Robo-Advisor using deep learning has a significant effect on finance industry. The Robo-Advisor collects a massive data such as earnings statements, news reports and regulatory filings, analyzes those and recommends investors how to view market trends and identify the best time to purchase financial assets. On the other hand, the Robo-Advisor allows businesses to learn more about their customers, develop better marketing strategies, increase sales and decrease costs.Artificial intelligence, deep learning, FinTech, Robo-Advisor, technical analysis."
137,딥러닝의 효율적인 학습을 위한 학습데이터 선별 기법에 관한 연구,주경돈 /Gyoung-don Joo,2017,한국정보과학회,"빅데이터의 증가와 병렬 하드웨어 처리, 학습 알고리즘의 개선은 딥러닝의 발전을 가져왔으며 인공지능이 적용 가능한 영역을 획기적으로 확장시키고 있다. 하지만 이러한 대규모의 학습 데이터에 기반한 딥러닝은 학습시간이 증가되고, 더 나아가 학습 데이터의 구축을 위한 레이블링(labeling) 비용이 데이터 수에 비례하여 증가하게 된다. 따라서 본 논문에서는, 딥러닝의 정확도를 최대한 유지하면서 필요한 학습 데이터의 수를 줄일수 있도록, 전체 데이터에서 제한된 수의 학습 데이터의 후보를 선별하는 기법에 대하여 연구한다. 특히 기존의 능동 학습에 기반한 기법들이 학습 데이터의 수가 매우 적어질 경우 딥러닝의 정확도가 급격히 떨어지는 문제를 해결할 수 있는 새로운 기법을 제안하였으며, 제안된 기법으로 선별된 학습 데이터는 그 수의 변화에 대해 딥러닝이 일관되게 경쟁력있는 정확도를 가짐을 다양한 실험을 통하여 검증하였다.The increase of big data, parallel hardware processing and enhanced learning algorithms have led to the development of deep learning and are dramatically expanding the domain of applications based on artificial intelligence. However, such large-scale training data extends the learning time, and furthermore, the labeling cost for building the training set increases in proportion to the volume of data. Therefore, in this paper, we study how to select a limited number of candidates for training set from given data so as to reduce the number of training set while maintaining the accuracy of deep learning. Especially, we found that, with the training set selected by the previous work, the accuracy of deep learning drastically decreases when the number of training set is very small and so proposed new methods to resolve this fall-off in accuracy. Through experiments, we showed that deep learning consistently has the competitive accuracy with the training set selected by the proposed method regardless of the size of training set.Deep Learning, Train Set Selection, Active Learning, Gamma distribution"
138,딥러닝을 이용한 범용적 스테그아날리시스,김현재 /Hyunjae Kim,2017,한국정보과학회,"스테그아날리시스(Steganalysis)란 이미지 등 일반적인 자료에 암호화된 정보를 은닉하는 스테가노그래피(Steganography)에 대한 검출 및 분석 방법으로, 기계학습 기반 방법론을 포함한다. 기존 기계학습 기반 스테그아날리시스는 영상(Image)의 특징(Feature) 추출 및 모델링에 기반하며, 최근 딥러닝(Deep Learning)의 적용으로 검출 정확도가 큰 폭으로 향상되었다. 하지만 현존하는 스테그아날리시스 모델은 단일 스테가노그래피 기법에 대해 국한되어 있어 학습에 사용되지 않은 스테고(Stego) 이미지의 경우 검출이 불가능한 결정적 한계를 가진다. 본 연구에서는 다양한 스테가노그래피 기법으로 생성된 스테고이미지에 딥러닝을 적용하여 스테그아날리시스를 학습하는 범용적 모델을 제안한다. 다양한 실험을 통해 제안 기법의 효용성 및 가능성을 확인하고, 범용적 스테그아날리시스 모델이 각각에 특화된 검출 기법과 유사한 정확도로 스테고 이미지를 검출할 수 있음을 보인다.Steganalysis is to detect information hidden by steganography inside general data such as images. There are stegoanalysis techniques that use machine learning (ML). Existing ML approaches to steganalysis are based on extracting features from stego images and modeling them.
Recently deep learning-based methodologies have shown significant improvements in detection accuracy. However, all the existing methods, including deep learning-based ones, have a critical limitation in that they can only detect stego images that are created by a specific steganography method. In this paper, we propose a generalized steganalysis method that can model multiple types of stego images using deep learning. Through various experiments, we confirm the effectiveness of our approach and envision directions for future research. In particular, we show that our method can detect each type of steganography with the same level of accuracy as that of a steganalysis method dedicated to that type of steganography, thereby demonstrating the general applicability of our approach to multiple types of stego images.steganalysis, steganography, deep learning, generalized steganalysis model"
139,"딥러닝 프레임워크의 비교: 티아노, 텐서플로, CNTK를 중심으로",정여진 /Yeojin Chung,2017,한국지능정보시스템학회,"딥러닝 프레임워크의 대표적인 기능으로는 ‘자동미분’과 ‘GPU의 활용’ 등을 들 수 있다. 본 논문은 파이썬의 라이브러리 형태로 사용 가능한 프레임워크 중에서 구글의 텐서플로와 마이크로소프트의 CNTK, 그리고 텐서 플로의 원조라고 할 수 있는 티아노를 비교하였다. 본문에서는 자동미분의 개념과 GPU의 활용형태를 간단히 설명하고, 그 다음에 logistic regression을 실행하는 예를 통하여 각 프레임워크의 문법을 알아본 뒤에, 마지막으로 대표적인 딥러닝 응용인 CNN의 예제를 실행시켜보고 코딩의 편의성과 실행속도 등을 확인해 보았다. 그 결과, 편의성의 관점에서 보면 티아노가 가장 코딩 하기가 어렵고, CNTK와 텐서플로는 많은 부분이 비슷하게 추상화 되어 있어서 코딩이 비슷하지만 가중치와 편향을 직접 정의하느냐의 여부에서 차이를 보였다. 그리고 각프레임워크의 실행속도에 대한 평가는 ‘큰 차이는 없다’는 것이다. 텐서플로는 티아노에 비하여 속도가 느리다는 평가가 있어왔는데, 본 연구의 실험에 의하면, 비록 CNN 모형에 국한되었지만, 텐서플로가 아주 조금이지만빠른 것으로 나타났다. CNTK의 경우에도, 비록 실험환경이 달랐지만, 실험환경의 차이에 의한 속도의 차이의 편차범위 이내에 있는 것으로 판단이 되었다. 본 연구에서는 세 종류의 딥러닝 프레임워크만을 살펴보았는데,위키피디아에 따르면 딥러닝 프레임워크의 종류는 12가지가 있으며, 각 프레임워크의 특징을 15가지 속성으로구분하여 차이를 특정하고 있다. 그 많은 속성 중에서 사용자의 입장에서 볼 때 중요한 속성은 어떤 언어(파이썬, C++, Java, 등)로 사용가능한지, 어떤 딥러닝 모형에 대한 라이브러리가 잘 구현되어 있는지 등일 것이다. 그리고 사용자가 대규모의 딥러닝 모형을 구축한다면, 다중 GPU 혹은 다중 서버를 지원하는지의 여부도 중요할 것이다. 또한 딥러닝 모형을 처음 학습하는 경우에는 사용설명서가 많은지 예제 프로그램이 많은지 여부도 중요한 기준이 될 것이다.The deep learning framework is software designed to help develop deep learning models. Some of its important functions include “automatic differentiation” and “utilization of GPU”. The list of popular deep learning framework includes Caffe (BVLC) and Theano (University of Montreal). And recently, Microsoft's deep learning framework, Microsoft Cognitive Toolkit, was released as open-source license, following Google’s Tensorflow a year earlier. The early deep learning frameworks have been developed mainly for research at universities. Beginning with the inception of Tensorflow, however, it seems that companies such as Microsoft and Facebook have started to join the competition of framework development. Given the trend, Google and other companies are expected to continue investing in the deep learning framework to bring forward the initiative in the artificial intelligence business. From this point of view, we think it is a good time to compare some of deep learning frameworks. So we compare three deep learning frameworks which can be used as a Python library. Those are Google's Tensorflow, Microsoft’s CNTK, and Theano which is sort of a predecessor of the preceding two.
The most common and important function of deep learning frameworks is the ability to perform automatic differentiation. Basically all the mathematical expressions of deep learning models can be represented as computational graphs, which consist of nodes and edges. Partial derivatives on each edge of a computational graph can then be obtained. With the partial derivatives, we can let software compute differentiation of any node with respect to any variable by utilizing chain rule of Calculus.
First of all, the convenience of coding is in the order of CNTK, Tensorflow, and Theano. The criterion is simply based on the lengths of the codes and the learning curve and the ease of coding are not the main concern. According to the criteria, Theano was the most difficult to implement with, and CNTK and Tensorflow were somewhat easier. With Tensorflow, we need to define weight variables and biases explicitly. The reason that CNTK and Tensorflow are easier to implement with is that those frameworks provide us with more abstraction than Theano. We, however, need to mention that low-level coding is not always bad. It gives us flexibility of coding. With the low-level coding such as in Theano, we can implement and test any new deep learning models or any new search methods that we can think of.
The assessment of the execution speed of each framework is that there is not meaningful difference. According to the experiment, execution speeds of Theano and Tensorflow are very similar, although the experiment was limited to a CNN model. In the case of CNTK, the experimental environment was not maintained as the same. The code written in CNTK has to be run in PC environment without GPU where codes execute as much as 50 times slower than with GPU. But we concluded that the difference of execution speed was within the range of variation caused by the different hardware setup.
In this study, we compared three types of deep learning framework: Theano, Tensorflow, and CNTK. According to Wikipedia, there are 12 available deep learning frameworks. And 15 different attributes differentiate each framework. Some of the important attributes would include interface language (Python, C ++, Java, etc.) and the availability of libraries on various deep learning models such as CNN, RNN, DBN, and etc. And if a user implements a large scale deep learning model, it will also be important to support multiple GPU or multiple servers. Also, if you are learning the deep learning model, it would also be important if there are enough examples and references.deep learning framework, Theano, TensorFlow, CNTK, computational graph, CIFAR-10"
140,딥러닝을 이용한 주변 무선단말 파악방안,이웅섭 /Lee， Woong-sup,2018,한국정보통신학회,"최근 단말-대-단말(Device-to-device, D2D) 통신기술이 차세대 무선통신시스템의 핵심기술로 큰 관심을 받고 있다. 이러한 단말간 통신에서는 자신의 주변에 어떠한 단말이 있는지 파악하는 주변단말 탐색(Neighbor discovery)이매우 중요하다. 본 논문에서는 최근 큰 관심을 받고 있는 딥러닝(Deep learning) 기술을 활용하여 단말간 통신에서 주변단말을 파악하는 방안에 대해서 제안한다. 제안 방안은 기존의 방안과 달리 무선채널의 공간적 연관성을 이용하여단말간의 신호 전송 없이 단말이 기지국으로 전송하는 상향링크 파일럿 신호를 기반으로 주변 단말을 찾고 따라서기존의 방식에 비해 신호전송 복잡도(signaling complexity)를 크게 줄일 수 있다. 또한 제안 방안에서는 떨어져 있는거리에 따라서 주변 단말을 분류 가능하여 기존 방안에 비해서 좀 더 세밀한 단말 탐색이 가능하다. 마지막으로 본 논문에서는 tensorflow를 이용한 컴퓨터 시뮬레이션을 통해 제안 방안의 성능을 검증하였다.Recently, the device-to-device (D2D) communication has been conceived as the key technology for the next-generationmobile communication systems. The neighbor discovery in which the nearby users are found, is essential for the properoperation of the D2D communication. In this paper, we propose new neighbor discovery scheme based on deep learningtechnology which has gained a lot of attention recently. In the proposed scheme, the neighboring users can be foundusing the uplink pilot transmission of users only, unlike conventional neighbor discovery schemes in which direct pilotcommunication among users is required, such that the signaling overhead can be greatly reduced in our proposed scheme.
Moreover, the neighbors with different proximity can also be classified accordingly which enables more accurate neighbordiscovery compared to the conventional schemes. The performance of our proposed scheme is verified through thetensorflow-based computer simulations.
키워드 : 딥러닝, 주변단말 탐색, 단말-대-단말 통신Deep learning, Neighbor discovery, Device-to-device communication, Wireless channel correlation"
141,신뢰성있는 딥러닝 기반 분석 모델을 참조하기 위한 딥러닝 기술 언어,문종혁 /Jong Hyeok Mun,2021,한국정보처리학회,"최근 딥러닝은 하드웨어 성능이 향상됨에 따라 자연어 처리, 영상 인식 등의 다양한 기술에 접목되어 활용되고 있다. 이러한 기술들을 활용해 지능형교통 시스템(ITS), 스마트홈, 헬스케어 등의 산업분야에서 데이터를 분석하여 고속도로 속도위반 차량 검출, 에너지 사용량 제어, 응급상황 등과 같은고품질의 서비스를 제공하며, 고품질의 서비스를 제공하기 위해서는 정확도가 향상된 딥러닝 모델이 적용되어야 한다. 이를 위해 서비스 환경의 데이터를분석하기 위한 딥러닝 모델을 개발할 때, 개발자는 신뢰성이 검증된 최신의 딥러닝 모델을 적용할 수 있어야 한다. 이는 개발자가 참조하는 딥러닝 모델에적용된 학습 데이터셋의 정확도를 측정하여 검증할 수 있다. 이러한 검증을 위해서 개발자는 학습 데이터셋, 딥러닝의 계층구조 및 개발 환경 등과 같은내용을 포함하는 딥러닝 모델을 문서화하여 적용하기 위한 구조적인 정보가 필요하다. 본 논문에서는 신뢰성있는 딥러닝 기반 데이터 분석 모델을 참조하기위한 딥러닝 기술 언어를 제안한다. 제안하는 기술 언어는 신뢰성 있는 딥러닝 모델을 개발하는데 필요한 학습데이터셋, 개발 환경 및 설정 등의 정보와더불어 딥러닝 모델의 계층구조를 표현할 수 있다. 제안하는 딥러닝 기술 언어를 이용하여 개발자는 지능형 교통 시스템에서 참조하는 분석 모델의정확도를 검증할 수 있다. 실험에서는 제안하는 언어의 유효성을 검증하기 위해, 번호판 인식 모델을 중심으로 딥러닝 기술 문서의 적용과정을 보인다With the recent advancements of deep learning, companies such as smart home, healthcare, and intelligent transportation systems areutilizing its functionality to provide high-quality services for vehicle detection, emergency situation detection, and controlling energyconsumption. To provide reliable services in such sensitive systems, deep learning models are required to have high accuracy. In orderto develop a deep learning model for analyzing previously mentioned services, developers should utilize the state of the art deep learningmodels that have already been verified for higher accuracy. The developers can verify the accuracy of the referenced model by validatingthe model on the dataset. For this validation, the developer needs structural information to document and apply deep learning models,including metadata such as learning dataset, network architecture, and development environments. In this paper, we propose a descriptionlanguage that represents the network architecture of the deep learning model along with its metadata that are necessary to develop adeep learning model. Through the proposed description language, developers can easily verify the accuracy of the referenced deep learningmodel. Our experiments demonstrate the application scenario of a deep learning description document that focuses on the license platerecognition for the detection of illegally parked vehicles.Trusted Deep Learning, Model Reference, Deep Learning Description Language, Traffic Situation Analysis Model"
142,딥러닝 분석을 이용한 중국 역내·외 위안화 변동성 예측,이우식 /Lee Woosik,2016,한국데이터정보과학회,"2008년 글로벌 금융위기 이후 중국은 위안화 국제화의 점진적 추진을 진행하면서 중국상하이 외환시장과 중국홍콩 외환시장에서 거래되는 통화인 역내위안화와 역외위안화를 형성시켰다. 본 연구는 위안화 국제화와 점진적인 중국 자본계정 개방에 따라 급변하는 외환시장상황의 변동성을 정확하게 파악하기 위해서 GARCH모형 (일반화된 자기회귀 조건부이분산성모형)에 다단계인공신경망을 결합한 MLP-GARCH 모형과 GARCH모형과 기계학습의 일종인 딥러닝 (deep learning)을 통합한 DL-GARCH을 가지고 위안화 변동성예측을 비교 실험과 분석을 하였다. 비교분석 결과 DLGARCH 모형은 MLP-GARCH보다 모형 위안화 역내·외 환율변동성 예측 면에서 더욱 더 개선된예측값을 제공하였다. 그래서 이분산시계열모형을 딥러닝과 결합한 DL-GARCH 모형은 시계열의환율 변동성 예측 문제에 딥러닝을 응용할 수 있음을 확인하였다. 향후 이분산시계열과 결합된 딥러닝 모형은 다른 금융시계열 데이터에 응용하여 그 일반화 가능성을 높일 수 있을 것이다.The People’s Republic of China has vigorously been pursuing the internationalization of the Chinese Yuan or Renminbi after the financial crisis of 2008. In this view, an abrupt increase of use of the Chinese Yuan in the onshore and offshore markets are important milestones to be one of important currencies. One of the most frequently used methods to forecast volatility is GARCH model. Since a prediction error of the GARCH model has been reported quite high, a lot of efforts have been made to improve forecasting capability of the GARCH model. In this paper, we have proposed MLPGARCH and a DL-GARCH by employing Artificial Neural Network to the GARCH. In an application to forecasting Chinese Yuan volatility, we have successfully shown their overall outperformance in forecasting over the GARCH.Deep learning, DL-GARCH, GARCH, The Chinese Yuan, volatility."
143,딥러닝 기법을 이용한 내일강수 예측,하지훈,2016,한국지능시스템학회,"정확한 강수예측을 위해서는 예측인자 선정과 예측방법에 대한 선택이 매우 중요하다. 최근에는 강수예측 방법으로기계학습 기법이 많이 사용되고 있으며, 그 중에서도 특히 인공신경망을 사용한 강수예측 방법은 좋은 성능을 보였다. 본논문에서는 딥러닝 기법 중 하나인 DBN(deep belief network)를 이용한 새로운 강수예측 방법을 제안한다. DBN는 비지도사전 학습을 통해 초기 가중치를 설정하여 기존 인공신경망의 문제점을 보완한다. 예측인자로는 기온, 전일-전주 강수일, 태양과 달 궤도 관련 자료를 선정하였다. 기온과 전일-전주 강수일은 서울에서의 1974년부터 2013년까지 총 40년간의AWS(automatic weather system) 관측 자료를 사용하였고, 태양과 달의 궤도 관련 자료는 서울을 중심으로 계산한 결과를사용하였다. 전체 기간에서 일부는 학습 자료로 사용하여 예측모델을 생성하였고, 나머지를 생성한 모델의 검증 자료로사용하였다. 모델 검증 결과로 나온 예측값들은 확률값을 가지며 임계치를 이용하여 강수유무를 판별하였다. 강수 정확도의척도로 양분예보기법 중 CSI(critical successive index)와 Bias(frequency bias)를 계산하였다. 이를 통해 DBN와 MLP(multilayer perceptron)의 성능을 비교한 결과 DBN의 강수 예측 정확도가 높았고, 수행속도 또한 2배 이상 빨랐다.For accurate precipitation forecasts the choice of weather faoctrs and prediction method is very important. Recently, machinele arning has been widely used for forecasting precipitation, and artificial neural network, one of machine learning techniques, showed good performance. In this paper, we suggest a new method for forecasting precipitation using DBN, one of deep learning techniques.
DBN has an advantage that initial weights are set by unsupervised learning, so this compensates for the defects of artificial neural networks. We used past precipitation, temperature, and the parameters of the sun and moon's motion as features for forecasting precipitation. The dataset consists of observation data which had been measured for 40 years from AWS in Seoul. Experiments were based on 8-fold cross validation. As a result of estimation, we got probabilities of test dataset, so threshold was used for the decision of precipitation. CSI and Bias were used for indicating the precision of precipitation. Our experimental results showed that DBN performed better than MLP.Deep learning, Deep belief network, Precipitation, Forecast"
144,딥러닝 신경망모형을 이용한 실시간 교통정보수집 연구,김대현 /KIM DAE HYON,2016,사단법인 인문사회과학기술융합학회,"비디오 기반의 실시간 교통정보수집 시스템은 자율주행을 포함한 지능형교통체계의 주요 서브시스템중 하나이며, 우수한 예측(추정)력 때문에 핵심 알고리즘으로 인공신경망 모형이 주로 사용되고 있다. 특히, 최근 과학 학술지 네이처에 구글 딥마인드의 알파고가 공개되면서 딥러닝과 인공지능에 대한 전 세계적인 관심이 집중되고 있다. 최근까지 가장 널리 사용된 인공신경망모형은 역전파 모형이었으나 최근 컴퓨터 하드웨어 및 알고리즘의 새로운 개발과 더불어 딥러닝 알고리즘을 이용한 딥인공신경망 모형에 대한 관심이 높아지고 있다. 그러나 아직 딥러닝 인공신경망에 대한 연구는 초기단계에 있는 실정이며 적용분야도 극히 한정되어 있다. 본 연구에서는 실시간 도로교통정보 수집을 위한 영상처리분야에서 딥러닝 알고리즘을 활용하고 실험결과를 통하여 향후 교통정보수집‧분석 분야에서의 딥러닝 인공신경망 모형의 적용가능성을 살펴보고자 한다. 특히, 본 연구에서 도출한 딥러닝 인공신경망모형의 실험결과를 기존 역전파 모형에 의한 실험결과와 비교하여 모형의 우수성을 비교‧평가하고자 한다.Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Normally, artificial neural networks have been used in real-time applications for collecting traffic information because of their remarkable performance in terms of predictive accuracy and real-time computing. Especially, the recent publication of a paper in the journal Nature and the success of Google DeepMind’s world-beating Go program AlphaGo caused significant increases of the public interest in artificial intelligence. Even though various types of learning models have been proposed, the Backpropagation has been the most popular neural network model. However, the Deep learning neural network model has attracted much interest recently. In this study, the Deep leaning neural network will be applied in image processing techniques for the real-time traffic information collection to investigate the performance of Deep learning algorithm in traffic engineering fields. In addition, the experimental results from Deep learing algorithm will be compared with the results from Backpropagation model to evaluate and propose better performing model.Intelligent Transportation Systems (ITS), artificial neural networks, Deep learning, traffic information, image processing"
145,딥러닝의 모형과 응용사례,안성만 /SungMahn Ahn,2016,한국지능정보시스템학회,"딥러닝은 인공신경망(neural network)이라는 인공지능분야의 모형이 발전된 형태로서, 계층구조로 이루어진 인공신경망의 내부계층(hidden layer)이 여러 단계로 이루어진 구조이다. 딥러닝에서의 주요 모형은 합성곱신경망(convolutional neural network), 순환신경망(recurrent neural network), 그리고 심층신뢰신경망(deep belief network)의 세가지라고 할 수있다. 그 중에서 현재 흥미로운 연구가 많이 발표되어서 관심이 집중되고 있는 모형은 지도학습(supervised learning)모형인 처음 두 개의 모형이다. 따라서 본 논문에서는 지도학습모형의 가중치를 최적화하는 기본적인 방법인 오류역전파 알고리즘을 살펴본 뒤에 합성곱신경망과 순환신경망의 구조와 응용사례 등을 살펴보고자 한다. 본문에서 다루지 않은 모형인 심층신뢰신경망은 아직까지는 합성곱신경망 이나 순환신경망보다는 상대적으로 주목을 덜 받고 있다. 그러나 심층신뢰신경망은 CNN이나 RNN과는 달리 비지도학습(unsupervised learning)모형이며, 사람이나 동물은 관찰을 통해서 스스로 학습한다는 점에서 궁극적으로는 비지도학습모형이 더 많이 연구되어야 할 주제가 될 것이다.Deep learning model is a kind of neural networks that allows multiple hidden layers. There are various deep learning architectures such as convolutional neural networks, deep belief networks and recurrent neural networks. Those have been applied to fields like computer vision, automatic speech recognition, natural language processing, audio recognition and bioinformatics where they have been shown to produce state-of-the-art results on various tasks. Among those architectures, convolutional neural networks and recurrent neural networks are classified as the supervised learning model. And in recent years, those supervised learning models have gained more popularity than unsupervised learning models such as deep belief networks, because supervised learning models have shown fashionable applications in such fields mentioned above.
Deep learning models can be trained with backpropagation algorithm. Backpropagation is an abbreviation for “backward propagation of errors” and a common method of training artificial neural networks used in conjunction with an optimization method such as gradient descent. The method calculates the gradient of an error function with respect to all the weights in the network. The gradient is fed to the optimization method which in turn uses it to update the weights, in an attempt to minimize the error function.
Convolutional neural networks use a special architecture which is particularly well-adapted to classify images. Using this architecture makes convolutional networks fast to train. This, in turn, helps us train deep, muti-layer networks, which are very good at classifying images. These days, deep convolutional networks are used in most neural networks for image recognition. Convolutional neural networks use three basic ideas: local receptive fields, shared weights, and pooling. By local receptive fields, we mean that each neuron in the first(or any) hidden layer will be connected to a small region of the input(or previous layer’s) neurons. Shared weights mean that we’re going to use the same weights and bias for each of the local receptive field. This means that all the neurons in the hidden layer detect exactly the same feature, just at different locations in the input image. In addition to the convolutional layers just described, convolutional neural networks also contain pooling layers. Pooling layers are usually used immediately after convolutional layers. What the pooling layers do is to simplify the information in the output from the convolutional layer.
Recent convolutional network architectures have 10 to 20 hidden layers and billions of connections between units. Training deep learning networks has taken weeks several years ago, but thanks to progress in GPU and algorithm enhancement, training time has reduced to several hours.
Neural networks with time-varying behavior are known as recurrent neural networks or RNNs. A recurrent neural network is a class of artificial neural network where connections between units form a directed cycle. This creates an internal state of the network which allows it to exhibit dynamic temporal behavior. Unlike feedforward neural networks, RNNs can use their internal memory to process arbitrary sequences of inputs. Early RNN models turned out to be very difficult to train, harder even than deep feedforward networks. The reason is the unstable gradient problem such as vanishing gradient and exploding gradient. The gradient can get smaller and smaller as it is propagated back through layers. This makes learning in early layers extremely slow. The problem actually gets worse in RNNs, since gradients aren’t just propagated backward through layers, they’re propagated backward through time. If the network runs for a long time, that can make the gradient extremely unstable and hard to learn from. It has been possible to incorporate an idea known as long short-term memory units (LSTMs) into RNNs. LSTMs make it much easier to get good results when training RNNs, and many recent papers make use of LSTMs or related ideas.Deep learning, Convolutional neural networks, Recurrent neural networks, Error backpropagation algorithm"
146,딥러닝분석과 기술적 분석 지표를 이용한 한국 코스피주가지수 방향성 예측,이우식 /Lee Woosik,2017,한국데이터정보과학회,"2016년 3월 구글 (Google)의 바둑인공지능 알파고 (AlphaGo)가 이세돌 9단과의 바둑대결에서승리한 이후 다양한 분야에서 인공지능 사용에 대한 관심이 높아지고 있는 가운데 금융투자 분야에서도 인공지능과 투자자문 전문가의 합성어인 로보어드바이저 (Robo-Advisor)에 대한 관심이 높아지고 있다. 인공지능 (artificial intelligence)기반의 의사결정은 비용 절감은 물론 효과적인 의사결정을가능하게 한다는 점에서 큰 장점이 있다. 본 연구에서는 기술적 분석 (technical analysis) 지표와 딥러닝 (deep learning) 모형을 결합하여 한국 코스피 지수를 예측하는 모형을 개발하고 제시한 모형들의 예측력을 비교, 분석한다. 분석 결과 기술적 분석 지표에 딥러닝 알고리즘을 결합한 모형이 주가지수 방향성 예측 문제에 응용될 수 있음을 확인하였다. 향후 본 연구에서 제안된 기술적 분석 지표와딥러닝모형을 결합한 기법은 로보어드바이저서비스에 응용할 수 있는 일반화 가능성을 보여준다Since Google’s AlphaGo defeated a world champion of Go players in 2016, there have been many interests in the deep learning. In the financial sector, a Robo-Advisor using deep learning gains a significant attention, which builds and manages portfolios of financial instruments for investors.In this paper, we have proposed the a deep learning algorithm geared toward identification and forecast of the KOSPI index direction,and we also have compared the accuracy of the prediction.In an application of forecasting the financial market index direction, we have shown that the Robo-Advisor using deep learning has a significant effect on finance industry. The Robo-Advisor collects a massive data such as earnings statements, news reports and regulatory filings, analyzes those and recommends investors how to view market trends and identify the best time to purchase financial assets. On the other hand, the Robo-Advisor allows businesses to learn more about their customers, develop better marketing strategies, increase sales and decrease costs.Artificial intelligence, deep learning, FinTech, Robo-Advisor, technical analysis."
147,딥러닝의 효율적인 학습을 위한 학습데이터 선별 기법에 관한 연구,주경돈 /Gyoung-don Joo,2017,한국정보과학회,"빅데이터의 증가와 병렬 하드웨어 처리, 학습 알고리즘의 개선은 딥러닝의 발전을 가져왔으며 인공지능이 적용 가능한 영역을 획기적으로 확장시키고 있다. 하지만 이러한 대규모의 학습 데이터에 기반한 딥러닝은 학습시간이 증가되고, 더 나아가 학습 데이터의 구축을 위한 레이블링(labeling) 비용이 데이터 수에 비례하여 증가하게 된다. 따라서 본 논문에서는, 딥러닝의 정확도를 최대한 유지하면서 필요한 학습 데이터의 수를 줄일수 있도록, 전체 데이터에서 제한된 수의 학습 데이터의 후보를 선별하는 기법에 대하여 연구한다. 특히 기존의 능동 학습에 기반한 기법들이 학습 데이터의 수가 매우 적어질 경우 딥러닝의 정확도가 급격히 떨어지는 문제를 해결할 수 있는 새로운 기법을 제안하였으며, 제안된 기법으로 선별된 학습 데이터는 그 수의 변화에 대해 딥러닝이 일관되게 경쟁력있는 정확도를 가짐을 다양한 실험을 통하여 검증하였다.The increase of big data, parallel hardware processing and enhanced learning algorithms have led to the development of deep learning and are dramatically expanding the domain of applications based on artificial intelligence. However, such large-scale training data extends the learning time, and furthermore, the labeling cost for building the training set increases in proportion to the volume of data. Therefore, in this paper, we study how to select a limited number of candidates for training set from given data so as to reduce the number of training set while maintaining the accuracy of deep learning. Especially, we found that, with the training set selected by the previous work, the accuracy of deep learning drastically decreases when the number of training set is very small and so proposed new methods to resolve this fall-off in accuracy. Through experiments, we showed that deep learning consistently has the competitive accuracy with the training set selected by the proposed method regardless of the size of training set.Deep Learning, Train Set Selection, Active Learning, Gamma distribution"
148,딥러닝을 이용한 범용적 스테그아날리시스,김현재 /Hyunjae Kim,2017,한국정보과학회,"스테그아날리시스(Steganalysis)란 이미지 등 일반적인 자료에 암호화된 정보를 은닉하는 스테가노그래피(Steganography)에 대한 검출 및 분석 방법으로, 기계학습 기반 방법론을 포함한다. 기존 기계학습 기반 스테그아날리시스는 영상(Image)의 특징(Feature) 추출 및 모델링에 기반하며, 최근 딥러닝(Deep Learning)의 적용으로 검출 정확도가 큰 폭으로 향상되었다. 하지만 현존하는 스테그아날리시스 모델은 단일 스테가노그래피 기법에 대해 국한되어 있어 학습에 사용되지 않은 스테고(Stego) 이미지의 경우 검출이 불가능한 결정적 한계를 가진다. 본 연구에서는 다양한 스테가노그래피 기법으로 생성된 스테고이미지에 딥러닝을 적용하여 스테그아날리시스를 학습하는 범용적 모델을 제안한다. 다양한 실험을 통해 제안 기법의 효용성 및 가능성을 확인하고, 범용적 스테그아날리시스 모델이 각각에 특화된 검출 기법과 유사한 정확도로 스테고 이미지를 검출할 수 있음을 보인다.Steganalysis is to detect information hidden by steganography inside general data such as images. There are stegoanalysis techniques that use machine learning (ML). Existing ML approaches to steganalysis are based on extracting features from stego images and modeling them.
Recently deep learning-based methodologies have shown significant improvements in detection accuracy. However, all the existing methods, including deep learning-based ones, have a critical limitation in that they can only detect stego images that are created by a specific steganography method. In this paper, we propose a generalized steganalysis method that can model multiple types of stego images using deep learning. Through various experiments, we confirm the effectiveness of our approach and envision directions for future research. In particular, we show that our method can detect each type of steganography with the same level of accuracy as that of a steganalysis method dedicated to that type of steganography, thereby demonstrating the general applicability of our approach to multiple types of stego images.steganalysis, steganography, deep learning, generalized steganalysis model"
149,"딥러닝 프레임워크의 비교: 티아노, 텐서플로, CNTK를 중심으로",정여진 /Yeojin Chung,2017,한국지능정보시스템학회,"딥러닝 프레임워크의 대표적인 기능으로는 ‘자동미분’과 ‘GPU의 활용’ 등을 들 수 있다. 본 논문은 파이썬의 라이브러리 형태로 사용 가능한 프레임워크 중에서 구글의 텐서플로와 마이크로소프트의 CNTK, 그리고 텐서 플로의 원조라고 할 수 있는 티아노를 비교하였다. 본문에서는 자동미분의 개념과 GPU의 활용형태를 간단히 설명하고, 그 다음에 logistic regression을 실행하는 예를 통하여 각 프레임워크의 문법을 알아본 뒤에, 마지막으로 대표적인 딥러닝 응용인 CNN의 예제를 실행시켜보고 코딩의 편의성과 실행속도 등을 확인해 보았다. 그 결과, 편의성의 관점에서 보면 티아노가 가장 코딩 하기가 어렵고, CNTK와 텐서플로는 많은 부분이 비슷하게 추상화 되어 있어서 코딩이 비슷하지만 가중치와 편향을 직접 정의하느냐의 여부에서 차이를 보였다. 그리고 각프레임워크의 실행속도에 대한 평가는 ‘큰 차이는 없다’는 것이다. 텐서플로는 티아노에 비하여 속도가 느리다는 평가가 있어왔는데, 본 연구의 실험에 의하면, 비록 CNN 모형에 국한되었지만, 텐서플로가 아주 조금이지만빠른 것으로 나타났다. CNTK의 경우에도, 비록 실험환경이 달랐지만, 실험환경의 차이에 의한 속도의 차이의 편차범위 이내에 있는 것으로 판단이 되었다. 본 연구에서는 세 종류의 딥러닝 프레임워크만을 살펴보았는데,위키피디아에 따르면 딥러닝 프레임워크의 종류는 12가지가 있으며, 각 프레임워크의 특징을 15가지 속성으로구분하여 차이를 특정하고 있다. 그 많은 속성 중에서 사용자의 입장에서 볼 때 중요한 속성은 어떤 언어(파이썬, C++, Java, 등)로 사용가능한지, 어떤 딥러닝 모형에 대한 라이브러리가 잘 구현되어 있는지 등일 것이다. 그리고 사용자가 대규모의 딥러닝 모형을 구축한다면, 다중 GPU 혹은 다중 서버를 지원하는지의 여부도 중요할 것이다. 또한 딥러닝 모형을 처음 학습하는 경우에는 사용설명서가 많은지 예제 프로그램이 많은지 여부도 중요한 기준이 될 것이다.The deep learning framework is software designed to help develop deep learning models. Some of its important functions include “automatic differentiation” and “utilization of GPU”. The list of popular deep learning framework includes Caffe (BVLC) and Theano (University of Montreal). And recently, Microsoft's deep learning framework, Microsoft Cognitive Toolkit, was released as open-source license, following Google’s Tensorflow a year earlier. The early deep learning frameworks have been developed mainly for research at universities. Beginning with the inception of Tensorflow, however, it seems that companies such as Microsoft and Facebook have started to join the competition of framework development. Given the trend, Google and other companies are expected to continue investing in the deep learning framework to bring forward the initiative in the artificial intelligence business. From this point of view, we think it is a good time to compare some of deep learning frameworks. So we compare three deep learning frameworks which can be used as a Python library. Those are Google's Tensorflow, Microsoft’s CNTK, and Theano which is sort of a predecessor of the preceding two.
The most common and important function of deep learning frameworks is the ability to perform automatic differentiation. Basically all the mathematical expressions of deep learning models can be represented as computational graphs, which consist of nodes and edges. Partial derivatives on each edge of a computational graph can then be obtained. With the partial derivatives, we can let software compute differentiation of any node with respect to any variable by utilizing chain rule of Calculus.
First of all, the convenience of coding is in the order of CNTK, Tensorflow, and Theano. The criterion is simply based on the lengths of the codes and the learning curve and the ease of coding are not the main concern. According to the criteria, Theano was the most difficult to implement with, and CNTK and Tensorflow were somewhat easier. With Tensorflow, we need to define weight variables and biases explicitly. The reason that CNTK and Tensorflow are easier to implement with is that those frameworks provide us with more abstraction than Theano. We, however, need to mention that low-level coding is not always bad. It gives us flexibility of coding. With the low-level coding such as in Theano, we can implement and test any new deep learning models or any new search methods that we can think of.
The assessment of the execution speed of each framework is that there is not meaningful difference. According to the experiment, execution speeds of Theano and Tensorflow are very similar, although the experiment was limited to a CNN model. In the case of CNTK, the experimental environment was not maintained as the same. The code written in CNTK has to be run in PC environment without GPU where codes execute as much as 50 times slower than with GPU. But we concluded that the difference of execution speed was within the range of variation caused by the different hardware setup.
In this study, we compared three types of deep learning framework: Theano, Tensorflow, and CNTK. According to Wikipedia, there are 12 available deep learning frameworks. And 15 different attributes differentiate each framework. Some of the important attributes would include interface language (Python, C ++, Java, etc.) and the availability of libraries on various deep learning models such as CNN, RNN, DBN, and etc. And if a user implements a large scale deep learning model, it will also be important to support multiple GPU or multiple servers. Also, if you are learning the deep learning model, it would also be important if there are enough examples and references.deep learning framework, Theano, TensorFlow, CNTK, computational graph, CIFAR-10"
150,딥러닝을 이용한 주변 무선단말 파악방안,이웅섭 /Lee， Woong-sup,2018,한국정보통신학회,"최근 단말-대-단말(Device-to-device, D2D) 통신기술이 차세대 무선통신시스템의 핵심기술로 큰 관심을 받고 있다. 이러한 단말간 통신에서는 자신의 주변에 어떠한 단말이 있는지 파악하는 주변단말 탐색(Neighbor discovery)이매우 중요하다. 본 논문에서는 최근 큰 관심을 받고 있는 딥러닝(Deep learning) 기술을 활용하여 단말간 통신에서 주변단말을 파악하는 방안에 대해서 제안한다. 제안 방안은 기존의 방안과 달리 무선채널의 공간적 연관성을 이용하여단말간의 신호 전송 없이 단말이 기지국으로 전송하는 상향링크 파일럿 신호를 기반으로 주변 단말을 찾고 따라서기존의 방식에 비해 신호전송 복잡도(signaling complexity)를 크게 줄일 수 있다. 또한 제안 방안에서는 떨어져 있는거리에 따라서 주변 단말을 분류 가능하여 기존 방안에 비해서 좀 더 세밀한 단말 탐색이 가능하다. 마지막으로 본 논문에서는 tensorflow를 이용한 컴퓨터 시뮬레이션을 통해 제안 방안의 성능을 검증하였다.Recently, the device-to-device (D2D) communication has been conceived as the key technology for the next-generationmobile communication systems. The neighbor discovery in which the nearby users are found, is essential for the properoperation of the D2D communication. In this paper, we propose new neighbor discovery scheme based on deep learningtechnology which has gained a lot of attention recently. In the proposed scheme, the neighboring users can be foundusing the uplink pilot transmission of users only, unlike conventional neighbor discovery schemes in which direct pilotcommunication among users is required, such that the signaling overhead can be greatly reduced in our proposed scheme.
Moreover, the neighbors with different proximity can also be classified accordingly which enables more accurate neighbordiscovery compared to the conventional schemes. The performance of our proposed scheme is verified through thetensorflow-based computer simulations.
키워드 : 딥러닝, 주변단말 탐색, 단말-대-단말 통신Deep learning, Neighbor discovery, Device-to-device communication, Wireless channel correlation"
151,신뢰성있는 딥러닝 기반 분석 모델을 참조하기 위한 딥러닝 기술 언어,문종혁 /Jong Hyeok Mun,2021,한국정보처리학회,"최근 딥러닝은 하드웨어 성능이 향상됨에 따라 자연어 처리, 영상 인식 등의 다양한 기술에 접목되어 활용되고 있다. 이러한 기술들을 활용해 지능형교통 시스템(ITS), 스마트홈, 헬스케어 등의 산업분야에서 데이터를 분석하여 고속도로 속도위반 차량 검출, 에너지 사용량 제어, 응급상황 등과 같은고품질의 서비스를 제공하며, 고품질의 서비스를 제공하기 위해서는 정확도가 향상된 딥러닝 모델이 적용되어야 한다. 이를 위해 서비스 환경의 데이터를분석하기 위한 딥러닝 모델을 개발할 때, 개발자는 신뢰성이 검증된 최신의 딥러닝 모델을 적용할 수 있어야 한다. 이는 개발자가 참조하는 딥러닝 모델에적용된 학습 데이터셋의 정확도를 측정하여 검증할 수 있다. 이러한 검증을 위해서 개발자는 학습 데이터셋, 딥러닝의 계층구조 및 개발 환경 등과 같은내용을 포함하는 딥러닝 모델을 문서화하여 적용하기 위한 구조적인 정보가 필요하다. 본 논문에서는 신뢰성있는 딥러닝 기반 데이터 분석 모델을 참조하기위한 딥러닝 기술 언어를 제안한다. 제안하는 기술 언어는 신뢰성 있는 딥러닝 모델을 개발하는데 필요한 학습데이터셋, 개발 환경 및 설정 등의 정보와더불어 딥러닝 모델의 계층구조를 표현할 수 있다. 제안하는 딥러닝 기술 언어를 이용하여 개발자는 지능형 교통 시스템에서 참조하는 분석 모델의정확도를 검증할 수 있다. 실험에서는 제안하는 언어의 유효성을 검증하기 위해, 번호판 인식 모델을 중심으로 딥러닝 기술 문서의 적용과정을 보인다With the recent advancements of deep learning, companies such as smart home, healthcare, and intelligent transportation systems areutilizing its functionality to provide high-quality services for vehicle detection, emergency situation detection, and controlling energyconsumption. To provide reliable services in such sensitive systems, deep learning models are required to have high accuracy. In orderto develop a deep learning model for analyzing previously mentioned services, developers should utilize the state of the art deep learningmodels that have already been verified for higher accuracy. The developers can verify the accuracy of the referenced model by validatingthe model on the dataset. For this validation, the developer needs structural information to document and apply deep learning models,including metadata such as learning dataset, network architecture, and development environments. In this paper, we propose a descriptionlanguage that represents the network architecture of the deep learning model along with its metadata that are necessary to develop adeep learning model. Through the proposed description language, developers can easily verify the accuracy of the referenced deep learningmodel. Our experiments demonstrate the application scenario of a deep learning description document that focuses on the license platerecognition for the detection of illegally parked vehicles.Trusted Deep Learning, Model Reference, Deep Learning Description Language, Traffic Situation Analysis Model"
152,딥러닝 분석을 이용한 중국 역내·외 위안화 변동성 예측,이우식 /Lee Woosik,2016,한국데이터정보과학회,"2008년 글로벌 금융위기 이후 중국은 위안화 국제화의 점진적 추진을 진행하면서 중국상하이 외환시장과 중국홍콩 외환시장에서 거래되는 통화인 역내위안화와 역외위안화를 형성시켰다. 본 연구는 위안화 국제화와 점진적인 중국 자본계정 개방에 따라 급변하는 외환시장상황의 변동성을 정확하게 파악하기 위해서 GARCH모형 (일반화된 자기회귀 조건부이분산성모형)에 다단계인공신경망을 결합한 MLP-GARCH 모형과 GARCH모형과 기계학습의 일종인 딥러닝 (deep learning)을 통합한 DL-GARCH을 가지고 위안화 변동성예측을 비교 실험과 분석을 하였다. 비교분석 결과 DLGARCH 모형은 MLP-GARCH보다 모형 위안화 역내·외 환율변동성 예측 면에서 더욱 더 개선된예측값을 제공하였다. 그래서 이분산시계열모형을 딥러닝과 결합한 DL-GARCH 모형은 시계열의환율 변동성 예측 문제에 딥러닝을 응용할 수 있음을 확인하였다. 향후 이분산시계열과 결합된 딥러닝 모형은 다른 금융시계열 데이터에 응용하여 그 일반화 가능성을 높일 수 있을 것이다.The People’s Republic of China has vigorously been pursuing the internationalization of the Chinese Yuan or Renminbi after the financial crisis of 2008. In this view, an abrupt increase of use of the Chinese Yuan in the onshore and offshore markets are important milestones to be one of important currencies. One of the most frequently used methods to forecast volatility is GARCH model. Since a prediction error of the GARCH model has been reported quite high, a lot of efforts have been made to improve forecasting capability of the GARCH model. In this paper, we have proposed MLPGARCH and a DL-GARCH by employing Artificial Neural Network to the GARCH. In an application to forecasting Chinese Yuan volatility, we have successfully shown their overall outperformance in forecasting over the GARCH.Deep learning, DL-GARCH, GARCH, The Chinese Yuan, volatility."
153,딥러닝 기법을 이용한 내일강수 예측,하지훈,2016,한국지능시스템학회,"정확한 강수예측을 위해서는 예측인자 선정과 예측방법에 대한 선택이 매우 중요하다. 최근에는 강수예측 방법으로기계학습 기법이 많이 사용되고 있으며, 그 중에서도 특히 인공신경망을 사용한 강수예측 방법은 좋은 성능을 보였다. 본논문에서는 딥러닝 기법 중 하나인 DBN(deep belief network)를 이용한 새로운 강수예측 방법을 제안한다. DBN는 비지도사전 학습을 통해 초기 가중치를 설정하여 기존 인공신경망의 문제점을 보완한다. 예측인자로는 기온, 전일-전주 강수일, 태양과 달 궤도 관련 자료를 선정하였다. 기온과 전일-전주 강수일은 서울에서의 1974년부터 2013년까지 총 40년간의AWS(automatic weather system) 관측 자료를 사용하였고, 태양과 달의 궤도 관련 자료는 서울을 중심으로 계산한 결과를사용하였다. 전체 기간에서 일부는 학습 자료로 사용하여 예측모델을 생성하였고, 나머지를 생성한 모델의 검증 자료로사용하였다. 모델 검증 결과로 나온 예측값들은 확률값을 가지며 임계치를 이용하여 강수유무를 판별하였다. 강수 정확도의척도로 양분예보기법 중 CSI(critical successive index)와 Bias(frequency bias)를 계산하였다. 이를 통해 DBN와 MLP(multilayer perceptron)의 성능을 비교한 결과 DBN의 강수 예측 정확도가 높았고, 수행속도 또한 2배 이상 빨랐다.For accurate precipitation forecasts the choice of weather faoctrs and prediction method is very important. Recently, machinele arning has been widely used for forecasting precipitation, and artificial neural network, one of machine learning techniques, showed good performance. In this paper, we suggest a new method for forecasting precipitation using DBN, one of deep learning techniques.
DBN has an advantage that initial weights are set by unsupervised learning, so this compensates for the defects of artificial neural networks. We used past precipitation, temperature, and the parameters of the sun and moon's motion as features for forecasting precipitation. The dataset consists of observation data which had been measured for 40 years from AWS in Seoul. Experiments were based on 8-fold cross validation. As a result of estimation, we got probabilities of test dataset, so threshold was used for the decision of precipitation. CSI and Bias were used for indicating the precision of precipitation. Our experimental results showed that DBN performed better than MLP.Deep learning, Deep belief network, Precipitation, Forecast"
154,딥러닝 신경망모형을 이용한 실시간 교통정보수집 연구,김대현 /KIM DAE HYON,2016,사단법인 인문사회과학기술융합학회,"비디오 기반의 실시간 교통정보수집 시스템은 자율주행을 포함한 지능형교통체계의 주요 서브시스템중 하나이며, 우수한 예측(추정)력 때문에 핵심 알고리즘으로 인공신경망 모형이 주로 사용되고 있다. 특히, 최근 과학 학술지 네이처에 구글 딥마인드의 알파고가 공개되면서 딥러닝과 인공지능에 대한 전 세계적인 관심이 집중되고 있다. 최근까지 가장 널리 사용된 인공신경망모형은 역전파 모형이었으나 최근 컴퓨터 하드웨어 및 알고리즘의 새로운 개발과 더불어 딥러닝 알고리즘을 이용한 딥인공신경망 모형에 대한 관심이 높아지고 있다. 그러나 아직 딥러닝 인공신경망에 대한 연구는 초기단계에 있는 실정이며 적용분야도 극히 한정되어 있다. 본 연구에서는 실시간 도로교통정보 수집을 위한 영상처리분야에서 딥러닝 알고리즘을 활용하고 실험결과를 통하여 향후 교통정보수집‧분석 분야에서의 딥러닝 인공신경망 모형의 적용가능성을 살펴보고자 한다. 특히, 본 연구에서 도출한 딥러닝 인공신경망모형의 실험결과를 기존 역전파 모형에 의한 실험결과와 비교하여 모형의 우수성을 비교‧평가하고자 한다.Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Normally, artificial neural networks have been used in real-time applications for collecting traffic information because of their remarkable performance in terms of predictive accuracy and real-time computing. Especially, the recent publication of a paper in the journal Nature and the success of Google DeepMind’s world-beating Go program AlphaGo caused significant increases of the public interest in artificial intelligence. Even though various types of learning models have been proposed, the Backpropagation has been the most popular neural network model. However, the Deep learning neural network model has attracted much interest recently. In this study, the Deep leaning neural network will be applied in image processing techniques for the real-time traffic information collection to investigate the performance of Deep learning algorithm in traffic engineering fields. In addition, the experimental results from Deep learing algorithm will be compared with the results from Backpropagation model to evaluate and propose better performing model.Intelligent Transportation Systems (ITS), artificial neural networks, Deep learning, traffic information, image processing"
155,딥러닝의 모형과 응용사례,안성만 /SungMahn Ahn,2016,한국지능정보시스템학회,"딥러닝은 인공신경망(neural network)이라는 인공지능분야의 모형이 발전된 형태로서, 계층구조로 이루어진 인공신경망의 내부계층(hidden layer)이 여러 단계로 이루어진 구조이다. 딥러닝에서의 주요 모형은 합성곱신경망(convolutional neural network), 순환신경망(recurrent neural network), 그리고 심층신뢰신경망(deep belief network)의 세가지라고 할 수있다. 그 중에서 현재 흥미로운 연구가 많이 발표되어서 관심이 집중되고 있는 모형은 지도학습(supervised learning)모형인 처음 두 개의 모형이다. 따라서 본 논문에서는 지도학습모형의 가중치를 최적화하는 기본적인 방법인 오류역전파 알고리즘을 살펴본 뒤에 합성곱신경망과 순환신경망의 구조와 응용사례 등을 살펴보고자 한다. 본문에서 다루지 않은 모형인 심층신뢰신경망은 아직까지는 합성곱신경망 이나 순환신경망보다는 상대적으로 주목을 덜 받고 있다. 그러나 심층신뢰신경망은 CNN이나 RNN과는 달리 비지도학습(unsupervised learning)모형이며, 사람이나 동물은 관찰을 통해서 스스로 학습한다는 점에서 궁극적으로는 비지도학습모형이 더 많이 연구되어야 할 주제가 될 것이다.Deep learning model is a kind of neural networks that allows multiple hidden layers. There are various deep learning architectures such as convolutional neural networks, deep belief networks and recurrent neural networks. Those have been applied to fields like computer vision, automatic speech recognition, natural language processing, audio recognition and bioinformatics where they have been shown to produce state-of-the-art results on various tasks. Among those architectures, convolutional neural networks and recurrent neural networks are classified as the supervised learning model. And in recent years, those supervised learning models have gained more popularity than unsupervised learning models such as deep belief networks, because supervised learning models have shown fashionable applications in such fields mentioned above.
Deep learning models can be trained with backpropagation algorithm. Backpropagation is an abbreviation for “backward propagation of errors” and a common method of training artificial neural networks used in conjunction with an optimization method such as gradient descent. The method calculates the gradient of an error function with respect to all the weights in the network. The gradient is fed to the optimization method which in turn uses it to update the weights, in an attempt to minimize the error function.
Convolutional neural networks use a special architecture which is particularly well-adapted to classify images. Using this architecture makes convolutional networks fast to train. This, in turn, helps us train deep, muti-layer networks, which are very good at classifying images. These days, deep convolutional networks are used in most neural networks for image recognition. Convolutional neural networks use three basic ideas: local receptive fields, shared weights, and pooling. By local receptive fields, we mean that each neuron in the first(or any) hidden layer will be connected to a small region of the input(or previous layer’s) neurons. Shared weights mean that we’re going to use the same weights and bias for each of the local receptive field. This means that all the neurons in the hidden layer detect exactly the same feature, just at different locations in the input image. In addition to the convolutional layers just described, convolutional neural networks also contain pooling layers. Pooling layers are usually used immediately after convolutional layers. What the pooling layers do is to simplify the information in the output from the convolutional layer.
Recent convolutional network architectures have 10 to 20 hidden layers and billions of connections between units. Training deep learning networks has taken weeks several years ago, but thanks to progress in GPU and algorithm enhancement, training time has reduced to several hours.
Neural networks with time-varying behavior are known as recurrent neural networks or RNNs. A recurrent neural network is a class of artificial neural network where connections between units form a directed cycle. This creates an internal state of the network which allows it to exhibit dynamic temporal behavior. Unlike feedforward neural networks, RNNs can use their internal memory to process arbitrary sequences of inputs. Early RNN models turned out to be very difficult to train, harder even than deep feedforward networks. The reason is the unstable gradient problem such as vanishing gradient and exploding gradient. The gradient can get smaller and smaller as it is propagated back through layers. This makes learning in early layers extremely slow. The problem actually gets worse in RNNs, since gradients aren’t just propagated backward through layers, they’re propagated backward through time. If the network runs for a long time, that can make the gradient extremely unstable and hard to learn from. It has been possible to incorporate an idea known as long short-term memory units (LSTMs) into RNNs. LSTMs make it much easier to get good results when training RNNs, and many recent papers make use of LSTMs or related ideas.Deep learning, Convolutional neural networks, Recurrent neural networks, Error backpropagation algorithm"
156,딥러닝분석과 기술적 분석 지표를 이용한 한국 코스피주가지수 방향성 예측,이우식 /Lee Woosik,2017,한국데이터정보과학회,"2016년 3월 구글 (Google)의 바둑인공지능 알파고 (AlphaGo)가 이세돌 9단과의 바둑대결에서승리한 이후 다양한 분야에서 인공지능 사용에 대한 관심이 높아지고 있는 가운데 금융투자 분야에서도 인공지능과 투자자문 전문가의 합성어인 로보어드바이저 (Robo-Advisor)에 대한 관심이 높아지고 있다. 인공지능 (artificial intelligence)기반의 의사결정은 비용 절감은 물론 효과적인 의사결정을가능하게 한다는 점에서 큰 장점이 있다. 본 연구에서는 기술적 분석 (technical analysis) 지표와 딥러닝 (deep learning) 모형을 결합하여 한국 코스피 지수를 예측하는 모형을 개발하고 제시한 모형들의 예측력을 비교, 분석한다. 분석 결과 기술적 분석 지표에 딥러닝 알고리즘을 결합한 모형이 주가지수 방향성 예측 문제에 응용될 수 있음을 확인하였다. 향후 본 연구에서 제안된 기술적 분석 지표와딥러닝모형을 결합한 기법은 로보어드바이저서비스에 응용할 수 있는 일반화 가능성을 보여준다Since Google’s AlphaGo defeated a world champion of Go players in 2016, there have been many interests in the deep learning. In the financial sector, a Robo-Advisor using deep learning gains a significant attention, which builds and manages portfolios of financial instruments for investors.In this paper, we have proposed the a deep learning algorithm geared toward identification and forecast of the KOSPI index direction,and we also have compared the accuracy of the prediction.In an application of forecasting the financial market index direction, we have shown that the Robo-Advisor using deep learning has a significant effect on finance industry. The Robo-Advisor collects a massive data such as earnings statements, news reports and regulatory filings, analyzes those and recommends investors how to view market trends and identify the best time to purchase financial assets. On the other hand, the Robo-Advisor allows businesses to learn more about their customers, develop better marketing strategies, increase sales and decrease costs.Artificial intelligence, deep learning, FinTech, Robo-Advisor, technical analysis."
157,딥러닝의 효율적인 학습을 위한 학습데이터 선별 기법에 관한 연구,주경돈 /Gyoung-don Joo,2017,한국정보과학회,"빅데이터의 증가와 병렬 하드웨어 처리, 학습 알고리즘의 개선은 딥러닝의 발전을 가져왔으며 인공지능이 적용 가능한 영역을 획기적으로 확장시키고 있다. 하지만 이러한 대규모의 학습 데이터에 기반한 딥러닝은 학습시간이 증가되고, 더 나아가 학습 데이터의 구축을 위한 레이블링(labeling) 비용이 데이터 수에 비례하여 증가하게 된다. 따라서 본 논문에서는, 딥러닝의 정확도를 최대한 유지하면서 필요한 학습 데이터의 수를 줄일수 있도록, 전체 데이터에서 제한된 수의 학습 데이터의 후보를 선별하는 기법에 대하여 연구한다. 특히 기존의 능동 학습에 기반한 기법들이 학습 데이터의 수가 매우 적어질 경우 딥러닝의 정확도가 급격히 떨어지는 문제를 해결할 수 있는 새로운 기법을 제안하였으며, 제안된 기법으로 선별된 학습 데이터는 그 수의 변화에 대해 딥러닝이 일관되게 경쟁력있는 정확도를 가짐을 다양한 실험을 통하여 검증하였다.The increase of big data, parallel hardware processing and enhanced learning algorithms have led to the development of deep learning and are dramatically expanding the domain of applications based on artificial intelligence. However, such large-scale training data extends the learning time, and furthermore, the labeling cost for building the training set increases in proportion to the volume of data. Therefore, in this paper, we study how to select a limited number of candidates for training set from given data so as to reduce the number of training set while maintaining the accuracy of deep learning. Especially, we found that, with the training set selected by the previous work, the accuracy of deep learning drastically decreases when the number of training set is very small and so proposed new methods to resolve this fall-off in accuracy. Through experiments, we showed that deep learning consistently has the competitive accuracy with the training set selected by the proposed method regardless of the size of training set.Deep Learning, Train Set Selection, Active Learning, Gamma distribution"
158,딥러닝을 이용한 범용적 스테그아날리시스,김현재 /Hyunjae Kim,2017,한국정보과학회,"스테그아날리시스(Steganalysis)란 이미지 등 일반적인 자료에 암호화된 정보를 은닉하는 스테가노그래피(Steganography)에 대한 검출 및 분석 방법으로, 기계학습 기반 방법론을 포함한다. 기존 기계학습 기반 스테그아날리시스는 영상(Image)의 특징(Feature) 추출 및 모델링에 기반하며, 최근 딥러닝(Deep Learning)의 적용으로 검출 정확도가 큰 폭으로 향상되었다. 하지만 현존하는 스테그아날리시스 모델은 단일 스테가노그래피 기법에 대해 국한되어 있어 학습에 사용되지 않은 스테고(Stego) 이미지의 경우 검출이 불가능한 결정적 한계를 가진다. 본 연구에서는 다양한 스테가노그래피 기법으로 생성된 스테고이미지에 딥러닝을 적용하여 스테그아날리시스를 학습하는 범용적 모델을 제안한다. 다양한 실험을 통해 제안 기법의 효용성 및 가능성을 확인하고, 범용적 스테그아날리시스 모델이 각각에 특화된 검출 기법과 유사한 정확도로 스테고 이미지를 검출할 수 있음을 보인다.Steganalysis is to detect information hidden by steganography inside general data such as images. There are stegoanalysis techniques that use machine learning (ML). Existing ML approaches to steganalysis are based on extracting features from stego images and modeling them.
Recently deep learning-based methodologies have shown significant improvements in detection accuracy. However, all the existing methods, including deep learning-based ones, have a critical limitation in that they can only detect stego images that are created by a specific steganography method. In this paper, we propose a generalized steganalysis method that can model multiple types of stego images using deep learning. Through various experiments, we confirm the effectiveness of our approach and envision directions for future research. In particular, we show that our method can detect each type of steganography with the same level of accuracy as that of a steganalysis method dedicated to that type of steganography, thereby demonstrating the general applicability of our approach to multiple types of stego images.steganalysis, steganography, deep learning, generalized steganalysis model"
159,"딥러닝 프레임워크의 비교: 티아노, 텐서플로, CNTK를 중심으로",정여진 /Yeojin Chung,2017,한국지능정보시스템학회,"딥러닝 프레임워크의 대표적인 기능으로는 ‘자동미분’과 ‘GPU의 활용’ 등을 들 수 있다. 본 논문은 파이썬의 라이브러리 형태로 사용 가능한 프레임워크 중에서 구글의 텐서플로와 마이크로소프트의 CNTK, 그리고 텐서 플로의 원조라고 할 수 있는 티아노를 비교하였다. 본문에서는 자동미분의 개념과 GPU의 활용형태를 간단히 설명하고, 그 다음에 logistic regression을 실행하는 예를 통하여 각 프레임워크의 문법을 알아본 뒤에, 마지막으로 대표적인 딥러닝 응용인 CNN의 예제를 실행시켜보고 코딩의 편의성과 실행속도 등을 확인해 보았다. 그 결과, 편의성의 관점에서 보면 티아노가 가장 코딩 하기가 어렵고, CNTK와 텐서플로는 많은 부분이 비슷하게 추상화 되어 있어서 코딩이 비슷하지만 가중치와 편향을 직접 정의하느냐의 여부에서 차이를 보였다. 그리고 각프레임워크의 실행속도에 대한 평가는 ‘큰 차이는 없다’는 것이다. 텐서플로는 티아노에 비하여 속도가 느리다는 평가가 있어왔는데, 본 연구의 실험에 의하면, 비록 CNN 모형에 국한되었지만, 텐서플로가 아주 조금이지만빠른 것으로 나타났다. CNTK의 경우에도, 비록 실험환경이 달랐지만, 실험환경의 차이에 의한 속도의 차이의 편차범위 이내에 있는 것으로 판단이 되었다. 본 연구에서는 세 종류의 딥러닝 프레임워크만을 살펴보았는데,위키피디아에 따르면 딥러닝 프레임워크의 종류는 12가지가 있으며, 각 프레임워크의 특징을 15가지 속성으로구분하여 차이를 특정하고 있다. 그 많은 속성 중에서 사용자의 입장에서 볼 때 중요한 속성은 어떤 언어(파이썬, C++, Java, 등)로 사용가능한지, 어떤 딥러닝 모형에 대한 라이브러리가 잘 구현되어 있는지 등일 것이다. 그리고 사용자가 대규모의 딥러닝 모형을 구축한다면, 다중 GPU 혹은 다중 서버를 지원하는지의 여부도 중요할 것이다. 또한 딥러닝 모형을 처음 학습하는 경우에는 사용설명서가 많은지 예제 프로그램이 많은지 여부도 중요한 기준이 될 것이다.The deep learning framework is software designed to help develop deep learning models. Some of its important functions include “automatic differentiation” and “utilization of GPU”. The list of popular deep learning framework includes Caffe (BVLC) and Theano (University of Montreal). And recently, Microsoft's deep learning framework, Microsoft Cognitive Toolkit, was released as open-source license, following Google’s Tensorflow a year earlier. The early deep learning frameworks have been developed mainly for research at universities. Beginning with the inception of Tensorflow, however, it seems that companies such as Microsoft and Facebook have started to join the competition of framework development. Given the trend, Google and other companies are expected to continue investing in the deep learning framework to bring forward the initiative in the artificial intelligence business. From this point of view, we think it is a good time to compare some of deep learning frameworks. So we compare three deep learning frameworks which can be used as a Python library. Those are Google's Tensorflow, Microsoft’s CNTK, and Theano which is sort of a predecessor of the preceding two.
The most common and important function of deep learning frameworks is the ability to perform automatic differentiation. Basically all the mathematical expressions of deep learning models can be represented as computational graphs, which consist of nodes and edges. Partial derivatives on each edge of a computational graph can then be obtained. With the partial derivatives, we can let software compute differentiation of any node with respect to any variable by utilizing chain rule of Calculus.
First of all, the convenience of coding is in the order of CNTK, Tensorflow, and Theano. The criterion is simply based on the lengths of the codes and the learning curve and the ease of coding are not the main concern. According to the criteria, Theano was the most difficult to implement with, and CNTK and Tensorflow were somewhat easier. With Tensorflow, we need to define weight variables and biases explicitly. The reason that CNTK and Tensorflow are easier to implement with is that those frameworks provide us with more abstraction than Theano. We, however, need to mention that low-level coding is not always bad. It gives us flexibility of coding. With the low-level coding such as in Theano, we can implement and test any new deep learning models or any new search methods that we can think of.
The assessment of the execution speed of each framework is that there is not meaningful difference. According to the experiment, execution speeds of Theano and Tensorflow are very similar, although the experiment was limited to a CNN model. In the case of CNTK, the experimental environment was not maintained as the same. The code written in CNTK has to be run in PC environment without GPU where codes execute as much as 50 times slower than with GPU. But we concluded that the difference of execution speed was within the range of variation caused by the different hardware setup.
In this study, we compared three types of deep learning framework: Theano, Tensorflow, and CNTK. According to Wikipedia, there are 12 available deep learning frameworks. And 15 different attributes differentiate each framework. Some of the important attributes would include interface language (Python, C ++, Java, etc.) and the availability of libraries on various deep learning models such as CNN, RNN, DBN, and etc. And if a user implements a large scale deep learning model, it will also be important to support multiple GPU or multiple servers. Also, if you are learning the deep learning model, it would also be important if there are enough examples and references.deep learning framework, Theano, TensorFlow, CNTK, computational graph, CIFAR-10"
160,딥러닝을 이용한 주변 무선단말 파악방안,이웅섭 /Lee， Woong-sup,2018,한국정보통신학회,"최근 단말-대-단말(Device-to-device, D2D) 통신기술이 차세대 무선통신시스템의 핵심기술로 큰 관심을 받고 있다. 이러한 단말간 통신에서는 자신의 주변에 어떠한 단말이 있는지 파악하는 주변단말 탐색(Neighbor discovery)이매우 중요하다. 본 논문에서는 최근 큰 관심을 받고 있는 딥러닝(Deep learning) 기술을 활용하여 단말간 통신에서 주변단말을 파악하는 방안에 대해서 제안한다. 제안 방안은 기존의 방안과 달리 무선채널의 공간적 연관성을 이용하여단말간의 신호 전송 없이 단말이 기지국으로 전송하는 상향링크 파일럿 신호를 기반으로 주변 단말을 찾고 따라서기존의 방식에 비해 신호전송 복잡도(signaling complexity)를 크게 줄일 수 있다. 또한 제안 방안에서는 떨어져 있는거리에 따라서 주변 단말을 분류 가능하여 기존 방안에 비해서 좀 더 세밀한 단말 탐색이 가능하다. 마지막으로 본 논문에서는 tensorflow를 이용한 컴퓨터 시뮬레이션을 통해 제안 방안의 성능을 검증하였다.Recently, the device-to-device (D2D) communication has been conceived as the key technology for the next-generationmobile communication systems. The neighbor discovery in which the nearby users are found, is essential for the properoperation of the D2D communication. In this paper, we propose new neighbor discovery scheme based on deep learningtechnology which has gained a lot of attention recently. In the proposed scheme, the neighboring users can be foundusing the uplink pilot transmission of users only, unlike conventional neighbor discovery schemes in which direct pilotcommunication among users is required, such that the signaling overhead can be greatly reduced in our proposed scheme.
Moreover, the neighbors with different proximity can also be classified accordingly which enables more accurate neighbordiscovery compared to the conventional schemes. The performance of our proposed scheme is verified through thetensorflow-based computer simulations.
키워드 : 딥러닝, 주변단말 탐색, 단말-대-단말 통신Deep learning, Neighbor discovery, Device-to-device communication, Wireless channel correlation"
161,신뢰성있는 딥러닝 기반 분석 모델을 참조하기 위한 딥러닝 기술 언어,문종혁 /Jong Hyeok Mun,2021,한국정보처리학회,"최근 딥러닝은 하드웨어 성능이 향상됨에 따라 자연어 처리, 영상 인식 등의 다양한 기술에 접목되어 활용되고 있다. 이러한 기술들을 활용해 지능형교통 시스템(ITS), 스마트홈, 헬스케어 등의 산업분야에서 데이터를 분석하여 고속도로 속도위반 차량 검출, 에너지 사용량 제어, 응급상황 등과 같은고품질의 서비스를 제공하며, 고품질의 서비스를 제공하기 위해서는 정확도가 향상된 딥러닝 모델이 적용되어야 한다. 이를 위해 서비스 환경의 데이터를분석하기 위한 딥러닝 모델을 개발할 때, 개발자는 신뢰성이 검증된 최신의 딥러닝 모델을 적용할 수 있어야 한다. 이는 개발자가 참조하는 딥러닝 모델에적용된 학습 데이터셋의 정확도를 측정하여 검증할 수 있다. 이러한 검증을 위해서 개발자는 학습 데이터셋, 딥러닝의 계층구조 및 개발 환경 등과 같은내용을 포함하는 딥러닝 모델을 문서화하여 적용하기 위한 구조적인 정보가 필요하다. 본 논문에서는 신뢰성있는 딥러닝 기반 데이터 분석 모델을 참조하기위한 딥러닝 기술 언어를 제안한다. 제안하는 기술 언어는 신뢰성 있는 딥러닝 모델을 개발하는데 필요한 학습데이터셋, 개발 환경 및 설정 등의 정보와더불어 딥러닝 모델의 계층구조를 표현할 수 있다. 제안하는 딥러닝 기술 언어를 이용하여 개발자는 지능형 교통 시스템에서 참조하는 분석 모델의정확도를 검증할 수 있다. 실험에서는 제안하는 언어의 유효성을 검증하기 위해, 번호판 인식 모델을 중심으로 딥러닝 기술 문서의 적용과정을 보인다With the recent advancements of deep learning, companies such as smart home, healthcare, and intelligent transportation systems areutilizing its functionality to provide high-quality services for vehicle detection, emergency situation detection, and controlling energyconsumption. To provide reliable services in such sensitive systems, deep learning models are required to have high accuracy. In orderto develop a deep learning model for analyzing previously mentioned services, developers should utilize the state of the art deep learningmodels that have already been verified for higher accuracy. The developers can verify the accuracy of the referenced model by validatingthe model on the dataset. For this validation, the developer needs structural information to document and apply deep learning models,including metadata such as learning dataset, network architecture, and development environments. In this paper, we propose a descriptionlanguage that represents the network architecture of the deep learning model along with its metadata that are necessary to develop adeep learning model. Through the proposed description language, developers can easily verify the accuracy of the referenced deep learningmodel. Our experiments demonstrate the application scenario of a deep learning description document that focuses on the license platerecognition for the detection of illegally parked vehicles.Trusted Deep Learning, Model Reference, Deep Learning Description Language, Traffic Situation Analysis Model"
162,딥러닝 분석을 이용한 중국 역내·외 위안화 변동성 예측,이우식 /Lee Woosik,2016,한국데이터정보과학회,"2008년 글로벌 금융위기 이후 중국은 위안화 국제화의 점진적 추진을 진행하면서 중국상하이 외환시장과 중국홍콩 외환시장에서 거래되는 통화인 역내위안화와 역외위안화를 형성시켰다. 본 연구는 위안화 국제화와 점진적인 중국 자본계정 개방에 따라 급변하는 외환시장상황의 변동성을 정확하게 파악하기 위해서 GARCH모형 (일반화된 자기회귀 조건부이분산성모형)에 다단계인공신경망을 결합한 MLP-GARCH 모형과 GARCH모형과 기계학습의 일종인 딥러닝 (deep learning)을 통합한 DL-GARCH을 가지고 위안화 변동성예측을 비교 실험과 분석을 하였다. 비교분석 결과 DLGARCH 모형은 MLP-GARCH보다 모형 위안화 역내·외 환율변동성 예측 면에서 더욱 더 개선된예측값을 제공하였다. 그래서 이분산시계열모형을 딥러닝과 결합한 DL-GARCH 모형은 시계열의환율 변동성 예측 문제에 딥러닝을 응용할 수 있음을 확인하였다. 향후 이분산시계열과 결합된 딥러닝 모형은 다른 금융시계열 데이터에 응용하여 그 일반화 가능성을 높일 수 있을 것이다.The People’s Republic of China has vigorously been pursuing the internationalization of the Chinese Yuan or Renminbi after the financial crisis of 2008. In this view, an abrupt increase of use of the Chinese Yuan in the onshore and offshore markets are important milestones to be one of important currencies. One of the most frequently used methods to forecast volatility is GARCH model. Since a prediction error of the GARCH model has been reported quite high, a lot of efforts have been made to improve forecasting capability of the GARCH model. In this paper, we have proposed MLPGARCH and a DL-GARCH by employing Artificial Neural Network to the GARCH. In an application to forecasting Chinese Yuan volatility, we have successfully shown their overall outperformance in forecasting over the GARCH.Deep learning, DL-GARCH, GARCH, The Chinese Yuan, volatility."
163,딥러닝 기법을 이용한 내일강수 예측,하지훈,2016,한국지능시스템학회,"정확한 강수예측을 위해서는 예측인자 선정과 예측방법에 대한 선택이 매우 중요하다. 최근에는 강수예측 방법으로기계학습 기법이 많이 사용되고 있으며, 그 중에서도 특히 인공신경망을 사용한 강수예측 방법은 좋은 성능을 보였다. 본논문에서는 딥러닝 기법 중 하나인 DBN(deep belief network)를 이용한 새로운 강수예측 방법을 제안한다. DBN는 비지도사전 학습을 통해 초기 가중치를 설정하여 기존 인공신경망의 문제점을 보완한다. 예측인자로는 기온, 전일-전주 강수일, 태양과 달 궤도 관련 자료를 선정하였다. 기온과 전일-전주 강수일은 서울에서의 1974년부터 2013년까지 총 40년간의AWS(automatic weather system) 관측 자료를 사용하였고, 태양과 달의 궤도 관련 자료는 서울을 중심으로 계산한 결과를사용하였다. 전체 기간에서 일부는 학습 자료로 사용하여 예측모델을 생성하였고, 나머지를 생성한 모델의 검증 자료로사용하였다. 모델 검증 결과로 나온 예측값들은 확률값을 가지며 임계치를 이용하여 강수유무를 판별하였다. 강수 정확도의척도로 양분예보기법 중 CSI(critical successive index)와 Bias(frequency bias)를 계산하였다. 이를 통해 DBN와 MLP(multilayer perceptron)의 성능을 비교한 결과 DBN의 강수 예측 정확도가 높았고, 수행속도 또한 2배 이상 빨랐다.For accurate precipitation forecasts the choice of weather faoctrs and prediction method is very important. Recently, machinele arning has been widely used for forecasting precipitation, and artificial neural network, one of machine learning techniques, showed good performance. In this paper, we suggest a new method for forecasting precipitation using DBN, one of deep learning techniques.
DBN has an advantage that initial weights are set by unsupervised learning, so this compensates for the defects of artificial neural networks. We used past precipitation, temperature, and the parameters of the sun and moon's motion as features for forecasting precipitation. The dataset consists of observation data which had been measured for 40 years from AWS in Seoul. Experiments were based on 8-fold cross validation. As a result of estimation, we got probabilities of test dataset, so threshold was used for the decision of precipitation. CSI and Bias were used for indicating the precision of precipitation. Our experimental results showed that DBN performed better than MLP.Deep learning, Deep belief network, Precipitation, Forecast"
164,딥러닝 신경망모형을 이용한 실시간 교통정보수집 연구,김대현 /KIM DAE HYON,2016,사단법인 인문사회과학기술융합학회,"비디오 기반의 실시간 교통정보수집 시스템은 자율주행을 포함한 지능형교통체계의 주요 서브시스템중 하나이며, 우수한 예측(추정)력 때문에 핵심 알고리즘으로 인공신경망 모형이 주로 사용되고 있다. 특히, 최근 과학 학술지 네이처에 구글 딥마인드의 알파고가 공개되면서 딥러닝과 인공지능에 대한 전 세계적인 관심이 집중되고 있다. 최근까지 가장 널리 사용된 인공신경망모형은 역전파 모형이었으나 최근 컴퓨터 하드웨어 및 알고리즘의 새로운 개발과 더불어 딥러닝 알고리즘을 이용한 딥인공신경망 모형에 대한 관심이 높아지고 있다. 그러나 아직 딥러닝 인공신경망에 대한 연구는 초기단계에 있는 실정이며 적용분야도 극히 한정되어 있다. 본 연구에서는 실시간 도로교통정보 수집을 위한 영상처리분야에서 딥러닝 알고리즘을 활용하고 실험결과를 통하여 향후 교통정보수집‧분석 분야에서의 딥러닝 인공신경망 모형의 적용가능성을 살펴보고자 한다. 특히, 본 연구에서 도출한 딥러닝 인공신경망모형의 실험결과를 기존 역전파 모형에 의한 실험결과와 비교하여 모형의 우수성을 비교‧평가하고자 한다.Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Normally, artificial neural networks have been used in real-time applications for collecting traffic information because of their remarkable performance in terms of predictive accuracy and real-time computing. Especially, the recent publication of a paper in the journal Nature and the success of Google DeepMind’s world-beating Go program AlphaGo caused significant increases of the public interest in artificial intelligence. Even though various types of learning models have been proposed, the Backpropagation has been the most popular neural network model. However, the Deep learning neural network model has attracted much interest recently. In this study, the Deep leaning neural network will be applied in image processing techniques for the real-time traffic information collection to investigate the performance of Deep learning algorithm in traffic engineering fields. In addition, the experimental results from Deep learing algorithm will be compared with the results from Backpropagation model to evaluate and propose better performing model.Intelligent Transportation Systems (ITS), artificial neural networks, Deep learning, traffic information, image processing"
165,딥러닝의 모형과 응용사례,안성만 /SungMahn Ahn,2016,한국지능정보시스템학회,"딥러닝은 인공신경망(neural network)이라는 인공지능분야의 모형이 발전된 형태로서, 계층구조로 이루어진 인공신경망의 내부계층(hidden layer)이 여러 단계로 이루어진 구조이다. 딥러닝에서의 주요 모형은 합성곱신경망(convolutional neural network), 순환신경망(recurrent neural network), 그리고 심층신뢰신경망(deep belief network)의 세가지라고 할 수있다. 그 중에서 현재 흥미로운 연구가 많이 발표되어서 관심이 집중되고 있는 모형은 지도학습(supervised learning)모형인 처음 두 개의 모형이다. 따라서 본 논문에서는 지도학습모형의 가중치를 최적화하는 기본적인 방법인 오류역전파 알고리즘을 살펴본 뒤에 합성곱신경망과 순환신경망의 구조와 응용사례 등을 살펴보고자 한다. 본문에서 다루지 않은 모형인 심층신뢰신경망은 아직까지는 합성곱신경망 이나 순환신경망보다는 상대적으로 주목을 덜 받고 있다. 그러나 심층신뢰신경망은 CNN이나 RNN과는 달리 비지도학습(unsupervised learning)모형이며, 사람이나 동물은 관찰을 통해서 스스로 학습한다는 점에서 궁극적으로는 비지도학습모형이 더 많이 연구되어야 할 주제가 될 것이다.Deep learning model is a kind of neural networks that allows multiple hidden layers. There are various deep learning architectures such as convolutional neural networks, deep belief networks and recurrent neural networks. Those have been applied to fields like computer vision, automatic speech recognition, natural language processing, audio recognition and bioinformatics where they have been shown to produce state-of-the-art results on various tasks. Among those architectures, convolutional neural networks and recurrent neural networks are classified as the supervised learning model. And in recent years, those supervised learning models have gained more popularity than unsupervised learning models such as deep belief networks, because supervised learning models have shown fashionable applications in such fields mentioned above.
Deep learning models can be trained with backpropagation algorithm. Backpropagation is an abbreviation for “backward propagation of errors” and a common method of training artificial neural networks used in conjunction with an optimization method such as gradient descent. The method calculates the gradient of an error function with respect to all the weights in the network. The gradient is fed to the optimization method which in turn uses it to update the weights, in an attempt to minimize the error function.
Convolutional neural networks use a special architecture which is particularly well-adapted to classify images. Using this architecture makes convolutional networks fast to train. This, in turn, helps us train deep, muti-layer networks, which are very good at classifying images. These days, deep convolutional networks are used in most neural networks for image recognition. Convolutional neural networks use three basic ideas: local receptive fields, shared weights, and pooling. By local receptive fields, we mean that each neuron in the first(or any) hidden layer will be connected to a small region of the input(or previous layer’s) neurons. Shared weights mean that we’re going to use the same weights and bias for each of the local receptive field. This means that all the neurons in the hidden layer detect exactly the same feature, just at different locations in the input image. In addition to the convolutional layers just described, convolutional neural networks also contain pooling layers. Pooling layers are usually used immediately after convolutional layers. What the pooling layers do is to simplify the information in the output from the convolutional layer.
Recent convolutional network architectures have 10 to 20 hidden layers and billions of connections between units. Training deep learning networks has taken weeks several years ago, but thanks to progress in GPU and algorithm enhancement, training time has reduced to several hours.
Neural networks with time-varying behavior are known as recurrent neural networks or RNNs. A recurrent neural network is a class of artificial neural network where connections between units form a directed cycle. This creates an internal state of the network which allows it to exhibit dynamic temporal behavior. Unlike feedforward neural networks, RNNs can use their internal memory to process arbitrary sequences of inputs. Early RNN models turned out to be very difficult to train, harder even than deep feedforward networks. The reason is the unstable gradient problem such as vanishing gradient and exploding gradient. The gradient can get smaller and smaller as it is propagated back through layers. This makes learning in early layers extremely slow. The problem actually gets worse in RNNs, since gradients aren’t just propagated backward through layers, they’re propagated backward through time. If the network runs for a long time, that can make the gradient extremely unstable and hard to learn from. It has been possible to incorporate an idea known as long short-term memory units (LSTMs) into RNNs. LSTMs make it much easier to get good results when training RNNs, and many recent papers make use of LSTMs or related ideas.Deep learning, Convolutional neural networks, Recurrent neural networks, Error backpropagation algorithm"
166,딥러닝분석과 기술적 분석 지표를 이용한 한국 코스피주가지수 방향성 예측,이우식 /Lee Woosik,2017,한국데이터정보과학회,"2016년 3월 구글 (Google)의 바둑인공지능 알파고 (AlphaGo)가 이세돌 9단과의 바둑대결에서승리한 이후 다양한 분야에서 인공지능 사용에 대한 관심이 높아지고 있는 가운데 금융투자 분야에서도 인공지능과 투자자문 전문가의 합성어인 로보어드바이저 (Robo-Advisor)에 대한 관심이 높아지고 있다. 인공지능 (artificial intelligence)기반의 의사결정은 비용 절감은 물론 효과적인 의사결정을가능하게 한다는 점에서 큰 장점이 있다. 본 연구에서는 기술적 분석 (technical analysis) 지표와 딥러닝 (deep learning) 모형을 결합하여 한국 코스피 지수를 예측하는 모형을 개발하고 제시한 모형들의 예측력을 비교, 분석한다. 분석 결과 기술적 분석 지표에 딥러닝 알고리즘을 결합한 모형이 주가지수 방향성 예측 문제에 응용될 수 있음을 확인하였다. 향후 본 연구에서 제안된 기술적 분석 지표와딥러닝모형을 결합한 기법은 로보어드바이저서비스에 응용할 수 있는 일반화 가능성을 보여준다Since Google’s AlphaGo defeated a world champion of Go players in 2016, there have been many interests in the deep learning. In the financial sector, a Robo-Advisor using deep learning gains a significant attention, which builds and manages portfolios of financial instruments for investors.In this paper, we have proposed the a deep learning algorithm geared toward identification and forecast of the KOSPI index direction,and we also have compared the accuracy of the prediction.In an application of forecasting the financial market index direction, we have shown that the Robo-Advisor using deep learning has a significant effect on finance industry. The Robo-Advisor collects a massive data such as earnings statements, news reports and regulatory filings, analyzes those and recommends investors how to view market trends and identify the best time to purchase financial assets. On the other hand, the Robo-Advisor allows businesses to learn more about their customers, develop better marketing strategies, increase sales and decrease costs.Artificial intelligence, deep learning, FinTech, Robo-Advisor, technical analysis."
167,딥러닝의 효율적인 학습을 위한 학습데이터 선별 기법에 관한 연구,주경돈 /Gyoung-don Joo,2017,한국정보과학회,"빅데이터의 증가와 병렬 하드웨어 처리, 학습 알고리즘의 개선은 딥러닝의 발전을 가져왔으며 인공지능이 적용 가능한 영역을 획기적으로 확장시키고 있다. 하지만 이러한 대규모의 학습 데이터에 기반한 딥러닝은 학습시간이 증가되고, 더 나아가 학습 데이터의 구축을 위한 레이블링(labeling) 비용이 데이터 수에 비례하여 증가하게 된다. 따라서 본 논문에서는, 딥러닝의 정확도를 최대한 유지하면서 필요한 학습 데이터의 수를 줄일수 있도록, 전체 데이터에서 제한된 수의 학습 데이터의 후보를 선별하는 기법에 대하여 연구한다. 특히 기존의 능동 학습에 기반한 기법들이 학습 데이터의 수가 매우 적어질 경우 딥러닝의 정확도가 급격히 떨어지는 문제를 해결할 수 있는 새로운 기법을 제안하였으며, 제안된 기법으로 선별된 학습 데이터는 그 수의 변화에 대해 딥러닝이 일관되게 경쟁력있는 정확도를 가짐을 다양한 실험을 통하여 검증하였다.The increase of big data, parallel hardware processing and enhanced learning algorithms have led to the development of deep learning and are dramatically expanding the domain of applications based on artificial intelligence. However, such large-scale training data extends the learning time, and furthermore, the labeling cost for building the training set increases in proportion to the volume of data. Therefore, in this paper, we study how to select a limited number of candidates for training set from given data so as to reduce the number of training set while maintaining the accuracy of deep learning. Especially, we found that, with the training set selected by the previous work, the accuracy of deep learning drastically decreases when the number of training set is very small and so proposed new methods to resolve this fall-off in accuracy. Through experiments, we showed that deep learning consistently has the competitive accuracy with the training set selected by the proposed method regardless of the size of training set.Deep Learning, Train Set Selection, Active Learning, Gamma distribution"
168,딥러닝을 이용한 범용적 스테그아날리시스,김현재 /Hyunjae Kim,2017,한국정보과학회,"스테그아날리시스(Steganalysis)란 이미지 등 일반적인 자료에 암호화된 정보를 은닉하는 스테가노그래피(Steganography)에 대한 검출 및 분석 방법으로, 기계학습 기반 방법론을 포함한다. 기존 기계학습 기반 스테그아날리시스는 영상(Image)의 특징(Feature) 추출 및 모델링에 기반하며, 최근 딥러닝(Deep Learning)의 적용으로 검출 정확도가 큰 폭으로 향상되었다. 하지만 현존하는 스테그아날리시스 모델은 단일 스테가노그래피 기법에 대해 국한되어 있어 학습에 사용되지 않은 스테고(Stego) 이미지의 경우 검출이 불가능한 결정적 한계를 가진다. 본 연구에서는 다양한 스테가노그래피 기법으로 생성된 스테고이미지에 딥러닝을 적용하여 스테그아날리시스를 학습하는 범용적 모델을 제안한다. 다양한 실험을 통해 제안 기법의 효용성 및 가능성을 확인하고, 범용적 스테그아날리시스 모델이 각각에 특화된 검출 기법과 유사한 정확도로 스테고 이미지를 검출할 수 있음을 보인다.Steganalysis is to detect information hidden by steganography inside general data such as images. There are stegoanalysis techniques that use machine learning (ML). Existing ML approaches to steganalysis are based on extracting features from stego images and modeling them.
Recently deep learning-based methodologies have shown significant improvements in detection accuracy. However, all the existing methods, including deep learning-based ones, have a critical limitation in that they can only detect stego images that are created by a specific steganography method. In this paper, we propose a generalized steganalysis method that can model multiple types of stego images using deep learning. Through various experiments, we confirm the effectiveness of our approach and envision directions for future research. In particular, we show that our method can detect each type of steganography with the same level of accuracy as that of a steganalysis method dedicated to that type of steganography, thereby demonstrating the general applicability of our approach to multiple types of stego images.steganalysis, steganography, deep learning, generalized steganalysis model"
169,"딥러닝 프레임워크의 비교: 티아노, 텐서플로, CNTK를 중심으로",정여진 /Yeojin Chung,2017,한국지능정보시스템학회,"딥러닝 프레임워크의 대표적인 기능으로는 ‘자동미분’과 ‘GPU의 활용’ 등을 들 수 있다. 본 논문은 파이썬의 라이브러리 형태로 사용 가능한 프레임워크 중에서 구글의 텐서플로와 마이크로소프트의 CNTK, 그리고 텐서 플로의 원조라고 할 수 있는 티아노를 비교하였다. 본문에서는 자동미분의 개념과 GPU의 활용형태를 간단히 설명하고, 그 다음에 logistic regression을 실행하는 예를 통하여 각 프레임워크의 문법을 알아본 뒤에, 마지막으로 대표적인 딥러닝 응용인 CNN의 예제를 실행시켜보고 코딩의 편의성과 실행속도 등을 확인해 보았다. 그 결과, 편의성의 관점에서 보면 티아노가 가장 코딩 하기가 어렵고, CNTK와 텐서플로는 많은 부분이 비슷하게 추상화 되어 있어서 코딩이 비슷하지만 가중치와 편향을 직접 정의하느냐의 여부에서 차이를 보였다. 그리고 각프레임워크의 실행속도에 대한 평가는 ‘큰 차이는 없다’는 것이다. 텐서플로는 티아노에 비하여 속도가 느리다는 평가가 있어왔는데, 본 연구의 실험에 의하면, 비록 CNN 모형에 국한되었지만, 텐서플로가 아주 조금이지만빠른 것으로 나타났다. CNTK의 경우에도, 비록 실험환경이 달랐지만, 실험환경의 차이에 의한 속도의 차이의 편차범위 이내에 있는 것으로 판단이 되었다. 본 연구에서는 세 종류의 딥러닝 프레임워크만을 살펴보았는데,위키피디아에 따르면 딥러닝 프레임워크의 종류는 12가지가 있으며, 각 프레임워크의 특징을 15가지 속성으로구분하여 차이를 특정하고 있다. 그 많은 속성 중에서 사용자의 입장에서 볼 때 중요한 속성은 어떤 언어(파이썬, C++, Java, 등)로 사용가능한지, 어떤 딥러닝 모형에 대한 라이브러리가 잘 구현되어 있는지 등일 것이다. 그리고 사용자가 대규모의 딥러닝 모형을 구축한다면, 다중 GPU 혹은 다중 서버를 지원하는지의 여부도 중요할 것이다. 또한 딥러닝 모형을 처음 학습하는 경우에는 사용설명서가 많은지 예제 프로그램이 많은지 여부도 중요한 기준이 될 것이다.The deep learning framework is software designed to help develop deep learning models. Some of its important functions include “automatic differentiation” and “utilization of GPU”. The list of popular deep learning framework includes Caffe (BVLC) and Theano (University of Montreal). And recently, Microsoft's deep learning framework, Microsoft Cognitive Toolkit, was released as open-source license, following Google’s Tensorflow a year earlier. The early deep learning frameworks have been developed mainly for research at universities. Beginning with the inception of Tensorflow, however, it seems that companies such as Microsoft and Facebook have started to join the competition of framework development. Given the trend, Google and other companies are expected to continue investing in the deep learning framework to bring forward the initiative in the artificial intelligence business. From this point of view, we think it is a good time to compare some of deep learning frameworks. So we compare three deep learning frameworks which can be used as a Python library. Those are Google's Tensorflow, Microsoft’s CNTK, and Theano which is sort of a predecessor of the preceding two.
The most common and important function of deep learning frameworks is the ability to perform automatic differentiation. Basically all the mathematical expressions of deep learning models can be represented as computational graphs, which consist of nodes and edges. Partial derivatives on each edge of a computational graph can then be obtained. With the partial derivatives, we can let software compute differentiation of any node with respect to any variable by utilizing chain rule of Calculus.
First of all, the convenience of coding is in the order of CNTK, Tensorflow, and Theano. The criterion is simply based on the lengths of the codes and the learning curve and the ease of coding are not the main concern. According to the criteria, Theano was the most difficult to implement with, and CNTK and Tensorflow were somewhat easier. With Tensorflow, we need to define weight variables and biases explicitly. The reason that CNTK and Tensorflow are easier to implement with is that those frameworks provide us with more abstraction than Theano. We, however, need to mention that low-level coding is not always bad. It gives us flexibility of coding. With the low-level coding such as in Theano, we can implement and test any new deep learning models or any new search methods that we can think of.
The assessment of the execution speed of each framework is that there is not meaningful difference. According to the experiment, execution speeds of Theano and Tensorflow are very similar, although the experiment was limited to a CNN model. In the case of CNTK, the experimental environment was not maintained as the same. The code written in CNTK has to be run in PC environment without GPU where codes execute as much as 50 times slower than with GPU. But we concluded that the difference of execution speed was within the range of variation caused by the different hardware setup.
In this study, we compared three types of deep learning framework: Theano, Tensorflow, and CNTK. According to Wikipedia, there are 12 available deep learning frameworks. And 15 different attributes differentiate each framework. Some of the important attributes would include interface language (Python, C ++, Java, etc.) and the availability of libraries on various deep learning models such as CNN, RNN, DBN, and etc. And if a user implements a large scale deep learning model, it will also be important to support multiple GPU or multiple servers. Also, if you are learning the deep learning model, it would also be important if there are enough examples and references.deep learning framework, Theano, TensorFlow, CNTK, computational graph, CIFAR-10"
170,딥러닝을 이용한 주변 무선단말 파악방안,이웅섭 /Lee， Woong-sup,2018,한국정보통신학회,"최근 단말-대-단말(Device-to-device, D2D) 통신기술이 차세대 무선통신시스템의 핵심기술로 큰 관심을 받고 있다. 이러한 단말간 통신에서는 자신의 주변에 어떠한 단말이 있는지 파악하는 주변단말 탐색(Neighbor discovery)이매우 중요하다. 본 논문에서는 최근 큰 관심을 받고 있는 딥러닝(Deep learning) 기술을 활용하여 단말간 통신에서 주변단말을 파악하는 방안에 대해서 제안한다. 제안 방안은 기존의 방안과 달리 무선채널의 공간적 연관성을 이용하여단말간의 신호 전송 없이 단말이 기지국으로 전송하는 상향링크 파일럿 신호를 기반으로 주변 단말을 찾고 따라서기존의 방식에 비해 신호전송 복잡도(signaling complexity)를 크게 줄일 수 있다. 또한 제안 방안에서는 떨어져 있는거리에 따라서 주변 단말을 분류 가능하여 기존 방안에 비해서 좀 더 세밀한 단말 탐색이 가능하다. 마지막으로 본 논문에서는 tensorflow를 이용한 컴퓨터 시뮬레이션을 통해 제안 방안의 성능을 검증하였다.Recently, the device-to-device (D2D) communication has been conceived as the key technology for the next-generationmobile communication systems. The neighbor discovery in which the nearby users are found, is essential for the properoperation of the D2D communication. In this paper, we propose new neighbor discovery scheme based on deep learningtechnology which has gained a lot of attention recently. In the proposed scheme, the neighboring users can be foundusing the uplink pilot transmission of users only, unlike conventional neighbor discovery schemes in which direct pilotcommunication among users is required, such that the signaling overhead can be greatly reduced in our proposed scheme.
Moreover, the neighbors with different proximity can also be classified accordingly which enables more accurate neighbordiscovery compared to the conventional schemes. The performance of our proposed scheme is verified through thetensorflow-based computer simulations.
키워드 : 딥러닝, 주변단말 탐색, 단말-대-단말 통신Deep learning, Neighbor discovery, Device-to-device communication, Wireless channel correlation"
171,신뢰성있는 딥러닝 기반 분석 모델을 참조하기 위한 딥러닝 기술 언어,문종혁 /Jong Hyeok Mun,2021,한국정보처리학회,"최근 딥러닝은 하드웨어 성능이 향상됨에 따라 자연어 처리, 영상 인식 등의 다양한 기술에 접목되어 활용되고 있다. 이러한 기술들을 활용해 지능형교통 시스템(ITS), 스마트홈, 헬스케어 등의 산업분야에서 데이터를 분석하여 고속도로 속도위반 차량 검출, 에너지 사용량 제어, 응급상황 등과 같은고품질의 서비스를 제공하며, 고품질의 서비스를 제공하기 위해서는 정확도가 향상된 딥러닝 모델이 적용되어야 한다. 이를 위해 서비스 환경의 데이터를분석하기 위한 딥러닝 모델을 개발할 때, 개발자는 신뢰성이 검증된 최신의 딥러닝 모델을 적용할 수 있어야 한다. 이는 개발자가 참조하는 딥러닝 모델에적용된 학습 데이터셋의 정확도를 측정하여 검증할 수 있다. 이러한 검증을 위해서 개발자는 학습 데이터셋, 딥러닝의 계층구조 및 개발 환경 등과 같은내용을 포함하는 딥러닝 모델을 문서화하여 적용하기 위한 구조적인 정보가 필요하다. 본 논문에서는 신뢰성있는 딥러닝 기반 데이터 분석 모델을 참조하기위한 딥러닝 기술 언어를 제안한다. 제안하는 기술 언어는 신뢰성 있는 딥러닝 모델을 개발하는데 필요한 학습데이터셋, 개발 환경 및 설정 등의 정보와더불어 딥러닝 모델의 계층구조를 표현할 수 있다. 제안하는 딥러닝 기술 언어를 이용하여 개발자는 지능형 교통 시스템에서 참조하는 분석 모델의정확도를 검증할 수 있다. 실험에서는 제안하는 언어의 유효성을 검증하기 위해, 번호판 인식 모델을 중심으로 딥러닝 기술 문서의 적용과정을 보인다With the recent advancements of deep learning, companies such as smart home, healthcare, and intelligent transportation systems areutilizing its functionality to provide high-quality services for vehicle detection, emergency situation detection, and controlling energyconsumption. To provide reliable services in such sensitive systems, deep learning models are required to have high accuracy. In orderto develop a deep learning model for analyzing previously mentioned services, developers should utilize the state of the art deep learningmodels that have already been verified for higher accuracy. The developers can verify the accuracy of the referenced model by validatingthe model on the dataset. For this validation, the developer needs structural information to document and apply deep learning models,including metadata such as learning dataset, network architecture, and development environments. In this paper, we propose a descriptionlanguage that represents the network architecture of the deep learning model along with its metadata that are necessary to develop adeep learning model. Through the proposed description language, developers can easily verify the accuracy of the referenced deep learningmodel. Our experiments demonstrate the application scenario of a deep learning description document that focuses on the license platerecognition for the detection of illegally parked vehicles.Trusted Deep Learning, Model Reference, Deep Learning Description Language, Traffic Situation Analysis Model"
172,딥러닝 분석을 이용한 중국 역내·외 위안화 변동성 예측,이우식 /Lee Woosik,2016,한국데이터정보과학회,"2008년 글로벌 금융위기 이후 중국은 위안화 국제화의 점진적 추진을 진행하면서 중국상하이 외환시장과 중국홍콩 외환시장에서 거래되는 통화인 역내위안화와 역외위안화를 형성시켰다. 본 연구는 위안화 국제화와 점진적인 중국 자본계정 개방에 따라 급변하는 외환시장상황의 변동성을 정확하게 파악하기 위해서 GARCH모형 (일반화된 자기회귀 조건부이분산성모형)에 다단계인공신경망을 결합한 MLP-GARCH 모형과 GARCH모형과 기계학습의 일종인 딥러닝 (deep learning)을 통합한 DL-GARCH을 가지고 위안화 변동성예측을 비교 실험과 분석을 하였다. 비교분석 결과 DLGARCH 모형은 MLP-GARCH보다 모형 위안화 역내·외 환율변동성 예측 면에서 더욱 더 개선된예측값을 제공하였다. 그래서 이분산시계열모형을 딥러닝과 결합한 DL-GARCH 모형은 시계열의환율 변동성 예측 문제에 딥러닝을 응용할 수 있음을 확인하였다. 향후 이분산시계열과 결합된 딥러닝 모형은 다른 금융시계열 데이터에 응용하여 그 일반화 가능성을 높일 수 있을 것이다.The People’s Republic of China has vigorously been pursuing the internationalization of the Chinese Yuan or Renminbi after the financial crisis of 2008. In this view, an abrupt increase of use of the Chinese Yuan in the onshore and offshore markets are important milestones to be one of important currencies. One of the most frequently used methods to forecast volatility is GARCH model. Since a prediction error of the GARCH model has been reported quite high, a lot of efforts have been made to improve forecasting capability of the GARCH model. In this paper, we have proposed MLPGARCH and a DL-GARCH by employing Artificial Neural Network to the GARCH. In an application to forecasting Chinese Yuan volatility, we have successfully shown their overall outperformance in forecasting over the GARCH.Deep learning, DL-GARCH, GARCH, The Chinese Yuan, volatility."
173,딥러닝 기법을 이용한 내일강수 예측,하지훈,2016,한국지능시스템학회,"정확한 강수예측을 위해서는 예측인자 선정과 예측방법에 대한 선택이 매우 중요하다. 최근에는 강수예측 방법으로기계학습 기법이 많이 사용되고 있으며, 그 중에서도 특히 인공신경망을 사용한 강수예측 방법은 좋은 성능을 보였다. 본논문에서는 딥러닝 기법 중 하나인 DBN(deep belief network)를 이용한 새로운 강수예측 방법을 제안한다. DBN는 비지도사전 학습을 통해 초기 가중치를 설정하여 기존 인공신경망의 문제점을 보완한다. 예측인자로는 기온, 전일-전주 강수일, 태양과 달 궤도 관련 자료를 선정하였다. 기온과 전일-전주 강수일은 서울에서의 1974년부터 2013년까지 총 40년간의AWS(automatic weather system) 관측 자료를 사용하였고, 태양과 달의 궤도 관련 자료는 서울을 중심으로 계산한 결과를사용하였다. 전체 기간에서 일부는 학습 자료로 사용하여 예측모델을 생성하였고, 나머지를 생성한 모델의 검증 자료로사용하였다. 모델 검증 결과로 나온 예측값들은 확률값을 가지며 임계치를 이용하여 강수유무를 판별하였다. 강수 정확도의척도로 양분예보기법 중 CSI(critical successive index)와 Bias(frequency bias)를 계산하였다. 이를 통해 DBN와 MLP(multilayer perceptron)의 성능을 비교한 결과 DBN의 강수 예측 정확도가 높았고, 수행속도 또한 2배 이상 빨랐다.For accurate precipitation forecasts the choice of weather faoctrs and prediction method is very important. Recently, machinele arning has been widely used for forecasting precipitation, and artificial neural network, one of machine learning techniques, showed good performance. In this paper, we suggest a new method for forecasting precipitation using DBN, one of deep learning techniques.
DBN has an advantage that initial weights are set by unsupervised learning, so this compensates for the defects of artificial neural networks. We used past precipitation, temperature, and the parameters of the sun and moon's motion as features for forecasting precipitation. The dataset consists of observation data which had been measured for 40 years from AWS in Seoul. Experiments were based on 8-fold cross validation. As a result of estimation, we got probabilities of test dataset, so threshold was used for the decision of precipitation. CSI and Bias were used for indicating the precision of precipitation. Our experimental results showed that DBN performed better than MLP.Deep learning, Deep belief network, Precipitation, Forecast"
174,딥러닝 신경망모형을 이용한 실시간 교통정보수집 연구,김대현 /KIM DAE HYON,2016,사단법인 인문사회과학기술융합학회,"비디오 기반의 실시간 교통정보수집 시스템은 자율주행을 포함한 지능형교통체계의 주요 서브시스템중 하나이며, 우수한 예측(추정)력 때문에 핵심 알고리즘으로 인공신경망 모형이 주로 사용되고 있다. 특히, 최근 과학 학술지 네이처에 구글 딥마인드의 알파고가 공개되면서 딥러닝과 인공지능에 대한 전 세계적인 관심이 집중되고 있다. 최근까지 가장 널리 사용된 인공신경망모형은 역전파 모형이었으나 최근 컴퓨터 하드웨어 및 알고리즘의 새로운 개발과 더불어 딥러닝 알고리즘을 이용한 딥인공신경망 모형에 대한 관심이 높아지고 있다. 그러나 아직 딥러닝 인공신경망에 대한 연구는 초기단계에 있는 실정이며 적용분야도 극히 한정되어 있다. 본 연구에서는 실시간 도로교통정보 수집을 위한 영상처리분야에서 딥러닝 알고리즘을 활용하고 실험결과를 통하여 향후 교통정보수집‧분석 분야에서의 딥러닝 인공신경망 모형의 적용가능성을 살펴보고자 한다. 특히, 본 연구에서 도출한 딥러닝 인공신경망모형의 실험결과를 기존 역전파 모형에 의한 실험결과와 비교하여 모형의 우수성을 비교‧평가하고자 한다.Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Normally, artificial neural networks have been used in real-time applications for collecting traffic information because of their remarkable performance in terms of predictive accuracy and real-time computing. Especially, the recent publication of a paper in the journal Nature and the success of Google DeepMind’s world-beating Go program AlphaGo caused significant increases of the public interest in artificial intelligence. Even though various types of learning models have been proposed, the Backpropagation has been the most popular neural network model. However, the Deep learning neural network model has attracted much interest recently. In this study, the Deep leaning neural network will be applied in image processing techniques for the real-time traffic information collection to investigate the performance of Deep learning algorithm in traffic engineering fields. In addition, the experimental results from Deep learing algorithm will be compared with the results from Backpropagation model to evaluate and propose better performing model.Intelligent Transportation Systems (ITS), artificial neural networks, Deep learning, traffic information, image processing"
175,딥러닝의 모형과 응용사례,안성만 /SungMahn Ahn,2016,한국지능정보시스템학회,"딥러닝은 인공신경망(neural network)이라는 인공지능분야의 모형이 발전된 형태로서, 계층구조로 이루어진 인공신경망의 내부계층(hidden layer)이 여러 단계로 이루어진 구조이다. 딥러닝에서의 주요 모형은 합성곱신경망(convolutional neural network), 순환신경망(recurrent neural network), 그리고 심층신뢰신경망(deep belief network)의 세가지라고 할 수있다. 그 중에서 현재 흥미로운 연구가 많이 발표되어서 관심이 집중되고 있는 모형은 지도학습(supervised learning)모형인 처음 두 개의 모형이다. 따라서 본 논문에서는 지도학습모형의 가중치를 최적화하는 기본적인 방법인 오류역전파 알고리즘을 살펴본 뒤에 합성곱신경망과 순환신경망의 구조와 응용사례 등을 살펴보고자 한다. 본문에서 다루지 않은 모형인 심층신뢰신경망은 아직까지는 합성곱신경망 이나 순환신경망보다는 상대적으로 주목을 덜 받고 있다. 그러나 심층신뢰신경망은 CNN이나 RNN과는 달리 비지도학습(unsupervised learning)모형이며, 사람이나 동물은 관찰을 통해서 스스로 학습한다는 점에서 궁극적으로는 비지도학습모형이 더 많이 연구되어야 할 주제가 될 것이다.Deep learning model is a kind of neural networks that allows multiple hidden layers. There are various deep learning architectures such as convolutional neural networks, deep belief networks and recurrent neural networks. Those have been applied to fields like computer vision, automatic speech recognition, natural language processing, audio recognition and bioinformatics where they have been shown to produce state-of-the-art results on various tasks. Among those architectures, convolutional neural networks and recurrent neural networks are classified as the supervised learning model. And in recent years, those supervised learning models have gained more popularity than unsupervised learning models such as deep belief networks, because supervised learning models have shown fashionable applications in such fields mentioned above.
Deep learning models can be trained with backpropagation algorithm. Backpropagation is an abbreviation for “backward propagation of errors” and a common method of training artificial neural networks used in conjunction with an optimization method such as gradient descent. The method calculates the gradient of an error function with respect to all the weights in the network. The gradient is fed to the optimization method which in turn uses it to update the weights, in an attempt to minimize the error function.
Convolutional neural networks use a special architecture which is particularly well-adapted to classify images. Using this architecture makes convolutional networks fast to train. This, in turn, helps us train deep, muti-layer networks, which are very good at classifying images. These days, deep convolutional networks are used in most neural networks for image recognition. Convolutional neural networks use three basic ideas: local receptive fields, shared weights, and pooling. By local receptive fields, we mean that each neuron in the first(or any) hidden layer will be connected to a small region of the input(or previous layer’s) neurons. Shared weights mean that we’re going to use the same weights and bias for each of the local receptive field. This means that all the neurons in the hidden layer detect exactly the same feature, just at different locations in the input image. In addition to the convolutional layers just described, convolutional neural networks also contain pooling layers. Pooling layers are usually used immediately after convolutional layers. What the pooling layers do is to simplify the information in the output from the convolutional layer.
Recent convolutional network architectures have 10 to 20 hidden layers and billions of connections between units. Training deep learning networks has taken weeks several years ago, but thanks to progress in GPU and algorithm enhancement, training time has reduced to several hours.
Neural networks with time-varying behavior are known as recurrent neural networks or RNNs. A recurrent neural network is a class of artificial neural network where connections between units form a directed cycle. This creates an internal state of the network which allows it to exhibit dynamic temporal behavior. Unlike feedforward neural networks, RNNs can use their internal memory to process arbitrary sequences of inputs. Early RNN models turned out to be very difficult to train, harder even than deep feedforward networks. The reason is the unstable gradient problem such as vanishing gradient and exploding gradient. The gradient can get smaller and smaller as it is propagated back through layers. This makes learning in early layers extremely slow. The problem actually gets worse in RNNs, since gradients aren’t just propagated backward through layers, they’re propagated backward through time. If the network runs for a long time, that can make the gradient extremely unstable and hard to learn from. It has been possible to incorporate an idea known as long short-term memory units (LSTMs) into RNNs. LSTMs make it much easier to get good results when training RNNs, and many recent papers make use of LSTMs or related ideas.Deep learning, Convolutional neural networks, Recurrent neural networks, Error backpropagation algorithm"
176,딥러닝분석과 기술적 분석 지표를 이용한 한국 코스피주가지수 방향성 예측,이우식 /Lee Woosik,2017,한국데이터정보과학회,"2016년 3월 구글 (Google)의 바둑인공지능 알파고 (AlphaGo)가 이세돌 9단과의 바둑대결에서승리한 이후 다양한 분야에서 인공지능 사용에 대한 관심이 높아지고 있는 가운데 금융투자 분야에서도 인공지능과 투자자문 전문가의 합성어인 로보어드바이저 (Robo-Advisor)에 대한 관심이 높아지고 있다. 인공지능 (artificial intelligence)기반의 의사결정은 비용 절감은 물론 효과적인 의사결정을가능하게 한다는 점에서 큰 장점이 있다. 본 연구에서는 기술적 분석 (technical analysis) 지표와 딥러닝 (deep learning) 모형을 결합하여 한국 코스피 지수를 예측하는 모형을 개발하고 제시한 모형들의 예측력을 비교, 분석한다. 분석 결과 기술적 분석 지표에 딥러닝 알고리즘을 결합한 모형이 주가지수 방향성 예측 문제에 응용될 수 있음을 확인하였다. 향후 본 연구에서 제안된 기술적 분석 지표와딥러닝모형을 결합한 기법은 로보어드바이저서비스에 응용할 수 있는 일반화 가능성을 보여준다Since Google’s AlphaGo defeated a world champion of Go players in 2016, there have been many interests in the deep learning. In the financial sector, a Robo-Advisor using deep learning gains a significant attention, which builds and manages portfolios of financial instruments for investors.In this paper, we have proposed the a deep learning algorithm geared toward identification and forecast of the KOSPI index direction,and we also have compared the accuracy of the prediction.In an application of forecasting the financial market index direction, we have shown that the Robo-Advisor using deep learning has a significant effect on finance industry. The Robo-Advisor collects a massive data such as earnings statements, news reports and regulatory filings, analyzes those and recommends investors how to view market trends and identify the best time to purchase financial assets. On the other hand, the Robo-Advisor allows businesses to learn more about their customers, develop better marketing strategies, increase sales and decrease costs.Artificial intelligence, deep learning, FinTech, Robo-Advisor, technical analysis."
177,딥러닝의 효율적인 학습을 위한 학습데이터 선별 기법에 관한 연구,주경돈 /Gyoung-don Joo,2017,한국정보과학회,"빅데이터의 증가와 병렬 하드웨어 처리, 학습 알고리즘의 개선은 딥러닝의 발전을 가져왔으며 인공지능이 적용 가능한 영역을 획기적으로 확장시키고 있다. 하지만 이러한 대규모의 학습 데이터에 기반한 딥러닝은 학습시간이 증가되고, 더 나아가 학습 데이터의 구축을 위한 레이블링(labeling) 비용이 데이터 수에 비례하여 증가하게 된다. 따라서 본 논문에서는, 딥러닝의 정확도를 최대한 유지하면서 필요한 학습 데이터의 수를 줄일수 있도록, 전체 데이터에서 제한된 수의 학습 데이터의 후보를 선별하는 기법에 대하여 연구한다. 특히 기존의 능동 학습에 기반한 기법들이 학습 데이터의 수가 매우 적어질 경우 딥러닝의 정확도가 급격히 떨어지는 문제를 해결할 수 있는 새로운 기법을 제안하였으며, 제안된 기법으로 선별된 학습 데이터는 그 수의 변화에 대해 딥러닝이 일관되게 경쟁력있는 정확도를 가짐을 다양한 실험을 통하여 검증하였다.The increase of big data, parallel hardware processing and enhanced learning algorithms have led to the development of deep learning and are dramatically expanding the domain of applications based on artificial intelligence. However, such large-scale training data extends the learning time, and furthermore, the labeling cost for building the training set increases in proportion to the volume of data. Therefore, in this paper, we study how to select a limited number of candidates for training set from given data so as to reduce the number of training set while maintaining the accuracy of deep learning. Especially, we found that, with the training set selected by the previous work, the accuracy of deep learning drastically decreases when the number of training set is very small and so proposed new methods to resolve this fall-off in accuracy. Through experiments, we showed that deep learning consistently has the competitive accuracy with the training set selected by the proposed method regardless of the size of training set.Deep Learning, Train Set Selection, Active Learning, Gamma distribution"
178,딥러닝을 이용한 범용적 스테그아날리시스,김현재 /Hyunjae Kim,2017,한국정보과학회,"스테그아날리시스(Steganalysis)란 이미지 등 일반적인 자료에 암호화된 정보를 은닉하는 스테가노그래피(Steganography)에 대한 검출 및 분석 방법으로, 기계학습 기반 방법론을 포함한다. 기존 기계학습 기반 스테그아날리시스는 영상(Image)의 특징(Feature) 추출 및 모델링에 기반하며, 최근 딥러닝(Deep Learning)의 적용으로 검출 정확도가 큰 폭으로 향상되었다. 하지만 현존하는 스테그아날리시스 모델은 단일 스테가노그래피 기법에 대해 국한되어 있어 학습에 사용되지 않은 스테고(Stego) 이미지의 경우 검출이 불가능한 결정적 한계를 가진다. 본 연구에서는 다양한 스테가노그래피 기법으로 생성된 스테고이미지에 딥러닝을 적용하여 스테그아날리시스를 학습하는 범용적 모델을 제안한다. 다양한 실험을 통해 제안 기법의 효용성 및 가능성을 확인하고, 범용적 스테그아날리시스 모델이 각각에 특화된 검출 기법과 유사한 정확도로 스테고 이미지를 검출할 수 있음을 보인다.Steganalysis is to detect information hidden by steganography inside general data such as images. There are stegoanalysis techniques that use machine learning (ML). Existing ML approaches to steganalysis are based on extracting features from stego images and modeling them.
Recently deep learning-based methodologies have shown significant improvements in detection accuracy. However, all the existing methods, including deep learning-based ones, have a critical limitation in that they can only detect stego images that are created by a specific steganography method. In this paper, we propose a generalized steganalysis method that can model multiple types of stego images using deep learning. Through various experiments, we confirm the effectiveness of our approach and envision directions for future research. In particular, we show that our method can detect each type of steganography with the same level of accuracy as that of a steganalysis method dedicated to that type of steganography, thereby demonstrating the general applicability of our approach to multiple types of stego images.steganalysis, steganography, deep learning, generalized steganalysis model"
179,"딥러닝 프레임워크의 비교: 티아노, 텐서플로, CNTK를 중심으로",정여진 /Yeojin Chung,2017,한국지능정보시스템학회,"딥러닝 프레임워크의 대표적인 기능으로는 ‘자동미분’과 ‘GPU의 활용’ 등을 들 수 있다. 본 논문은 파이썬의 라이브러리 형태로 사용 가능한 프레임워크 중에서 구글의 텐서플로와 마이크로소프트의 CNTK, 그리고 텐서 플로의 원조라고 할 수 있는 티아노를 비교하였다. 본문에서는 자동미분의 개념과 GPU의 활용형태를 간단히 설명하고, 그 다음에 logistic regression을 실행하는 예를 통하여 각 프레임워크의 문법을 알아본 뒤에, 마지막으로 대표적인 딥러닝 응용인 CNN의 예제를 실행시켜보고 코딩의 편의성과 실행속도 등을 확인해 보았다. 그 결과, 편의성의 관점에서 보면 티아노가 가장 코딩 하기가 어렵고, CNTK와 텐서플로는 많은 부분이 비슷하게 추상화 되어 있어서 코딩이 비슷하지만 가중치와 편향을 직접 정의하느냐의 여부에서 차이를 보였다. 그리고 각프레임워크의 실행속도에 대한 평가는 ‘큰 차이는 없다’는 것이다. 텐서플로는 티아노에 비하여 속도가 느리다는 평가가 있어왔는데, 본 연구의 실험에 의하면, 비록 CNN 모형에 국한되었지만, 텐서플로가 아주 조금이지만빠른 것으로 나타났다. CNTK의 경우에도, 비록 실험환경이 달랐지만, 실험환경의 차이에 의한 속도의 차이의 편차범위 이내에 있는 것으로 판단이 되었다. 본 연구에서는 세 종류의 딥러닝 프레임워크만을 살펴보았는데,위키피디아에 따르면 딥러닝 프레임워크의 종류는 12가지가 있으며, 각 프레임워크의 특징을 15가지 속성으로구분하여 차이를 특정하고 있다. 그 많은 속성 중에서 사용자의 입장에서 볼 때 중요한 속성은 어떤 언어(파이썬, C++, Java, 등)로 사용가능한지, 어떤 딥러닝 모형에 대한 라이브러리가 잘 구현되어 있는지 등일 것이다. 그리고 사용자가 대규모의 딥러닝 모형을 구축한다면, 다중 GPU 혹은 다중 서버를 지원하는지의 여부도 중요할 것이다. 또한 딥러닝 모형을 처음 학습하는 경우에는 사용설명서가 많은지 예제 프로그램이 많은지 여부도 중요한 기준이 될 것이다.The deep learning framework is software designed to help develop deep learning models. Some of its important functions include “automatic differentiation” and “utilization of GPU”. The list of popular deep learning framework includes Caffe (BVLC) and Theano (University of Montreal). And recently, Microsoft's deep learning framework, Microsoft Cognitive Toolkit, was released as open-source license, following Google’s Tensorflow a year earlier. The early deep learning frameworks have been developed mainly for research at universities. Beginning with the inception of Tensorflow, however, it seems that companies such as Microsoft and Facebook have started to join the competition of framework development. Given the trend, Google and other companies are expected to continue investing in the deep learning framework to bring forward the initiative in the artificial intelligence business. From this point of view, we think it is a good time to compare some of deep learning frameworks. So we compare three deep learning frameworks which can be used as a Python library. Those are Google's Tensorflow, Microsoft’s CNTK, and Theano which is sort of a predecessor of the preceding two.
The most common and important function of deep learning frameworks is the ability to perform automatic differentiation. Basically all the mathematical expressions of deep learning models can be represented as computational graphs, which consist of nodes and edges. Partial derivatives on each edge of a computational graph can then be obtained. With the partial derivatives, we can let software compute differentiation of any node with respect to any variable by utilizing chain rule of Calculus.
First of all, the convenience of coding is in the order of CNTK, Tensorflow, and Theano. The criterion is simply based on the lengths of the codes and the learning curve and the ease of coding are not the main concern. According to the criteria, Theano was the most difficult to implement with, and CNTK and Tensorflow were somewhat easier. With Tensorflow, we need to define weight variables and biases explicitly. The reason that CNTK and Tensorflow are easier to implement with is that those frameworks provide us with more abstraction than Theano. We, however, need to mention that low-level coding is not always bad. It gives us flexibility of coding. With the low-level coding such as in Theano, we can implement and test any new deep learning models or any new search methods that we can think of.
The assessment of the execution speed of each framework is that there is not meaningful difference. According to the experiment, execution speeds of Theano and Tensorflow are very similar, although the experiment was limited to a CNN model. In the case of CNTK, the experimental environment was not maintained as the same. The code written in CNTK has to be run in PC environment without GPU where codes execute as much as 50 times slower than with GPU. But we concluded that the difference of execution speed was within the range of variation caused by the different hardware setup.
In this study, we compared three types of deep learning framework: Theano, Tensorflow, and CNTK. According to Wikipedia, there are 12 available deep learning frameworks. And 15 different attributes differentiate each framework. Some of the important attributes would include interface language (Python, C ++, Java, etc.) and the availability of libraries on various deep learning models such as CNN, RNN, DBN, and etc. And if a user implements a large scale deep learning model, it will also be important to support multiple GPU or multiple servers. Also, if you are learning the deep learning model, it would also be important if there are enough examples and references.deep learning framework, Theano, TensorFlow, CNTK, computational graph, CIFAR-10"
180,딥러닝을 이용한 주변 무선단말 파악방안,이웅섭 /Lee， Woong-sup,2018,한국정보통신학회,"최근 단말-대-단말(Device-to-device, D2D) 통신기술이 차세대 무선통신시스템의 핵심기술로 큰 관심을 받고 있다. 이러한 단말간 통신에서는 자신의 주변에 어떠한 단말이 있는지 파악하는 주변단말 탐색(Neighbor discovery)이매우 중요하다. 본 논문에서는 최근 큰 관심을 받고 있는 딥러닝(Deep learning) 기술을 활용하여 단말간 통신에서 주변단말을 파악하는 방안에 대해서 제안한다. 제안 방안은 기존의 방안과 달리 무선채널의 공간적 연관성을 이용하여단말간의 신호 전송 없이 단말이 기지국으로 전송하는 상향링크 파일럿 신호를 기반으로 주변 단말을 찾고 따라서기존의 방식에 비해 신호전송 복잡도(signaling complexity)를 크게 줄일 수 있다. 또한 제안 방안에서는 떨어져 있는거리에 따라서 주변 단말을 분류 가능하여 기존 방안에 비해서 좀 더 세밀한 단말 탐색이 가능하다. 마지막으로 본 논문에서는 tensorflow를 이용한 컴퓨터 시뮬레이션을 통해 제안 방안의 성능을 검증하였다.Recently, the device-to-device (D2D) communication has been conceived as the key technology for the next-generationmobile communication systems. The neighbor discovery in which the nearby users are found, is essential for the properoperation of the D2D communication. In this paper, we propose new neighbor discovery scheme based on deep learningtechnology which has gained a lot of attention recently. In the proposed scheme, the neighboring users can be foundusing the uplink pilot transmission of users only, unlike conventional neighbor discovery schemes in which direct pilotcommunication among users is required, such that the signaling overhead can be greatly reduced in our proposed scheme.
Moreover, the neighbors with different proximity can also be classified accordingly which enables more accurate neighbordiscovery compared to the conventional schemes. The performance of our proposed scheme is verified through thetensorflow-based computer simulations.
키워드 : 딥러닝, 주변단말 탐색, 단말-대-단말 통신Deep learning, Neighbor discovery, Device-to-device communication, Wireless channel correlation"
181,신뢰성있는 딥러닝 기반 분석 모델을 참조하기 위한 딥러닝 기술 언어,문종혁 /Jong Hyeok Mun,2021,한국정보처리학회,"최근 딥러닝은 하드웨어 성능이 향상됨에 따라 자연어 처리, 영상 인식 등의 다양한 기술에 접목되어 활용되고 있다. 이러한 기술들을 활용해 지능형교통 시스템(ITS), 스마트홈, 헬스케어 등의 산업분야에서 데이터를 분석하여 고속도로 속도위반 차량 검출, 에너지 사용량 제어, 응급상황 등과 같은고품질의 서비스를 제공하며, 고품질의 서비스를 제공하기 위해서는 정확도가 향상된 딥러닝 모델이 적용되어야 한다. 이를 위해 서비스 환경의 데이터를분석하기 위한 딥러닝 모델을 개발할 때, 개발자는 신뢰성이 검증된 최신의 딥러닝 모델을 적용할 수 있어야 한다. 이는 개발자가 참조하는 딥러닝 모델에적용된 학습 데이터셋의 정확도를 측정하여 검증할 수 있다. 이러한 검증을 위해서 개발자는 학습 데이터셋, 딥러닝의 계층구조 및 개발 환경 등과 같은내용을 포함하는 딥러닝 모델을 문서화하여 적용하기 위한 구조적인 정보가 필요하다. 본 논문에서는 신뢰성있는 딥러닝 기반 데이터 분석 모델을 참조하기위한 딥러닝 기술 언어를 제안한다. 제안하는 기술 언어는 신뢰성 있는 딥러닝 모델을 개발하는데 필요한 학습데이터셋, 개발 환경 및 설정 등의 정보와더불어 딥러닝 모델의 계층구조를 표현할 수 있다. 제안하는 딥러닝 기술 언어를 이용하여 개발자는 지능형 교통 시스템에서 참조하는 분석 모델의정확도를 검증할 수 있다. 실험에서는 제안하는 언어의 유효성을 검증하기 위해, 번호판 인식 모델을 중심으로 딥러닝 기술 문서의 적용과정을 보인다With the recent advancements of deep learning, companies such as smart home, healthcare, and intelligent transportation systems areutilizing its functionality to provide high-quality services for vehicle detection, emergency situation detection, and controlling energyconsumption. To provide reliable services in such sensitive systems, deep learning models are required to have high accuracy. In orderto develop a deep learning model for analyzing previously mentioned services, developers should utilize the state of the art deep learningmodels that have already been verified for higher accuracy. The developers can verify the accuracy of the referenced model by validatingthe model on the dataset. For this validation, the developer needs structural information to document and apply deep learning models,including metadata such as learning dataset, network architecture, and development environments. In this paper, we propose a descriptionlanguage that represents the network architecture of the deep learning model along with its metadata that are necessary to develop adeep learning model. Through the proposed description language, developers can easily verify the accuracy of the referenced deep learningmodel. Our experiments demonstrate the application scenario of a deep learning description document that focuses on the license platerecognition for the detection of illegally parked vehicles.Trusted Deep Learning, Model Reference, Deep Learning Description Language, Traffic Situation Analysis Model"
182,딥러닝 분석을 이용한 중국 역내·외 위안화 변동성 예측,이우식 /Lee Woosik,2016,한국데이터정보과학회,"2008년 글로벌 금융위기 이후 중국은 위안화 국제화의 점진적 추진을 진행하면서 중국상하이 외환시장과 중국홍콩 외환시장에서 거래되는 통화인 역내위안화와 역외위안화를 형성시켰다. 본 연구는 위안화 국제화와 점진적인 중국 자본계정 개방에 따라 급변하는 외환시장상황의 변동성을 정확하게 파악하기 위해서 GARCH모형 (일반화된 자기회귀 조건부이분산성모형)에 다단계인공신경망을 결합한 MLP-GARCH 모형과 GARCH모형과 기계학습의 일종인 딥러닝 (deep learning)을 통합한 DL-GARCH을 가지고 위안화 변동성예측을 비교 실험과 분석을 하였다. 비교분석 결과 DLGARCH 모형은 MLP-GARCH보다 모형 위안화 역내·외 환율변동성 예측 면에서 더욱 더 개선된예측값을 제공하였다. 그래서 이분산시계열모형을 딥러닝과 결합한 DL-GARCH 모형은 시계열의환율 변동성 예측 문제에 딥러닝을 응용할 수 있음을 확인하였다. 향후 이분산시계열과 결합된 딥러닝 모형은 다른 금융시계열 데이터에 응용하여 그 일반화 가능성을 높일 수 있을 것이다.The People’s Republic of China has vigorously been pursuing the internationalization of the Chinese Yuan or Renminbi after the financial crisis of 2008. In this view, an abrupt increase of use of the Chinese Yuan in the onshore and offshore markets are important milestones to be one of important currencies. One of the most frequently used methods to forecast volatility is GARCH model. Since a prediction error of the GARCH model has been reported quite high, a lot of efforts have been made to improve forecasting capability of the GARCH model. In this paper, we have proposed MLPGARCH and a DL-GARCH by employing Artificial Neural Network to the GARCH. In an application to forecasting Chinese Yuan volatility, we have successfully shown their overall outperformance in forecasting over the GARCH.Deep learning, DL-GARCH, GARCH, The Chinese Yuan, volatility."
183,딥러닝 기법을 이용한 내일강수 예측,하지훈,2016,한국지능시스템학회,"정확한 강수예측을 위해서는 예측인자 선정과 예측방법에 대한 선택이 매우 중요하다. 최근에는 강수예측 방법으로기계학습 기법이 많이 사용되고 있으며, 그 중에서도 특히 인공신경망을 사용한 강수예측 방법은 좋은 성능을 보였다. 본논문에서는 딥러닝 기법 중 하나인 DBN(deep belief network)를 이용한 새로운 강수예측 방법을 제안한다. DBN는 비지도사전 학습을 통해 초기 가중치를 설정하여 기존 인공신경망의 문제점을 보완한다. 예측인자로는 기온, 전일-전주 강수일, 태양과 달 궤도 관련 자료를 선정하였다. 기온과 전일-전주 강수일은 서울에서의 1974년부터 2013년까지 총 40년간의AWS(automatic weather system) 관측 자료를 사용하였고, 태양과 달의 궤도 관련 자료는 서울을 중심으로 계산한 결과를사용하였다. 전체 기간에서 일부는 학습 자료로 사용하여 예측모델을 생성하였고, 나머지를 생성한 모델의 검증 자료로사용하였다. 모델 검증 결과로 나온 예측값들은 확률값을 가지며 임계치를 이용하여 강수유무를 판별하였다. 강수 정확도의척도로 양분예보기법 중 CSI(critical successive index)와 Bias(frequency bias)를 계산하였다. 이를 통해 DBN와 MLP(multilayer perceptron)의 성능을 비교한 결과 DBN의 강수 예측 정확도가 높았고, 수행속도 또한 2배 이상 빨랐다.For accurate precipitation forecasts the choice of weather faoctrs and prediction method is very important. Recently, machinele arning has been widely used for forecasting precipitation, and artificial neural network, one of machine learning techniques, showed good performance. In this paper, we suggest a new method for forecasting precipitation using DBN, one of deep learning techniques.
DBN has an advantage that initial weights are set by unsupervised learning, so this compensates for the defects of artificial neural networks. We used past precipitation, temperature, and the parameters of the sun and moon's motion as features for forecasting precipitation. The dataset consists of observation data which had been measured for 40 years from AWS in Seoul. Experiments were based on 8-fold cross validation. As a result of estimation, we got probabilities of test dataset, so threshold was used for the decision of precipitation. CSI and Bias were used for indicating the precision of precipitation. Our experimental results showed that DBN performed better than MLP.Deep learning, Deep belief network, Precipitation, Forecast"
184,딥러닝 신경망모형을 이용한 실시간 교통정보수집 연구,김대현 /KIM DAE HYON,2016,사단법인 인문사회과학기술융합학회,"비디오 기반의 실시간 교통정보수집 시스템은 자율주행을 포함한 지능형교통체계의 주요 서브시스템중 하나이며, 우수한 예측(추정)력 때문에 핵심 알고리즘으로 인공신경망 모형이 주로 사용되고 있다. 특히, 최근 과학 학술지 네이처에 구글 딥마인드의 알파고가 공개되면서 딥러닝과 인공지능에 대한 전 세계적인 관심이 집중되고 있다. 최근까지 가장 널리 사용된 인공신경망모형은 역전파 모형이었으나 최근 컴퓨터 하드웨어 및 알고리즘의 새로운 개발과 더불어 딥러닝 알고리즘을 이용한 딥인공신경망 모형에 대한 관심이 높아지고 있다. 그러나 아직 딥러닝 인공신경망에 대한 연구는 초기단계에 있는 실정이며 적용분야도 극히 한정되어 있다. 본 연구에서는 실시간 도로교통정보 수집을 위한 영상처리분야에서 딥러닝 알고리즘을 활용하고 실험결과를 통하여 향후 교통정보수집‧분석 분야에서의 딥러닝 인공신경망 모형의 적용가능성을 살펴보고자 한다. 특히, 본 연구에서 도출한 딥러닝 인공신경망모형의 실험결과를 기존 역전파 모형에 의한 실험결과와 비교하여 모형의 우수성을 비교‧평가하고자 한다.Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Normally, artificial neural networks have been used in real-time applications for collecting traffic information because of their remarkable performance in terms of predictive accuracy and real-time computing. Especially, the recent publication of a paper in the journal Nature and the success of Google DeepMind’s world-beating Go program AlphaGo caused significant increases of the public interest in artificial intelligence. Even though various types of learning models have been proposed, the Backpropagation has been the most popular neural network model. However, the Deep learning neural network model has attracted much interest recently. In this study, the Deep leaning neural network will be applied in image processing techniques for the real-time traffic information collection to investigate the performance of Deep learning algorithm in traffic engineering fields. In addition, the experimental results from Deep learing algorithm will be compared with the results from Backpropagation model to evaluate and propose better performing model.Intelligent Transportation Systems (ITS), artificial neural networks, Deep learning, traffic information, image processing"
185,딥러닝의 모형과 응용사례,안성만 /SungMahn Ahn,2016,한국지능정보시스템학회,"딥러닝은 인공신경망(neural network)이라는 인공지능분야의 모형이 발전된 형태로서, 계층구조로 이루어진 인공신경망의 내부계층(hidden layer)이 여러 단계로 이루어진 구조이다. 딥러닝에서의 주요 모형은 합성곱신경망(convolutional neural network), 순환신경망(recurrent neural network), 그리고 심층신뢰신경망(deep belief network)의 세가지라고 할 수있다. 그 중에서 현재 흥미로운 연구가 많이 발표되어서 관심이 집중되고 있는 모형은 지도학습(supervised learning)모형인 처음 두 개의 모형이다. 따라서 본 논문에서는 지도학습모형의 가중치를 최적화하는 기본적인 방법인 오류역전파 알고리즘을 살펴본 뒤에 합성곱신경망과 순환신경망의 구조와 응용사례 등을 살펴보고자 한다. 본문에서 다루지 않은 모형인 심층신뢰신경망은 아직까지는 합성곱신경망 이나 순환신경망보다는 상대적으로 주목을 덜 받고 있다. 그러나 심층신뢰신경망은 CNN이나 RNN과는 달리 비지도학습(unsupervised learning)모형이며, 사람이나 동물은 관찰을 통해서 스스로 학습한다는 점에서 궁극적으로는 비지도학습모형이 더 많이 연구되어야 할 주제가 될 것이다.Deep learning model is a kind of neural networks that allows multiple hidden layers. There are various deep learning architectures such as convolutional neural networks, deep belief networks and recurrent neural networks. Those have been applied to fields like computer vision, automatic speech recognition, natural language processing, audio recognition and bioinformatics where they have been shown to produce state-of-the-art results on various tasks. Among those architectures, convolutional neural networks and recurrent neural networks are classified as the supervised learning model. And in recent years, those supervised learning models have gained more popularity than unsupervised learning models such as deep belief networks, because supervised learning models have shown fashionable applications in such fields mentioned above.
Deep learning models can be trained with backpropagation algorithm. Backpropagation is an abbreviation for “backward propagation of errors” and a common method of training artificial neural networks used in conjunction with an optimization method such as gradient descent. The method calculates the gradient of an error function with respect to all the weights in the network. The gradient is fed to the optimization method which in turn uses it to update the weights, in an attempt to minimize the error function.
Convolutional neural networks use a special architecture which is particularly well-adapted to classify images. Using this architecture makes convolutional networks fast to train. This, in turn, helps us train deep, muti-layer networks, which are very good at classifying images. These days, deep convolutional networks are used in most neural networks for image recognition. Convolutional neural networks use three basic ideas: local receptive fields, shared weights, and pooling. By local receptive fields, we mean that each neuron in the first(or any) hidden layer will be connected to a small region of the input(or previous layer’s) neurons. Shared weights mean that we’re going to use the same weights and bias for each of the local receptive field. This means that all the neurons in the hidden layer detect exactly the same feature, just at different locations in the input image. In addition to the convolutional layers just described, convolutional neural networks also contain pooling layers. Pooling layers are usually used immediately after convolutional layers. What the pooling layers do is to simplify the information in the output from the convolutional layer.
Recent convolutional network architectures have 10 to 20 hidden layers and billions of connections between units. Training deep learning networks has taken weeks several years ago, but thanks to progress in GPU and algorithm enhancement, training time has reduced to several hours.
Neural networks with time-varying behavior are known as recurrent neural networks or RNNs. A recurrent neural network is a class of artificial neural network where connections between units form a directed cycle. This creates an internal state of the network which allows it to exhibit dynamic temporal behavior. Unlike feedforward neural networks, RNNs can use their internal memory to process arbitrary sequences of inputs. Early RNN models turned out to be very difficult to train, harder even than deep feedforward networks. The reason is the unstable gradient problem such as vanishing gradient and exploding gradient. The gradient can get smaller and smaller as it is propagated back through layers. This makes learning in early layers extremely slow. The problem actually gets worse in RNNs, since gradients aren’t just propagated backward through layers, they’re propagated backward through time. If the network runs for a long time, that can make the gradient extremely unstable and hard to learn from. It has been possible to incorporate an idea known as long short-term memory units (LSTMs) into RNNs. LSTMs make it much easier to get good results when training RNNs, and many recent papers make use of LSTMs or related ideas.Deep learning, Convolutional neural networks, Recurrent neural networks, Error backpropagation algorithm"
186,딥러닝분석과 기술적 분석 지표를 이용한 한국 코스피주가지수 방향성 예측,이우식 /Lee Woosik,2017,한국데이터정보과학회,"2016년 3월 구글 (Google)의 바둑인공지능 알파고 (AlphaGo)가 이세돌 9단과의 바둑대결에서승리한 이후 다양한 분야에서 인공지능 사용에 대한 관심이 높아지고 있는 가운데 금융투자 분야에서도 인공지능과 투자자문 전문가의 합성어인 로보어드바이저 (Robo-Advisor)에 대한 관심이 높아지고 있다. 인공지능 (artificial intelligence)기반의 의사결정은 비용 절감은 물론 효과적인 의사결정을가능하게 한다는 점에서 큰 장점이 있다. 본 연구에서는 기술적 분석 (technical analysis) 지표와 딥러닝 (deep learning) 모형을 결합하여 한국 코스피 지수를 예측하는 모형을 개발하고 제시한 모형들의 예측력을 비교, 분석한다. 분석 결과 기술적 분석 지표에 딥러닝 알고리즘을 결합한 모형이 주가지수 방향성 예측 문제에 응용될 수 있음을 확인하였다. 향후 본 연구에서 제안된 기술적 분석 지표와딥러닝모형을 결합한 기법은 로보어드바이저서비스에 응용할 수 있는 일반화 가능성을 보여준다Since Google’s AlphaGo defeated a world champion of Go players in 2016, there have been many interests in the deep learning. In the financial sector, a Robo-Advisor using deep learning gains a significant attention, which builds and manages portfolios of financial instruments for investors.In this paper, we have proposed the a deep learning algorithm geared toward identification and forecast of the KOSPI index direction,and we also have compared the accuracy of the prediction.In an application of forecasting the financial market index direction, we have shown that the Robo-Advisor using deep learning has a significant effect on finance industry. The Robo-Advisor collects a massive data such as earnings statements, news reports and regulatory filings, analyzes those and recommends investors how to view market trends and identify the best time to purchase financial assets. On the other hand, the Robo-Advisor allows businesses to learn more about their customers, develop better marketing strategies, increase sales and decrease costs.Artificial intelligence, deep learning, FinTech, Robo-Advisor, technical analysis."
187,딥러닝의 효율적인 학습을 위한 학습데이터 선별 기법에 관한 연구,주경돈 /Gyoung-don Joo,2017,한국정보과학회,"빅데이터의 증가와 병렬 하드웨어 처리, 학습 알고리즘의 개선은 딥러닝의 발전을 가져왔으며 인공지능이 적용 가능한 영역을 획기적으로 확장시키고 있다. 하지만 이러한 대규모의 학습 데이터에 기반한 딥러닝은 학습시간이 증가되고, 더 나아가 학습 데이터의 구축을 위한 레이블링(labeling) 비용이 데이터 수에 비례하여 증가하게 된다. 따라서 본 논문에서는, 딥러닝의 정확도를 최대한 유지하면서 필요한 학습 데이터의 수를 줄일수 있도록, 전체 데이터에서 제한된 수의 학습 데이터의 후보를 선별하는 기법에 대하여 연구한다. 특히 기존의 능동 학습에 기반한 기법들이 학습 데이터의 수가 매우 적어질 경우 딥러닝의 정확도가 급격히 떨어지는 문제를 해결할 수 있는 새로운 기법을 제안하였으며, 제안된 기법으로 선별된 학습 데이터는 그 수의 변화에 대해 딥러닝이 일관되게 경쟁력있는 정확도를 가짐을 다양한 실험을 통하여 검증하였다.The increase of big data, parallel hardware processing and enhanced learning algorithms have led to the development of deep learning and are dramatically expanding the domain of applications based on artificial intelligence. However, such large-scale training data extends the learning time, and furthermore, the labeling cost for building the training set increases in proportion to the volume of data. Therefore, in this paper, we study how to select a limited number of candidates for training set from given data so as to reduce the number of training set while maintaining the accuracy of deep learning. Especially, we found that, with the training set selected by the previous work, the accuracy of deep learning drastically decreases when the number of training set is very small and so proposed new methods to resolve this fall-off in accuracy. Through experiments, we showed that deep learning consistently has the competitive accuracy with the training set selected by the proposed method regardless of the size of training set.Deep Learning, Train Set Selection, Active Learning, Gamma distribution"
188,딥러닝을 이용한 범용적 스테그아날리시스,김현재 /Hyunjae Kim,2017,한국정보과학회,"스테그아날리시스(Steganalysis)란 이미지 등 일반적인 자료에 암호화된 정보를 은닉하는 스테가노그래피(Steganography)에 대한 검출 및 분석 방법으로, 기계학습 기반 방법론을 포함한다. 기존 기계학습 기반 스테그아날리시스는 영상(Image)의 특징(Feature) 추출 및 모델링에 기반하며, 최근 딥러닝(Deep Learning)의 적용으로 검출 정확도가 큰 폭으로 향상되었다. 하지만 현존하는 스테그아날리시스 모델은 단일 스테가노그래피 기법에 대해 국한되어 있어 학습에 사용되지 않은 스테고(Stego) 이미지의 경우 검출이 불가능한 결정적 한계를 가진다. 본 연구에서는 다양한 스테가노그래피 기법으로 생성된 스테고이미지에 딥러닝을 적용하여 스테그아날리시스를 학습하는 범용적 모델을 제안한다. 다양한 실험을 통해 제안 기법의 효용성 및 가능성을 확인하고, 범용적 스테그아날리시스 모델이 각각에 특화된 검출 기법과 유사한 정확도로 스테고 이미지를 검출할 수 있음을 보인다.Steganalysis is to detect information hidden by steganography inside general data such as images. There are stegoanalysis techniques that use machine learning (ML). Existing ML approaches to steganalysis are based on extracting features from stego images and modeling them.
Recently deep learning-based methodologies have shown significant improvements in detection accuracy. However, all the existing methods, including deep learning-based ones, have a critical limitation in that they can only detect stego images that are created by a specific steganography method. In this paper, we propose a generalized steganalysis method that can model multiple types of stego images using deep learning. Through various experiments, we confirm the effectiveness of our approach and envision directions for future research. In particular, we show that our method can detect each type of steganography with the same level of accuracy as that of a steganalysis method dedicated to that type of steganography, thereby demonstrating the general applicability of our approach to multiple types of stego images.steganalysis, steganography, deep learning, generalized steganalysis model"
189,"딥러닝 프레임워크의 비교: 티아노, 텐서플로, CNTK를 중심으로",정여진 /Yeojin Chung,2017,한국지능정보시스템학회,"딥러닝 프레임워크의 대표적인 기능으로는 ‘자동미분’과 ‘GPU의 활용’ 등을 들 수 있다. 본 논문은 파이썬의 라이브러리 형태로 사용 가능한 프레임워크 중에서 구글의 텐서플로와 마이크로소프트의 CNTK, 그리고 텐서 플로의 원조라고 할 수 있는 티아노를 비교하였다. 본문에서는 자동미분의 개념과 GPU의 활용형태를 간단히 설명하고, 그 다음에 logistic regression을 실행하는 예를 통하여 각 프레임워크의 문법을 알아본 뒤에, 마지막으로 대표적인 딥러닝 응용인 CNN의 예제를 실행시켜보고 코딩의 편의성과 실행속도 등을 확인해 보았다. 그 결과, 편의성의 관점에서 보면 티아노가 가장 코딩 하기가 어렵고, CNTK와 텐서플로는 많은 부분이 비슷하게 추상화 되어 있어서 코딩이 비슷하지만 가중치와 편향을 직접 정의하느냐의 여부에서 차이를 보였다. 그리고 각프레임워크의 실행속도에 대한 평가는 ‘큰 차이는 없다’는 것이다. 텐서플로는 티아노에 비하여 속도가 느리다는 평가가 있어왔는데, 본 연구의 실험에 의하면, 비록 CNN 모형에 국한되었지만, 텐서플로가 아주 조금이지만빠른 것으로 나타났다. CNTK의 경우에도, 비록 실험환경이 달랐지만, 실험환경의 차이에 의한 속도의 차이의 편차범위 이내에 있는 것으로 판단이 되었다. 본 연구에서는 세 종류의 딥러닝 프레임워크만을 살펴보았는데,위키피디아에 따르면 딥러닝 프레임워크의 종류는 12가지가 있으며, 각 프레임워크의 특징을 15가지 속성으로구분하여 차이를 특정하고 있다. 그 많은 속성 중에서 사용자의 입장에서 볼 때 중요한 속성은 어떤 언어(파이썬, C++, Java, 등)로 사용가능한지, 어떤 딥러닝 모형에 대한 라이브러리가 잘 구현되어 있는지 등일 것이다. 그리고 사용자가 대규모의 딥러닝 모형을 구축한다면, 다중 GPU 혹은 다중 서버를 지원하는지의 여부도 중요할 것이다. 또한 딥러닝 모형을 처음 학습하는 경우에는 사용설명서가 많은지 예제 프로그램이 많은지 여부도 중요한 기준이 될 것이다.The deep learning framework is software designed to help develop deep learning models. Some of its important functions include “automatic differentiation” and “utilization of GPU”. The list of popular deep learning framework includes Caffe (BVLC) and Theano (University of Montreal). And recently, Microsoft's deep learning framework, Microsoft Cognitive Toolkit, was released as open-source license, following Google’s Tensorflow a year earlier. The early deep learning frameworks have been developed mainly for research at universities. Beginning with the inception of Tensorflow, however, it seems that companies such as Microsoft and Facebook have started to join the competition of framework development. Given the trend, Google and other companies are expected to continue investing in the deep learning framework to bring forward the initiative in the artificial intelligence business. From this point of view, we think it is a good time to compare some of deep learning frameworks. So we compare three deep learning frameworks which can be used as a Python library. Those are Google's Tensorflow, Microsoft’s CNTK, and Theano which is sort of a predecessor of the preceding two.
The most common and important function of deep learning frameworks is the ability to perform automatic differentiation. Basically all the mathematical expressions of deep learning models can be represented as computational graphs, which consist of nodes and edges. Partial derivatives on each edge of a computational graph can then be obtained. With the partial derivatives, we can let software compute differentiation of any node with respect to any variable by utilizing chain rule of Calculus.
First of all, the convenience of coding is in the order of CNTK, Tensorflow, and Theano. The criterion is simply based on the lengths of the codes and the learning curve and the ease of coding are not the main concern. According to the criteria, Theano was the most difficult to implement with, and CNTK and Tensorflow were somewhat easier. With Tensorflow, we need to define weight variables and biases explicitly. The reason that CNTK and Tensorflow are easier to implement with is that those frameworks provide us with more abstraction than Theano. We, however, need to mention that low-level coding is not always bad. It gives us flexibility of coding. With the low-level coding such as in Theano, we can implement and test any new deep learning models or any new search methods that we can think of.
The assessment of the execution speed of each framework is that there is not meaningful difference. According to the experiment, execution speeds of Theano and Tensorflow are very similar, although the experiment was limited to a CNN model. In the case of CNTK, the experimental environment was not maintained as the same. The code written in CNTK has to be run in PC environment without GPU where codes execute as much as 50 times slower than with GPU. But we concluded that the difference of execution speed was within the range of variation caused by the different hardware setup.
In this study, we compared three types of deep learning framework: Theano, Tensorflow, and CNTK. According to Wikipedia, there are 12 available deep learning frameworks. And 15 different attributes differentiate each framework. Some of the important attributes would include interface language (Python, C ++, Java, etc.) and the availability of libraries on various deep learning models such as CNN, RNN, DBN, and etc. And if a user implements a large scale deep learning model, it will also be important to support multiple GPU or multiple servers. Also, if you are learning the deep learning model, it would also be important if there are enough examples and references.deep learning framework, Theano, TensorFlow, CNTK, computational graph, CIFAR-10"
190,딥러닝을 이용한 주변 무선단말 파악방안,이웅섭 /Lee， Woong-sup,2018,한국정보통신학회,"최근 단말-대-단말(Device-to-device, D2D) 통신기술이 차세대 무선통신시스템의 핵심기술로 큰 관심을 받고 있다. 이러한 단말간 통신에서는 자신의 주변에 어떠한 단말이 있는지 파악하는 주변단말 탐색(Neighbor discovery)이매우 중요하다. 본 논문에서는 최근 큰 관심을 받고 있는 딥러닝(Deep learning) 기술을 활용하여 단말간 통신에서 주변단말을 파악하는 방안에 대해서 제안한다. 제안 방안은 기존의 방안과 달리 무선채널의 공간적 연관성을 이용하여단말간의 신호 전송 없이 단말이 기지국으로 전송하는 상향링크 파일럿 신호를 기반으로 주변 단말을 찾고 따라서기존의 방식에 비해 신호전송 복잡도(signaling complexity)를 크게 줄일 수 있다. 또한 제안 방안에서는 떨어져 있는거리에 따라서 주변 단말을 분류 가능하여 기존 방안에 비해서 좀 더 세밀한 단말 탐색이 가능하다. 마지막으로 본 논문에서는 tensorflow를 이용한 컴퓨터 시뮬레이션을 통해 제안 방안의 성능을 검증하였다.Recently, the device-to-device (D2D) communication has been conceived as the key technology for the next-generationmobile communication systems. The neighbor discovery in which the nearby users are found, is essential for the properoperation of the D2D communication. In this paper, we propose new neighbor discovery scheme based on deep learningtechnology which has gained a lot of attention recently. In the proposed scheme, the neighboring users can be foundusing the uplink pilot transmission of users only, unlike conventional neighbor discovery schemes in which direct pilotcommunication among users is required, such that the signaling overhead can be greatly reduced in our proposed scheme.
Moreover, the neighbors with different proximity can also be classified accordingly which enables more accurate neighbordiscovery compared to the conventional schemes. The performance of our proposed scheme is verified through thetensorflow-based computer simulations.
키워드 : 딥러닝, 주변단말 탐색, 단말-대-단말 통신Deep learning, Neighbor discovery, Device-to-device communication, Wireless channel correlation"
191,신뢰성있는 딥러닝 기반 분석 모델을 참조하기 위한 딥러닝 기술 언어,문종혁 /Jong Hyeok Mun,2021,한국정보처리학회,"최근 딥러닝은 하드웨어 성능이 향상됨에 따라 자연어 처리, 영상 인식 등의 다양한 기술에 접목되어 활용되고 있다. 이러한 기술들을 활용해 지능형교통 시스템(ITS), 스마트홈, 헬스케어 등의 산업분야에서 데이터를 분석하여 고속도로 속도위반 차량 검출, 에너지 사용량 제어, 응급상황 등과 같은고품질의 서비스를 제공하며, 고품질의 서비스를 제공하기 위해서는 정확도가 향상된 딥러닝 모델이 적용되어야 한다. 이를 위해 서비스 환경의 데이터를분석하기 위한 딥러닝 모델을 개발할 때, 개발자는 신뢰성이 검증된 최신의 딥러닝 모델을 적용할 수 있어야 한다. 이는 개발자가 참조하는 딥러닝 모델에적용된 학습 데이터셋의 정확도를 측정하여 검증할 수 있다. 이러한 검증을 위해서 개발자는 학습 데이터셋, 딥러닝의 계층구조 및 개발 환경 등과 같은내용을 포함하는 딥러닝 모델을 문서화하여 적용하기 위한 구조적인 정보가 필요하다. 본 논문에서는 신뢰성있는 딥러닝 기반 데이터 분석 모델을 참조하기위한 딥러닝 기술 언어를 제안한다. 제안하는 기술 언어는 신뢰성 있는 딥러닝 모델을 개발하는데 필요한 학습데이터셋, 개발 환경 및 설정 등의 정보와더불어 딥러닝 모델의 계층구조를 표현할 수 있다. 제안하는 딥러닝 기술 언어를 이용하여 개발자는 지능형 교통 시스템에서 참조하는 분석 모델의정확도를 검증할 수 있다. 실험에서는 제안하는 언어의 유효성을 검증하기 위해, 번호판 인식 모델을 중심으로 딥러닝 기술 문서의 적용과정을 보인다With the recent advancements of deep learning, companies such as smart home, healthcare, and intelligent transportation systems areutilizing its functionality to provide high-quality services for vehicle detection, emergency situation detection, and controlling energyconsumption. To provide reliable services in such sensitive systems, deep learning models are required to have high accuracy. In orderto develop a deep learning model for analyzing previously mentioned services, developers should utilize the state of the art deep learningmodels that have already been verified for higher accuracy. The developers can verify the accuracy of the referenced model by validatingthe model on the dataset. For this validation, the developer needs structural information to document and apply deep learning models,including metadata such as learning dataset, network architecture, and development environments. In this paper, we propose a descriptionlanguage that represents the network architecture of the deep learning model along with its metadata that are necessary to develop adeep learning model. Through the proposed description language, developers can easily verify the accuracy of the referenced deep learningmodel. Our experiments demonstrate the application scenario of a deep learning description document that focuses on the license platerecognition for the detection of illegally parked vehicles.Trusted Deep Learning, Model Reference, Deep Learning Description Language, Traffic Situation Analysis Model"
192,딥러닝 분석을 이용한 중국 역내·외 위안화 변동성 예측,이우식 /Lee Woosik,2016,한국데이터정보과학회,"2008년 글로벌 금융위기 이후 중국은 위안화 국제화의 점진적 추진을 진행하면서 중국상하이 외환시장과 중국홍콩 외환시장에서 거래되는 통화인 역내위안화와 역외위안화를 형성시켰다. 본 연구는 위안화 국제화와 점진적인 중국 자본계정 개방에 따라 급변하는 외환시장상황의 변동성을 정확하게 파악하기 위해서 GARCH모형 (일반화된 자기회귀 조건부이분산성모형)에 다단계인공신경망을 결합한 MLP-GARCH 모형과 GARCH모형과 기계학습의 일종인 딥러닝 (deep learning)을 통합한 DL-GARCH을 가지고 위안화 변동성예측을 비교 실험과 분석을 하였다. 비교분석 결과 DLGARCH 모형은 MLP-GARCH보다 모형 위안화 역내·외 환율변동성 예측 면에서 더욱 더 개선된예측값을 제공하였다. 그래서 이분산시계열모형을 딥러닝과 결합한 DL-GARCH 모형은 시계열의환율 변동성 예측 문제에 딥러닝을 응용할 수 있음을 확인하였다. 향후 이분산시계열과 결합된 딥러닝 모형은 다른 금융시계열 데이터에 응용하여 그 일반화 가능성을 높일 수 있을 것이다.The People’s Republic of China has vigorously been pursuing the internationalization of the Chinese Yuan or Renminbi after the financial crisis of 2008. In this view, an abrupt increase of use of the Chinese Yuan in the onshore and offshore markets are important milestones to be one of important currencies. One of the most frequently used methods to forecast volatility is GARCH model. Since a prediction error of the GARCH model has been reported quite high, a lot of efforts have been made to improve forecasting capability of the GARCH model. In this paper, we have proposed MLPGARCH and a DL-GARCH by employing Artificial Neural Network to the GARCH. In an application to forecasting Chinese Yuan volatility, we have successfully shown their overall outperformance in forecasting over the GARCH.Deep learning, DL-GARCH, GARCH, The Chinese Yuan, volatility."
193,딥러닝 기법을 이용한 내일강수 예측,하지훈,2016,한국지능시스템학회,"정확한 강수예측을 위해서는 예측인자 선정과 예측방법에 대한 선택이 매우 중요하다. 최근에는 강수예측 방법으로기계학습 기법이 많이 사용되고 있으며, 그 중에서도 특히 인공신경망을 사용한 강수예측 방법은 좋은 성능을 보였다. 본논문에서는 딥러닝 기법 중 하나인 DBN(deep belief network)를 이용한 새로운 강수예측 방법을 제안한다. DBN는 비지도사전 학습을 통해 초기 가중치를 설정하여 기존 인공신경망의 문제점을 보완한다. 예측인자로는 기온, 전일-전주 강수일, 태양과 달 궤도 관련 자료를 선정하였다. 기온과 전일-전주 강수일은 서울에서의 1974년부터 2013년까지 총 40년간의AWS(automatic weather system) 관측 자료를 사용하였고, 태양과 달의 궤도 관련 자료는 서울을 중심으로 계산한 결과를사용하였다. 전체 기간에서 일부는 학습 자료로 사용하여 예측모델을 생성하였고, 나머지를 생성한 모델의 검증 자료로사용하였다. 모델 검증 결과로 나온 예측값들은 확률값을 가지며 임계치를 이용하여 강수유무를 판별하였다. 강수 정확도의척도로 양분예보기법 중 CSI(critical successive index)와 Bias(frequency bias)를 계산하였다. 이를 통해 DBN와 MLP(multilayer perceptron)의 성능을 비교한 결과 DBN의 강수 예측 정확도가 높았고, 수행속도 또한 2배 이상 빨랐다.For accurate precipitation forecasts the choice of weather faoctrs and prediction method is very important. Recently, machinele arning has been widely used for forecasting precipitation, and artificial neural network, one of machine learning techniques, showed good performance. In this paper, we suggest a new method for forecasting precipitation using DBN, one of deep learning techniques.
DBN has an advantage that initial weights are set by unsupervised learning, so this compensates for the defects of artificial neural networks. We used past precipitation, temperature, and the parameters of the sun and moon's motion as features for forecasting precipitation. The dataset consists of observation data which had been measured for 40 years from AWS in Seoul. Experiments were based on 8-fold cross validation. As a result of estimation, we got probabilities of test dataset, so threshold was used for the decision of precipitation. CSI and Bias were used for indicating the precision of precipitation. Our experimental results showed that DBN performed better than MLP.Deep learning, Deep belief network, Precipitation, Forecast"
194,딥러닝 신경망모형을 이용한 실시간 교통정보수집 연구,김대현 /KIM DAE HYON,2016,사단법인 인문사회과학기술융합학회,"비디오 기반의 실시간 교통정보수집 시스템은 자율주행을 포함한 지능형교통체계의 주요 서브시스템중 하나이며, 우수한 예측(추정)력 때문에 핵심 알고리즘으로 인공신경망 모형이 주로 사용되고 있다. 특히, 최근 과학 학술지 네이처에 구글 딥마인드의 알파고가 공개되면서 딥러닝과 인공지능에 대한 전 세계적인 관심이 집중되고 있다. 최근까지 가장 널리 사용된 인공신경망모형은 역전파 모형이었으나 최근 컴퓨터 하드웨어 및 알고리즘의 새로운 개발과 더불어 딥러닝 알고리즘을 이용한 딥인공신경망 모형에 대한 관심이 높아지고 있다. 그러나 아직 딥러닝 인공신경망에 대한 연구는 초기단계에 있는 실정이며 적용분야도 극히 한정되어 있다. 본 연구에서는 실시간 도로교통정보 수집을 위한 영상처리분야에서 딥러닝 알고리즘을 활용하고 실험결과를 통하여 향후 교통정보수집‧분석 분야에서의 딥러닝 인공신경망 모형의 적용가능성을 살펴보고자 한다. 특히, 본 연구에서 도출한 딥러닝 인공신경망모형의 실험결과를 기존 역전파 모형에 의한 실험결과와 비교하여 모형의 우수성을 비교‧평가하고자 한다.Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Normally, artificial neural networks have been used in real-time applications for collecting traffic information because of their remarkable performance in terms of predictive accuracy and real-time computing. Especially, the recent publication of a paper in the journal Nature and the success of Google DeepMind’s world-beating Go program AlphaGo caused significant increases of the public interest in artificial intelligence. Even though various types of learning models have been proposed, the Backpropagation has been the most popular neural network model. However, the Deep learning neural network model has attracted much interest recently. In this study, the Deep leaning neural network will be applied in image processing techniques for the real-time traffic information collection to investigate the performance of Deep learning algorithm in traffic engineering fields. In addition, the experimental results from Deep learing algorithm will be compared with the results from Backpropagation model to evaluate and propose better performing model.Intelligent Transportation Systems (ITS), artificial neural networks, Deep learning, traffic information, image processing"
195,딥러닝의 모형과 응용사례,안성만 /SungMahn Ahn,2016,한국지능정보시스템학회,"딥러닝은 인공신경망(neural network)이라는 인공지능분야의 모형이 발전된 형태로서, 계층구조로 이루어진 인공신경망의 내부계층(hidden layer)이 여러 단계로 이루어진 구조이다. 딥러닝에서의 주요 모형은 합성곱신경망(convolutional neural network), 순환신경망(recurrent neural network), 그리고 심층신뢰신경망(deep belief network)의 세가지라고 할 수있다. 그 중에서 현재 흥미로운 연구가 많이 발표되어서 관심이 집중되고 있는 모형은 지도학습(supervised learning)모형인 처음 두 개의 모형이다. 따라서 본 논문에서는 지도학습모형의 가중치를 최적화하는 기본적인 방법인 오류역전파 알고리즘을 살펴본 뒤에 합성곱신경망과 순환신경망의 구조와 응용사례 등을 살펴보고자 한다. 본문에서 다루지 않은 모형인 심층신뢰신경망은 아직까지는 합성곱신경망 이나 순환신경망보다는 상대적으로 주목을 덜 받고 있다. 그러나 심층신뢰신경망은 CNN이나 RNN과는 달리 비지도학습(unsupervised learning)모형이며, 사람이나 동물은 관찰을 통해서 스스로 학습한다는 점에서 궁극적으로는 비지도학습모형이 더 많이 연구되어야 할 주제가 될 것이다.Deep learning model is a kind of neural networks that allows multiple hidden layers. There are various deep learning architectures such as convolutional neural networks, deep belief networks and recurrent neural networks. Those have been applied to fields like computer vision, automatic speech recognition, natural language processing, audio recognition and bioinformatics where they have been shown to produce state-of-the-art results on various tasks. Among those architectures, convolutional neural networks and recurrent neural networks are classified as the supervised learning model. And in recent years, those supervised learning models have gained more popularity than unsupervised learning models such as deep belief networks, because supervised learning models have shown fashionable applications in such fields mentioned above.
Deep learning models can be trained with backpropagation algorithm. Backpropagation is an abbreviation for “backward propagation of errors” and a common method of training artificial neural networks used in conjunction with an optimization method such as gradient descent. The method calculates the gradient of an error function with respect to all the weights in the network. The gradient is fed to the optimization method which in turn uses it to update the weights, in an attempt to minimize the error function.
Convolutional neural networks use a special architecture which is particularly well-adapted to classify images. Using this architecture makes convolutional networks fast to train. This, in turn, helps us train deep, muti-layer networks, which are very good at classifying images. These days, deep convolutional networks are used in most neural networks for image recognition. Convolutional neural networks use three basic ideas: local receptive fields, shared weights, and pooling. By local receptive fields, we mean that each neuron in the first(or any) hidden layer will be connected to a small region of the input(or previous layer’s) neurons. Shared weights mean that we’re going to use the same weights and bias for each of the local receptive field. This means that all the neurons in the hidden layer detect exactly the same feature, just at different locations in the input image. In addition to the convolutional layers just described, convolutional neural networks also contain pooling layers. Pooling layers are usually used immediately after convolutional layers. What the pooling layers do is to simplify the information in the output from the convolutional layer.
Recent convolutional network architectures have 10 to 20 hidden layers and billions of connections between units. Training deep learning networks has taken weeks several years ago, but thanks to progress in GPU and algorithm enhancement, training time has reduced to several hours.
Neural networks with time-varying behavior are known as recurrent neural networks or RNNs. A recurrent neural network is a class of artificial neural network where connections between units form a directed cycle. This creates an internal state of the network which allows it to exhibit dynamic temporal behavior. Unlike feedforward neural networks, RNNs can use their internal memory to process arbitrary sequences of inputs. Early RNN models turned out to be very difficult to train, harder even than deep feedforward networks. The reason is the unstable gradient problem such as vanishing gradient and exploding gradient. The gradient can get smaller and smaller as it is propagated back through layers. This makes learning in early layers extremely slow. The problem actually gets worse in RNNs, since gradients aren’t just propagated backward through layers, they’re propagated backward through time. If the network runs for a long time, that can make the gradient extremely unstable and hard to learn from. It has been possible to incorporate an idea known as long short-term memory units (LSTMs) into RNNs. LSTMs make it much easier to get good results when training RNNs, and many recent papers make use of LSTMs or related ideas.Deep learning, Convolutional neural networks, Recurrent neural networks, Error backpropagation algorithm"
196,딥러닝분석과 기술적 분석 지표를 이용한 한국 코스피주가지수 방향성 예측,이우식 /Lee Woosik,2017,한국데이터정보과학회,"2016년 3월 구글 (Google)의 바둑인공지능 알파고 (AlphaGo)가 이세돌 9단과의 바둑대결에서승리한 이후 다양한 분야에서 인공지능 사용에 대한 관심이 높아지고 있는 가운데 금융투자 분야에서도 인공지능과 투자자문 전문가의 합성어인 로보어드바이저 (Robo-Advisor)에 대한 관심이 높아지고 있다. 인공지능 (artificial intelligence)기반의 의사결정은 비용 절감은 물론 효과적인 의사결정을가능하게 한다는 점에서 큰 장점이 있다. 본 연구에서는 기술적 분석 (technical analysis) 지표와 딥러닝 (deep learning) 모형을 결합하여 한국 코스피 지수를 예측하는 모형을 개발하고 제시한 모형들의 예측력을 비교, 분석한다. 분석 결과 기술적 분석 지표에 딥러닝 알고리즘을 결합한 모형이 주가지수 방향성 예측 문제에 응용될 수 있음을 확인하였다. 향후 본 연구에서 제안된 기술적 분석 지표와딥러닝모형을 결합한 기법은 로보어드바이저서비스에 응용할 수 있는 일반화 가능성을 보여준다Since Google’s AlphaGo defeated a world champion of Go players in 2016, there have been many interests in the deep learning. In the financial sector, a Robo-Advisor using deep learning gains a significant attention, which builds and manages portfolios of financial instruments for investors.In this paper, we have proposed the a deep learning algorithm geared toward identification and forecast of the KOSPI index direction,and we also have compared the accuracy of the prediction.In an application of forecasting the financial market index direction, we have shown that the Robo-Advisor using deep learning has a significant effect on finance industry. The Robo-Advisor collects a massive data such as earnings statements, news reports and regulatory filings, analyzes those and recommends investors how to view market trends and identify the best time to purchase financial assets. On the other hand, the Robo-Advisor allows businesses to learn more about their customers, develop better marketing strategies, increase sales and decrease costs.Artificial intelligence, deep learning, FinTech, Robo-Advisor, technical analysis."
197,딥러닝의 효율적인 학습을 위한 학습데이터 선별 기법에 관한 연구,주경돈 /Gyoung-don Joo,2017,한국정보과학회,"빅데이터의 증가와 병렬 하드웨어 처리, 학습 알고리즘의 개선은 딥러닝의 발전을 가져왔으며 인공지능이 적용 가능한 영역을 획기적으로 확장시키고 있다. 하지만 이러한 대규모의 학습 데이터에 기반한 딥러닝은 학습시간이 증가되고, 더 나아가 학습 데이터의 구축을 위한 레이블링(labeling) 비용이 데이터 수에 비례하여 증가하게 된다. 따라서 본 논문에서는, 딥러닝의 정확도를 최대한 유지하면서 필요한 학습 데이터의 수를 줄일수 있도록, 전체 데이터에서 제한된 수의 학습 데이터의 후보를 선별하는 기법에 대하여 연구한다. 특히 기존의 능동 학습에 기반한 기법들이 학습 데이터의 수가 매우 적어질 경우 딥러닝의 정확도가 급격히 떨어지는 문제를 해결할 수 있는 새로운 기법을 제안하였으며, 제안된 기법으로 선별된 학습 데이터는 그 수의 변화에 대해 딥러닝이 일관되게 경쟁력있는 정확도를 가짐을 다양한 실험을 통하여 검증하였다.The increase of big data, parallel hardware processing and enhanced learning algorithms have led to the development of deep learning and are dramatically expanding the domain of applications based on artificial intelligence. However, such large-scale training data extends the learning time, and furthermore, the labeling cost for building the training set increases in proportion to the volume of data. Therefore, in this paper, we study how to select a limited number of candidates for training set from given data so as to reduce the number of training set while maintaining the accuracy of deep learning. Especially, we found that, with the training set selected by the previous work, the accuracy of deep learning drastically decreases when the number of training set is very small and so proposed new methods to resolve this fall-off in accuracy. Through experiments, we showed that deep learning consistently has the competitive accuracy with the training set selected by the proposed method regardless of the size of training set.Deep Learning, Train Set Selection, Active Learning, Gamma distribution"
198,딥러닝을 이용한 범용적 스테그아날리시스,김현재 /Hyunjae Kim,2017,한국정보과학회,"스테그아날리시스(Steganalysis)란 이미지 등 일반적인 자료에 암호화된 정보를 은닉하는 스테가노그래피(Steganography)에 대한 검출 및 분석 방법으로, 기계학습 기반 방법론을 포함한다. 기존 기계학습 기반 스테그아날리시스는 영상(Image)의 특징(Feature) 추출 및 모델링에 기반하며, 최근 딥러닝(Deep Learning)의 적용으로 검출 정확도가 큰 폭으로 향상되었다. 하지만 현존하는 스테그아날리시스 모델은 단일 스테가노그래피 기법에 대해 국한되어 있어 학습에 사용되지 않은 스테고(Stego) 이미지의 경우 검출이 불가능한 결정적 한계를 가진다. 본 연구에서는 다양한 스테가노그래피 기법으로 생성된 스테고이미지에 딥러닝을 적용하여 스테그아날리시스를 학습하는 범용적 모델을 제안한다. 다양한 실험을 통해 제안 기법의 효용성 및 가능성을 확인하고, 범용적 스테그아날리시스 모델이 각각에 특화된 검출 기법과 유사한 정확도로 스테고 이미지를 검출할 수 있음을 보인다.Steganalysis is to detect information hidden by steganography inside general data such as images. There are stegoanalysis techniques that use machine learning (ML). Existing ML approaches to steganalysis are based on extracting features from stego images and modeling them.
Recently deep learning-based methodologies have shown significant improvements in detection accuracy. However, all the existing methods, including deep learning-based ones, have a critical limitation in that they can only detect stego images that are created by a specific steganography method. In this paper, we propose a generalized steganalysis method that can model multiple types of stego images using deep learning. Through various experiments, we confirm the effectiveness of our approach and envision directions for future research. In particular, we show that our method can detect each type of steganography with the same level of accuracy as that of a steganalysis method dedicated to that type of steganography, thereby demonstrating the general applicability of our approach to multiple types of stego images.steganalysis, steganography, deep learning, generalized steganalysis model"
199,"딥러닝 프레임워크의 비교: 티아노, 텐서플로, CNTK를 중심으로",정여진 /Yeojin Chung,2017,한국지능정보시스템학회,"딥러닝 프레임워크의 대표적인 기능으로는 ‘자동미분’과 ‘GPU의 활용’ 등을 들 수 있다. 본 논문은 파이썬의 라이브러리 형태로 사용 가능한 프레임워크 중에서 구글의 텐서플로와 마이크로소프트의 CNTK, 그리고 텐서 플로의 원조라고 할 수 있는 티아노를 비교하였다. 본문에서는 자동미분의 개념과 GPU의 활용형태를 간단히 설명하고, 그 다음에 logistic regression을 실행하는 예를 통하여 각 프레임워크의 문법을 알아본 뒤에, 마지막으로 대표적인 딥러닝 응용인 CNN의 예제를 실행시켜보고 코딩의 편의성과 실행속도 등을 확인해 보았다. 그 결과, 편의성의 관점에서 보면 티아노가 가장 코딩 하기가 어렵고, CNTK와 텐서플로는 많은 부분이 비슷하게 추상화 되어 있어서 코딩이 비슷하지만 가중치와 편향을 직접 정의하느냐의 여부에서 차이를 보였다. 그리고 각프레임워크의 실행속도에 대한 평가는 ‘큰 차이는 없다’는 것이다. 텐서플로는 티아노에 비하여 속도가 느리다는 평가가 있어왔는데, 본 연구의 실험에 의하면, 비록 CNN 모형에 국한되었지만, 텐서플로가 아주 조금이지만빠른 것으로 나타났다. CNTK의 경우에도, 비록 실험환경이 달랐지만, 실험환경의 차이에 의한 속도의 차이의 편차범위 이내에 있는 것으로 판단이 되었다. 본 연구에서는 세 종류의 딥러닝 프레임워크만을 살펴보았는데,위키피디아에 따르면 딥러닝 프레임워크의 종류는 12가지가 있으며, 각 프레임워크의 특징을 15가지 속성으로구분하여 차이를 특정하고 있다. 그 많은 속성 중에서 사용자의 입장에서 볼 때 중요한 속성은 어떤 언어(파이썬, C++, Java, 등)로 사용가능한지, 어떤 딥러닝 모형에 대한 라이브러리가 잘 구현되어 있는지 등일 것이다. 그리고 사용자가 대규모의 딥러닝 모형을 구축한다면, 다중 GPU 혹은 다중 서버를 지원하는지의 여부도 중요할 것이다. 또한 딥러닝 모형을 처음 학습하는 경우에는 사용설명서가 많은지 예제 프로그램이 많은지 여부도 중요한 기준이 될 것이다.The deep learning framework is software designed to help develop deep learning models. Some of its important functions include “automatic differentiation” and “utilization of GPU”. The list of popular deep learning framework includes Caffe (BVLC) and Theano (University of Montreal). And recently, Microsoft's deep learning framework, Microsoft Cognitive Toolkit, was released as open-source license, following Google’s Tensorflow a year earlier. The early deep learning frameworks have been developed mainly for research at universities. Beginning with the inception of Tensorflow, however, it seems that companies such as Microsoft and Facebook have started to join the competition of framework development. Given the trend, Google and other companies are expected to continue investing in the deep learning framework to bring forward the initiative in the artificial intelligence business. From this point of view, we think it is a good time to compare some of deep learning frameworks. So we compare three deep learning frameworks which can be used as a Python library. Those are Google's Tensorflow, Microsoft’s CNTK, and Theano which is sort of a predecessor of the preceding two.
The most common and important function of deep learning frameworks is the ability to perform automatic differentiation. Basically all the mathematical expressions of deep learning models can be represented as computational graphs, which consist of nodes and edges. Partial derivatives on each edge of a computational graph can then be obtained. With the partial derivatives, we can let software compute differentiation of any node with respect to any variable by utilizing chain rule of Calculus.
First of all, the convenience of coding is in the order of CNTK, Tensorflow, and Theano. The criterion is simply based on the lengths of the codes and the learning curve and the ease of coding are not the main concern. According to the criteria, Theano was the most difficult to implement with, and CNTK and Tensorflow were somewhat easier. With Tensorflow, we need to define weight variables and biases explicitly. The reason that CNTK and Tensorflow are easier to implement with is that those frameworks provide us with more abstraction than Theano. We, however, need to mention that low-level coding is not always bad. It gives us flexibility of coding. With the low-level coding such as in Theano, we can implement and test any new deep learning models or any new search methods that we can think of.
The assessment of the execution speed of each framework is that there is not meaningful difference. According to the experiment, execution speeds of Theano and Tensorflow are very similar, although the experiment was limited to a CNN model. In the case of CNTK, the experimental environment was not maintained as the same. The code written in CNTK has to be run in PC environment without GPU where codes execute as much as 50 times slower than with GPU. But we concluded that the difference of execution speed was within the range of variation caused by the different hardware setup.
In this study, we compared three types of deep learning framework: Theano, Tensorflow, and CNTK. According to Wikipedia, there are 12 available deep learning frameworks. And 15 different attributes differentiate each framework. Some of the important attributes would include interface language (Python, C ++, Java, etc.) and the availability of libraries on various deep learning models such as CNN, RNN, DBN, and etc. And if a user implements a large scale deep learning model, it will also be important to support multiple GPU or multiple servers. Also, if you are learning the deep learning model, it would also be important if there are enough examples and references.deep learning framework, Theano, TensorFlow, CNTK, computational graph, CIFAR-10"
200,딥러닝을 이용한 주변 무선단말 파악방안,이웅섭 /Lee， Woong-sup,2018,한국정보통신학회,"최근 단말-대-단말(Device-to-device, D2D) 통신기술이 차세대 무선통신시스템의 핵심기술로 큰 관심을 받고 있다. 이러한 단말간 통신에서는 자신의 주변에 어떠한 단말이 있는지 파악하는 주변단말 탐색(Neighbor discovery)이매우 중요하다. 본 논문에서는 최근 큰 관심을 받고 있는 딥러닝(Deep learning) 기술을 활용하여 단말간 통신에서 주변단말을 파악하는 방안에 대해서 제안한다. 제안 방안은 기존의 방안과 달리 무선채널의 공간적 연관성을 이용하여단말간의 신호 전송 없이 단말이 기지국으로 전송하는 상향링크 파일럿 신호를 기반으로 주변 단말을 찾고 따라서기존의 방식에 비해 신호전송 복잡도(signaling complexity)를 크게 줄일 수 있다. 또한 제안 방안에서는 떨어져 있는거리에 따라서 주변 단말을 분류 가능하여 기존 방안에 비해서 좀 더 세밀한 단말 탐색이 가능하다. 마지막으로 본 논문에서는 tensorflow를 이용한 컴퓨터 시뮬레이션을 통해 제안 방안의 성능을 검증하였다.Recently, the device-to-device (D2D) communication has been conceived as the key technology for the next-generationmobile communication systems. The neighbor discovery in which the nearby users are found, is essential for the properoperation of the D2D communication. In this paper, we propose new neighbor discovery scheme based on deep learningtechnology which has gained a lot of attention recently. In the proposed scheme, the neighboring users can be foundusing the uplink pilot transmission of users only, unlike conventional neighbor discovery schemes in which direct pilotcommunication among users is required, such that the signaling overhead can be greatly reduced in our proposed scheme.
Moreover, the neighbors with different proximity can also be classified accordingly which enables more accurate neighbordiscovery compared to the conventional schemes. The performance of our proposed scheme is verified through thetensorflow-based computer simulations.
키워드 : 딥러닝, 주변단말 탐색, 단말-대-단말 통신Deep learning, Neighbor discovery, Device-to-device communication, Wireless channel correlation"
201,신뢰성있는 딥러닝 기반 분석 모델을 참조하기 위한 딥러닝 기술 언어,문종혁 /Jong Hyeok Mun,2021,한국정보처리학회,"최근 딥러닝은 하드웨어 성능이 향상됨에 따라 자연어 처리, 영상 인식 등의 다양한 기술에 접목되어 활용되고 있다. 이러한 기술들을 활용해 지능형교통 시스템(ITS), 스마트홈, 헬스케어 등의 산업분야에서 데이터를 분석하여 고속도로 속도위반 차량 검출, 에너지 사용량 제어, 응급상황 등과 같은고품질의 서비스를 제공하며, 고품질의 서비스를 제공하기 위해서는 정확도가 향상된 딥러닝 모델이 적용되어야 한다. 이를 위해 서비스 환경의 데이터를분석하기 위한 딥러닝 모델을 개발할 때, 개발자는 신뢰성이 검증된 최신의 딥러닝 모델을 적용할 수 있어야 한다. 이는 개발자가 참조하는 딥러닝 모델에적용된 학습 데이터셋의 정확도를 측정하여 검증할 수 있다. 이러한 검증을 위해서 개발자는 학습 데이터셋, 딥러닝의 계층구조 및 개발 환경 등과 같은내용을 포함하는 딥러닝 모델을 문서화하여 적용하기 위한 구조적인 정보가 필요하다. 본 논문에서는 신뢰성있는 딥러닝 기반 데이터 분석 모델을 참조하기위한 딥러닝 기술 언어를 제안한다. 제안하는 기술 언어는 신뢰성 있는 딥러닝 모델을 개발하는데 필요한 학습데이터셋, 개발 환경 및 설정 등의 정보와더불어 딥러닝 모델의 계층구조를 표현할 수 있다. 제안하는 딥러닝 기술 언어를 이용하여 개발자는 지능형 교통 시스템에서 참조하는 분석 모델의정확도를 검증할 수 있다. 실험에서는 제안하는 언어의 유효성을 검증하기 위해, 번호판 인식 모델을 중심으로 딥러닝 기술 문서의 적용과정을 보인다With the recent advancements of deep learning, companies such as smart home, healthcare, and intelligent transportation systems areutilizing its functionality to provide high-quality services for vehicle detection, emergency situation detection, and controlling energyconsumption. To provide reliable services in such sensitive systems, deep learning models are required to have high accuracy. In orderto develop a deep learning model for analyzing previously mentioned services, developers should utilize the state of the art deep learningmodels that have already been verified for higher accuracy. The developers can verify the accuracy of the referenced model by validatingthe model on the dataset. For this validation, the developer needs structural information to document and apply deep learning models,including metadata such as learning dataset, network architecture, and development environments. In this paper, we propose a descriptionlanguage that represents the network architecture of the deep learning model along with its metadata that are necessary to develop adeep learning model. Through the proposed description language, developers can easily verify the accuracy of the referenced deep learningmodel. Our experiments demonstrate the application scenario of a deep learning description document that focuses on the license platerecognition for the detection of illegally parked vehicles.Trusted Deep Learning, Model Reference, Deep Learning Description Language, Traffic Situation Analysis Model"
202,딥러닝 분석을 이용한 중국 역내·외 위안화 변동성 예측,이우식 /Lee Woosik,2016,한국데이터정보과학회,"2008년 글로벌 금융위기 이후 중국은 위안화 국제화의 점진적 추진을 진행하면서 중국상하이 외환시장과 중국홍콩 외환시장에서 거래되는 통화인 역내위안화와 역외위안화를 형성시켰다. 본 연구는 위안화 국제화와 점진적인 중국 자본계정 개방에 따라 급변하는 외환시장상황의 변동성을 정확하게 파악하기 위해서 GARCH모형 (일반화된 자기회귀 조건부이분산성모형)에 다단계인공신경망을 결합한 MLP-GARCH 모형과 GARCH모형과 기계학습의 일종인 딥러닝 (deep learning)을 통합한 DL-GARCH을 가지고 위안화 변동성예측을 비교 실험과 분석을 하였다. 비교분석 결과 DLGARCH 모형은 MLP-GARCH보다 모형 위안화 역내·외 환율변동성 예측 면에서 더욱 더 개선된예측값을 제공하였다. 그래서 이분산시계열모형을 딥러닝과 결합한 DL-GARCH 모형은 시계열의환율 변동성 예측 문제에 딥러닝을 응용할 수 있음을 확인하였다. 향후 이분산시계열과 결합된 딥러닝 모형은 다른 금융시계열 데이터에 응용하여 그 일반화 가능성을 높일 수 있을 것이다.The People’s Republic of China has vigorously been pursuing the internationalization of the Chinese Yuan or Renminbi after the financial crisis of 2008. In this view, an abrupt increase of use of the Chinese Yuan in the onshore and offshore markets are important milestones to be one of important currencies. One of the most frequently used methods to forecast volatility is GARCH model. Since a prediction error of the GARCH model has been reported quite high, a lot of efforts have been made to improve forecasting capability of the GARCH model. In this paper, we have proposed MLPGARCH and a DL-GARCH by employing Artificial Neural Network to the GARCH. In an application to forecasting Chinese Yuan volatility, we have successfully shown their overall outperformance in forecasting over the GARCH.Deep learning, DL-GARCH, GARCH, The Chinese Yuan, volatility."
203,딥러닝 기법을 이용한 내일강수 예측,하지훈,2016,한국지능시스템학회,"정확한 강수예측을 위해서는 예측인자 선정과 예측방법에 대한 선택이 매우 중요하다. 최근에는 강수예측 방법으로기계학습 기법이 많이 사용되고 있으며, 그 중에서도 특히 인공신경망을 사용한 강수예측 방법은 좋은 성능을 보였다. 본논문에서는 딥러닝 기법 중 하나인 DBN(deep belief network)를 이용한 새로운 강수예측 방법을 제안한다. DBN는 비지도사전 학습을 통해 초기 가중치를 설정하여 기존 인공신경망의 문제점을 보완한다. 예측인자로는 기온, 전일-전주 강수일, 태양과 달 궤도 관련 자료를 선정하였다. 기온과 전일-전주 강수일은 서울에서의 1974년부터 2013년까지 총 40년간의AWS(automatic weather system) 관측 자료를 사용하였고, 태양과 달의 궤도 관련 자료는 서울을 중심으로 계산한 결과를사용하였다. 전체 기간에서 일부는 학습 자료로 사용하여 예측모델을 생성하였고, 나머지를 생성한 모델의 검증 자료로사용하였다. 모델 검증 결과로 나온 예측값들은 확률값을 가지며 임계치를 이용하여 강수유무를 판별하였다. 강수 정확도의척도로 양분예보기법 중 CSI(critical successive index)와 Bias(frequency bias)를 계산하였다. 이를 통해 DBN와 MLP(multilayer perceptron)의 성능을 비교한 결과 DBN의 강수 예측 정확도가 높았고, 수행속도 또한 2배 이상 빨랐다.For accurate precipitation forecasts the choice of weather faoctrs and prediction method is very important. Recently, machinele arning has been widely used for forecasting precipitation, and artificial neural network, one of machine learning techniques, showed good performance. In this paper, we suggest a new method for forecasting precipitation using DBN, one of deep learning techniques.
DBN has an advantage that initial weights are set by unsupervised learning, so this compensates for the defects of artificial neural networks. We used past precipitation, temperature, and the parameters of the sun and moon's motion as features for forecasting precipitation. The dataset consists of observation data which had been measured for 40 years from AWS in Seoul. Experiments were based on 8-fold cross validation. As a result of estimation, we got probabilities of test dataset, so threshold was used for the decision of precipitation. CSI and Bias were used for indicating the precision of precipitation. Our experimental results showed that DBN performed better than MLP.Deep learning, Deep belief network, Precipitation, Forecast"
204,딥러닝 신경망모형을 이용한 실시간 교통정보수집 연구,김대현 /KIM DAE HYON,2016,사단법인 인문사회과학기술융합학회,"비디오 기반의 실시간 교통정보수집 시스템은 자율주행을 포함한 지능형교통체계의 주요 서브시스템중 하나이며, 우수한 예측(추정)력 때문에 핵심 알고리즘으로 인공신경망 모형이 주로 사용되고 있다. 특히, 최근 과학 학술지 네이처에 구글 딥마인드의 알파고가 공개되면서 딥러닝과 인공지능에 대한 전 세계적인 관심이 집중되고 있다. 최근까지 가장 널리 사용된 인공신경망모형은 역전파 모형이었으나 최근 컴퓨터 하드웨어 및 알고리즘의 새로운 개발과 더불어 딥러닝 알고리즘을 이용한 딥인공신경망 모형에 대한 관심이 높아지고 있다. 그러나 아직 딥러닝 인공신경망에 대한 연구는 초기단계에 있는 실정이며 적용분야도 극히 한정되어 있다. 본 연구에서는 실시간 도로교통정보 수집을 위한 영상처리분야에서 딥러닝 알고리즘을 활용하고 실험결과를 통하여 향후 교통정보수집‧분석 분야에서의 딥러닝 인공신경망 모형의 적용가능성을 살펴보고자 한다. 특히, 본 연구에서 도출한 딥러닝 인공신경망모형의 실험결과를 기존 역전파 모형에 의한 실험결과와 비교하여 모형의 우수성을 비교‧평가하고자 한다.Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Normally, artificial neural networks have been used in real-time applications for collecting traffic information because of their remarkable performance in terms of predictive accuracy and real-time computing. Especially, the recent publication of a paper in the journal Nature and the success of Google DeepMind’s world-beating Go program AlphaGo caused significant increases of the public interest in artificial intelligence. Even though various types of learning models have been proposed, the Backpropagation has been the most popular neural network model. However, the Deep learning neural network model has attracted much interest recently. In this study, the Deep leaning neural network will be applied in image processing techniques for the real-time traffic information collection to investigate the performance of Deep learning algorithm in traffic engineering fields. In addition, the experimental results from Deep learing algorithm will be compared with the results from Backpropagation model to evaluate and propose better performing model.Intelligent Transportation Systems (ITS), artificial neural networks, Deep learning, traffic information, image processing"
205,딥러닝의 모형과 응용사례,안성만 /SungMahn Ahn,2016,한국지능정보시스템학회,"딥러닝은 인공신경망(neural network)이라는 인공지능분야의 모형이 발전된 형태로서, 계층구조로 이루어진 인공신경망의 내부계층(hidden layer)이 여러 단계로 이루어진 구조이다. 딥러닝에서의 주요 모형은 합성곱신경망(convolutional neural network), 순환신경망(recurrent neural network), 그리고 심층신뢰신경망(deep belief network)의 세가지라고 할 수있다. 그 중에서 현재 흥미로운 연구가 많이 발표되어서 관심이 집중되고 있는 모형은 지도학습(supervised learning)모형인 처음 두 개의 모형이다. 따라서 본 논문에서는 지도학습모형의 가중치를 최적화하는 기본적인 방법인 오류역전파 알고리즘을 살펴본 뒤에 합성곱신경망과 순환신경망의 구조와 응용사례 등을 살펴보고자 한다. 본문에서 다루지 않은 모형인 심층신뢰신경망은 아직까지는 합성곱신경망 이나 순환신경망보다는 상대적으로 주목을 덜 받고 있다. 그러나 심층신뢰신경망은 CNN이나 RNN과는 달리 비지도학습(unsupervised learning)모형이며, 사람이나 동물은 관찰을 통해서 스스로 학습한다는 점에서 궁극적으로는 비지도학습모형이 더 많이 연구되어야 할 주제가 될 것이다.Deep learning model is a kind of neural networks that allows multiple hidden layers. There are various deep learning architectures such as convolutional neural networks, deep belief networks and recurrent neural networks. Those have been applied to fields like computer vision, automatic speech recognition, natural language processing, audio recognition and bioinformatics where they have been shown to produce state-of-the-art results on various tasks. Among those architectures, convolutional neural networks and recurrent neural networks are classified as the supervised learning model. And in recent years, those supervised learning models have gained more popularity than unsupervised learning models such as deep belief networks, because supervised learning models have shown fashionable applications in such fields mentioned above.
Deep learning models can be trained with backpropagation algorithm. Backpropagation is an abbreviation for “backward propagation of errors” and a common method of training artificial neural networks used in conjunction with an optimization method such as gradient descent. The method calculates the gradient of an error function with respect to all the weights in the network. The gradient is fed to the optimization method which in turn uses it to update the weights, in an attempt to minimize the error function.
Convolutional neural networks use a special architecture which is particularly well-adapted to classify images. Using this architecture makes convolutional networks fast to train. This, in turn, helps us train deep, muti-layer networks, which are very good at classifying images. These days, deep convolutional networks are used in most neural networks for image recognition. Convolutional neural networks use three basic ideas: local receptive fields, shared weights, and pooling. By local receptive fields, we mean that each neuron in the first(or any) hidden layer will be connected to a small region of the input(or previous layer’s) neurons. Shared weights mean that we’re going to use the same weights and bias for each of the local receptive field. This means that all the neurons in the hidden layer detect exactly the same feature, just at different locations in the input image. In addition to the convolutional layers just described, convolutional neural networks also contain pooling layers. Pooling layers are usually used immediately after convolutional layers. What the pooling layers do is to simplify the information in the output from the convolutional layer.
Recent convolutional network architectures have 10 to 20 hidden layers and billions of connections between units. Training deep learning networks has taken weeks several years ago, but thanks to progress in GPU and algorithm enhancement, training time has reduced to several hours.
Neural networks with time-varying behavior are known as recurrent neural networks or RNNs. A recurrent neural network is a class of artificial neural network where connections between units form a directed cycle. This creates an internal state of the network which allows it to exhibit dynamic temporal behavior. Unlike feedforward neural networks, RNNs can use their internal memory to process arbitrary sequences of inputs. Early RNN models turned out to be very difficult to train, harder even than deep feedforward networks. The reason is the unstable gradient problem such as vanishing gradient and exploding gradient. The gradient can get smaller and smaller as it is propagated back through layers. This makes learning in early layers extremely slow. The problem actually gets worse in RNNs, since gradients aren’t just propagated backward through layers, they’re propagated backward through time. If the network runs for a long time, that can make the gradient extremely unstable and hard to learn from. It has been possible to incorporate an idea known as long short-term memory units (LSTMs) into RNNs. LSTMs make it much easier to get good results when training RNNs, and many recent papers make use of LSTMs or related ideas.Deep learning, Convolutional neural networks, Recurrent neural networks, Error backpropagation algorithm"
206,딥러닝분석과 기술적 분석 지표를 이용한 한국 코스피주가지수 방향성 예측,이우식 /Lee Woosik,2017,한국데이터정보과학회,"2016년 3월 구글 (Google)의 바둑인공지능 알파고 (AlphaGo)가 이세돌 9단과의 바둑대결에서승리한 이후 다양한 분야에서 인공지능 사용에 대한 관심이 높아지고 있는 가운데 금융투자 분야에서도 인공지능과 투자자문 전문가의 합성어인 로보어드바이저 (Robo-Advisor)에 대한 관심이 높아지고 있다. 인공지능 (artificial intelligence)기반의 의사결정은 비용 절감은 물론 효과적인 의사결정을가능하게 한다는 점에서 큰 장점이 있다. 본 연구에서는 기술적 분석 (technical analysis) 지표와 딥러닝 (deep learning) 모형을 결합하여 한국 코스피 지수를 예측하는 모형을 개발하고 제시한 모형들의 예측력을 비교, 분석한다. 분석 결과 기술적 분석 지표에 딥러닝 알고리즘을 결합한 모형이 주가지수 방향성 예측 문제에 응용될 수 있음을 확인하였다. 향후 본 연구에서 제안된 기술적 분석 지표와딥러닝모형을 결합한 기법은 로보어드바이저서비스에 응용할 수 있는 일반화 가능성을 보여준다Since Google’s AlphaGo defeated a world champion of Go players in 2016, there have been many interests in the deep learning. In the financial sector, a Robo-Advisor using deep learning gains a significant attention, which builds and manages portfolios of financial instruments for investors.In this paper, we have proposed the a deep learning algorithm geared toward identification and forecast of the KOSPI index direction,and we also have compared the accuracy of the prediction.In an application of forecasting the financial market index direction, we have shown that the Robo-Advisor using deep learning has a significant effect on finance industry. The Robo-Advisor collects a massive data such as earnings statements, news reports and regulatory filings, analyzes those and recommends investors how to view market trends and identify the best time to purchase financial assets. On the other hand, the Robo-Advisor allows businesses to learn more about their customers, develop better marketing strategies, increase sales and decrease costs.Artificial intelligence, deep learning, FinTech, Robo-Advisor, technical analysis."
207,딥러닝의 효율적인 학습을 위한 학습데이터 선별 기법에 관한 연구,주경돈 /Gyoung-don Joo,2017,한국정보과학회,"빅데이터의 증가와 병렬 하드웨어 처리, 학습 알고리즘의 개선은 딥러닝의 발전을 가져왔으며 인공지능이 적용 가능한 영역을 획기적으로 확장시키고 있다. 하지만 이러한 대규모의 학습 데이터에 기반한 딥러닝은 학습시간이 증가되고, 더 나아가 학습 데이터의 구축을 위한 레이블링(labeling) 비용이 데이터 수에 비례하여 증가하게 된다. 따라서 본 논문에서는, 딥러닝의 정확도를 최대한 유지하면서 필요한 학습 데이터의 수를 줄일수 있도록, 전체 데이터에서 제한된 수의 학습 데이터의 후보를 선별하는 기법에 대하여 연구한다. 특히 기존의 능동 학습에 기반한 기법들이 학습 데이터의 수가 매우 적어질 경우 딥러닝의 정확도가 급격히 떨어지는 문제를 해결할 수 있는 새로운 기법을 제안하였으며, 제안된 기법으로 선별된 학습 데이터는 그 수의 변화에 대해 딥러닝이 일관되게 경쟁력있는 정확도를 가짐을 다양한 실험을 통하여 검증하였다.The increase of big data, parallel hardware processing and enhanced learning algorithms have led to the development of deep learning and are dramatically expanding the domain of applications based on artificial intelligence. However, such large-scale training data extends the learning time, and furthermore, the labeling cost for building the training set increases in proportion to the volume of data. Therefore, in this paper, we study how to select a limited number of candidates for training set from given data so as to reduce the number of training set while maintaining the accuracy of deep learning. Especially, we found that, with the training set selected by the previous work, the accuracy of deep learning drastically decreases when the number of training set is very small and so proposed new methods to resolve this fall-off in accuracy. Through experiments, we showed that deep learning consistently has the competitive accuracy with the training set selected by the proposed method regardless of the size of training set.Deep Learning, Train Set Selection, Active Learning, Gamma distribution"
208,딥러닝을 이용한 범용적 스테그아날리시스,김현재 /Hyunjae Kim,2017,한국정보과학회,"스테그아날리시스(Steganalysis)란 이미지 등 일반적인 자료에 암호화된 정보를 은닉하는 스테가노그래피(Steganography)에 대한 검출 및 분석 방법으로, 기계학습 기반 방법론을 포함한다. 기존 기계학습 기반 스테그아날리시스는 영상(Image)의 특징(Feature) 추출 및 모델링에 기반하며, 최근 딥러닝(Deep Learning)의 적용으로 검출 정확도가 큰 폭으로 향상되었다. 하지만 현존하는 스테그아날리시스 모델은 단일 스테가노그래피 기법에 대해 국한되어 있어 학습에 사용되지 않은 스테고(Stego) 이미지의 경우 검출이 불가능한 결정적 한계를 가진다. 본 연구에서는 다양한 스테가노그래피 기법으로 생성된 스테고이미지에 딥러닝을 적용하여 스테그아날리시스를 학습하는 범용적 모델을 제안한다. 다양한 실험을 통해 제안 기법의 효용성 및 가능성을 확인하고, 범용적 스테그아날리시스 모델이 각각에 특화된 검출 기법과 유사한 정확도로 스테고 이미지를 검출할 수 있음을 보인다.Steganalysis is to detect information hidden by steganography inside general data such as images. There are stegoanalysis techniques that use machine learning (ML). Existing ML approaches to steganalysis are based on extracting features from stego images and modeling them.
Recently deep learning-based methodologies have shown significant improvements in detection accuracy. However, all the existing methods, including deep learning-based ones, have a critical limitation in that they can only detect stego images that are created by a specific steganography method. In this paper, we propose a generalized steganalysis method that can model multiple types of stego images using deep learning. Through various experiments, we confirm the effectiveness of our approach and envision directions for future research. In particular, we show that our method can detect each type of steganography with the same level of accuracy as that of a steganalysis method dedicated to that type of steganography, thereby demonstrating the general applicability of our approach to multiple types of stego images.steganalysis, steganography, deep learning, generalized steganalysis model"
209,"딥러닝 프레임워크의 비교: 티아노, 텐서플로, CNTK를 중심으로",정여진 /Yeojin Chung,2017,한국지능정보시스템학회,"딥러닝 프레임워크의 대표적인 기능으로는 ‘자동미분’과 ‘GPU의 활용’ 등을 들 수 있다. 본 논문은 파이썬의 라이브러리 형태로 사용 가능한 프레임워크 중에서 구글의 텐서플로와 마이크로소프트의 CNTK, 그리고 텐서 플로의 원조라고 할 수 있는 티아노를 비교하였다. 본문에서는 자동미분의 개념과 GPU의 활용형태를 간단히 설명하고, 그 다음에 logistic regression을 실행하는 예를 통하여 각 프레임워크의 문법을 알아본 뒤에, 마지막으로 대표적인 딥러닝 응용인 CNN의 예제를 실행시켜보고 코딩의 편의성과 실행속도 등을 확인해 보았다. 그 결과, 편의성의 관점에서 보면 티아노가 가장 코딩 하기가 어렵고, CNTK와 텐서플로는 많은 부분이 비슷하게 추상화 되어 있어서 코딩이 비슷하지만 가중치와 편향을 직접 정의하느냐의 여부에서 차이를 보였다. 그리고 각프레임워크의 실행속도에 대한 평가는 ‘큰 차이는 없다’는 것이다. 텐서플로는 티아노에 비하여 속도가 느리다는 평가가 있어왔는데, 본 연구의 실험에 의하면, 비록 CNN 모형에 국한되었지만, 텐서플로가 아주 조금이지만빠른 것으로 나타났다. CNTK의 경우에도, 비록 실험환경이 달랐지만, 실험환경의 차이에 의한 속도의 차이의 편차범위 이내에 있는 것으로 판단이 되었다. 본 연구에서는 세 종류의 딥러닝 프레임워크만을 살펴보았는데,위키피디아에 따르면 딥러닝 프레임워크의 종류는 12가지가 있으며, 각 프레임워크의 특징을 15가지 속성으로구분하여 차이를 특정하고 있다. 그 많은 속성 중에서 사용자의 입장에서 볼 때 중요한 속성은 어떤 언어(파이썬, C++, Java, 등)로 사용가능한지, 어떤 딥러닝 모형에 대한 라이브러리가 잘 구현되어 있는지 등일 것이다. 그리고 사용자가 대규모의 딥러닝 모형을 구축한다면, 다중 GPU 혹은 다중 서버를 지원하는지의 여부도 중요할 것이다. 또한 딥러닝 모형을 처음 학습하는 경우에는 사용설명서가 많은지 예제 프로그램이 많은지 여부도 중요한 기준이 될 것이다.The deep learning framework is software designed to help develop deep learning models. Some of its important functions include “automatic differentiation” and “utilization of GPU”. The list of popular deep learning framework includes Caffe (BVLC) and Theano (University of Montreal). And recently, Microsoft's deep learning framework, Microsoft Cognitive Toolkit, was released as open-source license, following Google’s Tensorflow a year earlier. The early deep learning frameworks have been developed mainly for research at universities. Beginning with the inception of Tensorflow, however, it seems that companies such as Microsoft and Facebook have started to join the competition of framework development. Given the trend, Google and other companies are expected to continue investing in the deep learning framework to bring forward the initiative in the artificial intelligence business. From this point of view, we think it is a good time to compare some of deep learning frameworks. So we compare three deep learning frameworks which can be used as a Python library. Those are Google's Tensorflow, Microsoft’s CNTK, and Theano which is sort of a predecessor of the preceding two.
The most common and important function of deep learning frameworks is the ability to perform automatic differentiation. Basically all the mathematical expressions of deep learning models can be represented as computational graphs, which consist of nodes and edges. Partial derivatives on each edge of a computational graph can then be obtained. With the partial derivatives, we can let software compute differentiation of any node with respect to any variable by utilizing chain rule of Calculus.
First of all, the convenience of coding is in the order of CNTK, Tensorflow, and Theano. The criterion is simply based on the lengths of the codes and the learning curve and the ease of coding are not the main concern. According to the criteria, Theano was the most difficult to implement with, and CNTK and Tensorflow were somewhat easier. With Tensorflow, we need to define weight variables and biases explicitly. The reason that CNTK and Tensorflow are easier to implement with is that those frameworks provide us with more abstraction than Theano. We, however, need to mention that low-level coding is not always bad. It gives us flexibility of coding. With the low-level coding such as in Theano, we can implement and test any new deep learning models or any new search methods that we can think of.
The assessment of the execution speed of each framework is that there is not meaningful difference. According to the experiment, execution speeds of Theano and Tensorflow are very similar, although the experiment was limited to a CNN model. In the case of CNTK, the experimental environment was not maintained as the same. The code written in CNTK has to be run in PC environment without GPU where codes execute as much as 50 times slower than with GPU. But we concluded that the difference of execution speed was within the range of variation caused by the different hardware setup.
In this study, we compared three types of deep learning framework: Theano, Tensorflow, and CNTK. According to Wikipedia, there are 12 available deep learning frameworks. And 15 different attributes differentiate each framework. Some of the important attributes would include interface language (Python, C ++, Java, etc.) and the availability of libraries on various deep learning models such as CNN, RNN, DBN, and etc. And if a user implements a large scale deep learning model, it will also be important to support multiple GPU or multiple servers. Also, if you are learning the deep learning model, it would also be important if there are enough examples and references.deep learning framework, Theano, TensorFlow, CNTK, computational graph, CIFAR-10"
210,딥러닝을 이용한 주변 무선단말 파악방안,이웅섭 /Lee， Woong-sup,2018,한국정보통신학회,"최근 단말-대-단말(Device-to-device, D2D) 통신기술이 차세대 무선통신시스템의 핵심기술로 큰 관심을 받고 있다. 이러한 단말간 통신에서는 자신의 주변에 어떠한 단말이 있는지 파악하는 주변단말 탐색(Neighbor discovery)이매우 중요하다. 본 논문에서는 최근 큰 관심을 받고 있는 딥러닝(Deep learning) 기술을 활용하여 단말간 통신에서 주변단말을 파악하는 방안에 대해서 제안한다. 제안 방안은 기존의 방안과 달리 무선채널의 공간적 연관성을 이용하여단말간의 신호 전송 없이 단말이 기지국으로 전송하는 상향링크 파일럿 신호를 기반으로 주변 단말을 찾고 따라서기존의 방식에 비해 신호전송 복잡도(signaling complexity)를 크게 줄일 수 있다. 또한 제안 방안에서는 떨어져 있는거리에 따라서 주변 단말을 분류 가능하여 기존 방안에 비해서 좀 더 세밀한 단말 탐색이 가능하다. 마지막으로 본 논문에서는 tensorflow를 이용한 컴퓨터 시뮬레이션을 통해 제안 방안의 성능을 검증하였다.Recently, the device-to-device (D2D) communication has been conceived as the key technology for the next-generationmobile communication systems. The neighbor discovery in which the nearby users are found, is essential for the properoperation of the D2D communication. In this paper, we propose new neighbor discovery scheme based on deep learningtechnology which has gained a lot of attention recently. In the proposed scheme, the neighboring users can be foundusing the uplink pilot transmission of users only, unlike conventional neighbor discovery schemes in which direct pilotcommunication among users is required, such that the signaling overhead can be greatly reduced in our proposed scheme.
Moreover, the neighbors with different proximity can also be classified accordingly which enables more accurate neighbordiscovery compared to the conventional schemes. The performance of our proposed scheme is verified through thetensorflow-based computer simulations.
키워드 : 딥러닝, 주변단말 탐색, 단말-대-단말 통신Deep learning, Neighbor discovery, Device-to-device communication, Wireless channel correlation"
211,신뢰성있는 딥러닝 기반 분석 모델을 참조하기 위한 딥러닝 기술 언어,문종혁 /Jong Hyeok Mun,2021,한국정보처리학회,"최근 딥러닝은 하드웨어 성능이 향상됨에 따라 자연어 처리, 영상 인식 등의 다양한 기술에 접목되어 활용되고 있다. 이러한 기술들을 활용해 지능형교통 시스템(ITS), 스마트홈, 헬스케어 등의 산업분야에서 데이터를 분석하여 고속도로 속도위반 차량 검출, 에너지 사용량 제어, 응급상황 등과 같은고품질의 서비스를 제공하며, 고품질의 서비스를 제공하기 위해서는 정확도가 향상된 딥러닝 모델이 적용되어야 한다. 이를 위해 서비스 환경의 데이터를분석하기 위한 딥러닝 모델을 개발할 때, 개발자는 신뢰성이 검증된 최신의 딥러닝 모델을 적용할 수 있어야 한다. 이는 개발자가 참조하는 딥러닝 모델에적용된 학습 데이터셋의 정확도를 측정하여 검증할 수 있다. 이러한 검증을 위해서 개발자는 학습 데이터셋, 딥러닝의 계층구조 및 개발 환경 등과 같은내용을 포함하는 딥러닝 모델을 문서화하여 적용하기 위한 구조적인 정보가 필요하다. 본 논문에서는 신뢰성있는 딥러닝 기반 데이터 분석 모델을 참조하기위한 딥러닝 기술 언어를 제안한다. 제안하는 기술 언어는 신뢰성 있는 딥러닝 모델을 개발하는데 필요한 학습데이터셋, 개발 환경 및 설정 등의 정보와더불어 딥러닝 모델의 계층구조를 표현할 수 있다. 제안하는 딥러닝 기술 언어를 이용하여 개발자는 지능형 교통 시스템에서 참조하는 분석 모델의정확도를 검증할 수 있다. 실험에서는 제안하는 언어의 유효성을 검증하기 위해, 번호판 인식 모델을 중심으로 딥러닝 기술 문서의 적용과정을 보인다With the recent advancements of deep learning, companies such as smart home, healthcare, and intelligent transportation systems areutilizing its functionality to provide high-quality services for vehicle detection, emergency situation detection, and controlling energyconsumption. To provide reliable services in such sensitive systems, deep learning models are required to have high accuracy. In orderto develop a deep learning model for analyzing previously mentioned services, developers should utilize the state of the art deep learningmodels that have already been verified for higher accuracy. The developers can verify the accuracy of the referenced model by validatingthe model on the dataset. For this validation, the developer needs structural information to document and apply deep learning models,including metadata such as learning dataset, network architecture, and development environments. In this paper, we propose a descriptionlanguage that represents the network architecture of the deep learning model along with its metadata that are necessary to develop adeep learning model. Through the proposed description language, developers can easily verify the accuracy of the referenced deep learningmodel. Our experiments demonstrate the application scenario of a deep learning description document that focuses on the license platerecognition for the detection of illegally parked vehicles.Trusted Deep Learning, Model Reference, Deep Learning Description Language, Traffic Situation Analysis Model"
212,딥러닝 분석을 이용한 중국 역내·외 위안화 변동성 예측,이우식 /Lee Woosik,2016,한국데이터정보과학회,"2008년 글로벌 금융위기 이후 중국은 위안화 국제화의 점진적 추진을 진행하면서 중국상하이 외환시장과 중국홍콩 외환시장에서 거래되는 통화인 역내위안화와 역외위안화를 형성시켰다. 본 연구는 위안화 국제화와 점진적인 중국 자본계정 개방에 따라 급변하는 외환시장상황의 변동성을 정확하게 파악하기 위해서 GARCH모형 (일반화된 자기회귀 조건부이분산성모형)에 다단계인공신경망을 결합한 MLP-GARCH 모형과 GARCH모형과 기계학습의 일종인 딥러닝 (deep learning)을 통합한 DL-GARCH을 가지고 위안화 변동성예측을 비교 실험과 분석을 하였다. 비교분석 결과 DLGARCH 모형은 MLP-GARCH보다 모형 위안화 역내·외 환율변동성 예측 면에서 더욱 더 개선된예측값을 제공하였다. 그래서 이분산시계열모형을 딥러닝과 결합한 DL-GARCH 모형은 시계열의환율 변동성 예측 문제에 딥러닝을 응용할 수 있음을 확인하였다. 향후 이분산시계열과 결합된 딥러닝 모형은 다른 금융시계열 데이터에 응용하여 그 일반화 가능성을 높일 수 있을 것이다.The People’s Republic of China has vigorously been pursuing the internationalization of the Chinese Yuan or Renminbi after the financial crisis of 2008. In this view, an abrupt increase of use of the Chinese Yuan in the onshore and offshore markets are important milestones to be one of important currencies. One of the most frequently used methods to forecast volatility is GARCH model. Since a prediction error of the GARCH model has been reported quite high, a lot of efforts have been made to improve forecasting capability of the GARCH model. In this paper, we have proposed MLPGARCH and a DL-GARCH by employing Artificial Neural Network to the GARCH. In an application to forecasting Chinese Yuan volatility, we have successfully shown their overall outperformance in forecasting over the GARCH.Deep learning, DL-GARCH, GARCH, The Chinese Yuan, volatility."
213,딥러닝 기법을 이용한 내일강수 예측,하지훈,2016,한국지능시스템학회,"정확한 강수예측을 위해서는 예측인자 선정과 예측방법에 대한 선택이 매우 중요하다. 최근에는 강수예측 방법으로기계학습 기법이 많이 사용되고 있으며, 그 중에서도 특히 인공신경망을 사용한 강수예측 방법은 좋은 성능을 보였다. 본논문에서는 딥러닝 기법 중 하나인 DBN(deep belief network)를 이용한 새로운 강수예측 방법을 제안한다. DBN는 비지도사전 학습을 통해 초기 가중치를 설정하여 기존 인공신경망의 문제점을 보완한다. 예측인자로는 기온, 전일-전주 강수일, 태양과 달 궤도 관련 자료를 선정하였다. 기온과 전일-전주 강수일은 서울에서의 1974년부터 2013년까지 총 40년간의AWS(automatic weather system) 관측 자료를 사용하였고, 태양과 달의 궤도 관련 자료는 서울을 중심으로 계산한 결과를사용하였다. 전체 기간에서 일부는 학습 자료로 사용하여 예측모델을 생성하였고, 나머지를 생성한 모델의 검증 자료로사용하였다. 모델 검증 결과로 나온 예측값들은 확률값을 가지며 임계치를 이용하여 강수유무를 판별하였다. 강수 정확도의척도로 양분예보기법 중 CSI(critical successive index)와 Bias(frequency bias)를 계산하였다. 이를 통해 DBN와 MLP(multilayer perceptron)의 성능을 비교한 결과 DBN의 강수 예측 정확도가 높았고, 수행속도 또한 2배 이상 빨랐다.For accurate precipitation forecasts the choice of weather faoctrs and prediction method is very important. Recently, machinele arning has been widely used for forecasting precipitation, and artificial neural network, one of machine learning techniques, showed good performance. In this paper, we suggest a new method for forecasting precipitation using DBN, one of deep learning techniques.
DBN has an advantage that initial weights are set by unsupervised learning, so this compensates for the defects of artificial neural networks. We used past precipitation, temperature, and the parameters of the sun and moon's motion as features for forecasting precipitation. The dataset consists of observation data which had been measured for 40 years from AWS in Seoul. Experiments were based on 8-fold cross validation. As a result of estimation, we got probabilities of test dataset, so threshold was used for the decision of precipitation. CSI and Bias were used for indicating the precision of precipitation. Our experimental results showed that DBN performed better than MLP.Deep learning, Deep belief network, Precipitation, Forecast"
214,딥러닝 신경망모형을 이용한 실시간 교통정보수집 연구,김대현 /KIM DAE HYON,2016,사단법인 인문사회과학기술융합학회,"비디오 기반의 실시간 교통정보수집 시스템은 자율주행을 포함한 지능형교통체계의 주요 서브시스템중 하나이며, 우수한 예측(추정)력 때문에 핵심 알고리즘으로 인공신경망 모형이 주로 사용되고 있다. 특히, 최근 과학 학술지 네이처에 구글 딥마인드의 알파고가 공개되면서 딥러닝과 인공지능에 대한 전 세계적인 관심이 집중되고 있다. 최근까지 가장 널리 사용된 인공신경망모형은 역전파 모형이었으나 최근 컴퓨터 하드웨어 및 알고리즘의 새로운 개발과 더불어 딥러닝 알고리즘을 이용한 딥인공신경망 모형에 대한 관심이 높아지고 있다. 그러나 아직 딥러닝 인공신경망에 대한 연구는 초기단계에 있는 실정이며 적용분야도 극히 한정되어 있다. 본 연구에서는 실시간 도로교통정보 수집을 위한 영상처리분야에서 딥러닝 알고리즘을 활용하고 실험결과를 통하여 향후 교통정보수집‧분석 분야에서의 딥러닝 인공신경망 모형의 적용가능성을 살펴보고자 한다. 특히, 본 연구에서 도출한 딥러닝 인공신경망모형의 실험결과를 기존 역전파 모형에 의한 실험결과와 비교하여 모형의 우수성을 비교‧평가하고자 한다.Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Normally, artificial neural networks have been used in real-time applications for collecting traffic information because of their remarkable performance in terms of predictive accuracy and real-time computing. Especially, the recent publication of a paper in the journal Nature and the success of Google DeepMind’s world-beating Go program AlphaGo caused significant increases of the public interest in artificial intelligence. Even though various types of learning models have been proposed, the Backpropagation has been the most popular neural network model. However, the Deep learning neural network model has attracted much interest recently. In this study, the Deep leaning neural network will be applied in image processing techniques for the real-time traffic information collection to investigate the performance of Deep learning algorithm in traffic engineering fields. In addition, the experimental results from Deep learing algorithm will be compared with the results from Backpropagation model to evaluate and propose better performing model.Intelligent Transportation Systems (ITS), artificial neural networks, Deep learning, traffic information, image processing"
215,딥러닝의 모형과 응용사례,안성만 /SungMahn Ahn,2016,한국지능정보시스템학회,"딥러닝은 인공신경망(neural network)이라는 인공지능분야의 모형이 발전된 형태로서, 계층구조로 이루어진 인공신경망의 내부계층(hidden layer)이 여러 단계로 이루어진 구조이다. 딥러닝에서의 주요 모형은 합성곱신경망(convolutional neural network), 순환신경망(recurrent neural network), 그리고 심층신뢰신경망(deep belief network)의 세가지라고 할 수있다. 그 중에서 현재 흥미로운 연구가 많이 발표되어서 관심이 집중되고 있는 모형은 지도학습(supervised learning)모형인 처음 두 개의 모형이다. 따라서 본 논문에서는 지도학습모형의 가중치를 최적화하는 기본적인 방법인 오류역전파 알고리즘을 살펴본 뒤에 합성곱신경망과 순환신경망의 구조와 응용사례 등을 살펴보고자 한다. 본문에서 다루지 않은 모형인 심층신뢰신경망은 아직까지는 합성곱신경망 이나 순환신경망보다는 상대적으로 주목을 덜 받고 있다. 그러나 심층신뢰신경망은 CNN이나 RNN과는 달리 비지도학습(unsupervised learning)모형이며, 사람이나 동물은 관찰을 통해서 스스로 학습한다는 점에서 궁극적으로는 비지도학습모형이 더 많이 연구되어야 할 주제가 될 것이다.Deep learning model is a kind of neural networks that allows multiple hidden layers. There are various deep learning architectures such as convolutional neural networks, deep belief networks and recurrent neural networks. Those have been applied to fields like computer vision, automatic speech recognition, natural language processing, audio recognition and bioinformatics where they have been shown to produce state-of-the-art results on various tasks. Among those architectures, convolutional neural networks and recurrent neural networks are classified as the supervised learning model. And in recent years, those supervised learning models have gained more popularity than unsupervised learning models such as deep belief networks, because supervised learning models have shown fashionable applications in such fields mentioned above.
Deep learning models can be trained with backpropagation algorithm. Backpropagation is an abbreviation for “backward propagation of errors” and a common method of training artificial neural networks used in conjunction with an optimization method such as gradient descent. The method calculates the gradient of an error function with respect to all the weights in the network. The gradient is fed to the optimization method which in turn uses it to update the weights, in an attempt to minimize the error function.
Convolutional neural networks use a special architecture which is particularly well-adapted to classify images. Using this architecture makes convolutional networks fast to train. This, in turn, helps us train deep, muti-layer networks, which are very good at classifying images. These days, deep convolutional networks are used in most neural networks for image recognition. Convolutional neural networks use three basic ideas: local receptive fields, shared weights, and pooling. By local receptive fields, we mean that each neuron in the first(or any) hidden layer will be connected to a small region of the input(or previous layer’s) neurons. Shared weights mean that we’re going to use the same weights and bias for each of the local receptive field. This means that all the neurons in the hidden layer detect exactly the same feature, just at different locations in the input image. In addition to the convolutional layers just described, convolutional neural networks also contain pooling layers. Pooling layers are usually used immediately after convolutional layers. What the pooling layers do is to simplify the information in the output from the convolutional layer.
Recent convolutional network architectures have 10 to 20 hidden layers and billions of connections between units. Training deep learning networks has taken weeks several years ago, but thanks to progress in GPU and algorithm enhancement, training time has reduced to several hours.
Neural networks with time-varying behavior are known as recurrent neural networks or RNNs. A recurrent neural network is a class of artificial neural network where connections between units form a directed cycle. This creates an internal state of the network which allows it to exhibit dynamic temporal behavior. Unlike feedforward neural networks, RNNs can use their internal memory to process arbitrary sequences of inputs. Early RNN models turned out to be very difficult to train, harder even than deep feedforward networks. The reason is the unstable gradient problem such as vanishing gradient and exploding gradient. The gradient can get smaller and smaller as it is propagated back through layers. This makes learning in early layers extremely slow. The problem actually gets worse in RNNs, since gradients aren’t just propagated backward through layers, they’re propagated backward through time. If the network runs for a long time, that can make the gradient extremely unstable and hard to learn from. It has been possible to incorporate an idea known as long short-term memory units (LSTMs) into RNNs. LSTMs make it much easier to get good results when training RNNs, and many recent papers make use of LSTMs or related ideas.Deep learning, Convolutional neural networks, Recurrent neural networks, Error backpropagation algorithm"
216,딥러닝분석과 기술적 분석 지표를 이용한 한국 코스피주가지수 방향성 예측,이우식 /Lee Woosik,2017,한국데이터정보과학회,"2016년 3월 구글 (Google)의 바둑인공지능 알파고 (AlphaGo)가 이세돌 9단과의 바둑대결에서승리한 이후 다양한 분야에서 인공지능 사용에 대한 관심이 높아지고 있는 가운데 금융투자 분야에서도 인공지능과 투자자문 전문가의 합성어인 로보어드바이저 (Robo-Advisor)에 대한 관심이 높아지고 있다. 인공지능 (artificial intelligence)기반의 의사결정은 비용 절감은 물론 효과적인 의사결정을가능하게 한다는 점에서 큰 장점이 있다. 본 연구에서는 기술적 분석 (technical analysis) 지표와 딥러닝 (deep learning) 모형을 결합하여 한국 코스피 지수를 예측하는 모형을 개발하고 제시한 모형들의 예측력을 비교, 분석한다. 분석 결과 기술적 분석 지표에 딥러닝 알고리즘을 결합한 모형이 주가지수 방향성 예측 문제에 응용될 수 있음을 확인하였다. 향후 본 연구에서 제안된 기술적 분석 지표와딥러닝모형을 결합한 기법은 로보어드바이저서비스에 응용할 수 있는 일반화 가능성을 보여준다Since Google’s AlphaGo defeated a world champion of Go players in 2016, there have been many interests in the deep learning. In the financial sector, a Robo-Advisor using deep learning gains a significant attention, which builds and manages portfolios of financial instruments for investors.In this paper, we have proposed the a deep learning algorithm geared toward identification and forecast of the KOSPI index direction,and we also have compared the accuracy of the prediction.In an application of forecasting the financial market index direction, we have shown that the Robo-Advisor using deep learning has a significant effect on finance industry. The Robo-Advisor collects a massive data such as earnings statements, news reports and regulatory filings, analyzes those and recommends investors how to view market trends and identify the best time to purchase financial assets. On the other hand, the Robo-Advisor allows businesses to learn more about their customers, develop better marketing strategies, increase sales and decrease costs.Artificial intelligence, deep learning, FinTech, Robo-Advisor, technical analysis."
217,딥러닝의 효율적인 학습을 위한 학습데이터 선별 기법에 관한 연구,주경돈 /Gyoung-don Joo,2017,한국정보과학회,"빅데이터의 증가와 병렬 하드웨어 처리, 학습 알고리즘의 개선은 딥러닝의 발전을 가져왔으며 인공지능이 적용 가능한 영역을 획기적으로 확장시키고 있다. 하지만 이러한 대규모의 학습 데이터에 기반한 딥러닝은 학습시간이 증가되고, 더 나아가 학습 데이터의 구축을 위한 레이블링(labeling) 비용이 데이터 수에 비례하여 증가하게 된다. 따라서 본 논문에서는, 딥러닝의 정확도를 최대한 유지하면서 필요한 학습 데이터의 수를 줄일수 있도록, 전체 데이터에서 제한된 수의 학습 데이터의 후보를 선별하는 기법에 대하여 연구한다. 특히 기존의 능동 학습에 기반한 기법들이 학습 데이터의 수가 매우 적어질 경우 딥러닝의 정확도가 급격히 떨어지는 문제를 해결할 수 있는 새로운 기법을 제안하였으며, 제안된 기법으로 선별된 학습 데이터는 그 수의 변화에 대해 딥러닝이 일관되게 경쟁력있는 정확도를 가짐을 다양한 실험을 통하여 검증하였다.The increase of big data, parallel hardware processing and enhanced learning algorithms have led to the development of deep learning and are dramatically expanding the domain of applications based on artificial intelligence. However, such large-scale training data extends the learning time, and furthermore, the labeling cost for building the training set increases in proportion to the volume of data. Therefore, in this paper, we study how to select a limited number of candidates for training set from given data so as to reduce the number of training set while maintaining the accuracy of deep learning. Especially, we found that, with the training set selected by the previous work, the accuracy of deep learning drastically decreases when the number of training set is very small and so proposed new methods to resolve this fall-off in accuracy. Through experiments, we showed that deep learning consistently has the competitive accuracy with the training set selected by the proposed method regardless of the size of training set.Deep Learning, Train Set Selection, Active Learning, Gamma distribution"
218,딥러닝을 이용한 범용적 스테그아날리시스,김현재 /Hyunjae Kim,2017,한국정보과학회,"스테그아날리시스(Steganalysis)란 이미지 등 일반적인 자료에 암호화된 정보를 은닉하는 스테가노그래피(Steganography)에 대한 검출 및 분석 방법으로, 기계학습 기반 방법론을 포함한다. 기존 기계학습 기반 스테그아날리시스는 영상(Image)의 특징(Feature) 추출 및 모델링에 기반하며, 최근 딥러닝(Deep Learning)의 적용으로 검출 정확도가 큰 폭으로 향상되었다. 하지만 현존하는 스테그아날리시스 모델은 단일 스테가노그래피 기법에 대해 국한되어 있어 학습에 사용되지 않은 스테고(Stego) 이미지의 경우 검출이 불가능한 결정적 한계를 가진다. 본 연구에서는 다양한 스테가노그래피 기법으로 생성된 스테고이미지에 딥러닝을 적용하여 스테그아날리시스를 학습하는 범용적 모델을 제안한다. 다양한 실험을 통해 제안 기법의 효용성 및 가능성을 확인하고, 범용적 스테그아날리시스 모델이 각각에 특화된 검출 기법과 유사한 정확도로 스테고 이미지를 검출할 수 있음을 보인다.Steganalysis is to detect information hidden by steganography inside general data such as images. There are stegoanalysis techniques that use machine learning (ML). Existing ML approaches to steganalysis are based on extracting features from stego images and modeling them.
Recently deep learning-based methodologies have shown significant improvements in detection accuracy. However, all the existing methods, including deep learning-based ones, have a critical limitation in that they can only detect stego images that are created by a specific steganography method. In this paper, we propose a generalized steganalysis method that can model multiple types of stego images using deep learning. Through various experiments, we confirm the effectiveness of our approach and envision directions for future research. In particular, we show that our method can detect each type of steganography with the same level of accuracy as that of a steganalysis method dedicated to that type of steganography, thereby demonstrating the general applicability of our approach to multiple types of stego images.steganalysis, steganography, deep learning, generalized steganalysis model"
219,"딥러닝 프레임워크의 비교: 티아노, 텐서플로, CNTK를 중심으로",정여진 /Yeojin Chung,2017,한국지능정보시스템학회,"딥러닝 프레임워크의 대표적인 기능으로는 ‘자동미분’과 ‘GPU의 활용’ 등을 들 수 있다. 본 논문은 파이썬의 라이브러리 형태로 사용 가능한 프레임워크 중에서 구글의 텐서플로와 마이크로소프트의 CNTK, 그리고 텐서 플로의 원조라고 할 수 있는 티아노를 비교하였다. 본문에서는 자동미분의 개념과 GPU의 활용형태를 간단히 설명하고, 그 다음에 logistic regression을 실행하는 예를 통하여 각 프레임워크의 문법을 알아본 뒤에, 마지막으로 대표적인 딥러닝 응용인 CNN의 예제를 실행시켜보고 코딩의 편의성과 실행속도 등을 확인해 보았다. 그 결과, 편의성의 관점에서 보면 티아노가 가장 코딩 하기가 어렵고, CNTK와 텐서플로는 많은 부분이 비슷하게 추상화 되어 있어서 코딩이 비슷하지만 가중치와 편향을 직접 정의하느냐의 여부에서 차이를 보였다. 그리고 각프레임워크의 실행속도에 대한 평가는 ‘큰 차이는 없다’는 것이다. 텐서플로는 티아노에 비하여 속도가 느리다는 평가가 있어왔는데, 본 연구의 실험에 의하면, 비록 CNN 모형에 국한되었지만, 텐서플로가 아주 조금이지만빠른 것으로 나타났다. CNTK의 경우에도, 비록 실험환경이 달랐지만, 실험환경의 차이에 의한 속도의 차이의 편차범위 이내에 있는 것으로 판단이 되었다. 본 연구에서는 세 종류의 딥러닝 프레임워크만을 살펴보았는데,위키피디아에 따르면 딥러닝 프레임워크의 종류는 12가지가 있으며, 각 프레임워크의 특징을 15가지 속성으로구분하여 차이를 특정하고 있다. 그 많은 속성 중에서 사용자의 입장에서 볼 때 중요한 속성은 어떤 언어(파이썬, C++, Java, 등)로 사용가능한지, 어떤 딥러닝 모형에 대한 라이브러리가 잘 구현되어 있는지 등일 것이다. 그리고 사용자가 대규모의 딥러닝 모형을 구축한다면, 다중 GPU 혹은 다중 서버를 지원하는지의 여부도 중요할 것이다. 또한 딥러닝 모형을 처음 학습하는 경우에는 사용설명서가 많은지 예제 프로그램이 많은지 여부도 중요한 기준이 될 것이다.The deep learning framework is software designed to help develop deep learning models. Some of its important functions include “automatic differentiation” and “utilization of GPU”. The list of popular deep learning framework includes Caffe (BVLC) and Theano (University of Montreal). And recently, Microsoft's deep learning framework, Microsoft Cognitive Toolkit, was released as open-source license, following Google’s Tensorflow a year earlier. The early deep learning frameworks have been developed mainly for research at universities. Beginning with the inception of Tensorflow, however, it seems that companies such as Microsoft and Facebook have started to join the competition of framework development. Given the trend, Google and other companies are expected to continue investing in the deep learning framework to bring forward the initiative in the artificial intelligence business. From this point of view, we think it is a good time to compare some of deep learning frameworks. So we compare three deep learning frameworks which can be used as a Python library. Those are Google's Tensorflow, Microsoft’s CNTK, and Theano which is sort of a predecessor of the preceding two.
The most common and important function of deep learning frameworks is the ability to perform automatic differentiation. Basically all the mathematical expressions of deep learning models can be represented as computational graphs, which consist of nodes and edges. Partial derivatives on each edge of a computational graph can then be obtained. With the partial derivatives, we can let software compute differentiation of any node with respect to any variable by utilizing chain rule of Calculus.
First of all, the convenience of coding is in the order of CNTK, Tensorflow, and Theano. The criterion is simply based on the lengths of the codes and the learning curve and the ease of coding are not the main concern. According to the criteria, Theano was the most difficult to implement with, and CNTK and Tensorflow were somewhat easier. With Tensorflow, we need to define weight variables and biases explicitly. The reason that CNTK and Tensorflow are easier to implement with is that those frameworks provide us with more abstraction than Theano. We, however, need to mention that low-level coding is not always bad. It gives us flexibility of coding. With the low-level coding such as in Theano, we can implement and test any new deep learning models or any new search methods that we can think of.
The assessment of the execution speed of each framework is that there is not meaningful difference. According to the experiment, execution speeds of Theano and Tensorflow are very similar, although the experiment was limited to a CNN model. In the case of CNTK, the experimental environment was not maintained as the same. The code written in CNTK has to be run in PC environment without GPU where codes execute as much as 50 times slower than with GPU. But we concluded that the difference of execution speed was within the range of variation caused by the different hardware setup.
In this study, we compared three types of deep learning framework: Theano, Tensorflow, and CNTK. According to Wikipedia, there are 12 available deep learning frameworks. And 15 different attributes differentiate each framework. Some of the important attributes would include interface language (Python, C ++, Java, etc.) and the availability of libraries on various deep learning models such as CNN, RNN, DBN, and etc. And if a user implements a large scale deep learning model, it will also be important to support multiple GPU or multiple servers. Also, if you are learning the deep learning model, it would also be important if there are enough examples and references.deep learning framework, Theano, TensorFlow, CNTK, computational graph, CIFAR-10"
220,딥러닝을 이용한 주변 무선단말 파악방안,이웅섭 /Lee， Woong-sup,2018,한국정보통신학회,"최근 단말-대-단말(Device-to-device, D2D) 통신기술이 차세대 무선통신시스템의 핵심기술로 큰 관심을 받고 있다. 이러한 단말간 통신에서는 자신의 주변에 어떠한 단말이 있는지 파악하는 주변단말 탐색(Neighbor discovery)이매우 중요하다. 본 논문에서는 최근 큰 관심을 받고 있는 딥러닝(Deep learning) 기술을 활용하여 단말간 통신에서 주변단말을 파악하는 방안에 대해서 제안한다. 제안 방안은 기존의 방안과 달리 무선채널의 공간적 연관성을 이용하여단말간의 신호 전송 없이 단말이 기지국으로 전송하는 상향링크 파일럿 신호를 기반으로 주변 단말을 찾고 따라서기존의 방식에 비해 신호전송 복잡도(signaling complexity)를 크게 줄일 수 있다. 또한 제안 방안에서는 떨어져 있는거리에 따라서 주변 단말을 분류 가능하여 기존 방안에 비해서 좀 더 세밀한 단말 탐색이 가능하다. 마지막으로 본 논문에서는 tensorflow를 이용한 컴퓨터 시뮬레이션을 통해 제안 방안의 성능을 검증하였다.Recently, the device-to-device (D2D) communication has been conceived as the key technology for the next-generationmobile communication systems. The neighbor discovery in which the nearby users are found, is essential for the properoperation of the D2D communication. In this paper, we propose new neighbor discovery scheme based on deep learningtechnology which has gained a lot of attention recently. In the proposed scheme, the neighboring users can be foundusing the uplink pilot transmission of users only, unlike conventional neighbor discovery schemes in which direct pilotcommunication among users is required, such that the signaling overhead can be greatly reduced in our proposed scheme.
Moreover, the neighbors with different proximity can also be classified accordingly which enables more accurate neighbordiscovery compared to the conventional schemes. The performance of our proposed scheme is verified through thetensorflow-based computer simulations.
키워드 : 딥러닝, 주변단말 탐색, 단말-대-단말 통신Deep learning, Neighbor discovery, Device-to-device communication, Wireless channel correlation"
221,신뢰성있는 딥러닝 기반 분석 모델을 참조하기 위한 딥러닝 기술 언어,문종혁 /Jong Hyeok Mun,2021,한국정보처리학회,"최근 딥러닝은 하드웨어 성능이 향상됨에 따라 자연어 처리, 영상 인식 등의 다양한 기술에 접목되어 활용되고 있다. 이러한 기술들을 활용해 지능형교통 시스템(ITS), 스마트홈, 헬스케어 등의 산업분야에서 데이터를 분석하여 고속도로 속도위반 차량 검출, 에너지 사용량 제어, 응급상황 등과 같은고품질의 서비스를 제공하며, 고품질의 서비스를 제공하기 위해서는 정확도가 향상된 딥러닝 모델이 적용되어야 한다. 이를 위해 서비스 환경의 데이터를분석하기 위한 딥러닝 모델을 개발할 때, 개발자는 신뢰성이 검증된 최신의 딥러닝 모델을 적용할 수 있어야 한다. 이는 개발자가 참조하는 딥러닝 모델에적용된 학습 데이터셋의 정확도를 측정하여 검증할 수 있다. 이러한 검증을 위해서 개발자는 학습 데이터셋, 딥러닝의 계층구조 및 개발 환경 등과 같은내용을 포함하는 딥러닝 모델을 문서화하여 적용하기 위한 구조적인 정보가 필요하다. 본 논문에서는 신뢰성있는 딥러닝 기반 데이터 분석 모델을 참조하기위한 딥러닝 기술 언어를 제안한다. 제안하는 기술 언어는 신뢰성 있는 딥러닝 모델을 개발하는데 필요한 학습데이터셋, 개발 환경 및 설정 등의 정보와더불어 딥러닝 모델의 계층구조를 표현할 수 있다. 제안하는 딥러닝 기술 언어를 이용하여 개발자는 지능형 교통 시스템에서 참조하는 분석 모델의정확도를 검증할 수 있다. 실험에서는 제안하는 언어의 유효성을 검증하기 위해, 번호판 인식 모델을 중심으로 딥러닝 기술 문서의 적용과정을 보인다With the recent advancements of deep learning, companies such as smart home, healthcare, and intelligent transportation systems areutilizing its functionality to provide high-quality services for vehicle detection, emergency situation detection, and controlling energyconsumption. To provide reliable services in such sensitive systems, deep learning models are required to have high accuracy. In orderto develop a deep learning model for analyzing previously mentioned services, developers should utilize the state of the art deep learningmodels that have already been verified for higher accuracy. The developers can verify the accuracy of the referenced model by validatingthe model on the dataset. For this validation, the developer needs structural information to document and apply deep learning models,including metadata such as learning dataset, network architecture, and development environments. In this paper, we propose a descriptionlanguage that represents the network architecture of the deep learning model along with its metadata that are necessary to develop adeep learning model. Through the proposed description language, developers can easily verify the accuracy of the referenced deep learningmodel. Our experiments demonstrate the application scenario of a deep learning description document that focuses on the license platerecognition for the detection of illegally parked vehicles.Trusted Deep Learning, Model Reference, Deep Learning Description Language, Traffic Situation Analysis Model"
222,딥러닝 분석을 이용한 중국 역내·외 위안화 변동성 예측,이우식 /Lee Woosik,2016,한국데이터정보과학회,"2008년 글로벌 금융위기 이후 중국은 위안화 국제화의 점진적 추진을 진행하면서 중국상하이 외환시장과 중국홍콩 외환시장에서 거래되는 통화인 역내위안화와 역외위안화를 형성시켰다. 본 연구는 위안화 국제화와 점진적인 중국 자본계정 개방에 따라 급변하는 외환시장상황의 변동성을 정확하게 파악하기 위해서 GARCH모형 (일반화된 자기회귀 조건부이분산성모형)에 다단계인공신경망을 결합한 MLP-GARCH 모형과 GARCH모형과 기계학습의 일종인 딥러닝 (deep learning)을 통합한 DL-GARCH을 가지고 위안화 변동성예측을 비교 실험과 분석을 하였다. 비교분석 결과 DLGARCH 모형은 MLP-GARCH보다 모형 위안화 역내·외 환율변동성 예측 면에서 더욱 더 개선된예측값을 제공하였다. 그래서 이분산시계열모형을 딥러닝과 결합한 DL-GARCH 모형은 시계열의환율 변동성 예측 문제에 딥러닝을 응용할 수 있음을 확인하였다. 향후 이분산시계열과 결합된 딥러닝 모형은 다른 금융시계열 데이터에 응용하여 그 일반화 가능성을 높일 수 있을 것이다.The People’s Republic of China has vigorously been pursuing the internationalization of the Chinese Yuan or Renminbi after the financial crisis of 2008. In this view, an abrupt increase of use of the Chinese Yuan in the onshore and offshore markets are important milestones to be one of important currencies. One of the most frequently used methods to forecast volatility is GARCH model. Since a prediction error of the GARCH model has been reported quite high, a lot of efforts have been made to improve forecasting capability of the GARCH model. In this paper, we have proposed MLPGARCH and a DL-GARCH by employing Artificial Neural Network to the GARCH. In an application to forecasting Chinese Yuan volatility, we have successfully shown their overall outperformance in forecasting over the GARCH.Deep learning, DL-GARCH, GARCH, The Chinese Yuan, volatility."
223,딥러닝 기법을 이용한 내일강수 예측,하지훈,2016,한국지능시스템학회,"정확한 강수예측을 위해서는 예측인자 선정과 예측방법에 대한 선택이 매우 중요하다. 최근에는 강수예측 방법으로기계학습 기법이 많이 사용되고 있으며, 그 중에서도 특히 인공신경망을 사용한 강수예측 방법은 좋은 성능을 보였다. 본논문에서는 딥러닝 기법 중 하나인 DBN(deep belief network)를 이용한 새로운 강수예측 방법을 제안한다. DBN는 비지도사전 학습을 통해 초기 가중치를 설정하여 기존 인공신경망의 문제점을 보완한다. 예측인자로는 기온, 전일-전주 강수일, 태양과 달 궤도 관련 자료를 선정하였다. 기온과 전일-전주 강수일은 서울에서의 1974년부터 2013년까지 총 40년간의AWS(automatic weather system) 관측 자료를 사용하였고, 태양과 달의 궤도 관련 자료는 서울을 중심으로 계산한 결과를사용하였다. 전체 기간에서 일부는 학습 자료로 사용하여 예측모델을 생성하였고, 나머지를 생성한 모델의 검증 자료로사용하였다. 모델 검증 결과로 나온 예측값들은 확률값을 가지며 임계치를 이용하여 강수유무를 판별하였다. 강수 정확도의척도로 양분예보기법 중 CSI(critical successive index)와 Bias(frequency bias)를 계산하였다. 이를 통해 DBN와 MLP(multilayer perceptron)의 성능을 비교한 결과 DBN의 강수 예측 정확도가 높았고, 수행속도 또한 2배 이상 빨랐다.For accurate precipitation forecasts the choice of weather faoctrs and prediction method is very important. Recently, machinele arning has been widely used for forecasting precipitation, and artificial neural network, one of machine learning techniques, showed good performance. In this paper, we suggest a new method for forecasting precipitation using DBN, one of deep learning techniques.
DBN has an advantage that initial weights are set by unsupervised learning, so this compensates for the defects of artificial neural networks. We used past precipitation, temperature, and the parameters of the sun and moon's motion as features for forecasting precipitation. The dataset consists of observation data which had been measured for 40 years from AWS in Seoul. Experiments were based on 8-fold cross validation. As a result of estimation, we got probabilities of test dataset, so threshold was used for the decision of precipitation. CSI and Bias were used for indicating the precision of precipitation. Our experimental results showed that DBN performed better than MLP.Deep learning, Deep belief network, Precipitation, Forecast"
224,딥러닝 신경망모형을 이용한 실시간 교통정보수집 연구,김대현 /KIM DAE HYON,2016,사단법인 인문사회과학기술융합학회,"비디오 기반의 실시간 교통정보수집 시스템은 자율주행을 포함한 지능형교통체계의 주요 서브시스템중 하나이며, 우수한 예측(추정)력 때문에 핵심 알고리즘으로 인공신경망 모형이 주로 사용되고 있다. 특히, 최근 과학 학술지 네이처에 구글 딥마인드의 알파고가 공개되면서 딥러닝과 인공지능에 대한 전 세계적인 관심이 집중되고 있다. 최근까지 가장 널리 사용된 인공신경망모형은 역전파 모형이었으나 최근 컴퓨터 하드웨어 및 알고리즘의 새로운 개발과 더불어 딥러닝 알고리즘을 이용한 딥인공신경망 모형에 대한 관심이 높아지고 있다. 그러나 아직 딥러닝 인공신경망에 대한 연구는 초기단계에 있는 실정이며 적용분야도 극히 한정되어 있다. 본 연구에서는 실시간 도로교통정보 수집을 위한 영상처리분야에서 딥러닝 알고리즘을 활용하고 실험결과를 통하여 향후 교통정보수집‧분석 분야에서의 딥러닝 인공신경망 모형의 적용가능성을 살펴보고자 한다. 특히, 본 연구에서 도출한 딥러닝 인공신경망모형의 실험결과를 기존 역전파 모형에 의한 실험결과와 비교하여 모형의 우수성을 비교‧평가하고자 한다.Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Normally, artificial neural networks have been used in real-time applications for collecting traffic information because of their remarkable performance in terms of predictive accuracy and real-time computing. Especially, the recent publication of a paper in the journal Nature and the success of Google DeepMind’s world-beating Go program AlphaGo caused significant increases of the public interest in artificial intelligence. Even though various types of learning models have been proposed, the Backpropagation has been the most popular neural network model. However, the Deep learning neural network model has attracted much interest recently. In this study, the Deep leaning neural network will be applied in image processing techniques for the real-time traffic information collection to investigate the performance of Deep learning algorithm in traffic engineering fields. In addition, the experimental results from Deep learing algorithm will be compared with the results from Backpropagation model to evaluate and propose better performing model.Intelligent Transportation Systems (ITS), artificial neural networks, Deep learning, traffic information, image processing"
225,딥러닝의 모형과 응용사례,안성만 /SungMahn Ahn,2016,한국지능정보시스템학회,"딥러닝은 인공신경망(neural network)이라는 인공지능분야의 모형이 발전된 형태로서, 계층구조로 이루어진 인공신경망의 내부계층(hidden layer)이 여러 단계로 이루어진 구조이다. 딥러닝에서의 주요 모형은 합성곱신경망(convolutional neural network), 순환신경망(recurrent neural network), 그리고 심층신뢰신경망(deep belief network)의 세가지라고 할 수있다. 그 중에서 현재 흥미로운 연구가 많이 발표되어서 관심이 집중되고 있는 모형은 지도학습(supervised learning)모형인 처음 두 개의 모형이다. 따라서 본 논문에서는 지도학습모형의 가중치를 최적화하는 기본적인 방법인 오류역전파 알고리즘을 살펴본 뒤에 합성곱신경망과 순환신경망의 구조와 응용사례 등을 살펴보고자 한다. 본문에서 다루지 않은 모형인 심층신뢰신경망은 아직까지는 합성곱신경망 이나 순환신경망보다는 상대적으로 주목을 덜 받고 있다. 그러나 심층신뢰신경망은 CNN이나 RNN과는 달리 비지도학습(unsupervised learning)모형이며, 사람이나 동물은 관찰을 통해서 스스로 학습한다는 점에서 궁극적으로는 비지도학습모형이 더 많이 연구되어야 할 주제가 될 것이다.Deep learning model is a kind of neural networks that allows multiple hidden layers. There are various deep learning architectures such as convolutional neural networks, deep belief networks and recurrent neural networks. Those have been applied to fields like computer vision, automatic speech recognition, natural language processing, audio recognition and bioinformatics where they have been shown to produce state-of-the-art results on various tasks. Among those architectures, convolutional neural networks and recurrent neural networks are classified as the supervised learning model. And in recent years, those supervised learning models have gained more popularity than unsupervised learning models such as deep belief networks, because supervised learning models have shown fashionable applications in such fields mentioned above.
Deep learning models can be trained with backpropagation algorithm. Backpropagation is an abbreviation for “backward propagation of errors” and a common method of training artificial neural networks used in conjunction with an optimization method such as gradient descent. The method calculates the gradient of an error function with respect to all the weights in the network. The gradient is fed to the optimization method which in turn uses it to update the weights, in an attempt to minimize the error function.
Convolutional neural networks use a special architecture which is particularly well-adapted to classify images. Using this architecture makes convolutional networks fast to train. This, in turn, helps us train deep, muti-layer networks, which are very good at classifying images. These days, deep convolutional networks are used in most neural networks for image recognition. Convolutional neural networks use three basic ideas: local receptive fields, shared weights, and pooling. By local receptive fields, we mean that each neuron in the first(or any) hidden layer will be connected to a small region of the input(or previous layer’s) neurons. Shared weights mean that we’re going to use the same weights and bias for each of the local receptive field. This means that all the neurons in the hidden layer detect exactly the same feature, just at different locations in the input image. In addition to the convolutional layers just described, convolutional neural networks also contain pooling layers. Pooling layers are usually used immediately after convolutional layers. What the pooling layers do is to simplify the information in the output from the convolutional layer.
Recent convolutional network architectures have 10 to 20 hidden layers and billions of connections between units. Training deep learning networks has taken weeks several years ago, but thanks to progress in GPU and algorithm enhancement, training time has reduced to several hours.
Neural networks with time-varying behavior are known as recurrent neural networks or RNNs. A recurrent neural network is a class of artificial neural network where connections between units form a directed cycle. This creates an internal state of the network which allows it to exhibit dynamic temporal behavior. Unlike feedforward neural networks, RNNs can use their internal memory to process arbitrary sequences of inputs. Early RNN models turned out to be very difficult to train, harder even than deep feedforward networks. The reason is the unstable gradient problem such as vanishing gradient and exploding gradient. The gradient can get smaller and smaller as it is propagated back through layers. This makes learning in early layers extremely slow. The problem actually gets worse in RNNs, since gradients aren’t just propagated backward through layers, they’re propagated backward through time. If the network runs for a long time, that can make the gradient extremely unstable and hard to learn from. It has been possible to incorporate an idea known as long short-term memory units (LSTMs) into RNNs. LSTMs make it much easier to get good results when training RNNs, and many recent papers make use of LSTMs or related ideas.Deep learning, Convolutional neural networks, Recurrent neural networks, Error backpropagation algorithm"
226,딥러닝분석과 기술적 분석 지표를 이용한 한국 코스피주가지수 방향성 예측,이우식 /Lee Woosik,2017,한국데이터정보과학회,"2016년 3월 구글 (Google)의 바둑인공지능 알파고 (AlphaGo)가 이세돌 9단과의 바둑대결에서승리한 이후 다양한 분야에서 인공지능 사용에 대한 관심이 높아지고 있는 가운데 금융투자 분야에서도 인공지능과 투자자문 전문가의 합성어인 로보어드바이저 (Robo-Advisor)에 대한 관심이 높아지고 있다. 인공지능 (artificial intelligence)기반의 의사결정은 비용 절감은 물론 효과적인 의사결정을가능하게 한다는 점에서 큰 장점이 있다. 본 연구에서는 기술적 분석 (technical analysis) 지표와 딥러닝 (deep learning) 모형을 결합하여 한국 코스피 지수를 예측하는 모형을 개발하고 제시한 모형들의 예측력을 비교, 분석한다. 분석 결과 기술적 분석 지표에 딥러닝 알고리즘을 결합한 모형이 주가지수 방향성 예측 문제에 응용될 수 있음을 확인하였다. 향후 본 연구에서 제안된 기술적 분석 지표와딥러닝모형을 결합한 기법은 로보어드바이저서비스에 응용할 수 있는 일반화 가능성을 보여준다Since Google’s AlphaGo defeated a world champion of Go players in 2016, there have been many interests in the deep learning. In the financial sector, a Robo-Advisor using deep learning gains a significant attention, which builds and manages portfolios of financial instruments for investors.In this paper, we have proposed the a deep learning algorithm geared toward identification and forecast of the KOSPI index direction,and we also have compared the accuracy of the prediction.In an application of forecasting the financial market index direction, we have shown that the Robo-Advisor using deep learning has a significant effect on finance industry. The Robo-Advisor collects a massive data such as earnings statements, news reports and regulatory filings, analyzes those and recommends investors how to view market trends and identify the best time to purchase financial assets. On the other hand, the Robo-Advisor allows businesses to learn more about their customers, develop better marketing strategies, increase sales and decrease costs.Artificial intelligence, deep learning, FinTech, Robo-Advisor, technical analysis."
227,딥러닝의 효율적인 학습을 위한 학습데이터 선별 기법에 관한 연구,주경돈 /Gyoung-don Joo,2017,한국정보과학회,"빅데이터의 증가와 병렬 하드웨어 처리, 학습 알고리즘의 개선은 딥러닝의 발전을 가져왔으며 인공지능이 적용 가능한 영역을 획기적으로 확장시키고 있다. 하지만 이러한 대규모의 학습 데이터에 기반한 딥러닝은 학습시간이 증가되고, 더 나아가 학습 데이터의 구축을 위한 레이블링(labeling) 비용이 데이터 수에 비례하여 증가하게 된다. 따라서 본 논문에서는, 딥러닝의 정확도를 최대한 유지하면서 필요한 학습 데이터의 수를 줄일수 있도록, 전체 데이터에서 제한된 수의 학습 데이터의 후보를 선별하는 기법에 대하여 연구한다. 특히 기존의 능동 학습에 기반한 기법들이 학습 데이터의 수가 매우 적어질 경우 딥러닝의 정확도가 급격히 떨어지는 문제를 해결할 수 있는 새로운 기법을 제안하였으며, 제안된 기법으로 선별된 학습 데이터는 그 수의 변화에 대해 딥러닝이 일관되게 경쟁력있는 정확도를 가짐을 다양한 실험을 통하여 검증하였다.The increase of big data, parallel hardware processing and enhanced learning algorithms have led to the development of deep learning and are dramatically expanding the domain of applications based on artificial intelligence. However, such large-scale training data extends the learning time, and furthermore, the labeling cost for building the training set increases in proportion to the volume of data. Therefore, in this paper, we study how to select a limited number of candidates for training set from given data so as to reduce the number of training set while maintaining the accuracy of deep learning. Especially, we found that, with the training set selected by the previous work, the accuracy of deep learning drastically decreases when the number of training set is very small and so proposed new methods to resolve this fall-off in accuracy. Through experiments, we showed that deep learning consistently has the competitive accuracy with the training set selected by the proposed method regardless of the size of training set.Deep Learning, Train Set Selection, Active Learning, Gamma distribution"
228,딥러닝을 이용한 범용적 스테그아날리시스,김현재 /Hyunjae Kim,2017,한국정보과학회,"스테그아날리시스(Steganalysis)란 이미지 등 일반적인 자료에 암호화된 정보를 은닉하는 스테가노그래피(Steganography)에 대한 검출 및 분석 방법으로, 기계학습 기반 방법론을 포함한다. 기존 기계학습 기반 스테그아날리시스는 영상(Image)의 특징(Feature) 추출 및 모델링에 기반하며, 최근 딥러닝(Deep Learning)의 적용으로 검출 정확도가 큰 폭으로 향상되었다. 하지만 현존하는 스테그아날리시스 모델은 단일 스테가노그래피 기법에 대해 국한되어 있어 학습에 사용되지 않은 스테고(Stego) 이미지의 경우 검출이 불가능한 결정적 한계를 가진다. 본 연구에서는 다양한 스테가노그래피 기법으로 생성된 스테고이미지에 딥러닝을 적용하여 스테그아날리시스를 학습하는 범용적 모델을 제안한다. 다양한 실험을 통해 제안 기법의 효용성 및 가능성을 확인하고, 범용적 스테그아날리시스 모델이 각각에 특화된 검출 기법과 유사한 정확도로 스테고 이미지를 검출할 수 있음을 보인다.Steganalysis is to detect information hidden by steganography inside general data such as images. There are stegoanalysis techniques that use machine learning (ML). Existing ML approaches to steganalysis are based on extracting features from stego images and modeling them.
Recently deep learning-based methodologies have shown significant improvements in detection accuracy. However, all the existing methods, including deep learning-based ones, have a critical limitation in that they can only detect stego images that are created by a specific steganography method. In this paper, we propose a generalized steganalysis method that can model multiple types of stego images using deep learning. Through various experiments, we confirm the effectiveness of our approach and envision directions for future research. In particular, we show that our method can detect each type of steganography with the same level of accuracy as that of a steganalysis method dedicated to that type of steganography, thereby demonstrating the general applicability of our approach to multiple types of stego images.steganalysis, steganography, deep learning, generalized steganalysis model"
229,"딥러닝 프레임워크의 비교: 티아노, 텐서플로, CNTK를 중심으로",정여진 /Yeojin Chung,2017,한국지능정보시스템학회,"딥러닝 프레임워크의 대표적인 기능으로는 ‘자동미분’과 ‘GPU의 활용’ 등을 들 수 있다. 본 논문은 파이썬의 라이브러리 형태로 사용 가능한 프레임워크 중에서 구글의 텐서플로와 마이크로소프트의 CNTK, 그리고 텐서 플로의 원조라고 할 수 있는 티아노를 비교하였다. 본문에서는 자동미분의 개념과 GPU의 활용형태를 간단히 설명하고, 그 다음에 logistic regression을 실행하는 예를 통하여 각 프레임워크의 문법을 알아본 뒤에, 마지막으로 대표적인 딥러닝 응용인 CNN의 예제를 실행시켜보고 코딩의 편의성과 실행속도 등을 확인해 보았다. 그 결과, 편의성의 관점에서 보면 티아노가 가장 코딩 하기가 어렵고, CNTK와 텐서플로는 많은 부분이 비슷하게 추상화 되어 있어서 코딩이 비슷하지만 가중치와 편향을 직접 정의하느냐의 여부에서 차이를 보였다. 그리고 각프레임워크의 실행속도에 대한 평가는 ‘큰 차이는 없다’는 것이다. 텐서플로는 티아노에 비하여 속도가 느리다는 평가가 있어왔는데, 본 연구의 실험에 의하면, 비록 CNN 모형에 국한되었지만, 텐서플로가 아주 조금이지만빠른 것으로 나타났다. CNTK의 경우에도, 비록 실험환경이 달랐지만, 실험환경의 차이에 의한 속도의 차이의 편차범위 이내에 있는 것으로 판단이 되었다. 본 연구에서는 세 종류의 딥러닝 프레임워크만을 살펴보았는데,위키피디아에 따르면 딥러닝 프레임워크의 종류는 12가지가 있으며, 각 프레임워크의 특징을 15가지 속성으로구분하여 차이를 특정하고 있다. 그 많은 속성 중에서 사용자의 입장에서 볼 때 중요한 속성은 어떤 언어(파이썬, C++, Java, 등)로 사용가능한지, 어떤 딥러닝 모형에 대한 라이브러리가 잘 구현되어 있는지 등일 것이다. 그리고 사용자가 대규모의 딥러닝 모형을 구축한다면, 다중 GPU 혹은 다중 서버를 지원하는지의 여부도 중요할 것이다. 또한 딥러닝 모형을 처음 학습하는 경우에는 사용설명서가 많은지 예제 프로그램이 많은지 여부도 중요한 기준이 될 것이다.The deep learning framework is software designed to help develop deep learning models. Some of its important functions include “automatic differentiation” and “utilization of GPU”. The list of popular deep learning framework includes Caffe (BVLC) and Theano (University of Montreal). And recently, Microsoft's deep learning framework, Microsoft Cognitive Toolkit, was released as open-source license, following Google’s Tensorflow a year earlier. The early deep learning frameworks have been developed mainly for research at universities. Beginning with the inception of Tensorflow, however, it seems that companies such as Microsoft and Facebook have started to join the competition of framework development. Given the trend, Google and other companies are expected to continue investing in the deep learning framework to bring forward the initiative in the artificial intelligence business. From this point of view, we think it is a good time to compare some of deep learning frameworks. So we compare three deep learning frameworks which can be used as a Python library. Those are Google's Tensorflow, Microsoft’s CNTK, and Theano which is sort of a predecessor of the preceding two.
The most common and important function of deep learning frameworks is the ability to perform automatic differentiation. Basically all the mathematical expressions of deep learning models can be represented as computational graphs, which consist of nodes and edges. Partial derivatives on each edge of a computational graph can then be obtained. With the partial derivatives, we can let software compute differentiation of any node with respect to any variable by utilizing chain rule of Calculus.
First of all, the convenience of coding is in the order of CNTK, Tensorflow, and Theano. The criterion is simply based on the lengths of the codes and the learning curve and the ease of coding are not the main concern. According to the criteria, Theano was the most difficult to implement with, and CNTK and Tensorflow were somewhat easier. With Tensorflow, we need to define weight variables and biases explicitly. The reason that CNTK and Tensorflow are easier to implement with is that those frameworks provide us with more abstraction than Theano. We, however, need to mention that low-level coding is not always bad. It gives us flexibility of coding. With the low-level coding such as in Theano, we can implement and test any new deep learning models or any new search methods that we can think of.
The assessment of the execution speed of each framework is that there is not meaningful difference. According to the experiment, execution speeds of Theano and Tensorflow are very similar, although the experiment was limited to a CNN model. In the case of CNTK, the experimental environment was not maintained as the same. The code written in CNTK has to be run in PC environment without GPU where codes execute as much as 50 times slower than with GPU. But we concluded that the difference of execution speed was within the range of variation caused by the different hardware setup.
In this study, we compared three types of deep learning framework: Theano, Tensorflow, and CNTK. According to Wikipedia, there are 12 available deep learning frameworks. And 15 different attributes differentiate each framework. Some of the important attributes would include interface language (Python, C ++, Java, etc.) and the availability of libraries on various deep learning models such as CNN, RNN, DBN, and etc. And if a user implements a large scale deep learning model, it will also be important to support multiple GPU or multiple servers. Also, if you are learning the deep learning model, it would also be important if there are enough examples and references.deep learning framework, Theano, TensorFlow, CNTK, computational graph, CIFAR-10"
230,딥러닝을 이용한 주변 무선단말 파악방안,이웅섭 /Lee， Woong-sup,2018,한국정보통신학회,"최근 단말-대-단말(Device-to-device, D2D) 통신기술이 차세대 무선통신시스템의 핵심기술로 큰 관심을 받고 있다. 이러한 단말간 통신에서는 자신의 주변에 어떠한 단말이 있는지 파악하는 주변단말 탐색(Neighbor discovery)이매우 중요하다. 본 논문에서는 최근 큰 관심을 받고 있는 딥러닝(Deep learning) 기술을 활용하여 단말간 통신에서 주변단말을 파악하는 방안에 대해서 제안한다. 제안 방안은 기존의 방안과 달리 무선채널의 공간적 연관성을 이용하여단말간의 신호 전송 없이 단말이 기지국으로 전송하는 상향링크 파일럿 신호를 기반으로 주변 단말을 찾고 따라서기존의 방식에 비해 신호전송 복잡도(signaling complexity)를 크게 줄일 수 있다. 또한 제안 방안에서는 떨어져 있는거리에 따라서 주변 단말을 분류 가능하여 기존 방안에 비해서 좀 더 세밀한 단말 탐색이 가능하다. 마지막으로 본 논문에서는 tensorflow를 이용한 컴퓨터 시뮬레이션을 통해 제안 방안의 성능을 검증하였다.Recently, the device-to-device (D2D) communication has been conceived as the key technology for the next-generationmobile communication systems. The neighbor discovery in which the nearby users are found, is essential for the properoperation of the D2D communication. In this paper, we propose new neighbor discovery scheme based on deep learningtechnology which has gained a lot of attention recently. In the proposed scheme, the neighboring users can be foundusing the uplink pilot transmission of users only, unlike conventional neighbor discovery schemes in which direct pilotcommunication among users is required, such that the signaling overhead can be greatly reduced in our proposed scheme.
Moreover, the neighbors with different proximity can also be classified accordingly which enables more accurate neighbordiscovery compared to the conventional schemes. The performance of our proposed scheme is verified through thetensorflow-based computer simulations.
키워드 : 딥러닝, 주변단말 탐색, 단말-대-단말 통신Deep learning, Neighbor discovery, Device-to-device communication, Wireless channel correlation"
231,신뢰성있는 딥러닝 기반 분석 모델을 참조하기 위한 딥러닝 기술 언어,문종혁 /Jong Hyeok Mun,2021,한국정보처리학회,"최근 딥러닝은 하드웨어 성능이 향상됨에 따라 자연어 처리, 영상 인식 등의 다양한 기술에 접목되어 활용되고 있다. 이러한 기술들을 활용해 지능형교통 시스템(ITS), 스마트홈, 헬스케어 등의 산업분야에서 데이터를 분석하여 고속도로 속도위반 차량 검출, 에너지 사용량 제어, 응급상황 등과 같은고품질의 서비스를 제공하며, 고품질의 서비스를 제공하기 위해서는 정확도가 향상된 딥러닝 모델이 적용되어야 한다. 이를 위해 서비스 환경의 데이터를분석하기 위한 딥러닝 모델을 개발할 때, 개발자는 신뢰성이 검증된 최신의 딥러닝 모델을 적용할 수 있어야 한다. 이는 개발자가 참조하는 딥러닝 모델에적용된 학습 데이터셋의 정확도를 측정하여 검증할 수 있다. 이러한 검증을 위해서 개발자는 학습 데이터셋, 딥러닝의 계층구조 및 개발 환경 등과 같은내용을 포함하는 딥러닝 모델을 문서화하여 적용하기 위한 구조적인 정보가 필요하다. 본 논문에서는 신뢰성있는 딥러닝 기반 데이터 분석 모델을 참조하기위한 딥러닝 기술 언어를 제안한다. 제안하는 기술 언어는 신뢰성 있는 딥러닝 모델을 개발하는데 필요한 학습데이터셋, 개발 환경 및 설정 등의 정보와더불어 딥러닝 모델의 계층구조를 표현할 수 있다. 제안하는 딥러닝 기술 언어를 이용하여 개발자는 지능형 교통 시스템에서 참조하는 분석 모델의정확도를 검증할 수 있다. 실험에서는 제안하는 언어의 유효성을 검증하기 위해, 번호판 인식 모델을 중심으로 딥러닝 기술 문서의 적용과정을 보인다With the recent advancements of deep learning, companies such as smart home, healthcare, and intelligent transportation systems areutilizing its functionality to provide high-quality services for vehicle detection, emergency situation detection, and controlling energyconsumption. To provide reliable services in such sensitive systems, deep learning models are required to have high accuracy. In orderto develop a deep learning model for analyzing previously mentioned services, developers should utilize the state of the art deep learningmodels that have already been verified for higher accuracy. The developers can verify the accuracy of the referenced model by validatingthe model on the dataset. For this validation, the developer needs structural information to document and apply deep learning models,including metadata such as learning dataset, network architecture, and development environments. In this paper, we propose a descriptionlanguage that represents the network architecture of the deep learning model along with its metadata that are necessary to develop adeep learning model. Through the proposed description language, developers can easily verify the accuracy of the referenced deep learningmodel. Our experiments demonstrate the application scenario of a deep learning description document that focuses on the license platerecognition for the detection of illegally parked vehicles.Trusted Deep Learning, Model Reference, Deep Learning Description Language, Traffic Situation Analysis Model"
232,딥러닝 분석을 이용한 중국 역내·외 위안화 변동성 예측,이우식 /Lee Woosik,2016,한국데이터정보과학회,"2008년 글로벌 금융위기 이후 중국은 위안화 국제화의 점진적 추진을 진행하면서 중국상하이 외환시장과 중국홍콩 외환시장에서 거래되는 통화인 역내위안화와 역외위안화를 형성시켰다. 본 연구는 위안화 국제화와 점진적인 중국 자본계정 개방에 따라 급변하는 외환시장상황의 변동성을 정확하게 파악하기 위해서 GARCH모형 (일반화된 자기회귀 조건부이분산성모형)에 다단계인공신경망을 결합한 MLP-GARCH 모형과 GARCH모형과 기계학습의 일종인 딥러닝 (deep learning)을 통합한 DL-GARCH을 가지고 위안화 변동성예측을 비교 실험과 분석을 하였다. 비교분석 결과 DLGARCH 모형은 MLP-GARCH보다 모형 위안화 역내·외 환율변동성 예측 면에서 더욱 더 개선된예측값을 제공하였다. 그래서 이분산시계열모형을 딥러닝과 결합한 DL-GARCH 모형은 시계열의환율 변동성 예측 문제에 딥러닝을 응용할 수 있음을 확인하였다. 향후 이분산시계열과 결합된 딥러닝 모형은 다른 금융시계열 데이터에 응용하여 그 일반화 가능성을 높일 수 있을 것이다.The People’s Republic of China has vigorously been pursuing the internationalization of the Chinese Yuan or Renminbi after the financial crisis of 2008. In this view, an abrupt increase of use of the Chinese Yuan in the onshore and offshore markets are important milestones to be one of important currencies. One of the most frequently used methods to forecast volatility is GARCH model. Since a prediction error of the GARCH model has been reported quite high, a lot of efforts have been made to improve forecasting capability of the GARCH model. In this paper, we have proposed MLPGARCH and a DL-GARCH by employing Artificial Neural Network to the GARCH. In an application to forecasting Chinese Yuan volatility, we have successfully shown their overall outperformance in forecasting over the GARCH.Deep learning, DL-GARCH, GARCH, The Chinese Yuan, volatility."
233,딥러닝 기법을 이용한 내일강수 예측,하지훈,2016,한국지능시스템학회,"정확한 강수예측을 위해서는 예측인자 선정과 예측방법에 대한 선택이 매우 중요하다. 최근에는 강수예측 방법으로기계학습 기법이 많이 사용되고 있으며, 그 중에서도 특히 인공신경망을 사용한 강수예측 방법은 좋은 성능을 보였다. 본논문에서는 딥러닝 기법 중 하나인 DBN(deep belief network)를 이용한 새로운 강수예측 방법을 제안한다. DBN는 비지도사전 학습을 통해 초기 가중치를 설정하여 기존 인공신경망의 문제점을 보완한다. 예측인자로는 기온, 전일-전주 강수일, 태양과 달 궤도 관련 자료를 선정하였다. 기온과 전일-전주 강수일은 서울에서의 1974년부터 2013년까지 총 40년간의AWS(automatic weather system) 관측 자료를 사용하였고, 태양과 달의 궤도 관련 자료는 서울을 중심으로 계산한 결과를사용하였다. 전체 기간에서 일부는 학습 자료로 사용하여 예측모델을 생성하였고, 나머지를 생성한 모델의 검증 자료로사용하였다. 모델 검증 결과로 나온 예측값들은 확률값을 가지며 임계치를 이용하여 강수유무를 판별하였다. 강수 정확도의척도로 양분예보기법 중 CSI(critical successive index)와 Bias(frequency bias)를 계산하였다. 이를 통해 DBN와 MLP(multilayer perceptron)의 성능을 비교한 결과 DBN의 강수 예측 정확도가 높았고, 수행속도 또한 2배 이상 빨랐다.For accurate precipitation forecasts the choice of weather faoctrs and prediction method is very important. Recently, machinele arning has been widely used for forecasting precipitation, and artificial neural network, one of machine learning techniques, showed good performance. In this paper, we suggest a new method for forecasting precipitation using DBN, one of deep learning techniques.
DBN has an advantage that initial weights are set by unsupervised learning, so this compensates for the defects of artificial neural networks. We used past precipitation, temperature, and the parameters of the sun and moon's motion as features for forecasting precipitation. The dataset consists of observation data which had been measured for 40 years from AWS in Seoul. Experiments were based on 8-fold cross validation. As a result of estimation, we got probabilities of test dataset, so threshold was used for the decision of precipitation. CSI and Bias were used for indicating the precision of precipitation. Our experimental results showed that DBN performed better than MLP.Deep learning, Deep belief network, Precipitation, Forecast"
234,딥러닝 신경망모형을 이용한 실시간 교통정보수집 연구,김대현 /KIM DAE HYON,2016,사단법인 인문사회과학기술융합학회,"비디오 기반의 실시간 교통정보수집 시스템은 자율주행을 포함한 지능형교통체계의 주요 서브시스템중 하나이며, 우수한 예측(추정)력 때문에 핵심 알고리즘으로 인공신경망 모형이 주로 사용되고 있다. 특히, 최근 과학 학술지 네이처에 구글 딥마인드의 알파고가 공개되면서 딥러닝과 인공지능에 대한 전 세계적인 관심이 집중되고 있다. 최근까지 가장 널리 사용된 인공신경망모형은 역전파 모형이었으나 최근 컴퓨터 하드웨어 및 알고리즘의 새로운 개발과 더불어 딥러닝 알고리즘을 이용한 딥인공신경망 모형에 대한 관심이 높아지고 있다. 그러나 아직 딥러닝 인공신경망에 대한 연구는 초기단계에 있는 실정이며 적용분야도 극히 한정되어 있다. 본 연구에서는 실시간 도로교통정보 수집을 위한 영상처리분야에서 딥러닝 알고리즘을 활용하고 실험결과를 통하여 향후 교통정보수집‧분석 분야에서의 딥러닝 인공신경망 모형의 적용가능성을 살펴보고자 한다. 특히, 본 연구에서 도출한 딥러닝 인공신경망모형의 실험결과를 기존 역전파 모형에 의한 실험결과와 비교하여 모형의 우수성을 비교‧평가하고자 한다.Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Normally, artificial neural networks have been used in real-time applications for collecting traffic information because of their remarkable performance in terms of predictive accuracy and real-time computing. Especially, the recent publication of a paper in the journal Nature and the success of Google DeepMind’s world-beating Go program AlphaGo caused significant increases of the public interest in artificial intelligence. Even though various types of learning models have been proposed, the Backpropagation has been the most popular neural network model. However, the Deep learning neural network model has attracted much interest recently. In this study, the Deep leaning neural network will be applied in image processing techniques for the real-time traffic information collection to investigate the performance of Deep learning algorithm in traffic engineering fields. In addition, the experimental results from Deep learing algorithm will be compared with the results from Backpropagation model to evaluate and propose better performing model.Intelligent Transportation Systems (ITS), artificial neural networks, Deep learning, traffic information, image processing"
235,딥러닝의 모형과 응용사례,안성만 /SungMahn Ahn,2016,한국지능정보시스템학회,"딥러닝은 인공신경망(neural network)이라는 인공지능분야의 모형이 발전된 형태로서, 계층구조로 이루어진 인공신경망의 내부계층(hidden layer)이 여러 단계로 이루어진 구조이다. 딥러닝에서의 주요 모형은 합성곱신경망(convolutional neural network), 순환신경망(recurrent neural network), 그리고 심층신뢰신경망(deep belief network)의 세가지라고 할 수있다. 그 중에서 현재 흥미로운 연구가 많이 발표되어서 관심이 집중되고 있는 모형은 지도학습(supervised learning)모형인 처음 두 개의 모형이다. 따라서 본 논문에서는 지도학습모형의 가중치를 최적화하는 기본적인 방법인 오류역전파 알고리즘을 살펴본 뒤에 합성곱신경망과 순환신경망의 구조와 응용사례 등을 살펴보고자 한다. 본문에서 다루지 않은 모형인 심층신뢰신경망은 아직까지는 합성곱신경망 이나 순환신경망보다는 상대적으로 주목을 덜 받고 있다. 그러나 심층신뢰신경망은 CNN이나 RNN과는 달리 비지도학습(unsupervised learning)모형이며, 사람이나 동물은 관찰을 통해서 스스로 학습한다는 점에서 궁극적으로는 비지도학습모형이 더 많이 연구되어야 할 주제가 될 것이다.Deep learning model is a kind of neural networks that allows multiple hidden layers. There are various deep learning architectures such as convolutional neural networks, deep belief networks and recurrent neural networks. Those have been applied to fields like computer vision, automatic speech recognition, natural language processing, audio recognition and bioinformatics where they have been shown to produce state-of-the-art results on various tasks. Among those architectures, convolutional neural networks and recurrent neural networks are classified as the supervised learning model. And in recent years, those supervised learning models have gained more popularity than unsupervised learning models such as deep belief networks, because supervised learning models have shown fashionable applications in such fields mentioned above.
Deep learning models can be trained with backpropagation algorithm. Backpropagation is an abbreviation for “backward propagation of errors” and a common method of training artificial neural networks used in conjunction with an optimization method such as gradient descent. The method calculates the gradient of an error function with respect to all the weights in the network. The gradient is fed to the optimization method which in turn uses it to update the weights, in an attempt to minimize the error function.
Convolutional neural networks use a special architecture which is particularly well-adapted to classify images. Using this architecture makes convolutional networks fast to train. This, in turn, helps us train deep, muti-layer networks, which are very good at classifying images. These days, deep convolutional networks are used in most neural networks for image recognition. Convolutional neural networks use three basic ideas: local receptive fields, shared weights, and pooling. By local receptive fields, we mean that each neuron in the first(or any) hidden layer will be connected to a small region of the input(or previous layer’s) neurons. Shared weights mean that we’re going to use the same weights and bias for each of the local receptive field. This means that all the neurons in the hidden layer detect exactly the same feature, just at different locations in the input image. In addition to the convolutional layers just described, convolutional neural networks also contain pooling layers. Pooling layers are usually used immediately after convolutional layers. What the pooling layers do is to simplify the information in the output from the convolutional layer.
Recent convolutional network architectures have 10 to 20 hidden layers and billions of connections between units. Training deep learning networks has taken weeks several years ago, but thanks to progress in GPU and algorithm enhancement, training time has reduced to several hours.
Neural networks with time-varying behavior are known as recurrent neural networks or RNNs. A recurrent neural network is a class of artificial neural network where connections between units form a directed cycle. This creates an internal state of the network which allows it to exhibit dynamic temporal behavior. Unlike feedforward neural networks, RNNs can use their internal memory to process arbitrary sequences of inputs. Early RNN models turned out to be very difficult to train, harder even than deep feedforward networks. The reason is the unstable gradient problem such as vanishing gradient and exploding gradient. The gradient can get smaller and smaller as it is propagated back through layers. This makes learning in early layers extremely slow. The problem actually gets worse in RNNs, since gradients aren’t just propagated backward through layers, they’re propagated backward through time. If the network runs for a long time, that can make the gradient extremely unstable and hard to learn from. It has been possible to incorporate an idea known as long short-term memory units (LSTMs) into RNNs. LSTMs make it much easier to get good results when training RNNs, and many recent papers make use of LSTMs or related ideas.Deep learning, Convolutional neural networks, Recurrent neural networks, Error backpropagation algorithm"
236,딥러닝분석과 기술적 분석 지표를 이용한 한국 코스피주가지수 방향성 예측,이우식 /Lee Woosik,2017,한국데이터정보과학회,"2016년 3월 구글 (Google)의 바둑인공지능 알파고 (AlphaGo)가 이세돌 9단과의 바둑대결에서승리한 이후 다양한 분야에서 인공지능 사용에 대한 관심이 높아지고 있는 가운데 금융투자 분야에서도 인공지능과 투자자문 전문가의 합성어인 로보어드바이저 (Robo-Advisor)에 대한 관심이 높아지고 있다. 인공지능 (artificial intelligence)기반의 의사결정은 비용 절감은 물론 효과적인 의사결정을가능하게 한다는 점에서 큰 장점이 있다. 본 연구에서는 기술적 분석 (technical analysis) 지표와 딥러닝 (deep learning) 모형을 결합하여 한국 코스피 지수를 예측하는 모형을 개발하고 제시한 모형들의 예측력을 비교, 분석한다. 분석 결과 기술적 분석 지표에 딥러닝 알고리즘을 결합한 모형이 주가지수 방향성 예측 문제에 응용될 수 있음을 확인하였다. 향후 본 연구에서 제안된 기술적 분석 지표와딥러닝모형을 결합한 기법은 로보어드바이저서비스에 응용할 수 있는 일반화 가능성을 보여준다Since Google’s AlphaGo defeated a world champion of Go players in 2016, there have been many interests in the deep learning. In the financial sector, a Robo-Advisor using deep learning gains a significant attention, which builds and manages portfolios of financial instruments for investors.In this paper, we have proposed the a deep learning algorithm geared toward identification and forecast of the KOSPI index direction,and we also have compared the accuracy of the prediction.In an application of forecasting the financial market index direction, we have shown that the Robo-Advisor using deep learning has a significant effect on finance industry. The Robo-Advisor collects a massive data such as earnings statements, news reports and regulatory filings, analyzes those and recommends investors how to view market trends and identify the best time to purchase financial assets. On the other hand, the Robo-Advisor allows businesses to learn more about their customers, develop better marketing strategies, increase sales and decrease costs.Artificial intelligence, deep learning, FinTech, Robo-Advisor, technical analysis."
237,딥러닝의 효율적인 학습을 위한 학습데이터 선별 기법에 관한 연구,주경돈 /Gyoung-don Joo,2017,한국정보과학회,"빅데이터의 증가와 병렬 하드웨어 처리, 학습 알고리즘의 개선은 딥러닝의 발전을 가져왔으며 인공지능이 적용 가능한 영역을 획기적으로 확장시키고 있다. 하지만 이러한 대규모의 학습 데이터에 기반한 딥러닝은 학습시간이 증가되고, 더 나아가 학습 데이터의 구축을 위한 레이블링(labeling) 비용이 데이터 수에 비례하여 증가하게 된다. 따라서 본 논문에서는, 딥러닝의 정확도를 최대한 유지하면서 필요한 학습 데이터의 수를 줄일수 있도록, 전체 데이터에서 제한된 수의 학습 데이터의 후보를 선별하는 기법에 대하여 연구한다. 특히 기존의 능동 학습에 기반한 기법들이 학습 데이터의 수가 매우 적어질 경우 딥러닝의 정확도가 급격히 떨어지는 문제를 해결할 수 있는 새로운 기법을 제안하였으며, 제안된 기법으로 선별된 학습 데이터는 그 수의 변화에 대해 딥러닝이 일관되게 경쟁력있는 정확도를 가짐을 다양한 실험을 통하여 검증하였다.The increase of big data, parallel hardware processing and enhanced learning algorithms have led to the development of deep learning and are dramatically expanding the domain of applications based on artificial intelligence. However, such large-scale training data extends the learning time, and furthermore, the labeling cost for building the training set increases in proportion to the volume of data. Therefore, in this paper, we study how to select a limited number of candidates for training set from given data so as to reduce the number of training set while maintaining the accuracy of deep learning. Especially, we found that, with the training set selected by the previous work, the accuracy of deep learning drastically decreases when the number of training set is very small and so proposed new methods to resolve this fall-off in accuracy. Through experiments, we showed that deep learning consistently has the competitive accuracy with the training set selected by the proposed method regardless of the size of training set.Deep Learning, Train Set Selection, Active Learning, Gamma distribution"
238,딥러닝을 이용한 범용적 스테그아날리시스,김현재 /Hyunjae Kim,2017,한국정보과학회,"스테그아날리시스(Steganalysis)란 이미지 등 일반적인 자료에 암호화된 정보를 은닉하는 스테가노그래피(Steganography)에 대한 검출 및 분석 방법으로, 기계학습 기반 방법론을 포함한다. 기존 기계학습 기반 스테그아날리시스는 영상(Image)의 특징(Feature) 추출 및 모델링에 기반하며, 최근 딥러닝(Deep Learning)의 적용으로 검출 정확도가 큰 폭으로 향상되었다. 하지만 현존하는 스테그아날리시스 모델은 단일 스테가노그래피 기법에 대해 국한되어 있어 학습에 사용되지 않은 스테고(Stego) 이미지의 경우 검출이 불가능한 결정적 한계를 가진다. 본 연구에서는 다양한 스테가노그래피 기법으로 생성된 스테고이미지에 딥러닝을 적용하여 스테그아날리시스를 학습하는 범용적 모델을 제안한다. 다양한 실험을 통해 제안 기법의 효용성 및 가능성을 확인하고, 범용적 스테그아날리시스 모델이 각각에 특화된 검출 기법과 유사한 정확도로 스테고 이미지를 검출할 수 있음을 보인다.Steganalysis is to detect information hidden by steganography inside general data such as images. There are stegoanalysis techniques that use machine learning (ML). Existing ML approaches to steganalysis are based on extracting features from stego images and modeling them.
Recently deep learning-based methodologies have shown significant improvements in detection accuracy. However, all the existing methods, including deep learning-based ones, have a critical limitation in that they can only detect stego images that are created by a specific steganography method. In this paper, we propose a generalized steganalysis method that can model multiple types of stego images using deep learning. Through various experiments, we confirm the effectiveness of our approach and envision directions for future research. In particular, we show that our method can detect each type of steganography with the same level of accuracy as that of a steganalysis method dedicated to that type of steganography, thereby demonstrating the general applicability of our approach to multiple types of stego images.steganalysis, steganography, deep learning, generalized steganalysis model"
239,"딥러닝 프레임워크의 비교: 티아노, 텐서플로, CNTK를 중심으로",정여진 /Yeojin Chung,2017,한국지능정보시스템학회,"딥러닝 프레임워크의 대표적인 기능으로는 ‘자동미분’과 ‘GPU의 활용’ 등을 들 수 있다. 본 논문은 파이썬의 라이브러리 형태로 사용 가능한 프레임워크 중에서 구글의 텐서플로와 마이크로소프트의 CNTK, 그리고 텐서 플로의 원조라고 할 수 있는 티아노를 비교하였다. 본문에서는 자동미분의 개념과 GPU의 활용형태를 간단히 설명하고, 그 다음에 logistic regression을 실행하는 예를 통하여 각 프레임워크의 문법을 알아본 뒤에, 마지막으로 대표적인 딥러닝 응용인 CNN의 예제를 실행시켜보고 코딩의 편의성과 실행속도 등을 확인해 보았다. 그 결과, 편의성의 관점에서 보면 티아노가 가장 코딩 하기가 어렵고, CNTK와 텐서플로는 많은 부분이 비슷하게 추상화 되어 있어서 코딩이 비슷하지만 가중치와 편향을 직접 정의하느냐의 여부에서 차이를 보였다. 그리고 각프레임워크의 실행속도에 대한 평가는 ‘큰 차이는 없다’는 것이다. 텐서플로는 티아노에 비하여 속도가 느리다는 평가가 있어왔는데, 본 연구의 실험에 의하면, 비록 CNN 모형에 국한되었지만, 텐서플로가 아주 조금이지만빠른 것으로 나타났다. CNTK의 경우에도, 비록 실험환경이 달랐지만, 실험환경의 차이에 의한 속도의 차이의 편차범위 이내에 있는 것으로 판단이 되었다. 본 연구에서는 세 종류의 딥러닝 프레임워크만을 살펴보았는데,위키피디아에 따르면 딥러닝 프레임워크의 종류는 12가지가 있으며, 각 프레임워크의 특징을 15가지 속성으로구분하여 차이를 특정하고 있다. 그 많은 속성 중에서 사용자의 입장에서 볼 때 중요한 속성은 어떤 언어(파이썬, C++, Java, 등)로 사용가능한지, 어떤 딥러닝 모형에 대한 라이브러리가 잘 구현되어 있는지 등일 것이다. 그리고 사용자가 대규모의 딥러닝 모형을 구축한다면, 다중 GPU 혹은 다중 서버를 지원하는지의 여부도 중요할 것이다. 또한 딥러닝 모형을 처음 학습하는 경우에는 사용설명서가 많은지 예제 프로그램이 많은지 여부도 중요한 기준이 될 것이다.The deep learning framework is software designed to help develop deep learning models. Some of its important functions include “automatic differentiation” and “utilization of GPU”. The list of popular deep learning framework includes Caffe (BVLC) and Theano (University of Montreal). And recently, Microsoft's deep learning framework, Microsoft Cognitive Toolkit, was released as open-source license, following Google’s Tensorflow a year earlier. The early deep learning frameworks have been developed mainly for research at universities. Beginning with the inception of Tensorflow, however, it seems that companies such as Microsoft and Facebook have started to join the competition of framework development. Given the trend, Google and other companies are expected to continue investing in the deep learning framework to bring forward the initiative in the artificial intelligence business. From this point of view, we think it is a good time to compare some of deep learning frameworks. So we compare three deep learning frameworks which can be used as a Python library. Those are Google's Tensorflow, Microsoft’s CNTK, and Theano which is sort of a predecessor of the preceding two.
The most common and important function of deep learning frameworks is the ability to perform automatic differentiation. Basically all the mathematical expressions of deep learning models can be represented as computational graphs, which consist of nodes and edges. Partial derivatives on each edge of a computational graph can then be obtained. With the partial derivatives, we can let software compute differentiation of any node with respect to any variable by utilizing chain rule of Calculus.
First of all, the convenience of coding is in the order of CNTK, Tensorflow, and Theano. The criterion is simply based on the lengths of the codes and the learning curve and the ease of coding are not the main concern. According to the criteria, Theano was the most difficult to implement with, and CNTK and Tensorflow were somewhat easier. With Tensorflow, we need to define weight variables and biases explicitly. The reason that CNTK and Tensorflow are easier to implement with is that those frameworks provide us with more abstraction than Theano. We, however, need to mention that low-level coding is not always bad. It gives us flexibility of coding. With the low-level coding such as in Theano, we can implement and test any new deep learning models or any new search methods that we can think of.
The assessment of the execution speed of each framework is that there is not meaningful difference. According to the experiment, execution speeds of Theano and Tensorflow are very similar, although the experiment was limited to a CNN model. In the case of CNTK, the experimental environment was not maintained as the same. The code written in CNTK has to be run in PC environment without GPU where codes execute as much as 50 times slower than with GPU. But we concluded that the difference of execution speed was within the range of variation caused by the different hardware setup.
In this study, we compared three types of deep learning framework: Theano, Tensorflow, and CNTK. According to Wikipedia, there are 12 available deep learning frameworks. And 15 different attributes differentiate each framework. Some of the important attributes would include interface language (Python, C ++, Java, etc.) and the availability of libraries on various deep learning models such as CNN, RNN, DBN, and etc. And if a user implements a large scale deep learning model, it will also be important to support multiple GPU or multiple servers. Also, if you are learning the deep learning model, it would also be important if there are enough examples and references.deep learning framework, Theano, TensorFlow, CNTK, computational graph, CIFAR-10"
240,딥러닝을 이용한 주변 무선단말 파악방안,이웅섭 /Lee， Woong-sup,2018,한국정보통신학회,"최근 단말-대-단말(Device-to-device, D2D) 통신기술이 차세대 무선통신시스템의 핵심기술로 큰 관심을 받고 있다. 이러한 단말간 통신에서는 자신의 주변에 어떠한 단말이 있는지 파악하는 주변단말 탐색(Neighbor discovery)이매우 중요하다. 본 논문에서는 최근 큰 관심을 받고 있는 딥러닝(Deep learning) 기술을 활용하여 단말간 통신에서 주변단말을 파악하는 방안에 대해서 제안한다. 제안 방안은 기존의 방안과 달리 무선채널의 공간적 연관성을 이용하여단말간의 신호 전송 없이 단말이 기지국으로 전송하는 상향링크 파일럿 신호를 기반으로 주변 단말을 찾고 따라서기존의 방식에 비해 신호전송 복잡도(signaling complexity)를 크게 줄일 수 있다. 또한 제안 방안에서는 떨어져 있는거리에 따라서 주변 단말을 분류 가능하여 기존 방안에 비해서 좀 더 세밀한 단말 탐색이 가능하다. 마지막으로 본 논문에서는 tensorflow를 이용한 컴퓨터 시뮬레이션을 통해 제안 방안의 성능을 검증하였다.Recently, the device-to-device (D2D) communication has been conceived as the key technology for the next-generationmobile communication systems. The neighbor discovery in which the nearby users are found, is essential for the properoperation of the D2D communication. In this paper, we propose new neighbor discovery scheme based on deep learningtechnology which has gained a lot of attention recently. In the proposed scheme, the neighboring users can be foundusing the uplink pilot transmission of users only, unlike conventional neighbor discovery schemes in which direct pilotcommunication among users is required, such that the signaling overhead can be greatly reduced in our proposed scheme.
Moreover, the neighbors with different proximity can also be classified accordingly which enables more accurate neighbordiscovery compared to the conventional schemes. The performance of our proposed scheme is verified through thetensorflow-based computer simulations.
키워드 : 딥러닝, 주변단말 탐색, 단말-대-단말 통신Deep learning, Neighbor discovery, Device-to-device communication, Wireless channel correlation"
241,신뢰성있는 딥러닝 기반 분석 모델을 참조하기 위한 딥러닝 기술 언어,문종혁 /Jong Hyeok Mun,2021,한국정보처리학회,"최근 딥러닝은 하드웨어 성능이 향상됨에 따라 자연어 처리, 영상 인식 등의 다양한 기술에 접목되어 활용되고 있다. 이러한 기술들을 활용해 지능형교통 시스템(ITS), 스마트홈, 헬스케어 등의 산업분야에서 데이터를 분석하여 고속도로 속도위반 차량 검출, 에너지 사용량 제어, 응급상황 등과 같은고품질의 서비스를 제공하며, 고품질의 서비스를 제공하기 위해서는 정확도가 향상된 딥러닝 모델이 적용되어야 한다. 이를 위해 서비스 환경의 데이터를분석하기 위한 딥러닝 모델을 개발할 때, 개발자는 신뢰성이 검증된 최신의 딥러닝 모델을 적용할 수 있어야 한다. 이는 개발자가 참조하는 딥러닝 모델에적용된 학습 데이터셋의 정확도를 측정하여 검증할 수 있다. 이러한 검증을 위해서 개발자는 학습 데이터셋, 딥러닝의 계층구조 및 개발 환경 등과 같은내용을 포함하는 딥러닝 모델을 문서화하여 적용하기 위한 구조적인 정보가 필요하다. 본 논문에서는 신뢰성있는 딥러닝 기반 데이터 분석 모델을 참조하기위한 딥러닝 기술 언어를 제안한다. 제안하는 기술 언어는 신뢰성 있는 딥러닝 모델을 개발하는데 필요한 학습데이터셋, 개발 환경 및 설정 등의 정보와더불어 딥러닝 모델의 계층구조를 표현할 수 있다. 제안하는 딥러닝 기술 언어를 이용하여 개발자는 지능형 교통 시스템에서 참조하는 분석 모델의정확도를 검증할 수 있다. 실험에서는 제안하는 언어의 유효성을 검증하기 위해, 번호판 인식 모델을 중심으로 딥러닝 기술 문서의 적용과정을 보인다With the recent advancements of deep learning, companies such as smart home, healthcare, and intelligent transportation systems areutilizing its functionality to provide high-quality services for vehicle detection, emergency situation detection, and controlling energyconsumption. To provide reliable services in such sensitive systems, deep learning models are required to have high accuracy. In orderto develop a deep learning model for analyzing previously mentioned services, developers should utilize the state of the art deep learningmodels that have already been verified for higher accuracy. The developers can verify the accuracy of the referenced model by validatingthe model on the dataset. For this validation, the developer needs structural information to document and apply deep learning models,including metadata such as learning dataset, network architecture, and development environments. In this paper, we propose a descriptionlanguage that represents the network architecture of the deep learning model along with its metadata that are necessary to develop adeep learning model. Through the proposed description language, developers can easily verify the accuracy of the referenced deep learningmodel. Our experiments demonstrate the application scenario of a deep learning description document that focuses on the license platerecognition for the detection of illegally parked vehicles.Trusted Deep Learning, Model Reference, Deep Learning Description Language, Traffic Situation Analysis Model"
242,딥러닝 분석을 이용한 중국 역내·외 위안화 변동성 예측,이우식 /Lee Woosik,2016,한국데이터정보과학회,"2008년 글로벌 금융위기 이후 중국은 위안화 국제화의 점진적 추진을 진행하면서 중국상하이 외환시장과 중국홍콩 외환시장에서 거래되는 통화인 역내위안화와 역외위안화를 형성시켰다. 본 연구는 위안화 국제화와 점진적인 중국 자본계정 개방에 따라 급변하는 외환시장상황의 변동성을 정확하게 파악하기 위해서 GARCH모형 (일반화된 자기회귀 조건부이분산성모형)에 다단계인공신경망을 결합한 MLP-GARCH 모형과 GARCH모형과 기계학습의 일종인 딥러닝 (deep learning)을 통합한 DL-GARCH을 가지고 위안화 변동성예측을 비교 실험과 분석을 하였다. 비교분석 결과 DLGARCH 모형은 MLP-GARCH보다 모형 위안화 역내·외 환율변동성 예측 면에서 더욱 더 개선된예측값을 제공하였다. 그래서 이분산시계열모형을 딥러닝과 결합한 DL-GARCH 모형은 시계열의환율 변동성 예측 문제에 딥러닝을 응용할 수 있음을 확인하였다. 향후 이분산시계열과 결합된 딥러닝 모형은 다른 금융시계열 데이터에 응용하여 그 일반화 가능성을 높일 수 있을 것이다.The People’s Republic of China has vigorously been pursuing the internationalization of the Chinese Yuan or Renminbi after the financial crisis of 2008. In this view, an abrupt increase of use of the Chinese Yuan in the onshore and offshore markets are important milestones to be one of important currencies. One of the most frequently used methods to forecast volatility is GARCH model. Since a prediction error of the GARCH model has been reported quite high, a lot of efforts have been made to improve forecasting capability of the GARCH model. In this paper, we have proposed MLPGARCH and a DL-GARCH by employing Artificial Neural Network to the GARCH. In an application to forecasting Chinese Yuan volatility, we have successfully shown their overall outperformance in forecasting over the GARCH.Deep learning, DL-GARCH, GARCH, The Chinese Yuan, volatility."
243,딥러닝 기법을 이용한 내일강수 예측,하지훈,2016,한국지능시스템학회,"정확한 강수예측을 위해서는 예측인자 선정과 예측방법에 대한 선택이 매우 중요하다. 최근에는 강수예측 방법으로기계학습 기법이 많이 사용되고 있으며, 그 중에서도 특히 인공신경망을 사용한 강수예측 방법은 좋은 성능을 보였다. 본논문에서는 딥러닝 기법 중 하나인 DBN(deep belief network)를 이용한 새로운 강수예측 방법을 제안한다. DBN는 비지도사전 학습을 통해 초기 가중치를 설정하여 기존 인공신경망의 문제점을 보완한다. 예측인자로는 기온, 전일-전주 강수일, 태양과 달 궤도 관련 자료를 선정하였다. 기온과 전일-전주 강수일은 서울에서의 1974년부터 2013년까지 총 40년간의AWS(automatic weather system) 관측 자료를 사용하였고, 태양과 달의 궤도 관련 자료는 서울을 중심으로 계산한 결과를사용하였다. 전체 기간에서 일부는 학습 자료로 사용하여 예측모델을 생성하였고, 나머지를 생성한 모델의 검증 자료로사용하였다. 모델 검증 결과로 나온 예측값들은 확률값을 가지며 임계치를 이용하여 강수유무를 판별하였다. 강수 정확도의척도로 양분예보기법 중 CSI(critical successive index)와 Bias(frequency bias)를 계산하였다. 이를 통해 DBN와 MLP(multilayer perceptron)의 성능을 비교한 결과 DBN의 강수 예측 정확도가 높았고, 수행속도 또한 2배 이상 빨랐다.For accurate precipitation forecasts the choice of weather faoctrs and prediction method is very important. Recently, machinele arning has been widely used for forecasting precipitation, and artificial neural network, one of machine learning techniques, showed good performance. In this paper, we suggest a new method for forecasting precipitation using DBN, one of deep learning techniques.
DBN has an advantage that initial weights are set by unsupervised learning, so this compensates for the defects of artificial neural networks. We used past precipitation, temperature, and the parameters of the sun and moon's motion as features for forecasting precipitation. The dataset consists of observation data which had been measured for 40 years from AWS in Seoul. Experiments were based on 8-fold cross validation. As a result of estimation, we got probabilities of test dataset, so threshold was used for the decision of precipitation. CSI and Bias were used for indicating the precision of precipitation. Our experimental results showed that DBN performed better than MLP.Deep learning, Deep belief network, Precipitation, Forecast"
244,딥러닝 신경망모형을 이용한 실시간 교통정보수집 연구,김대현 /KIM DAE HYON,2016,사단법인 인문사회과학기술융합학회,"비디오 기반의 실시간 교통정보수집 시스템은 자율주행을 포함한 지능형교통체계의 주요 서브시스템중 하나이며, 우수한 예측(추정)력 때문에 핵심 알고리즘으로 인공신경망 모형이 주로 사용되고 있다. 특히, 최근 과학 학술지 네이처에 구글 딥마인드의 알파고가 공개되면서 딥러닝과 인공지능에 대한 전 세계적인 관심이 집중되고 있다. 최근까지 가장 널리 사용된 인공신경망모형은 역전파 모형이었으나 최근 컴퓨터 하드웨어 및 알고리즘의 새로운 개발과 더불어 딥러닝 알고리즘을 이용한 딥인공신경망 모형에 대한 관심이 높아지고 있다. 그러나 아직 딥러닝 인공신경망에 대한 연구는 초기단계에 있는 실정이며 적용분야도 극히 한정되어 있다. 본 연구에서는 실시간 도로교통정보 수집을 위한 영상처리분야에서 딥러닝 알고리즘을 활용하고 실험결과를 통하여 향후 교통정보수집‧분석 분야에서의 딥러닝 인공신경망 모형의 적용가능성을 살펴보고자 한다. 특히, 본 연구에서 도출한 딥러닝 인공신경망모형의 실험결과를 기존 역전파 모형에 의한 실험결과와 비교하여 모형의 우수성을 비교‧평가하고자 한다.Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Normally, artificial neural networks have been used in real-time applications for collecting traffic information because of their remarkable performance in terms of predictive accuracy and real-time computing. Especially, the recent publication of a paper in the journal Nature and the success of Google DeepMind’s world-beating Go program AlphaGo caused significant increases of the public interest in artificial intelligence. Even though various types of learning models have been proposed, the Backpropagation has been the most popular neural network model. However, the Deep learning neural network model has attracted much interest recently. In this study, the Deep leaning neural network will be applied in image processing techniques for the real-time traffic information collection to investigate the performance of Deep learning algorithm in traffic engineering fields. In addition, the experimental results from Deep learing algorithm will be compared with the results from Backpropagation model to evaluate and propose better performing model.Intelligent Transportation Systems (ITS), artificial neural networks, Deep learning, traffic information, image processing"
245,딥러닝의 모형과 응용사례,안성만 /SungMahn Ahn,2016,한국지능정보시스템학회,"딥러닝은 인공신경망(neural network)이라는 인공지능분야의 모형이 발전된 형태로서, 계층구조로 이루어진 인공신경망의 내부계층(hidden layer)이 여러 단계로 이루어진 구조이다. 딥러닝에서의 주요 모형은 합성곱신경망(convolutional neural network), 순환신경망(recurrent neural network), 그리고 심층신뢰신경망(deep belief network)의 세가지라고 할 수있다. 그 중에서 현재 흥미로운 연구가 많이 발표되어서 관심이 집중되고 있는 모형은 지도학습(supervised learning)모형인 처음 두 개의 모형이다. 따라서 본 논문에서는 지도학습모형의 가중치를 최적화하는 기본적인 방법인 오류역전파 알고리즘을 살펴본 뒤에 합성곱신경망과 순환신경망의 구조와 응용사례 등을 살펴보고자 한다. 본문에서 다루지 않은 모형인 심층신뢰신경망은 아직까지는 합성곱신경망 이나 순환신경망보다는 상대적으로 주목을 덜 받고 있다. 그러나 심층신뢰신경망은 CNN이나 RNN과는 달리 비지도학습(unsupervised learning)모형이며, 사람이나 동물은 관찰을 통해서 스스로 학습한다는 점에서 궁극적으로는 비지도학습모형이 더 많이 연구되어야 할 주제가 될 것이다.Deep learning model is a kind of neural networks that allows multiple hidden layers. There are various deep learning architectures such as convolutional neural networks, deep belief networks and recurrent neural networks. Those have been applied to fields like computer vision, automatic speech recognition, natural language processing, audio recognition and bioinformatics where they have been shown to produce state-of-the-art results on various tasks. Among those architectures, convolutional neural networks and recurrent neural networks are classified as the supervised learning model. And in recent years, those supervised learning models have gained more popularity than unsupervised learning models such as deep belief networks, because supervised learning models have shown fashionable applications in such fields mentioned above.
Deep learning models can be trained with backpropagation algorithm. Backpropagation is an abbreviation for “backward propagation of errors” and a common method of training artificial neural networks used in conjunction with an optimization method such as gradient descent. The method calculates the gradient of an error function with respect to all the weights in the network. The gradient is fed to the optimization method which in turn uses it to update the weights, in an attempt to minimize the error function.
Convolutional neural networks use a special architecture which is particularly well-adapted to classify images. Using this architecture makes convolutional networks fast to train. This, in turn, helps us train deep, muti-layer networks, which are very good at classifying images. These days, deep convolutional networks are used in most neural networks for image recognition. Convolutional neural networks use three basic ideas: local receptive fields, shared weights, and pooling. By local receptive fields, we mean that each neuron in the first(or any) hidden layer will be connected to a small region of the input(or previous layer’s) neurons. Shared weights mean that we’re going to use the same weights and bias for each of the local receptive field. This means that all the neurons in the hidden layer detect exactly the same feature, just at different locations in the input image. In addition to the convolutional layers just described, convolutional neural networks also contain pooling layers. Pooling layers are usually used immediately after convolutional layers. What the pooling layers do is to simplify the information in the output from the convolutional layer.
Recent convolutional network architectures have 10 to 20 hidden layers and billions of connections between units. Training deep learning networks has taken weeks several years ago, but thanks to progress in GPU and algorithm enhancement, training time has reduced to several hours.
Neural networks with time-varying behavior are known as recurrent neural networks or RNNs. A recurrent neural network is a class of artificial neural network where connections between units form a directed cycle. This creates an internal state of the network which allows it to exhibit dynamic temporal behavior. Unlike feedforward neural networks, RNNs can use their internal memory to process arbitrary sequences of inputs. Early RNN models turned out to be very difficult to train, harder even than deep feedforward networks. The reason is the unstable gradient problem such as vanishing gradient and exploding gradient. The gradient can get smaller and smaller as it is propagated back through layers. This makes learning in early layers extremely slow. The problem actually gets worse in RNNs, since gradients aren’t just propagated backward through layers, they’re propagated backward through time. If the network runs for a long time, that can make the gradient extremely unstable and hard to learn from. It has been possible to incorporate an idea known as long short-term memory units (LSTMs) into RNNs. LSTMs make it much easier to get good results when training RNNs, and many recent papers make use of LSTMs or related ideas.Deep learning, Convolutional neural networks, Recurrent neural networks, Error backpropagation algorithm"
246,딥러닝분석과 기술적 분석 지표를 이용한 한국 코스피주가지수 방향성 예측,이우식 /Lee Woosik,2017,한국데이터정보과학회,"2016년 3월 구글 (Google)의 바둑인공지능 알파고 (AlphaGo)가 이세돌 9단과의 바둑대결에서승리한 이후 다양한 분야에서 인공지능 사용에 대한 관심이 높아지고 있는 가운데 금융투자 분야에서도 인공지능과 투자자문 전문가의 합성어인 로보어드바이저 (Robo-Advisor)에 대한 관심이 높아지고 있다. 인공지능 (artificial intelligence)기반의 의사결정은 비용 절감은 물론 효과적인 의사결정을가능하게 한다는 점에서 큰 장점이 있다. 본 연구에서는 기술적 분석 (technical analysis) 지표와 딥러닝 (deep learning) 모형을 결합하여 한국 코스피 지수를 예측하는 모형을 개발하고 제시한 모형들의 예측력을 비교, 분석한다. 분석 결과 기술적 분석 지표에 딥러닝 알고리즘을 결합한 모형이 주가지수 방향성 예측 문제에 응용될 수 있음을 확인하였다. 향후 본 연구에서 제안된 기술적 분석 지표와딥러닝모형을 결합한 기법은 로보어드바이저서비스에 응용할 수 있는 일반화 가능성을 보여준다Since Google’s AlphaGo defeated a world champion of Go players in 2016, there have been many interests in the deep learning. In the financial sector, a Robo-Advisor using deep learning gains a significant attention, which builds and manages portfolios of financial instruments for investors.In this paper, we have proposed the a deep learning algorithm geared toward identification and forecast of the KOSPI index direction,and we also have compared the accuracy of the prediction.In an application of forecasting the financial market index direction, we have shown that the Robo-Advisor using deep learning has a significant effect on finance industry. The Robo-Advisor collects a massive data such as earnings statements, news reports and regulatory filings, analyzes those and recommends investors how to view market trends and identify the best time to purchase financial assets. On the other hand, the Robo-Advisor allows businesses to learn more about their customers, develop better marketing strategies, increase sales and decrease costs.Artificial intelligence, deep learning, FinTech, Robo-Advisor, technical analysis."
247,딥러닝의 효율적인 학습을 위한 학습데이터 선별 기법에 관한 연구,주경돈 /Gyoung-don Joo,2017,한국정보과학회,"빅데이터의 증가와 병렬 하드웨어 처리, 학습 알고리즘의 개선은 딥러닝의 발전을 가져왔으며 인공지능이 적용 가능한 영역을 획기적으로 확장시키고 있다. 하지만 이러한 대규모의 학습 데이터에 기반한 딥러닝은 학습시간이 증가되고, 더 나아가 학습 데이터의 구축을 위한 레이블링(labeling) 비용이 데이터 수에 비례하여 증가하게 된다. 따라서 본 논문에서는, 딥러닝의 정확도를 최대한 유지하면서 필요한 학습 데이터의 수를 줄일수 있도록, 전체 데이터에서 제한된 수의 학습 데이터의 후보를 선별하는 기법에 대하여 연구한다. 특히 기존의 능동 학습에 기반한 기법들이 학습 데이터의 수가 매우 적어질 경우 딥러닝의 정확도가 급격히 떨어지는 문제를 해결할 수 있는 새로운 기법을 제안하였으며, 제안된 기법으로 선별된 학습 데이터는 그 수의 변화에 대해 딥러닝이 일관되게 경쟁력있는 정확도를 가짐을 다양한 실험을 통하여 검증하였다.The increase of big data, parallel hardware processing and enhanced learning algorithms have led to the development of deep learning and are dramatically expanding the domain of applications based on artificial intelligence. However, such large-scale training data extends the learning time, and furthermore, the labeling cost for building the training set increases in proportion to the volume of data. Therefore, in this paper, we study how to select a limited number of candidates for training set from given data so as to reduce the number of training set while maintaining the accuracy of deep learning. Especially, we found that, with the training set selected by the previous work, the accuracy of deep learning drastically decreases when the number of training set is very small and so proposed new methods to resolve this fall-off in accuracy. Through experiments, we showed that deep learning consistently has the competitive accuracy with the training set selected by the proposed method regardless of the size of training set.Deep Learning, Train Set Selection, Active Learning, Gamma distribution"
248,딥러닝을 이용한 범용적 스테그아날리시스,김현재 /Hyunjae Kim,2017,한국정보과학회,"스테그아날리시스(Steganalysis)란 이미지 등 일반적인 자료에 암호화된 정보를 은닉하는 스테가노그래피(Steganography)에 대한 검출 및 분석 방법으로, 기계학습 기반 방법론을 포함한다. 기존 기계학습 기반 스테그아날리시스는 영상(Image)의 특징(Feature) 추출 및 모델링에 기반하며, 최근 딥러닝(Deep Learning)의 적용으로 검출 정확도가 큰 폭으로 향상되었다. 하지만 현존하는 스테그아날리시스 모델은 단일 스테가노그래피 기법에 대해 국한되어 있어 학습에 사용되지 않은 스테고(Stego) 이미지의 경우 검출이 불가능한 결정적 한계를 가진다. 본 연구에서는 다양한 스테가노그래피 기법으로 생성된 스테고이미지에 딥러닝을 적용하여 스테그아날리시스를 학습하는 범용적 모델을 제안한다. 다양한 실험을 통해 제안 기법의 효용성 및 가능성을 확인하고, 범용적 스테그아날리시스 모델이 각각에 특화된 검출 기법과 유사한 정확도로 스테고 이미지를 검출할 수 있음을 보인다.Steganalysis is to detect information hidden by steganography inside general data such as images. There are stegoanalysis techniques that use machine learning (ML). Existing ML approaches to steganalysis are based on extracting features from stego images and modeling them.
Recently deep learning-based methodologies have shown significant improvements in detection accuracy. However, all the existing methods, including deep learning-based ones, have a critical limitation in that they can only detect stego images that are created by a specific steganography method. In this paper, we propose a generalized steganalysis method that can model multiple types of stego images using deep learning. Through various experiments, we confirm the effectiveness of our approach and envision directions for future research. In particular, we show that our method can detect each type of steganography with the same level of accuracy as that of a steganalysis method dedicated to that type of steganography, thereby demonstrating the general applicability of our approach to multiple types of stego images.steganalysis, steganography, deep learning, generalized steganalysis model"
249,"딥러닝 프레임워크의 비교: 티아노, 텐서플로, CNTK를 중심으로",정여진 /Yeojin Chung,2017,한국지능정보시스템학회,"딥러닝 프레임워크의 대표적인 기능으로는 ‘자동미분’과 ‘GPU의 활용’ 등을 들 수 있다. 본 논문은 파이썬의 라이브러리 형태로 사용 가능한 프레임워크 중에서 구글의 텐서플로와 마이크로소프트의 CNTK, 그리고 텐서 플로의 원조라고 할 수 있는 티아노를 비교하였다. 본문에서는 자동미분의 개념과 GPU의 활용형태를 간단히 설명하고, 그 다음에 logistic regression을 실행하는 예를 통하여 각 프레임워크의 문법을 알아본 뒤에, 마지막으로 대표적인 딥러닝 응용인 CNN의 예제를 실행시켜보고 코딩의 편의성과 실행속도 등을 확인해 보았다. 그 결과, 편의성의 관점에서 보면 티아노가 가장 코딩 하기가 어렵고, CNTK와 텐서플로는 많은 부분이 비슷하게 추상화 되어 있어서 코딩이 비슷하지만 가중치와 편향을 직접 정의하느냐의 여부에서 차이를 보였다. 그리고 각프레임워크의 실행속도에 대한 평가는 ‘큰 차이는 없다’는 것이다. 텐서플로는 티아노에 비하여 속도가 느리다는 평가가 있어왔는데, 본 연구의 실험에 의하면, 비록 CNN 모형에 국한되었지만, 텐서플로가 아주 조금이지만빠른 것으로 나타났다. CNTK의 경우에도, 비록 실험환경이 달랐지만, 실험환경의 차이에 의한 속도의 차이의 편차범위 이내에 있는 것으로 판단이 되었다. 본 연구에서는 세 종류의 딥러닝 프레임워크만을 살펴보았는데,위키피디아에 따르면 딥러닝 프레임워크의 종류는 12가지가 있으며, 각 프레임워크의 특징을 15가지 속성으로구분하여 차이를 특정하고 있다. 그 많은 속성 중에서 사용자의 입장에서 볼 때 중요한 속성은 어떤 언어(파이썬, C++, Java, 등)로 사용가능한지, 어떤 딥러닝 모형에 대한 라이브러리가 잘 구현되어 있는지 등일 것이다. 그리고 사용자가 대규모의 딥러닝 모형을 구축한다면, 다중 GPU 혹은 다중 서버를 지원하는지의 여부도 중요할 것이다. 또한 딥러닝 모형을 처음 학습하는 경우에는 사용설명서가 많은지 예제 프로그램이 많은지 여부도 중요한 기준이 될 것이다.The deep learning framework is software designed to help develop deep learning models. Some of its important functions include “automatic differentiation” and “utilization of GPU”. The list of popular deep learning framework includes Caffe (BVLC) and Theano (University of Montreal). And recently, Microsoft's deep learning framework, Microsoft Cognitive Toolkit, was released as open-source license, following Google’s Tensorflow a year earlier. The early deep learning frameworks have been developed mainly for research at universities. Beginning with the inception of Tensorflow, however, it seems that companies such as Microsoft and Facebook have started to join the competition of framework development. Given the trend, Google and other companies are expected to continue investing in the deep learning framework to bring forward the initiative in the artificial intelligence business. From this point of view, we think it is a good time to compare some of deep learning frameworks. So we compare three deep learning frameworks which can be used as a Python library. Those are Google's Tensorflow, Microsoft’s CNTK, and Theano which is sort of a predecessor of the preceding two.
The most common and important function of deep learning frameworks is the ability to perform automatic differentiation. Basically all the mathematical expressions of deep learning models can be represented as computational graphs, which consist of nodes and edges. Partial derivatives on each edge of a computational graph can then be obtained. With the partial derivatives, we can let software compute differentiation of any node with respect to any variable by utilizing chain rule of Calculus.
First of all, the convenience of coding is in the order of CNTK, Tensorflow, and Theano. The criterion is simply based on the lengths of the codes and the learning curve and the ease of coding are not the main concern. According to the criteria, Theano was the most difficult to implement with, and CNTK and Tensorflow were somewhat easier. With Tensorflow, we need to define weight variables and biases explicitly. The reason that CNTK and Tensorflow are easier to implement with is that those frameworks provide us with more abstraction than Theano. We, however, need to mention that low-level coding is not always bad. It gives us flexibility of coding. With the low-level coding such as in Theano, we can implement and test any new deep learning models or any new search methods that we can think of.
The assessment of the execution speed of each framework is that there is not meaningful difference. According to the experiment, execution speeds of Theano and Tensorflow are very similar, although the experiment was limited to a CNN model. In the case of CNTK, the experimental environment was not maintained as the same. The code written in CNTK has to be run in PC environment without GPU where codes execute as much as 50 times slower than with GPU. But we concluded that the difference of execution speed was within the range of variation caused by the different hardware setup.
In this study, we compared three types of deep learning framework: Theano, Tensorflow, and CNTK. According to Wikipedia, there are 12 available deep learning frameworks. And 15 different attributes differentiate each framework. Some of the important attributes would include interface language (Python, C ++, Java, etc.) and the availability of libraries on various deep learning models such as CNN, RNN, DBN, and etc. And if a user implements a large scale deep learning model, it will also be important to support multiple GPU or multiple servers. Also, if you are learning the deep learning model, it would also be important if there are enough examples and references.deep learning framework, Theano, TensorFlow, CNTK, computational graph, CIFAR-10"
250,딥러닝을 이용한 주변 무선단말 파악방안,이웅섭 /Lee， Woong-sup,2018,한국정보통신학회,"최근 단말-대-단말(Device-to-device, D2D) 통신기술이 차세대 무선통신시스템의 핵심기술로 큰 관심을 받고 있다. 이러한 단말간 통신에서는 자신의 주변에 어떠한 단말이 있는지 파악하는 주변단말 탐색(Neighbor discovery)이매우 중요하다. 본 논문에서는 최근 큰 관심을 받고 있는 딥러닝(Deep learning) 기술을 활용하여 단말간 통신에서 주변단말을 파악하는 방안에 대해서 제안한다. 제안 방안은 기존의 방안과 달리 무선채널의 공간적 연관성을 이용하여단말간의 신호 전송 없이 단말이 기지국으로 전송하는 상향링크 파일럿 신호를 기반으로 주변 단말을 찾고 따라서기존의 방식에 비해 신호전송 복잡도(signaling complexity)를 크게 줄일 수 있다. 또한 제안 방안에서는 떨어져 있는거리에 따라서 주변 단말을 분류 가능하여 기존 방안에 비해서 좀 더 세밀한 단말 탐색이 가능하다. 마지막으로 본 논문에서는 tensorflow를 이용한 컴퓨터 시뮬레이션을 통해 제안 방안의 성능을 검증하였다.Recently, the device-to-device (D2D) communication has been conceived as the key technology for the next-generationmobile communication systems. The neighbor discovery in which the nearby users are found, is essential for the properoperation of the D2D communication. In this paper, we propose new neighbor discovery scheme based on deep learningtechnology which has gained a lot of attention recently. In the proposed scheme, the neighboring users can be foundusing the uplink pilot transmission of users only, unlike conventional neighbor discovery schemes in which direct pilotcommunication among users is required, such that the signaling overhead can be greatly reduced in our proposed scheme.
Moreover, the neighbors with different proximity can also be classified accordingly which enables more accurate neighbordiscovery compared to the conventional schemes. The performance of our proposed scheme is verified through thetensorflow-based computer simulations.
키워드 : 딥러닝, 주변단말 탐색, 단말-대-단말 통신Deep learning, Neighbor discovery, Device-to-device communication, Wireless channel correlation"
251,신뢰성있는 딥러닝 기반 분석 모델을 참조하기 위한 딥러닝 기술 언어,문종혁 /Jong Hyeok Mun,2021,한국정보처리학회,"최근 딥러닝은 하드웨어 성능이 향상됨에 따라 자연어 처리, 영상 인식 등의 다양한 기술에 접목되어 활용되고 있다. 이러한 기술들을 활용해 지능형교통 시스템(ITS), 스마트홈, 헬스케어 등의 산업분야에서 데이터를 분석하여 고속도로 속도위반 차량 검출, 에너지 사용량 제어, 응급상황 등과 같은고품질의 서비스를 제공하며, 고품질의 서비스를 제공하기 위해서는 정확도가 향상된 딥러닝 모델이 적용되어야 한다. 이를 위해 서비스 환경의 데이터를분석하기 위한 딥러닝 모델을 개발할 때, 개발자는 신뢰성이 검증된 최신의 딥러닝 모델을 적용할 수 있어야 한다. 이는 개발자가 참조하는 딥러닝 모델에적용된 학습 데이터셋의 정확도를 측정하여 검증할 수 있다. 이러한 검증을 위해서 개발자는 학습 데이터셋, 딥러닝의 계층구조 및 개발 환경 등과 같은내용을 포함하는 딥러닝 모델을 문서화하여 적용하기 위한 구조적인 정보가 필요하다. 본 논문에서는 신뢰성있는 딥러닝 기반 데이터 분석 모델을 참조하기위한 딥러닝 기술 언어를 제안한다. 제안하는 기술 언어는 신뢰성 있는 딥러닝 모델을 개발하는데 필요한 학습데이터셋, 개발 환경 및 설정 등의 정보와더불어 딥러닝 모델의 계층구조를 표현할 수 있다. 제안하는 딥러닝 기술 언어를 이용하여 개발자는 지능형 교통 시스템에서 참조하는 분석 모델의정확도를 검증할 수 있다. 실험에서는 제안하는 언어의 유효성을 검증하기 위해, 번호판 인식 모델을 중심으로 딥러닝 기술 문서의 적용과정을 보인다With the recent advancements of deep learning, companies such as smart home, healthcare, and intelligent transportation systems areutilizing its functionality to provide high-quality services for vehicle detection, emergency situation detection, and controlling energyconsumption. To provide reliable services in such sensitive systems, deep learning models are required to have high accuracy. In orderto develop a deep learning model for analyzing previously mentioned services, developers should utilize the state of the art deep learningmodels that have already been verified for higher accuracy. The developers can verify the accuracy of the referenced model by validatingthe model on the dataset. For this validation, the developer needs structural information to document and apply deep learning models,including metadata such as learning dataset, network architecture, and development environments. In this paper, we propose a descriptionlanguage that represents the network architecture of the deep learning model along with its metadata that are necessary to develop adeep learning model. Through the proposed description language, developers can easily verify the accuracy of the referenced deep learningmodel. Our experiments demonstrate the application scenario of a deep learning description document that focuses on the license platerecognition for the detection of illegally parked vehicles.Trusted Deep Learning, Model Reference, Deep Learning Description Language, Traffic Situation Analysis Model"
252,딥러닝 분석을 이용한 중국 역내·외 위안화 변동성 예측,이우식 /Lee Woosik,2016,한국데이터정보과학회,"2008년 글로벌 금융위기 이후 중국은 위안화 국제화의 점진적 추진을 진행하면서 중국상하이 외환시장과 중국홍콩 외환시장에서 거래되는 통화인 역내위안화와 역외위안화를 형성시켰다. 본 연구는 위안화 국제화와 점진적인 중국 자본계정 개방에 따라 급변하는 외환시장상황의 변동성을 정확하게 파악하기 위해서 GARCH모형 (일반화된 자기회귀 조건부이분산성모형)에 다단계인공신경망을 결합한 MLP-GARCH 모형과 GARCH모형과 기계학습의 일종인 딥러닝 (deep learning)을 통합한 DL-GARCH을 가지고 위안화 변동성예측을 비교 실험과 분석을 하였다. 비교분석 결과 DLGARCH 모형은 MLP-GARCH보다 모형 위안화 역내·외 환율변동성 예측 면에서 더욱 더 개선된예측값을 제공하였다. 그래서 이분산시계열모형을 딥러닝과 결합한 DL-GARCH 모형은 시계열의환율 변동성 예측 문제에 딥러닝을 응용할 수 있음을 확인하였다. 향후 이분산시계열과 결합된 딥러닝 모형은 다른 금융시계열 데이터에 응용하여 그 일반화 가능성을 높일 수 있을 것이다.The People’s Republic of China has vigorously been pursuing the internationalization of the Chinese Yuan or Renminbi after the financial crisis of 2008. In this view, an abrupt increase of use of the Chinese Yuan in the onshore and offshore markets are important milestones to be one of important currencies. One of the most frequently used methods to forecast volatility is GARCH model. Since a prediction error of the GARCH model has been reported quite high, a lot of efforts have been made to improve forecasting capability of the GARCH model. In this paper, we have proposed MLPGARCH and a DL-GARCH by employing Artificial Neural Network to the GARCH. In an application to forecasting Chinese Yuan volatility, we have successfully shown their overall outperformance in forecasting over the GARCH.Deep learning, DL-GARCH, GARCH, The Chinese Yuan, volatility."
253,딥러닝 기법을 이용한 내일강수 예측,하지훈,2016,한국지능시스템학회,"정확한 강수예측을 위해서는 예측인자 선정과 예측방법에 대한 선택이 매우 중요하다. 최근에는 강수예측 방법으로기계학습 기법이 많이 사용되고 있으며, 그 중에서도 특히 인공신경망을 사용한 강수예측 방법은 좋은 성능을 보였다. 본논문에서는 딥러닝 기법 중 하나인 DBN(deep belief network)를 이용한 새로운 강수예측 방법을 제안한다. DBN는 비지도사전 학습을 통해 초기 가중치를 설정하여 기존 인공신경망의 문제점을 보완한다. 예측인자로는 기온, 전일-전주 강수일, 태양과 달 궤도 관련 자료를 선정하였다. 기온과 전일-전주 강수일은 서울에서의 1974년부터 2013년까지 총 40년간의AWS(automatic weather system) 관측 자료를 사용하였고, 태양과 달의 궤도 관련 자료는 서울을 중심으로 계산한 결과를사용하였다. 전체 기간에서 일부는 학습 자료로 사용하여 예측모델을 생성하였고, 나머지를 생성한 모델의 검증 자료로사용하였다. 모델 검증 결과로 나온 예측값들은 확률값을 가지며 임계치를 이용하여 강수유무를 판별하였다. 강수 정확도의척도로 양분예보기법 중 CSI(critical successive index)와 Bias(frequency bias)를 계산하였다. 이를 통해 DBN와 MLP(multilayer perceptron)의 성능을 비교한 결과 DBN의 강수 예측 정확도가 높았고, 수행속도 또한 2배 이상 빨랐다.For accurate precipitation forecasts the choice of weather faoctrs and prediction method is very important. Recently, machinele arning has been widely used for forecasting precipitation, and artificial neural network, one of machine learning techniques, showed good performance. In this paper, we suggest a new method for forecasting precipitation using DBN, one of deep learning techniques.
DBN has an advantage that initial weights are set by unsupervised learning, so this compensates for the defects of artificial neural networks. We used past precipitation, temperature, and the parameters of the sun and moon's motion as features for forecasting precipitation. The dataset consists of observation data which had been measured for 40 years from AWS in Seoul. Experiments were based on 8-fold cross validation. As a result of estimation, we got probabilities of test dataset, so threshold was used for the decision of precipitation. CSI and Bias were used for indicating the precision of precipitation. Our experimental results showed that DBN performed better than MLP.Deep learning, Deep belief network, Precipitation, Forecast"
254,딥러닝 신경망모형을 이용한 실시간 교통정보수집 연구,김대현 /KIM DAE HYON,2016,사단법인 인문사회과학기술융합학회,"비디오 기반의 실시간 교통정보수집 시스템은 자율주행을 포함한 지능형교통체계의 주요 서브시스템중 하나이며, 우수한 예측(추정)력 때문에 핵심 알고리즘으로 인공신경망 모형이 주로 사용되고 있다. 특히, 최근 과학 학술지 네이처에 구글 딥마인드의 알파고가 공개되면서 딥러닝과 인공지능에 대한 전 세계적인 관심이 집중되고 있다. 최근까지 가장 널리 사용된 인공신경망모형은 역전파 모형이었으나 최근 컴퓨터 하드웨어 및 알고리즘의 새로운 개발과 더불어 딥러닝 알고리즘을 이용한 딥인공신경망 모형에 대한 관심이 높아지고 있다. 그러나 아직 딥러닝 인공신경망에 대한 연구는 초기단계에 있는 실정이며 적용분야도 극히 한정되어 있다. 본 연구에서는 실시간 도로교통정보 수집을 위한 영상처리분야에서 딥러닝 알고리즘을 활용하고 실험결과를 통하여 향후 교통정보수집‧분석 분야에서의 딥러닝 인공신경망 모형의 적용가능성을 살펴보고자 한다. 특히, 본 연구에서 도출한 딥러닝 인공신경망모형의 실험결과를 기존 역전파 모형에 의한 실험결과와 비교하여 모형의 우수성을 비교‧평가하고자 한다.Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Normally, artificial neural networks have been used in real-time applications for collecting traffic information because of their remarkable performance in terms of predictive accuracy and real-time computing. Especially, the recent publication of a paper in the journal Nature and the success of Google DeepMind’s world-beating Go program AlphaGo caused significant increases of the public interest in artificial intelligence. Even though various types of learning models have been proposed, the Backpropagation has been the most popular neural network model. However, the Deep learning neural network model has attracted much interest recently. In this study, the Deep leaning neural network will be applied in image processing techniques for the real-time traffic information collection to investigate the performance of Deep learning algorithm in traffic engineering fields. In addition, the experimental results from Deep learing algorithm will be compared with the results from Backpropagation model to evaluate and propose better performing model.Intelligent Transportation Systems (ITS), artificial neural networks, Deep learning, traffic information, image processing"
255,딥러닝의 모형과 응용사례,안성만 /SungMahn Ahn,2016,한국지능정보시스템학회,"딥러닝은 인공신경망(neural network)이라는 인공지능분야의 모형이 발전된 형태로서, 계층구조로 이루어진 인공신경망의 내부계층(hidden layer)이 여러 단계로 이루어진 구조이다. 딥러닝에서의 주요 모형은 합성곱신경망(convolutional neural network), 순환신경망(recurrent neural network), 그리고 심층신뢰신경망(deep belief network)의 세가지라고 할 수있다. 그 중에서 현재 흥미로운 연구가 많이 발표되어서 관심이 집중되고 있는 모형은 지도학습(supervised learning)모형인 처음 두 개의 모형이다. 따라서 본 논문에서는 지도학습모형의 가중치를 최적화하는 기본적인 방법인 오류역전파 알고리즘을 살펴본 뒤에 합성곱신경망과 순환신경망의 구조와 응용사례 등을 살펴보고자 한다. 본문에서 다루지 않은 모형인 심층신뢰신경망은 아직까지는 합성곱신경망 이나 순환신경망보다는 상대적으로 주목을 덜 받고 있다. 그러나 심층신뢰신경망은 CNN이나 RNN과는 달리 비지도학습(unsupervised learning)모형이며, 사람이나 동물은 관찰을 통해서 스스로 학습한다는 점에서 궁극적으로는 비지도학습모형이 더 많이 연구되어야 할 주제가 될 것이다.Deep learning model is a kind of neural networks that allows multiple hidden layers. There are various deep learning architectures such as convolutional neural networks, deep belief networks and recurrent neural networks. Those have been applied to fields like computer vision, automatic speech recognition, natural language processing, audio recognition and bioinformatics where they have been shown to produce state-of-the-art results on various tasks. Among those architectures, convolutional neural networks and recurrent neural networks are classified as the supervised learning model. And in recent years, those supervised learning models have gained more popularity than unsupervised learning models such as deep belief networks, because supervised learning models have shown fashionable applications in such fields mentioned above.
Deep learning models can be trained with backpropagation algorithm. Backpropagation is an abbreviation for “backward propagation of errors” and a common method of training artificial neural networks used in conjunction with an optimization method such as gradient descent. The method calculates the gradient of an error function with respect to all the weights in the network. The gradient is fed to the optimization method which in turn uses it to update the weights, in an attempt to minimize the error function.
Convolutional neural networks use a special architecture which is particularly well-adapted to classify images. Using this architecture makes convolutional networks fast to train. This, in turn, helps us train deep, muti-layer networks, which are very good at classifying images. These days, deep convolutional networks are used in most neural networks for image recognition. Convolutional neural networks use three basic ideas: local receptive fields, shared weights, and pooling. By local receptive fields, we mean that each neuron in the first(or any) hidden layer will be connected to a small region of the input(or previous layer’s) neurons. Shared weights mean that we’re going to use the same weights and bias for each of the local receptive field. This means that all the neurons in the hidden layer detect exactly the same feature, just at different locations in the input image. In addition to the convolutional layers just described, convolutional neural networks also contain pooling layers. Pooling layers are usually used immediately after convolutional layers. What the pooling layers do is to simplify the information in the output from the convolutional layer.
Recent convolutional network architectures have 10 to 20 hidden layers and billions of connections between units. Training deep learning networks has taken weeks several years ago, but thanks to progress in GPU and algorithm enhancement, training time has reduced to several hours.
Neural networks with time-varying behavior are known as recurrent neural networks or RNNs. A recurrent neural network is a class of artificial neural network where connections between units form a directed cycle. This creates an internal state of the network which allows it to exhibit dynamic temporal behavior. Unlike feedforward neural networks, RNNs can use their internal memory to process arbitrary sequences of inputs. Early RNN models turned out to be very difficult to train, harder even than deep feedforward networks. The reason is the unstable gradient problem such as vanishing gradient and exploding gradient. The gradient can get smaller and smaller as it is propagated back through layers. This makes learning in early layers extremely slow. The problem actually gets worse in RNNs, since gradients aren’t just propagated backward through layers, they’re propagated backward through time. If the network runs for a long time, that can make the gradient extremely unstable and hard to learn from. It has been possible to incorporate an idea known as long short-term memory units (LSTMs) into RNNs. LSTMs make it much easier to get good results when training RNNs, and many recent papers make use of LSTMs or related ideas.Deep learning, Convolutional neural networks, Recurrent neural networks, Error backpropagation algorithm"
256,딥러닝분석과 기술적 분석 지표를 이용한 한국 코스피주가지수 방향성 예측,이우식 /Lee Woosik,2017,한국데이터정보과학회,"2016년 3월 구글 (Google)의 바둑인공지능 알파고 (AlphaGo)가 이세돌 9단과의 바둑대결에서승리한 이후 다양한 분야에서 인공지능 사용에 대한 관심이 높아지고 있는 가운데 금융투자 분야에서도 인공지능과 투자자문 전문가의 합성어인 로보어드바이저 (Robo-Advisor)에 대한 관심이 높아지고 있다. 인공지능 (artificial intelligence)기반의 의사결정은 비용 절감은 물론 효과적인 의사결정을가능하게 한다는 점에서 큰 장점이 있다. 본 연구에서는 기술적 분석 (technical analysis) 지표와 딥러닝 (deep learning) 모형을 결합하여 한국 코스피 지수를 예측하는 모형을 개발하고 제시한 모형들의 예측력을 비교, 분석한다. 분석 결과 기술적 분석 지표에 딥러닝 알고리즘을 결합한 모형이 주가지수 방향성 예측 문제에 응용될 수 있음을 확인하였다. 향후 본 연구에서 제안된 기술적 분석 지표와딥러닝모형을 결합한 기법은 로보어드바이저서비스에 응용할 수 있는 일반화 가능성을 보여준다Since Google’s AlphaGo defeated a world champion of Go players in 2016, there have been many interests in the deep learning. In the financial sector, a Robo-Advisor using deep learning gains a significant attention, which builds and manages portfolios of financial instruments for investors.In this paper, we have proposed the a deep learning algorithm geared toward identification and forecast of the KOSPI index direction,and we also have compared the accuracy of the prediction.In an application of forecasting the financial market index direction, we have shown that the Robo-Advisor using deep learning has a significant effect on finance industry. The Robo-Advisor collects a massive data such as earnings statements, news reports and regulatory filings, analyzes those and recommends investors how to view market trends and identify the best time to purchase financial assets. On the other hand, the Robo-Advisor allows businesses to learn more about their customers, develop better marketing strategies, increase sales and decrease costs.Artificial intelligence, deep learning, FinTech, Robo-Advisor, technical analysis."
257,딥러닝의 효율적인 학습을 위한 학습데이터 선별 기법에 관한 연구,주경돈 /Gyoung-don Joo,2017,한국정보과학회,"빅데이터의 증가와 병렬 하드웨어 처리, 학습 알고리즘의 개선은 딥러닝의 발전을 가져왔으며 인공지능이 적용 가능한 영역을 획기적으로 확장시키고 있다. 하지만 이러한 대규모의 학습 데이터에 기반한 딥러닝은 학습시간이 증가되고, 더 나아가 학습 데이터의 구축을 위한 레이블링(labeling) 비용이 데이터 수에 비례하여 증가하게 된다. 따라서 본 논문에서는, 딥러닝의 정확도를 최대한 유지하면서 필요한 학습 데이터의 수를 줄일수 있도록, 전체 데이터에서 제한된 수의 학습 데이터의 후보를 선별하는 기법에 대하여 연구한다. 특히 기존의 능동 학습에 기반한 기법들이 학습 데이터의 수가 매우 적어질 경우 딥러닝의 정확도가 급격히 떨어지는 문제를 해결할 수 있는 새로운 기법을 제안하였으며, 제안된 기법으로 선별된 학습 데이터는 그 수의 변화에 대해 딥러닝이 일관되게 경쟁력있는 정확도를 가짐을 다양한 실험을 통하여 검증하였다.The increase of big data, parallel hardware processing and enhanced learning algorithms have led to the development of deep learning and are dramatically expanding the domain of applications based on artificial intelligence. However, such large-scale training data extends the learning time, and furthermore, the labeling cost for building the training set increases in proportion to the volume of data. Therefore, in this paper, we study how to select a limited number of candidates for training set from given data so as to reduce the number of training set while maintaining the accuracy of deep learning. Especially, we found that, with the training set selected by the previous work, the accuracy of deep learning drastically decreases when the number of training set is very small and so proposed new methods to resolve this fall-off in accuracy. Through experiments, we showed that deep learning consistently has the competitive accuracy with the training set selected by the proposed method regardless of the size of training set.Deep Learning, Train Set Selection, Active Learning, Gamma distribution"
258,딥러닝을 이용한 범용적 스테그아날리시스,김현재 /Hyunjae Kim,2017,한국정보과학회,"스테그아날리시스(Steganalysis)란 이미지 등 일반적인 자료에 암호화된 정보를 은닉하는 스테가노그래피(Steganography)에 대한 검출 및 분석 방법으로, 기계학습 기반 방법론을 포함한다. 기존 기계학습 기반 스테그아날리시스는 영상(Image)의 특징(Feature) 추출 및 모델링에 기반하며, 최근 딥러닝(Deep Learning)의 적용으로 검출 정확도가 큰 폭으로 향상되었다. 하지만 현존하는 스테그아날리시스 모델은 단일 스테가노그래피 기법에 대해 국한되어 있어 학습에 사용되지 않은 스테고(Stego) 이미지의 경우 검출이 불가능한 결정적 한계를 가진다. 본 연구에서는 다양한 스테가노그래피 기법으로 생성된 스테고이미지에 딥러닝을 적용하여 스테그아날리시스를 학습하는 범용적 모델을 제안한다. 다양한 실험을 통해 제안 기법의 효용성 및 가능성을 확인하고, 범용적 스테그아날리시스 모델이 각각에 특화된 검출 기법과 유사한 정확도로 스테고 이미지를 검출할 수 있음을 보인다.Steganalysis is to detect information hidden by steganography inside general data such as images. There are stegoanalysis techniques that use machine learning (ML). Existing ML approaches to steganalysis are based on extracting features from stego images and modeling them.
Recently deep learning-based methodologies have shown significant improvements in detection accuracy. However, all the existing methods, including deep learning-based ones, have a critical limitation in that they can only detect stego images that are created by a specific steganography method. In this paper, we propose a generalized steganalysis method that can model multiple types of stego images using deep learning. Through various experiments, we confirm the effectiveness of our approach and envision directions for future research. In particular, we show that our method can detect each type of steganography with the same level of accuracy as that of a steganalysis method dedicated to that type of steganography, thereby demonstrating the general applicability of our approach to multiple types of stego images.steganalysis, steganography, deep learning, generalized steganalysis model"
259,"딥러닝 프레임워크의 비교: 티아노, 텐서플로, CNTK를 중심으로",정여진 /Yeojin Chung,2017,한국지능정보시스템학회,"딥러닝 프레임워크의 대표적인 기능으로는 ‘자동미분’과 ‘GPU의 활용’ 등을 들 수 있다. 본 논문은 파이썬의 라이브러리 형태로 사용 가능한 프레임워크 중에서 구글의 텐서플로와 마이크로소프트의 CNTK, 그리고 텐서 플로의 원조라고 할 수 있는 티아노를 비교하였다. 본문에서는 자동미분의 개념과 GPU의 활용형태를 간단히 설명하고, 그 다음에 logistic regression을 실행하는 예를 통하여 각 프레임워크의 문법을 알아본 뒤에, 마지막으로 대표적인 딥러닝 응용인 CNN의 예제를 실행시켜보고 코딩의 편의성과 실행속도 등을 확인해 보았다. 그 결과, 편의성의 관점에서 보면 티아노가 가장 코딩 하기가 어렵고, CNTK와 텐서플로는 많은 부분이 비슷하게 추상화 되어 있어서 코딩이 비슷하지만 가중치와 편향을 직접 정의하느냐의 여부에서 차이를 보였다. 그리고 각프레임워크의 실행속도에 대한 평가는 ‘큰 차이는 없다’는 것이다. 텐서플로는 티아노에 비하여 속도가 느리다는 평가가 있어왔는데, 본 연구의 실험에 의하면, 비록 CNN 모형에 국한되었지만, 텐서플로가 아주 조금이지만빠른 것으로 나타났다. CNTK의 경우에도, 비록 실험환경이 달랐지만, 실험환경의 차이에 의한 속도의 차이의 편차범위 이내에 있는 것으로 판단이 되었다. 본 연구에서는 세 종류의 딥러닝 프레임워크만을 살펴보았는데,위키피디아에 따르면 딥러닝 프레임워크의 종류는 12가지가 있으며, 각 프레임워크의 특징을 15가지 속성으로구분하여 차이를 특정하고 있다. 그 많은 속성 중에서 사용자의 입장에서 볼 때 중요한 속성은 어떤 언어(파이썬, C++, Java, 등)로 사용가능한지, 어떤 딥러닝 모형에 대한 라이브러리가 잘 구현되어 있는지 등일 것이다. 그리고 사용자가 대규모의 딥러닝 모형을 구축한다면, 다중 GPU 혹은 다중 서버를 지원하는지의 여부도 중요할 것이다. 또한 딥러닝 모형을 처음 학습하는 경우에는 사용설명서가 많은지 예제 프로그램이 많은지 여부도 중요한 기준이 될 것이다.The deep learning framework is software designed to help develop deep learning models. Some of its important functions include “automatic differentiation” and “utilization of GPU”. The list of popular deep learning framework includes Caffe (BVLC) and Theano (University of Montreal). And recently, Microsoft's deep learning framework, Microsoft Cognitive Toolkit, was released as open-source license, following Google’s Tensorflow a year earlier. The early deep learning frameworks have been developed mainly for research at universities. Beginning with the inception of Tensorflow, however, it seems that companies such as Microsoft and Facebook have started to join the competition of framework development. Given the trend, Google and other companies are expected to continue investing in the deep learning framework to bring forward the initiative in the artificial intelligence business. From this point of view, we think it is a good time to compare some of deep learning frameworks. So we compare three deep learning frameworks which can be used as a Python library. Those are Google's Tensorflow, Microsoft’s CNTK, and Theano which is sort of a predecessor of the preceding two.
The most common and important function of deep learning frameworks is the ability to perform automatic differentiation. Basically all the mathematical expressions of deep learning models can be represented as computational graphs, which consist of nodes and edges. Partial derivatives on each edge of a computational graph can then be obtained. With the partial derivatives, we can let software compute differentiation of any node with respect to any variable by utilizing chain rule of Calculus.
First of all, the convenience of coding is in the order of CNTK, Tensorflow, and Theano. The criterion is simply based on the lengths of the codes and the learning curve and the ease of coding are not the main concern. According to the criteria, Theano was the most difficult to implement with, and CNTK and Tensorflow were somewhat easier. With Tensorflow, we need to define weight variables and biases explicitly. The reason that CNTK and Tensorflow are easier to implement with is that those frameworks provide us with more abstraction than Theano. We, however, need to mention that low-level coding is not always bad. It gives us flexibility of coding. With the low-level coding such as in Theano, we can implement and test any new deep learning models or any new search methods that we can think of.
The assessment of the execution speed of each framework is that there is not meaningful difference. According to the experiment, execution speeds of Theano and Tensorflow are very similar, although the experiment was limited to a CNN model. In the case of CNTK, the experimental environment was not maintained as the same. The code written in CNTK has to be run in PC environment without GPU where codes execute as much as 50 times slower than with GPU. But we concluded that the difference of execution speed was within the range of variation caused by the different hardware setup.
In this study, we compared three types of deep learning framework: Theano, Tensorflow, and CNTK. According to Wikipedia, there are 12 available deep learning frameworks. And 15 different attributes differentiate each framework. Some of the important attributes would include interface language (Python, C ++, Java, etc.) and the availability of libraries on various deep learning models such as CNN, RNN, DBN, and etc. And if a user implements a large scale deep learning model, it will also be important to support multiple GPU or multiple servers. Also, if you are learning the deep learning model, it would also be important if there are enough examples and references.deep learning framework, Theano, TensorFlow, CNTK, computational graph, CIFAR-10"
260,딥러닝을 이용한 주변 무선단말 파악방안,이웅섭 /Lee， Woong-sup,2018,한국정보통신학회,"최근 단말-대-단말(Device-to-device, D2D) 통신기술이 차세대 무선통신시스템의 핵심기술로 큰 관심을 받고 있다. 이러한 단말간 통신에서는 자신의 주변에 어떠한 단말이 있는지 파악하는 주변단말 탐색(Neighbor discovery)이매우 중요하다. 본 논문에서는 최근 큰 관심을 받고 있는 딥러닝(Deep learning) 기술을 활용하여 단말간 통신에서 주변단말을 파악하는 방안에 대해서 제안한다. 제안 방안은 기존의 방안과 달리 무선채널의 공간적 연관성을 이용하여단말간의 신호 전송 없이 단말이 기지국으로 전송하는 상향링크 파일럿 신호를 기반으로 주변 단말을 찾고 따라서기존의 방식에 비해 신호전송 복잡도(signaling complexity)를 크게 줄일 수 있다. 또한 제안 방안에서는 떨어져 있는거리에 따라서 주변 단말을 분류 가능하여 기존 방안에 비해서 좀 더 세밀한 단말 탐색이 가능하다. 마지막으로 본 논문에서는 tensorflow를 이용한 컴퓨터 시뮬레이션을 통해 제안 방안의 성능을 검증하였다.Recently, the device-to-device (D2D) communication has been conceived as the key technology for the next-generationmobile communication systems. The neighbor discovery in which the nearby users are found, is essential for the properoperation of the D2D communication. In this paper, we propose new neighbor discovery scheme based on deep learningtechnology which has gained a lot of attention recently. In the proposed scheme, the neighboring users can be foundusing the uplink pilot transmission of users only, unlike conventional neighbor discovery schemes in which direct pilotcommunication among users is required, such that the signaling overhead can be greatly reduced in our proposed scheme.
Moreover, the neighbors with different proximity can also be classified accordingly which enables more accurate neighbordiscovery compared to the conventional schemes. The performance of our proposed scheme is verified through thetensorflow-based computer simulations.
키워드 : 딥러닝, 주변단말 탐색, 단말-대-단말 통신Deep learning, Neighbor discovery, Device-to-device communication, Wireless channel correlation"
261,신뢰성있는 딥러닝 기반 분석 모델을 참조하기 위한 딥러닝 기술 언어,문종혁 /Jong Hyeok Mun,2021,한국정보처리학회,"최근 딥러닝은 하드웨어 성능이 향상됨에 따라 자연어 처리, 영상 인식 등의 다양한 기술에 접목되어 활용되고 있다. 이러한 기술들을 활용해 지능형교통 시스템(ITS), 스마트홈, 헬스케어 등의 산업분야에서 데이터를 분석하여 고속도로 속도위반 차량 검출, 에너지 사용량 제어, 응급상황 등과 같은고품질의 서비스를 제공하며, 고품질의 서비스를 제공하기 위해서는 정확도가 향상된 딥러닝 모델이 적용되어야 한다. 이를 위해 서비스 환경의 데이터를분석하기 위한 딥러닝 모델을 개발할 때, 개발자는 신뢰성이 검증된 최신의 딥러닝 모델을 적용할 수 있어야 한다. 이는 개발자가 참조하는 딥러닝 모델에적용된 학습 데이터셋의 정확도를 측정하여 검증할 수 있다. 이러한 검증을 위해서 개발자는 학습 데이터셋, 딥러닝의 계층구조 및 개발 환경 등과 같은내용을 포함하는 딥러닝 모델을 문서화하여 적용하기 위한 구조적인 정보가 필요하다. 본 논문에서는 신뢰성있는 딥러닝 기반 데이터 분석 모델을 참조하기위한 딥러닝 기술 언어를 제안한다. 제안하는 기술 언어는 신뢰성 있는 딥러닝 모델을 개발하는데 필요한 학습데이터셋, 개발 환경 및 설정 등의 정보와더불어 딥러닝 모델의 계층구조를 표현할 수 있다. 제안하는 딥러닝 기술 언어를 이용하여 개발자는 지능형 교통 시스템에서 참조하는 분석 모델의정확도를 검증할 수 있다. 실험에서는 제안하는 언어의 유효성을 검증하기 위해, 번호판 인식 모델을 중심으로 딥러닝 기술 문서의 적용과정을 보인다With the recent advancements of deep learning, companies such as smart home, healthcare, and intelligent transportation systems areutilizing its functionality to provide high-quality services for vehicle detection, emergency situation detection, and controlling energyconsumption. To provide reliable services in such sensitive systems, deep learning models are required to have high accuracy. In orderto develop a deep learning model for analyzing previously mentioned services, developers should utilize the state of the art deep learningmodels that have already been verified for higher accuracy. The developers can verify the accuracy of the referenced model by validatingthe model on the dataset. For this validation, the developer needs structural information to document and apply deep learning models,including metadata such as learning dataset, network architecture, and development environments. In this paper, we propose a descriptionlanguage that represents the network architecture of the deep learning model along with its metadata that are necessary to develop adeep learning model. Through the proposed description language, developers can easily verify the accuracy of the referenced deep learningmodel. Our experiments demonstrate the application scenario of a deep learning description document that focuses on the license platerecognition for the detection of illegally parked vehicles.Trusted Deep Learning, Model Reference, Deep Learning Description Language, Traffic Situation Analysis Model"
262,딥러닝 분석을 이용한 중국 역내·외 위안화 변동성 예측,이우식 /Lee Woosik,2016,한국데이터정보과학회,"2008년 글로벌 금융위기 이후 중국은 위안화 국제화의 점진적 추진을 진행하면서 중국상하이 외환시장과 중국홍콩 외환시장에서 거래되는 통화인 역내위안화와 역외위안화를 형성시켰다. 본 연구는 위안화 국제화와 점진적인 중국 자본계정 개방에 따라 급변하는 외환시장상황의 변동성을 정확하게 파악하기 위해서 GARCH모형 (일반화된 자기회귀 조건부이분산성모형)에 다단계인공신경망을 결합한 MLP-GARCH 모형과 GARCH모형과 기계학습의 일종인 딥러닝 (deep learning)을 통합한 DL-GARCH을 가지고 위안화 변동성예측을 비교 실험과 분석을 하였다. 비교분석 결과 DLGARCH 모형은 MLP-GARCH보다 모형 위안화 역내·외 환율변동성 예측 면에서 더욱 더 개선된예측값을 제공하였다. 그래서 이분산시계열모형을 딥러닝과 결합한 DL-GARCH 모형은 시계열의환율 변동성 예측 문제에 딥러닝을 응용할 수 있음을 확인하였다. 향후 이분산시계열과 결합된 딥러닝 모형은 다른 금융시계열 데이터에 응용하여 그 일반화 가능성을 높일 수 있을 것이다.The People’s Republic of China has vigorously been pursuing the internationalization of the Chinese Yuan or Renminbi after the financial crisis of 2008. In this view, an abrupt increase of use of the Chinese Yuan in the onshore and offshore markets are important milestones to be one of important currencies. One of the most frequently used methods to forecast volatility is GARCH model. Since a prediction error of the GARCH model has been reported quite high, a lot of efforts have been made to improve forecasting capability of the GARCH model. In this paper, we have proposed MLPGARCH and a DL-GARCH by employing Artificial Neural Network to the GARCH. In an application to forecasting Chinese Yuan volatility, we have successfully shown their overall outperformance in forecasting over the GARCH.Deep learning, DL-GARCH, GARCH, The Chinese Yuan, volatility."
263,딥러닝 기법을 이용한 내일강수 예측,하지훈,2016,한국지능시스템학회,"정확한 강수예측을 위해서는 예측인자 선정과 예측방법에 대한 선택이 매우 중요하다. 최근에는 강수예측 방법으로기계학습 기법이 많이 사용되고 있으며, 그 중에서도 특히 인공신경망을 사용한 강수예측 방법은 좋은 성능을 보였다. 본논문에서는 딥러닝 기법 중 하나인 DBN(deep belief network)를 이용한 새로운 강수예측 방법을 제안한다. DBN는 비지도사전 학습을 통해 초기 가중치를 설정하여 기존 인공신경망의 문제점을 보완한다. 예측인자로는 기온, 전일-전주 강수일, 태양과 달 궤도 관련 자료를 선정하였다. 기온과 전일-전주 강수일은 서울에서의 1974년부터 2013년까지 총 40년간의AWS(automatic weather system) 관측 자료를 사용하였고, 태양과 달의 궤도 관련 자료는 서울을 중심으로 계산한 결과를사용하였다. 전체 기간에서 일부는 학습 자료로 사용하여 예측모델을 생성하였고, 나머지를 생성한 모델의 검증 자료로사용하였다. 모델 검증 결과로 나온 예측값들은 확률값을 가지며 임계치를 이용하여 강수유무를 판별하였다. 강수 정확도의척도로 양분예보기법 중 CSI(critical successive index)와 Bias(frequency bias)를 계산하였다. 이를 통해 DBN와 MLP(multilayer perceptron)의 성능을 비교한 결과 DBN의 강수 예측 정확도가 높았고, 수행속도 또한 2배 이상 빨랐다.For accurate precipitation forecasts the choice of weather faoctrs and prediction method is very important. Recently, machinele arning has been widely used for forecasting precipitation, and artificial neural network, one of machine learning techniques, showed good performance. In this paper, we suggest a new method for forecasting precipitation using DBN, one of deep learning techniques.
DBN has an advantage that initial weights are set by unsupervised learning, so this compensates for the defects of artificial neural networks. We used past precipitation, temperature, and the parameters of the sun and moon's motion as features for forecasting precipitation. The dataset consists of observation data which had been measured for 40 years from AWS in Seoul. Experiments were based on 8-fold cross validation. As a result of estimation, we got probabilities of test dataset, so threshold was used for the decision of precipitation. CSI and Bias were used for indicating the precision of precipitation. Our experimental results showed that DBN performed better than MLP.Deep learning, Deep belief network, Precipitation, Forecast"
264,딥러닝 신경망모형을 이용한 실시간 교통정보수집 연구,김대현 /KIM DAE HYON,2016,사단법인 인문사회과학기술융합학회,"비디오 기반의 실시간 교통정보수집 시스템은 자율주행을 포함한 지능형교통체계의 주요 서브시스템중 하나이며, 우수한 예측(추정)력 때문에 핵심 알고리즘으로 인공신경망 모형이 주로 사용되고 있다. 특히, 최근 과학 학술지 네이처에 구글 딥마인드의 알파고가 공개되면서 딥러닝과 인공지능에 대한 전 세계적인 관심이 집중되고 있다. 최근까지 가장 널리 사용된 인공신경망모형은 역전파 모형이었으나 최근 컴퓨터 하드웨어 및 알고리즘의 새로운 개발과 더불어 딥러닝 알고리즘을 이용한 딥인공신경망 모형에 대한 관심이 높아지고 있다. 그러나 아직 딥러닝 인공신경망에 대한 연구는 초기단계에 있는 실정이며 적용분야도 극히 한정되어 있다. 본 연구에서는 실시간 도로교통정보 수집을 위한 영상처리분야에서 딥러닝 알고리즘을 활용하고 실험결과를 통하여 향후 교통정보수집‧분석 분야에서의 딥러닝 인공신경망 모형의 적용가능성을 살펴보고자 한다. 특히, 본 연구에서 도출한 딥러닝 인공신경망모형의 실험결과를 기존 역전파 모형에 의한 실험결과와 비교하여 모형의 우수성을 비교‧평가하고자 한다.Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Normally, artificial neural networks have been used in real-time applications for collecting traffic information because of their remarkable performance in terms of predictive accuracy and real-time computing. Especially, the recent publication of a paper in the journal Nature and the success of Google DeepMind’s world-beating Go program AlphaGo caused significant increases of the public interest in artificial intelligence. Even though various types of learning models have been proposed, the Backpropagation has been the most popular neural network model. However, the Deep learning neural network model has attracted much interest recently. In this study, the Deep leaning neural network will be applied in image processing techniques for the real-time traffic information collection to investigate the performance of Deep learning algorithm in traffic engineering fields. In addition, the experimental results from Deep learing algorithm will be compared with the results from Backpropagation model to evaluate and propose better performing model.Intelligent Transportation Systems (ITS), artificial neural networks, Deep learning, traffic information, image processing"
265,딥러닝의 모형과 응용사례,안성만 /SungMahn Ahn,2016,한국지능정보시스템학회,"딥러닝은 인공신경망(neural network)이라는 인공지능분야의 모형이 발전된 형태로서, 계층구조로 이루어진 인공신경망의 내부계층(hidden layer)이 여러 단계로 이루어진 구조이다. 딥러닝에서의 주요 모형은 합성곱신경망(convolutional neural network), 순환신경망(recurrent neural network), 그리고 심층신뢰신경망(deep belief network)의 세가지라고 할 수있다. 그 중에서 현재 흥미로운 연구가 많이 발표되어서 관심이 집중되고 있는 모형은 지도학습(supervised learning)모형인 처음 두 개의 모형이다. 따라서 본 논문에서는 지도학습모형의 가중치를 최적화하는 기본적인 방법인 오류역전파 알고리즘을 살펴본 뒤에 합성곱신경망과 순환신경망의 구조와 응용사례 등을 살펴보고자 한다. 본문에서 다루지 않은 모형인 심층신뢰신경망은 아직까지는 합성곱신경망 이나 순환신경망보다는 상대적으로 주목을 덜 받고 있다. 그러나 심층신뢰신경망은 CNN이나 RNN과는 달리 비지도학습(unsupervised learning)모형이며, 사람이나 동물은 관찰을 통해서 스스로 학습한다는 점에서 궁극적으로는 비지도학습모형이 더 많이 연구되어야 할 주제가 될 것이다.Deep learning model is a kind of neural networks that allows multiple hidden layers. There are various deep learning architectures such as convolutional neural networks, deep belief networks and recurrent neural networks. Those have been applied to fields like computer vision, automatic speech recognition, natural language processing, audio recognition and bioinformatics where they have been shown to produce state-of-the-art results on various tasks. Among those architectures, convolutional neural networks and recurrent neural networks are classified as the supervised learning model. And in recent years, those supervised learning models have gained more popularity than unsupervised learning models such as deep belief networks, because supervised learning models have shown fashionable applications in such fields mentioned above.
Deep learning models can be trained with backpropagation algorithm. Backpropagation is an abbreviation for “backward propagation of errors” and a common method of training artificial neural networks used in conjunction with an optimization method such as gradient descent. The method calculates the gradient of an error function with respect to all the weights in the network. The gradient is fed to the optimization method which in turn uses it to update the weights, in an attempt to minimize the error function.
Convolutional neural networks use a special architecture which is particularly well-adapted to classify images. Using this architecture makes convolutional networks fast to train. This, in turn, helps us train deep, muti-layer networks, which are very good at classifying images. These days, deep convolutional networks are used in most neural networks for image recognition. Convolutional neural networks use three basic ideas: local receptive fields, shared weights, and pooling. By local receptive fields, we mean that each neuron in the first(or any) hidden layer will be connected to a small region of the input(or previous layer’s) neurons. Shared weights mean that we’re going to use the same weights and bias for each of the local receptive field. This means that all the neurons in the hidden layer detect exactly the same feature, just at different locations in the input image. In addition to the convolutional layers just described, convolutional neural networks also contain pooling layers. Pooling layers are usually used immediately after convolutional layers. What the pooling layers do is to simplify the information in the output from the convolutional layer.
Recent convolutional network architectures have 10 to 20 hidden layers and billions of connections between units. Training deep learning networks has taken weeks several years ago, but thanks to progress in GPU and algorithm enhancement, training time has reduced to several hours.
Neural networks with time-varying behavior are known as recurrent neural networks or RNNs. A recurrent neural network is a class of artificial neural network where connections between units form a directed cycle. This creates an internal state of the network which allows it to exhibit dynamic temporal behavior. Unlike feedforward neural networks, RNNs can use their internal memory to process arbitrary sequences of inputs. Early RNN models turned out to be very difficult to train, harder even than deep feedforward networks. The reason is the unstable gradient problem such as vanishing gradient and exploding gradient. The gradient can get smaller and smaller as it is propagated back through layers. This makes learning in early layers extremely slow. The problem actually gets worse in RNNs, since gradients aren’t just propagated backward through layers, they’re propagated backward through time. If the network runs for a long time, that can make the gradient extremely unstable and hard to learn from. It has been possible to incorporate an idea known as long short-term memory units (LSTMs) into RNNs. LSTMs make it much easier to get good results when training RNNs, and many recent papers make use of LSTMs or related ideas.Deep learning, Convolutional neural networks, Recurrent neural networks, Error backpropagation algorithm"
266,딥러닝분석과 기술적 분석 지표를 이용한 한국 코스피주가지수 방향성 예측,이우식 /Lee Woosik,2017,한국데이터정보과학회,"2016년 3월 구글 (Google)의 바둑인공지능 알파고 (AlphaGo)가 이세돌 9단과의 바둑대결에서승리한 이후 다양한 분야에서 인공지능 사용에 대한 관심이 높아지고 있는 가운데 금융투자 분야에서도 인공지능과 투자자문 전문가의 합성어인 로보어드바이저 (Robo-Advisor)에 대한 관심이 높아지고 있다. 인공지능 (artificial intelligence)기반의 의사결정은 비용 절감은 물론 효과적인 의사결정을가능하게 한다는 점에서 큰 장점이 있다. 본 연구에서는 기술적 분석 (technical analysis) 지표와 딥러닝 (deep learning) 모형을 결합하여 한국 코스피 지수를 예측하는 모형을 개발하고 제시한 모형들의 예측력을 비교, 분석한다. 분석 결과 기술적 분석 지표에 딥러닝 알고리즘을 결합한 모형이 주가지수 방향성 예측 문제에 응용될 수 있음을 확인하였다. 향후 본 연구에서 제안된 기술적 분석 지표와딥러닝모형을 결합한 기법은 로보어드바이저서비스에 응용할 수 있는 일반화 가능성을 보여준다Since Google’s AlphaGo defeated a world champion of Go players in 2016, there have been many interests in the deep learning. In the financial sector, a Robo-Advisor using deep learning gains a significant attention, which builds and manages portfolios of financial instruments for investors.In this paper, we have proposed the a deep learning algorithm geared toward identification and forecast of the KOSPI index direction,and we also have compared the accuracy of the prediction.In an application of forecasting the financial market index direction, we have shown that the Robo-Advisor using deep learning has a significant effect on finance industry. The Robo-Advisor collects a massive data such as earnings statements, news reports and regulatory filings, analyzes those and recommends investors how to view market trends and identify the best time to purchase financial assets. On the other hand, the Robo-Advisor allows businesses to learn more about their customers, develop better marketing strategies, increase sales and decrease costs.Artificial intelligence, deep learning, FinTech, Robo-Advisor, technical analysis."
267,딥러닝의 효율적인 학습을 위한 학습데이터 선별 기법에 관한 연구,주경돈 /Gyoung-don Joo,2017,한국정보과학회,"빅데이터의 증가와 병렬 하드웨어 처리, 학습 알고리즘의 개선은 딥러닝의 발전을 가져왔으며 인공지능이 적용 가능한 영역을 획기적으로 확장시키고 있다. 하지만 이러한 대규모의 학습 데이터에 기반한 딥러닝은 학습시간이 증가되고, 더 나아가 학습 데이터의 구축을 위한 레이블링(labeling) 비용이 데이터 수에 비례하여 증가하게 된다. 따라서 본 논문에서는, 딥러닝의 정확도를 최대한 유지하면서 필요한 학습 데이터의 수를 줄일수 있도록, 전체 데이터에서 제한된 수의 학습 데이터의 후보를 선별하는 기법에 대하여 연구한다. 특히 기존의 능동 학습에 기반한 기법들이 학습 데이터의 수가 매우 적어질 경우 딥러닝의 정확도가 급격히 떨어지는 문제를 해결할 수 있는 새로운 기법을 제안하였으며, 제안된 기법으로 선별된 학습 데이터는 그 수의 변화에 대해 딥러닝이 일관되게 경쟁력있는 정확도를 가짐을 다양한 실험을 통하여 검증하였다.The increase of big data, parallel hardware processing and enhanced learning algorithms have led to the development of deep learning and are dramatically expanding the domain of applications based on artificial intelligence. However, such large-scale training data extends the learning time, and furthermore, the labeling cost for building the training set increases in proportion to the volume of data. Therefore, in this paper, we study how to select a limited number of candidates for training set from given data so as to reduce the number of training set while maintaining the accuracy of deep learning. Especially, we found that, with the training set selected by the previous work, the accuracy of deep learning drastically decreases when the number of training set is very small and so proposed new methods to resolve this fall-off in accuracy. Through experiments, we showed that deep learning consistently has the competitive accuracy with the training set selected by the proposed method regardless of the size of training set.Deep Learning, Train Set Selection, Active Learning, Gamma distribution"
268,딥러닝을 이용한 범용적 스테그아날리시스,김현재 /Hyunjae Kim,2017,한국정보과학회,"스테그아날리시스(Steganalysis)란 이미지 등 일반적인 자료에 암호화된 정보를 은닉하는 스테가노그래피(Steganography)에 대한 검출 및 분석 방법으로, 기계학습 기반 방법론을 포함한다. 기존 기계학습 기반 스테그아날리시스는 영상(Image)의 특징(Feature) 추출 및 모델링에 기반하며, 최근 딥러닝(Deep Learning)의 적용으로 검출 정확도가 큰 폭으로 향상되었다. 하지만 현존하는 스테그아날리시스 모델은 단일 스테가노그래피 기법에 대해 국한되어 있어 학습에 사용되지 않은 스테고(Stego) 이미지의 경우 검출이 불가능한 결정적 한계를 가진다. 본 연구에서는 다양한 스테가노그래피 기법으로 생성된 스테고이미지에 딥러닝을 적용하여 스테그아날리시스를 학습하는 범용적 모델을 제안한다. 다양한 실험을 통해 제안 기법의 효용성 및 가능성을 확인하고, 범용적 스테그아날리시스 모델이 각각에 특화된 검출 기법과 유사한 정확도로 스테고 이미지를 검출할 수 있음을 보인다.Steganalysis is to detect information hidden by steganography inside general data such as images. There are stegoanalysis techniques that use machine learning (ML). Existing ML approaches to steganalysis are based on extracting features from stego images and modeling them.
Recently deep learning-based methodologies have shown significant improvements in detection accuracy. However, all the existing methods, including deep learning-based ones, have a critical limitation in that they can only detect stego images that are created by a specific steganography method. In this paper, we propose a generalized steganalysis method that can model multiple types of stego images using deep learning. Through various experiments, we confirm the effectiveness of our approach and envision directions for future research. In particular, we show that our method can detect each type of steganography with the same level of accuracy as that of a steganalysis method dedicated to that type of steganography, thereby demonstrating the general applicability of our approach to multiple types of stego images.steganalysis, steganography, deep learning, generalized steganalysis model"
269,"딥러닝 프레임워크의 비교: 티아노, 텐서플로, CNTK를 중심으로",정여진 /Yeojin Chung,2017,한국지능정보시스템학회,"딥러닝 프레임워크의 대표적인 기능으로는 ‘자동미분’과 ‘GPU의 활용’ 등을 들 수 있다. 본 논문은 파이썬의 라이브러리 형태로 사용 가능한 프레임워크 중에서 구글의 텐서플로와 마이크로소프트의 CNTK, 그리고 텐서 플로의 원조라고 할 수 있는 티아노를 비교하였다. 본문에서는 자동미분의 개념과 GPU의 활용형태를 간단히 설명하고, 그 다음에 logistic regression을 실행하는 예를 통하여 각 프레임워크의 문법을 알아본 뒤에, 마지막으로 대표적인 딥러닝 응용인 CNN의 예제를 실행시켜보고 코딩의 편의성과 실행속도 등을 확인해 보았다. 그 결과, 편의성의 관점에서 보면 티아노가 가장 코딩 하기가 어렵고, CNTK와 텐서플로는 많은 부분이 비슷하게 추상화 되어 있어서 코딩이 비슷하지만 가중치와 편향을 직접 정의하느냐의 여부에서 차이를 보였다. 그리고 각프레임워크의 실행속도에 대한 평가는 ‘큰 차이는 없다’는 것이다. 텐서플로는 티아노에 비하여 속도가 느리다는 평가가 있어왔는데, 본 연구의 실험에 의하면, 비록 CNN 모형에 국한되었지만, 텐서플로가 아주 조금이지만빠른 것으로 나타났다. CNTK의 경우에도, 비록 실험환경이 달랐지만, 실험환경의 차이에 의한 속도의 차이의 편차범위 이내에 있는 것으로 판단이 되었다. 본 연구에서는 세 종류의 딥러닝 프레임워크만을 살펴보았는데,위키피디아에 따르면 딥러닝 프레임워크의 종류는 12가지가 있으며, 각 프레임워크의 특징을 15가지 속성으로구분하여 차이를 특정하고 있다. 그 많은 속성 중에서 사용자의 입장에서 볼 때 중요한 속성은 어떤 언어(파이썬, C++, Java, 등)로 사용가능한지, 어떤 딥러닝 모형에 대한 라이브러리가 잘 구현되어 있는지 등일 것이다. 그리고 사용자가 대규모의 딥러닝 모형을 구축한다면, 다중 GPU 혹은 다중 서버를 지원하는지의 여부도 중요할 것이다. 또한 딥러닝 모형을 처음 학습하는 경우에는 사용설명서가 많은지 예제 프로그램이 많은지 여부도 중요한 기준이 될 것이다.The deep learning framework is software designed to help develop deep learning models. Some of its important functions include “automatic differentiation” and “utilization of GPU”. The list of popular deep learning framework includes Caffe (BVLC) and Theano (University of Montreal). And recently, Microsoft's deep learning framework, Microsoft Cognitive Toolkit, was released as open-source license, following Google’s Tensorflow a year earlier. The early deep learning frameworks have been developed mainly for research at universities. Beginning with the inception of Tensorflow, however, it seems that companies such as Microsoft and Facebook have started to join the competition of framework development. Given the trend, Google and other companies are expected to continue investing in the deep learning framework to bring forward the initiative in the artificial intelligence business. From this point of view, we think it is a good time to compare some of deep learning frameworks. So we compare three deep learning frameworks which can be used as a Python library. Those are Google's Tensorflow, Microsoft’s CNTK, and Theano which is sort of a predecessor of the preceding two.
The most common and important function of deep learning frameworks is the ability to perform automatic differentiation. Basically all the mathematical expressions of deep learning models can be represented as computational graphs, which consist of nodes and edges. Partial derivatives on each edge of a computational graph can then be obtained. With the partial derivatives, we can let software compute differentiation of any node with respect to any variable by utilizing chain rule of Calculus.
First of all, the convenience of coding is in the order of CNTK, Tensorflow, and Theano. The criterion is simply based on the lengths of the codes and the learning curve and the ease of coding are not the main concern. According to the criteria, Theano was the most difficult to implement with, and CNTK and Tensorflow were somewhat easier. With Tensorflow, we need to define weight variables and biases explicitly. The reason that CNTK and Tensorflow are easier to implement with is that those frameworks provide us with more abstraction than Theano. We, however, need to mention that low-level coding is not always bad. It gives us flexibility of coding. With the low-level coding such as in Theano, we can implement and test any new deep learning models or any new search methods that we can think of.
The assessment of the execution speed of each framework is that there is not meaningful difference. According to the experiment, execution speeds of Theano and Tensorflow are very similar, although the experiment was limited to a CNN model. In the case of CNTK, the experimental environment was not maintained as the same. The code written in CNTK has to be run in PC environment without GPU where codes execute as much as 50 times slower than with GPU. But we concluded that the difference of execution speed was within the range of variation caused by the different hardware setup.
In this study, we compared three types of deep learning framework: Theano, Tensorflow, and CNTK. According to Wikipedia, there are 12 available deep learning frameworks. And 15 different attributes differentiate each framework. Some of the important attributes would include interface language (Python, C ++, Java, etc.) and the availability of libraries on various deep learning models such as CNN, RNN, DBN, and etc. And if a user implements a large scale deep learning model, it will also be important to support multiple GPU or multiple servers. Also, if you are learning the deep learning model, it would also be important if there are enough examples and references.deep learning framework, Theano, TensorFlow, CNTK, computational graph, CIFAR-10"
270,딥러닝을 이용한 주변 무선단말 파악방안,이웅섭 /Lee， Woong-sup,2018,한국정보통신학회,"최근 단말-대-단말(Device-to-device, D2D) 통신기술이 차세대 무선통신시스템의 핵심기술로 큰 관심을 받고 있다. 이러한 단말간 통신에서는 자신의 주변에 어떠한 단말이 있는지 파악하는 주변단말 탐색(Neighbor discovery)이매우 중요하다. 본 논문에서는 최근 큰 관심을 받고 있는 딥러닝(Deep learning) 기술을 활용하여 단말간 통신에서 주변단말을 파악하는 방안에 대해서 제안한다. 제안 방안은 기존의 방안과 달리 무선채널의 공간적 연관성을 이용하여단말간의 신호 전송 없이 단말이 기지국으로 전송하는 상향링크 파일럿 신호를 기반으로 주변 단말을 찾고 따라서기존의 방식에 비해 신호전송 복잡도(signaling complexity)를 크게 줄일 수 있다. 또한 제안 방안에서는 떨어져 있는거리에 따라서 주변 단말을 분류 가능하여 기존 방안에 비해서 좀 더 세밀한 단말 탐색이 가능하다. 마지막으로 본 논문에서는 tensorflow를 이용한 컴퓨터 시뮬레이션을 통해 제안 방안의 성능을 검증하였다.Recently, the device-to-device (D2D) communication has been conceived as the key technology for the next-generationmobile communication systems. The neighbor discovery in which the nearby users are found, is essential for the properoperation of the D2D communication. In this paper, we propose new neighbor discovery scheme based on deep learningtechnology which has gained a lot of attention recently. In the proposed scheme, the neighboring users can be foundusing the uplink pilot transmission of users only, unlike conventional neighbor discovery schemes in which direct pilotcommunication among users is required, such that the signaling overhead can be greatly reduced in our proposed scheme.
Moreover, the neighbors with different proximity can also be classified accordingly which enables more accurate neighbordiscovery compared to the conventional schemes. The performance of our proposed scheme is verified through thetensorflow-based computer simulations.
키워드 : 딥러닝, 주변단말 탐색, 단말-대-단말 통신Deep learning, Neighbor discovery, Device-to-device communication, Wireless channel correlation"
271,신뢰성있는 딥러닝 기반 분석 모델을 참조하기 위한 딥러닝 기술 언어,문종혁 /Jong Hyeok Mun,2021,한국정보처리학회,"최근 딥러닝은 하드웨어 성능이 향상됨에 따라 자연어 처리, 영상 인식 등의 다양한 기술에 접목되어 활용되고 있다. 이러한 기술들을 활용해 지능형교통 시스템(ITS), 스마트홈, 헬스케어 등의 산업분야에서 데이터를 분석하여 고속도로 속도위반 차량 검출, 에너지 사용량 제어, 응급상황 등과 같은고품질의 서비스를 제공하며, 고품질의 서비스를 제공하기 위해서는 정확도가 향상된 딥러닝 모델이 적용되어야 한다. 이를 위해 서비스 환경의 데이터를분석하기 위한 딥러닝 모델을 개발할 때, 개발자는 신뢰성이 검증된 최신의 딥러닝 모델을 적용할 수 있어야 한다. 이는 개발자가 참조하는 딥러닝 모델에적용된 학습 데이터셋의 정확도를 측정하여 검증할 수 있다. 이러한 검증을 위해서 개발자는 학습 데이터셋, 딥러닝의 계층구조 및 개발 환경 등과 같은내용을 포함하는 딥러닝 모델을 문서화하여 적용하기 위한 구조적인 정보가 필요하다. 본 논문에서는 신뢰성있는 딥러닝 기반 데이터 분석 모델을 참조하기위한 딥러닝 기술 언어를 제안한다. 제안하는 기술 언어는 신뢰성 있는 딥러닝 모델을 개발하는데 필요한 학습데이터셋, 개발 환경 및 설정 등의 정보와더불어 딥러닝 모델의 계층구조를 표현할 수 있다. 제안하는 딥러닝 기술 언어를 이용하여 개발자는 지능형 교통 시스템에서 참조하는 분석 모델의정확도를 검증할 수 있다. 실험에서는 제안하는 언어의 유효성을 검증하기 위해, 번호판 인식 모델을 중심으로 딥러닝 기술 문서의 적용과정을 보인다With the recent advancements of deep learning, companies such as smart home, healthcare, and intelligent transportation systems areutilizing its functionality to provide high-quality services for vehicle detection, emergency situation detection, and controlling energyconsumption. To provide reliable services in such sensitive systems, deep learning models are required to have high accuracy. In orderto develop a deep learning model for analyzing previously mentioned services, developers should utilize the state of the art deep learningmodels that have already been verified for higher accuracy. The developers can verify the accuracy of the referenced model by validatingthe model on the dataset. For this validation, the developer needs structural information to document and apply deep learning models,including metadata such as learning dataset, network architecture, and development environments. In this paper, we propose a descriptionlanguage that represents the network architecture of the deep learning model along with its metadata that are necessary to develop adeep learning model. Through the proposed description language, developers can easily verify the accuracy of the referenced deep learningmodel. Our experiments demonstrate the application scenario of a deep learning description document that focuses on the license platerecognition for the detection of illegally parked vehicles.Trusted Deep Learning, Model Reference, Deep Learning Description Language, Traffic Situation Analysis Model"
272,딥러닝 분석을 이용한 중국 역내·외 위안화 변동성 예측,이우식 /Lee Woosik,2016,한국데이터정보과학회,"2008년 글로벌 금융위기 이후 중국은 위안화 국제화의 점진적 추진을 진행하면서 중국상하이 외환시장과 중국홍콩 외환시장에서 거래되는 통화인 역내위안화와 역외위안화를 형성시켰다. 본 연구는 위안화 국제화와 점진적인 중국 자본계정 개방에 따라 급변하는 외환시장상황의 변동성을 정확하게 파악하기 위해서 GARCH모형 (일반화된 자기회귀 조건부이분산성모형)에 다단계인공신경망을 결합한 MLP-GARCH 모형과 GARCH모형과 기계학습의 일종인 딥러닝 (deep learning)을 통합한 DL-GARCH을 가지고 위안화 변동성예측을 비교 실험과 분석을 하였다. 비교분석 결과 DLGARCH 모형은 MLP-GARCH보다 모형 위안화 역내·외 환율변동성 예측 면에서 더욱 더 개선된예측값을 제공하였다. 그래서 이분산시계열모형을 딥러닝과 결합한 DL-GARCH 모형은 시계열의환율 변동성 예측 문제에 딥러닝을 응용할 수 있음을 확인하였다. 향후 이분산시계열과 결합된 딥러닝 모형은 다른 금융시계열 데이터에 응용하여 그 일반화 가능성을 높일 수 있을 것이다.The People’s Republic of China has vigorously been pursuing the internationalization of the Chinese Yuan or Renminbi after the financial crisis of 2008. In this view, an abrupt increase of use of the Chinese Yuan in the onshore and offshore markets are important milestones to be one of important currencies. One of the most frequently used methods to forecast volatility is GARCH model. Since a prediction error of the GARCH model has been reported quite high, a lot of efforts have been made to improve forecasting capability of the GARCH model. In this paper, we have proposed MLPGARCH and a DL-GARCH by employing Artificial Neural Network to the GARCH. In an application to forecasting Chinese Yuan volatility, we have successfully shown their overall outperformance in forecasting over the GARCH.Deep learning, DL-GARCH, GARCH, The Chinese Yuan, volatility."
273,딥러닝 기법을 이용한 내일강수 예측,하지훈,2016,한국지능시스템학회,"정확한 강수예측을 위해서는 예측인자 선정과 예측방법에 대한 선택이 매우 중요하다. 최근에는 강수예측 방법으로기계학습 기법이 많이 사용되고 있으며, 그 중에서도 특히 인공신경망을 사용한 강수예측 방법은 좋은 성능을 보였다. 본논문에서는 딥러닝 기법 중 하나인 DBN(deep belief network)를 이용한 새로운 강수예측 방법을 제안한다. DBN는 비지도사전 학습을 통해 초기 가중치를 설정하여 기존 인공신경망의 문제점을 보완한다. 예측인자로는 기온, 전일-전주 강수일, 태양과 달 궤도 관련 자료를 선정하였다. 기온과 전일-전주 강수일은 서울에서의 1974년부터 2013년까지 총 40년간의AWS(automatic weather system) 관측 자료를 사용하였고, 태양과 달의 궤도 관련 자료는 서울을 중심으로 계산한 결과를사용하였다. 전체 기간에서 일부는 학습 자료로 사용하여 예측모델을 생성하였고, 나머지를 생성한 모델의 검증 자료로사용하였다. 모델 검증 결과로 나온 예측값들은 확률값을 가지며 임계치를 이용하여 강수유무를 판별하였다. 강수 정확도의척도로 양분예보기법 중 CSI(critical successive index)와 Bias(frequency bias)를 계산하였다. 이를 통해 DBN와 MLP(multilayer perceptron)의 성능을 비교한 결과 DBN의 강수 예측 정확도가 높았고, 수행속도 또한 2배 이상 빨랐다.For accurate precipitation forecasts the choice of weather faoctrs and prediction method is very important. Recently, machinele arning has been widely used for forecasting precipitation, and artificial neural network, one of machine learning techniques, showed good performance. In this paper, we suggest a new method for forecasting precipitation using DBN, one of deep learning techniques.
DBN has an advantage that initial weights are set by unsupervised learning, so this compensates for the defects of artificial neural networks. We used past precipitation, temperature, and the parameters of the sun and moon's motion as features for forecasting precipitation. The dataset consists of observation data which had been measured for 40 years from AWS in Seoul. Experiments were based on 8-fold cross validation. As a result of estimation, we got probabilities of test dataset, so threshold was used for the decision of precipitation. CSI and Bias were used for indicating the precision of precipitation. Our experimental results showed that DBN performed better than MLP.Deep learning, Deep belief network, Precipitation, Forecast"
274,딥러닝 신경망모형을 이용한 실시간 교통정보수집 연구,김대현 /KIM DAE HYON,2016,사단법인 인문사회과학기술융합학회,"비디오 기반의 실시간 교통정보수집 시스템은 자율주행을 포함한 지능형교통체계의 주요 서브시스템중 하나이며, 우수한 예측(추정)력 때문에 핵심 알고리즘으로 인공신경망 모형이 주로 사용되고 있다. 특히, 최근 과학 학술지 네이처에 구글 딥마인드의 알파고가 공개되면서 딥러닝과 인공지능에 대한 전 세계적인 관심이 집중되고 있다. 최근까지 가장 널리 사용된 인공신경망모형은 역전파 모형이었으나 최근 컴퓨터 하드웨어 및 알고리즘의 새로운 개발과 더불어 딥러닝 알고리즘을 이용한 딥인공신경망 모형에 대한 관심이 높아지고 있다. 그러나 아직 딥러닝 인공신경망에 대한 연구는 초기단계에 있는 실정이며 적용분야도 극히 한정되어 있다. 본 연구에서는 실시간 도로교통정보 수집을 위한 영상처리분야에서 딥러닝 알고리즘을 활용하고 실험결과를 통하여 향후 교통정보수집‧분석 분야에서의 딥러닝 인공신경망 모형의 적용가능성을 살펴보고자 한다. 특히, 본 연구에서 도출한 딥러닝 인공신경망모형의 실험결과를 기존 역전파 모형에 의한 실험결과와 비교하여 모형의 우수성을 비교‧평가하고자 한다.Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Normally, artificial neural networks have been used in real-time applications for collecting traffic information because of their remarkable performance in terms of predictive accuracy and real-time computing. Especially, the recent publication of a paper in the journal Nature and the success of Google DeepMind’s world-beating Go program AlphaGo caused significant increases of the public interest in artificial intelligence. Even though various types of learning models have been proposed, the Backpropagation has been the most popular neural network model. However, the Deep learning neural network model has attracted much interest recently. In this study, the Deep leaning neural network will be applied in image processing techniques for the real-time traffic information collection to investigate the performance of Deep learning algorithm in traffic engineering fields. In addition, the experimental results from Deep learing algorithm will be compared with the results from Backpropagation model to evaluate and propose better performing model.Intelligent Transportation Systems (ITS), artificial neural networks, Deep learning, traffic information, image processing"
275,딥러닝의 모형과 응용사례,안성만 /SungMahn Ahn,2016,한국지능정보시스템학회,"딥러닝은 인공신경망(neural network)이라는 인공지능분야의 모형이 발전된 형태로서, 계층구조로 이루어진 인공신경망의 내부계층(hidden layer)이 여러 단계로 이루어진 구조이다. 딥러닝에서의 주요 모형은 합성곱신경망(convolutional neural network), 순환신경망(recurrent neural network), 그리고 심층신뢰신경망(deep belief network)의 세가지라고 할 수있다. 그 중에서 현재 흥미로운 연구가 많이 발표되어서 관심이 집중되고 있는 모형은 지도학습(supervised learning)모형인 처음 두 개의 모형이다. 따라서 본 논문에서는 지도학습모형의 가중치를 최적화하는 기본적인 방법인 오류역전파 알고리즘을 살펴본 뒤에 합성곱신경망과 순환신경망의 구조와 응용사례 등을 살펴보고자 한다. 본문에서 다루지 않은 모형인 심층신뢰신경망은 아직까지는 합성곱신경망 이나 순환신경망보다는 상대적으로 주목을 덜 받고 있다. 그러나 심층신뢰신경망은 CNN이나 RNN과는 달리 비지도학습(unsupervised learning)모형이며, 사람이나 동물은 관찰을 통해서 스스로 학습한다는 점에서 궁극적으로는 비지도학습모형이 더 많이 연구되어야 할 주제가 될 것이다.Deep learning model is a kind of neural networks that allows multiple hidden layers. There are various deep learning architectures such as convolutional neural networks, deep belief networks and recurrent neural networks. Those have been applied to fields like computer vision, automatic speech recognition, natural language processing, audio recognition and bioinformatics where they have been shown to produce state-of-the-art results on various tasks. Among those architectures, convolutional neural networks and recurrent neural networks are classified as the supervised learning model. And in recent years, those supervised learning models have gained more popularity than unsupervised learning models such as deep belief networks, because supervised learning models have shown fashionable applications in such fields mentioned above.
Deep learning models can be trained with backpropagation algorithm. Backpropagation is an abbreviation for “backward propagation of errors” and a common method of training artificial neural networks used in conjunction with an optimization method such as gradient descent. The method calculates the gradient of an error function with respect to all the weights in the network. The gradient is fed to the optimization method which in turn uses it to update the weights, in an attempt to minimize the error function.
Convolutional neural networks use a special architecture which is particularly well-adapted to classify images. Using this architecture makes convolutional networks fast to train. This, in turn, helps us train deep, muti-layer networks, which are very good at classifying images. These days, deep convolutional networks are used in most neural networks for image recognition. Convolutional neural networks use three basic ideas: local receptive fields, shared weights, and pooling. By local receptive fields, we mean that each neuron in the first(or any) hidden layer will be connected to a small region of the input(or previous layer’s) neurons. Shared weights mean that we’re going to use the same weights and bias for each of the local receptive field. This means that all the neurons in the hidden layer detect exactly the same feature, just at different locations in the input image. In addition to the convolutional layers just described, convolutional neural networks also contain pooling layers. Pooling layers are usually used immediately after convolutional layers. What the pooling layers do is to simplify the information in the output from the convolutional layer.
Recent convolutional network architectures have 10 to 20 hidden layers and billions of connections between units. Training deep learning networks has taken weeks several years ago, but thanks to progress in GPU and algorithm enhancement, training time has reduced to several hours.
Neural networks with time-varying behavior are known as recurrent neural networks or RNNs. A recurrent neural network is a class of artificial neural network where connections between units form a directed cycle. This creates an internal state of the network which allows it to exhibit dynamic temporal behavior. Unlike feedforward neural networks, RNNs can use their internal memory to process arbitrary sequences of inputs. Early RNN models turned out to be very difficult to train, harder even than deep feedforward networks. The reason is the unstable gradient problem such as vanishing gradient and exploding gradient. The gradient can get smaller and smaller as it is propagated back through layers. This makes learning in early layers extremely slow. The problem actually gets worse in RNNs, since gradients aren’t just propagated backward through layers, they’re propagated backward through time. If the network runs for a long time, that can make the gradient extremely unstable and hard to learn from. It has been possible to incorporate an idea known as long short-term memory units (LSTMs) into RNNs. LSTMs make it much easier to get good results when training RNNs, and many recent papers make use of LSTMs or related ideas.Deep learning, Convolutional neural networks, Recurrent neural networks, Error backpropagation algorithm"
276,딥러닝분석과 기술적 분석 지표를 이용한 한국 코스피주가지수 방향성 예측,이우식 /Lee Woosik,2017,한국데이터정보과학회,"2016년 3월 구글 (Google)의 바둑인공지능 알파고 (AlphaGo)가 이세돌 9단과의 바둑대결에서승리한 이후 다양한 분야에서 인공지능 사용에 대한 관심이 높아지고 있는 가운데 금융투자 분야에서도 인공지능과 투자자문 전문가의 합성어인 로보어드바이저 (Robo-Advisor)에 대한 관심이 높아지고 있다. 인공지능 (artificial intelligence)기반의 의사결정은 비용 절감은 물론 효과적인 의사결정을가능하게 한다는 점에서 큰 장점이 있다. 본 연구에서는 기술적 분석 (technical analysis) 지표와 딥러닝 (deep learning) 모형을 결합하여 한국 코스피 지수를 예측하는 모형을 개발하고 제시한 모형들의 예측력을 비교, 분석한다. 분석 결과 기술적 분석 지표에 딥러닝 알고리즘을 결합한 모형이 주가지수 방향성 예측 문제에 응용될 수 있음을 확인하였다. 향후 본 연구에서 제안된 기술적 분석 지표와딥러닝모형을 결합한 기법은 로보어드바이저서비스에 응용할 수 있는 일반화 가능성을 보여준다Since Google’s AlphaGo defeated a world champion of Go players in 2016, there have been many interests in the deep learning. In the financial sector, a Robo-Advisor using deep learning gains a significant attention, which builds and manages portfolios of financial instruments for investors.In this paper, we have proposed the a deep learning algorithm geared toward identification and forecast of the KOSPI index direction,and we also have compared the accuracy of the prediction.In an application of forecasting the financial market index direction, we have shown that the Robo-Advisor using deep learning has a significant effect on finance industry. The Robo-Advisor collects a massive data such as earnings statements, news reports and regulatory filings, analyzes those and recommends investors how to view market trends and identify the best time to purchase financial assets. On the other hand, the Robo-Advisor allows businesses to learn more about their customers, develop better marketing strategies, increase sales and decrease costs.Artificial intelligence, deep learning, FinTech, Robo-Advisor, technical analysis."
277,딥러닝의 효율적인 학습을 위한 학습데이터 선별 기법에 관한 연구,주경돈 /Gyoung-don Joo,2017,한국정보과학회,"빅데이터의 증가와 병렬 하드웨어 처리, 학습 알고리즘의 개선은 딥러닝의 발전을 가져왔으며 인공지능이 적용 가능한 영역을 획기적으로 확장시키고 있다. 하지만 이러한 대규모의 학습 데이터에 기반한 딥러닝은 학습시간이 증가되고, 더 나아가 학습 데이터의 구축을 위한 레이블링(labeling) 비용이 데이터 수에 비례하여 증가하게 된다. 따라서 본 논문에서는, 딥러닝의 정확도를 최대한 유지하면서 필요한 학습 데이터의 수를 줄일수 있도록, 전체 데이터에서 제한된 수의 학습 데이터의 후보를 선별하는 기법에 대하여 연구한다. 특히 기존의 능동 학습에 기반한 기법들이 학습 데이터의 수가 매우 적어질 경우 딥러닝의 정확도가 급격히 떨어지는 문제를 해결할 수 있는 새로운 기법을 제안하였으며, 제안된 기법으로 선별된 학습 데이터는 그 수의 변화에 대해 딥러닝이 일관되게 경쟁력있는 정확도를 가짐을 다양한 실험을 통하여 검증하였다.The increase of big data, parallel hardware processing and enhanced learning algorithms have led to the development of deep learning and are dramatically expanding the domain of applications based on artificial intelligence. However, such large-scale training data extends the learning time, and furthermore, the labeling cost for building the training set increases in proportion to the volume of data. Therefore, in this paper, we study how to select a limited number of candidates for training set from given data so as to reduce the number of training set while maintaining the accuracy of deep learning. Especially, we found that, with the training set selected by the previous work, the accuracy of deep learning drastically decreases when the number of training set is very small and so proposed new methods to resolve this fall-off in accuracy. Through experiments, we showed that deep learning consistently has the competitive accuracy with the training set selected by the proposed method regardless of the size of training set.Deep Learning, Train Set Selection, Active Learning, Gamma distribution"
278,딥러닝을 이용한 범용적 스테그아날리시스,김현재 /Hyunjae Kim,2017,한국정보과학회,"스테그아날리시스(Steganalysis)란 이미지 등 일반적인 자료에 암호화된 정보를 은닉하는 스테가노그래피(Steganography)에 대한 검출 및 분석 방법으로, 기계학습 기반 방법론을 포함한다. 기존 기계학습 기반 스테그아날리시스는 영상(Image)의 특징(Feature) 추출 및 모델링에 기반하며, 최근 딥러닝(Deep Learning)의 적용으로 검출 정확도가 큰 폭으로 향상되었다. 하지만 현존하는 스테그아날리시스 모델은 단일 스테가노그래피 기법에 대해 국한되어 있어 학습에 사용되지 않은 스테고(Stego) 이미지의 경우 검출이 불가능한 결정적 한계를 가진다. 본 연구에서는 다양한 스테가노그래피 기법으로 생성된 스테고이미지에 딥러닝을 적용하여 스테그아날리시스를 학습하는 범용적 모델을 제안한다. 다양한 실험을 통해 제안 기법의 효용성 및 가능성을 확인하고, 범용적 스테그아날리시스 모델이 각각에 특화된 검출 기법과 유사한 정확도로 스테고 이미지를 검출할 수 있음을 보인다.Steganalysis is to detect information hidden by steganography inside general data such as images. There are stegoanalysis techniques that use machine learning (ML). Existing ML approaches to steganalysis are based on extracting features from stego images and modeling them.
Recently deep learning-based methodologies have shown significant improvements in detection accuracy. However, all the existing methods, including deep learning-based ones, have a critical limitation in that they can only detect stego images that are created by a specific steganography method. In this paper, we propose a generalized steganalysis method that can model multiple types of stego images using deep learning. Through various experiments, we confirm the effectiveness of our approach and envision directions for future research. In particular, we show that our method can detect each type of steganography with the same level of accuracy as that of a steganalysis method dedicated to that type of steganography, thereby demonstrating the general applicability of our approach to multiple types of stego images.steganalysis, steganography, deep learning, generalized steganalysis model"
279,"딥러닝 프레임워크의 비교: 티아노, 텐서플로, CNTK를 중심으로",정여진 /Yeojin Chung,2017,한국지능정보시스템학회,"딥러닝 프레임워크의 대표적인 기능으로는 ‘자동미분’과 ‘GPU의 활용’ 등을 들 수 있다. 본 논문은 파이썬의 라이브러리 형태로 사용 가능한 프레임워크 중에서 구글의 텐서플로와 마이크로소프트의 CNTK, 그리고 텐서 플로의 원조라고 할 수 있는 티아노를 비교하였다. 본문에서는 자동미분의 개념과 GPU의 활용형태를 간단히 설명하고, 그 다음에 logistic regression을 실행하는 예를 통하여 각 프레임워크의 문법을 알아본 뒤에, 마지막으로 대표적인 딥러닝 응용인 CNN의 예제를 실행시켜보고 코딩의 편의성과 실행속도 등을 확인해 보았다. 그 결과, 편의성의 관점에서 보면 티아노가 가장 코딩 하기가 어렵고, CNTK와 텐서플로는 많은 부분이 비슷하게 추상화 되어 있어서 코딩이 비슷하지만 가중치와 편향을 직접 정의하느냐의 여부에서 차이를 보였다. 그리고 각프레임워크의 실행속도에 대한 평가는 ‘큰 차이는 없다’는 것이다. 텐서플로는 티아노에 비하여 속도가 느리다는 평가가 있어왔는데, 본 연구의 실험에 의하면, 비록 CNN 모형에 국한되었지만, 텐서플로가 아주 조금이지만빠른 것으로 나타났다. CNTK의 경우에도, 비록 실험환경이 달랐지만, 실험환경의 차이에 의한 속도의 차이의 편차범위 이내에 있는 것으로 판단이 되었다. 본 연구에서는 세 종류의 딥러닝 프레임워크만을 살펴보았는데,위키피디아에 따르면 딥러닝 프레임워크의 종류는 12가지가 있으며, 각 프레임워크의 특징을 15가지 속성으로구분하여 차이를 특정하고 있다. 그 많은 속성 중에서 사용자의 입장에서 볼 때 중요한 속성은 어떤 언어(파이썬, C++, Java, 등)로 사용가능한지, 어떤 딥러닝 모형에 대한 라이브러리가 잘 구현되어 있는지 등일 것이다. 그리고 사용자가 대규모의 딥러닝 모형을 구축한다면, 다중 GPU 혹은 다중 서버를 지원하는지의 여부도 중요할 것이다. 또한 딥러닝 모형을 처음 학습하는 경우에는 사용설명서가 많은지 예제 프로그램이 많은지 여부도 중요한 기준이 될 것이다.The deep learning framework is software designed to help develop deep learning models. Some of its important functions include “automatic differentiation” and “utilization of GPU”. The list of popular deep learning framework includes Caffe (BVLC) and Theano (University of Montreal). And recently, Microsoft's deep learning framework, Microsoft Cognitive Toolkit, was released as open-source license, following Google’s Tensorflow a year earlier. The early deep learning frameworks have been developed mainly for research at universities. Beginning with the inception of Tensorflow, however, it seems that companies such as Microsoft and Facebook have started to join the competition of framework development. Given the trend, Google and other companies are expected to continue investing in the deep learning framework to bring forward the initiative in the artificial intelligence business. From this point of view, we think it is a good time to compare some of deep learning frameworks. So we compare three deep learning frameworks which can be used as a Python library. Those are Google's Tensorflow, Microsoft’s CNTK, and Theano which is sort of a predecessor of the preceding two.
The most common and important function of deep learning frameworks is the ability to perform automatic differentiation. Basically all the mathematical expressions of deep learning models can be represented as computational graphs, which consist of nodes and edges. Partial derivatives on each edge of a computational graph can then be obtained. With the partial derivatives, we can let software compute differentiation of any node with respect to any variable by utilizing chain rule of Calculus.
First of all, the convenience of coding is in the order of CNTK, Tensorflow, and Theano. The criterion is simply based on the lengths of the codes and the learning curve and the ease of coding are not the main concern. According to the criteria, Theano was the most difficult to implement with, and CNTK and Tensorflow were somewhat easier. With Tensorflow, we need to define weight variables and biases explicitly. The reason that CNTK and Tensorflow are easier to implement with is that those frameworks provide us with more abstraction than Theano. We, however, need to mention that low-level coding is not always bad. It gives us flexibility of coding. With the low-level coding such as in Theano, we can implement and test any new deep learning models or any new search methods that we can think of.
The assessment of the execution speed of each framework is that there is not meaningful difference. According to the experiment, execution speeds of Theano and Tensorflow are very similar, although the experiment was limited to a CNN model. In the case of CNTK, the experimental environment was not maintained as the same. The code written in CNTK has to be run in PC environment without GPU where codes execute as much as 50 times slower than with GPU. But we concluded that the difference of execution speed was within the range of variation caused by the different hardware setup.
In this study, we compared three types of deep learning framework: Theano, Tensorflow, and CNTK. According to Wikipedia, there are 12 available deep learning frameworks. And 15 different attributes differentiate each framework. Some of the important attributes would include interface language (Python, C ++, Java, etc.) and the availability of libraries on various deep learning models such as CNN, RNN, DBN, and etc. And if a user implements a large scale deep learning model, it will also be important to support multiple GPU or multiple servers. Also, if you are learning the deep learning model, it would also be important if there are enough examples and references.deep learning framework, Theano, TensorFlow, CNTK, computational graph, CIFAR-10"
280,딥러닝을 이용한 주변 무선단말 파악방안,이웅섭 /Lee， Woong-sup,2018,한국정보통신학회,"최근 단말-대-단말(Device-to-device, D2D) 통신기술이 차세대 무선통신시스템의 핵심기술로 큰 관심을 받고 있다. 이러한 단말간 통신에서는 자신의 주변에 어떠한 단말이 있는지 파악하는 주변단말 탐색(Neighbor discovery)이매우 중요하다. 본 논문에서는 최근 큰 관심을 받고 있는 딥러닝(Deep learning) 기술을 활용하여 단말간 통신에서 주변단말을 파악하는 방안에 대해서 제안한다. 제안 방안은 기존의 방안과 달리 무선채널의 공간적 연관성을 이용하여단말간의 신호 전송 없이 단말이 기지국으로 전송하는 상향링크 파일럿 신호를 기반으로 주변 단말을 찾고 따라서기존의 방식에 비해 신호전송 복잡도(signaling complexity)를 크게 줄일 수 있다. 또한 제안 방안에서는 떨어져 있는거리에 따라서 주변 단말을 분류 가능하여 기존 방안에 비해서 좀 더 세밀한 단말 탐색이 가능하다. 마지막으로 본 논문에서는 tensorflow를 이용한 컴퓨터 시뮬레이션을 통해 제안 방안의 성능을 검증하였다.Recently, the device-to-device (D2D) communication has been conceived as the key technology for the next-generationmobile communication systems. The neighbor discovery in which the nearby users are found, is essential for the properoperation of the D2D communication. In this paper, we propose new neighbor discovery scheme based on deep learningtechnology which has gained a lot of attention recently. In the proposed scheme, the neighboring users can be foundusing the uplink pilot transmission of users only, unlike conventional neighbor discovery schemes in which direct pilotcommunication among users is required, such that the signaling overhead can be greatly reduced in our proposed scheme.
Moreover, the neighbors with different proximity can also be classified accordingly which enables more accurate neighbordiscovery compared to the conventional schemes. The performance of our proposed scheme is verified through thetensorflow-based computer simulations.
키워드 : 딥러닝, 주변단말 탐색, 단말-대-단말 통신Deep learning, Neighbor discovery, Device-to-device communication, Wireless channel correlation"
281,신뢰성있는 딥러닝 기반 분석 모델을 참조하기 위한 딥러닝 기술 언어,문종혁 /Jong Hyeok Mun,2021,한국정보처리학회,"최근 딥러닝은 하드웨어 성능이 향상됨에 따라 자연어 처리, 영상 인식 등의 다양한 기술에 접목되어 활용되고 있다. 이러한 기술들을 활용해 지능형교통 시스템(ITS), 스마트홈, 헬스케어 등의 산업분야에서 데이터를 분석하여 고속도로 속도위반 차량 검출, 에너지 사용량 제어, 응급상황 등과 같은고품질의 서비스를 제공하며, 고품질의 서비스를 제공하기 위해서는 정확도가 향상된 딥러닝 모델이 적용되어야 한다. 이를 위해 서비스 환경의 데이터를분석하기 위한 딥러닝 모델을 개발할 때, 개발자는 신뢰성이 검증된 최신의 딥러닝 모델을 적용할 수 있어야 한다. 이는 개발자가 참조하는 딥러닝 모델에적용된 학습 데이터셋의 정확도를 측정하여 검증할 수 있다. 이러한 검증을 위해서 개발자는 학습 데이터셋, 딥러닝의 계층구조 및 개발 환경 등과 같은내용을 포함하는 딥러닝 모델을 문서화하여 적용하기 위한 구조적인 정보가 필요하다. 본 논문에서는 신뢰성있는 딥러닝 기반 데이터 분석 모델을 참조하기위한 딥러닝 기술 언어를 제안한다. 제안하는 기술 언어는 신뢰성 있는 딥러닝 모델을 개발하는데 필요한 학습데이터셋, 개발 환경 및 설정 등의 정보와더불어 딥러닝 모델의 계층구조를 표현할 수 있다. 제안하는 딥러닝 기술 언어를 이용하여 개발자는 지능형 교통 시스템에서 참조하는 분석 모델의정확도를 검증할 수 있다. 실험에서는 제안하는 언어의 유효성을 검증하기 위해, 번호판 인식 모델을 중심으로 딥러닝 기술 문서의 적용과정을 보인다With the recent advancements of deep learning, companies such as smart home, healthcare, and intelligent transportation systems areutilizing its functionality to provide high-quality services for vehicle detection, emergency situation detection, and controlling energyconsumption. To provide reliable services in such sensitive systems, deep learning models are required to have high accuracy. In orderto develop a deep learning model for analyzing previously mentioned services, developers should utilize the state of the art deep learningmodels that have already been verified for higher accuracy. The developers can verify the accuracy of the referenced model by validatingthe model on the dataset. For this validation, the developer needs structural information to document and apply deep learning models,including metadata such as learning dataset, network architecture, and development environments. In this paper, we propose a descriptionlanguage that represents the network architecture of the deep learning model along with its metadata that are necessary to develop adeep learning model. Through the proposed description language, developers can easily verify the accuracy of the referenced deep learningmodel. Our experiments demonstrate the application scenario of a deep learning description document that focuses on the license platerecognition for the detection of illegally parked vehicles.Trusted Deep Learning, Model Reference, Deep Learning Description Language, Traffic Situation Analysis Model"
282,딥러닝 분석을 이용한 중국 역내·외 위안화 변동성 예측,이우식 /Lee Woosik,2016,한국데이터정보과학회,"2008년 글로벌 금융위기 이후 중국은 위안화 국제화의 점진적 추진을 진행하면서 중국상하이 외환시장과 중국홍콩 외환시장에서 거래되는 통화인 역내위안화와 역외위안화를 형성시켰다. 본 연구는 위안화 국제화와 점진적인 중국 자본계정 개방에 따라 급변하는 외환시장상황의 변동성을 정확하게 파악하기 위해서 GARCH모형 (일반화된 자기회귀 조건부이분산성모형)에 다단계인공신경망을 결합한 MLP-GARCH 모형과 GARCH모형과 기계학습의 일종인 딥러닝 (deep learning)을 통합한 DL-GARCH을 가지고 위안화 변동성예측을 비교 실험과 분석을 하였다. 비교분석 결과 DLGARCH 모형은 MLP-GARCH보다 모형 위안화 역내·외 환율변동성 예측 면에서 더욱 더 개선된예측값을 제공하였다. 그래서 이분산시계열모형을 딥러닝과 결합한 DL-GARCH 모형은 시계열의환율 변동성 예측 문제에 딥러닝을 응용할 수 있음을 확인하였다. 향후 이분산시계열과 결합된 딥러닝 모형은 다른 금융시계열 데이터에 응용하여 그 일반화 가능성을 높일 수 있을 것이다.The People’s Republic of China has vigorously been pursuing the internationalization of the Chinese Yuan or Renminbi after the financial crisis of 2008. In this view, an abrupt increase of use of the Chinese Yuan in the onshore and offshore markets are important milestones to be one of important currencies. One of the most frequently used methods to forecast volatility is GARCH model. Since a prediction error of the GARCH model has been reported quite high, a lot of efforts have been made to improve forecasting capability of the GARCH model. In this paper, we have proposed MLPGARCH and a DL-GARCH by employing Artificial Neural Network to the GARCH. In an application to forecasting Chinese Yuan volatility, we have successfully shown their overall outperformance in forecasting over the GARCH.Deep learning, DL-GARCH, GARCH, The Chinese Yuan, volatility."
283,딥러닝 기법을 이용한 내일강수 예측,하지훈,2016,한국지능시스템학회,"정확한 강수예측을 위해서는 예측인자 선정과 예측방법에 대한 선택이 매우 중요하다. 최근에는 강수예측 방법으로기계학습 기법이 많이 사용되고 있으며, 그 중에서도 특히 인공신경망을 사용한 강수예측 방법은 좋은 성능을 보였다. 본논문에서는 딥러닝 기법 중 하나인 DBN(deep belief network)를 이용한 새로운 강수예측 방법을 제안한다. DBN는 비지도사전 학습을 통해 초기 가중치를 설정하여 기존 인공신경망의 문제점을 보완한다. 예측인자로는 기온, 전일-전주 강수일, 태양과 달 궤도 관련 자료를 선정하였다. 기온과 전일-전주 강수일은 서울에서의 1974년부터 2013년까지 총 40년간의AWS(automatic weather system) 관측 자료를 사용하였고, 태양과 달의 궤도 관련 자료는 서울을 중심으로 계산한 결과를사용하였다. 전체 기간에서 일부는 학습 자료로 사용하여 예측모델을 생성하였고, 나머지를 생성한 모델의 검증 자료로사용하였다. 모델 검증 결과로 나온 예측값들은 확률값을 가지며 임계치를 이용하여 강수유무를 판별하였다. 강수 정확도의척도로 양분예보기법 중 CSI(critical successive index)와 Bias(frequency bias)를 계산하였다. 이를 통해 DBN와 MLP(multilayer perceptron)의 성능을 비교한 결과 DBN의 강수 예측 정확도가 높았고, 수행속도 또한 2배 이상 빨랐다.For accurate precipitation forecasts the choice of weather faoctrs and prediction method is very important. Recently, machinele arning has been widely used for forecasting precipitation, and artificial neural network, one of machine learning techniques, showed good performance. In this paper, we suggest a new method for forecasting precipitation using DBN, one of deep learning techniques.
DBN has an advantage that initial weights are set by unsupervised learning, so this compensates for the defects of artificial neural networks. We used past precipitation, temperature, and the parameters of the sun and moon's motion as features for forecasting precipitation. The dataset consists of observation data which had been measured for 40 years from AWS in Seoul. Experiments were based on 8-fold cross validation. As a result of estimation, we got probabilities of test dataset, so threshold was used for the decision of precipitation. CSI and Bias were used for indicating the precision of precipitation. Our experimental results showed that DBN performed better than MLP.Deep learning, Deep belief network, Precipitation, Forecast"
284,딥러닝 신경망모형을 이용한 실시간 교통정보수집 연구,김대현 /KIM DAE HYON,2016,사단법인 인문사회과학기술융합학회,"비디오 기반의 실시간 교통정보수집 시스템은 자율주행을 포함한 지능형교통체계의 주요 서브시스템중 하나이며, 우수한 예측(추정)력 때문에 핵심 알고리즘으로 인공신경망 모형이 주로 사용되고 있다. 특히, 최근 과학 학술지 네이처에 구글 딥마인드의 알파고가 공개되면서 딥러닝과 인공지능에 대한 전 세계적인 관심이 집중되고 있다. 최근까지 가장 널리 사용된 인공신경망모형은 역전파 모형이었으나 최근 컴퓨터 하드웨어 및 알고리즘의 새로운 개발과 더불어 딥러닝 알고리즘을 이용한 딥인공신경망 모형에 대한 관심이 높아지고 있다. 그러나 아직 딥러닝 인공신경망에 대한 연구는 초기단계에 있는 실정이며 적용분야도 극히 한정되어 있다. 본 연구에서는 실시간 도로교통정보 수집을 위한 영상처리분야에서 딥러닝 알고리즘을 활용하고 실험결과를 통하여 향후 교통정보수집‧분석 분야에서의 딥러닝 인공신경망 모형의 적용가능성을 살펴보고자 한다. 특히, 본 연구에서 도출한 딥러닝 인공신경망모형의 실험결과를 기존 역전파 모형에 의한 실험결과와 비교하여 모형의 우수성을 비교‧평가하고자 한다.Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Normally, artificial neural networks have been used in real-time applications for collecting traffic information because of their remarkable performance in terms of predictive accuracy and real-time computing. Especially, the recent publication of a paper in the journal Nature and the success of Google DeepMind’s world-beating Go program AlphaGo caused significant increases of the public interest in artificial intelligence. Even though various types of learning models have been proposed, the Backpropagation has been the most popular neural network model. However, the Deep learning neural network model has attracted much interest recently. In this study, the Deep leaning neural network will be applied in image processing techniques for the real-time traffic information collection to investigate the performance of Deep learning algorithm in traffic engineering fields. In addition, the experimental results from Deep learing algorithm will be compared with the results from Backpropagation model to evaluate and propose better performing model.Intelligent Transportation Systems (ITS), artificial neural networks, Deep learning, traffic information, image processing"
285,딥러닝의 모형과 응용사례,안성만 /SungMahn Ahn,2016,한국지능정보시스템학회,"딥러닝은 인공신경망(neural network)이라는 인공지능분야의 모형이 발전된 형태로서, 계층구조로 이루어진 인공신경망의 내부계층(hidden layer)이 여러 단계로 이루어진 구조이다. 딥러닝에서의 주요 모형은 합성곱신경망(convolutional neural network), 순환신경망(recurrent neural network), 그리고 심층신뢰신경망(deep belief network)의 세가지라고 할 수있다. 그 중에서 현재 흥미로운 연구가 많이 발표되어서 관심이 집중되고 있는 모형은 지도학습(supervised learning)모형인 처음 두 개의 모형이다. 따라서 본 논문에서는 지도학습모형의 가중치를 최적화하는 기본적인 방법인 오류역전파 알고리즘을 살펴본 뒤에 합성곱신경망과 순환신경망의 구조와 응용사례 등을 살펴보고자 한다. 본문에서 다루지 않은 모형인 심층신뢰신경망은 아직까지는 합성곱신경망 이나 순환신경망보다는 상대적으로 주목을 덜 받고 있다. 그러나 심층신뢰신경망은 CNN이나 RNN과는 달리 비지도학습(unsupervised learning)모형이며, 사람이나 동물은 관찰을 통해서 스스로 학습한다는 점에서 궁극적으로는 비지도학습모형이 더 많이 연구되어야 할 주제가 될 것이다.Deep learning model is a kind of neural networks that allows multiple hidden layers. There are various deep learning architectures such as convolutional neural networks, deep belief networks and recurrent neural networks. Those have been applied to fields like computer vision, automatic speech recognition, natural language processing, audio recognition and bioinformatics where they have been shown to produce state-of-the-art results on various tasks. Among those architectures, convolutional neural networks and recurrent neural networks are classified as the supervised learning model. And in recent years, those supervised learning models have gained more popularity than unsupervised learning models such as deep belief networks, because supervised learning models have shown fashionable applications in such fields mentioned above.
Deep learning models can be trained with backpropagation algorithm. Backpropagation is an abbreviation for “backward propagation of errors” and a common method of training artificial neural networks used in conjunction with an optimization method such as gradient descent. The method calculates the gradient of an error function with respect to all the weights in the network. The gradient is fed to the optimization method which in turn uses it to update the weights, in an attempt to minimize the error function.
Convolutional neural networks use a special architecture which is particularly well-adapted to classify images. Using this architecture makes convolutional networks fast to train. This, in turn, helps us train deep, muti-layer networks, which are very good at classifying images. These days, deep convolutional networks are used in most neural networks for image recognition. Convolutional neural networks use three basic ideas: local receptive fields, shared weights, and pooling. By local receptive fields, we mean that each neuron in the first(or any) hidden layer will be connected to a small region of the input(or previous layer’s) neurons. Shared weights mean that we’re going to use the same weights and bias for each of the local receptive field. This means that all the neurons in the hidden layer detect exactly the same feature, just at different locations in the input image. In addition to the convolutional layers just described, convolutional neural networks also contain pooling layers. Pooling layers are usually used immediately after convolutional layers. What the pooling layers do is to simplify the information in the output from the convolutional layer.
Recent convolutional network architectures have 10 to 20 hidden layers and billions of connections between units. Training deep learning networks has taken weeks several years ago, but thanks to progress in GPU and algorithm enhancement, training time has reduced to several hours.
Neural networks with time-varying behavior are known as recurrent neural networks or RNNs. A recurrent neural network is a class of artificial neural network where connections between units form a directed cycle. This creates an internal state of the network which allows it to exhibit dynamic temporal behavior. Unlike feedforward neural networks, RNNs can use their internal memory to process arbitrary sequences of inputs. Early RNN models turned out to be very difficult to train, harder even than deep feedforward networks. The reason is the unstable gradient problem such as vanishing gradient and exploding gradient. The gradient can get smaller and smaller as it is propagated back through layers. This makes learning in early layers extremely slow. The problem actually gets worse in RNNs, since gradients aren’t just propagated backward through layers, they’re propagated backward through time. If the network runs for a long time, that can make the gradient extremely unstable and hard to learn from. It has been possible to incorporate an idea known as long short-term memory units (LSTMs) into RNNs. LSTMs make it much easier to get good results when training RNNs, and many recent papers make use of LSTMs or related ideas.Deep learning, Convolutional neural networks, Recurrent neural networks, Error backpropagation algorithm"
286,딥러닝분석과 기술적 분석 지표를 이용한 한국 코스피주가지수 방향성 예측,이우식 /Lee Woosik,2017,한국데이터정보과학회,"2016년 3월 구글 (Google)의 바둑인공지능 알파고 (AlphaGo)가 이세돌 9단과의 바둑대결에서승리한 이후 다양한 분야에서 인공지능 사용에 대한 관심이 높아지고 있는 가운데 금융투자 분야에서도 인공지능과 투자자문 전문가의 합성어인 로보어드바이저 (Robo-Advisor)에 대한 관심이 높아지고 있다. 인공지능 (artificial intelligence)기반의 의사결정은 비용 절감은 물론 효과적인 의사결정을가능하게 한다는 점에서 큰 장점이 있다. 본 연구에서는 기술적 분석 (technical analysis) 지표와 딥러닝 (deep learning) 모형을 결합하여 한국 코스피 지수를 예측하는 모형을 개발하고 제시한 모형들의 예측력을 비교, 분석한다. 분석 결과 기술적 분석 지표에 딥러닝 알고리즘을 결합한 모형이 주가지수 방향성 예측 문제에 응용될 수 있음을 확인하였다. 향후 본 연구에서 제안된 기술적 분석 지표와딥러닝모형을 결합한 기법은 로보어드바이저서비스에 응용할 수 있는 일반화 가능성을 보여준다Since Google’s AlphaGo defeated a world champion of Go players in 2016, there have been many interests in the deep learning. In the financial sector, a Robo-Advisor using deep learning gains a significant attention, which builds and manages portfolios of financial instruments for investors.In this paper, we have proposed the a deep learning algorithm geared toward identification and forecast of the KOSPI index direction,and we also have compared the accuracy of the prediction.In an application of forecasting the financial market index direction, we have shown that the Robo-Advisor using deep learning has a significant effect on finance industry. The Robo-Advisor collects a massive data such as earnings statements, news reports and regulatory filings, analyzes those and recommends investors how to view market trends and identify the best time to purchase financial assets. On the other hand, the Robo-Advisor allows businesses to learn more about their customers, develop better marketing strategies, increase sales and decrease costs.Artificial intelligence, deep learning, FinTech, Robo-Advisor, technical analysis."
287,딥러닝의 효율적인 학습을 위한 학습데이터 선별 기법에 관한 연구,주경돈 /Gyoung-don Joo,2017,한국정보과학회,"빅데이터의 증가와 병렬 하드웨어 처리, 학습 알고리즘의 개선은 딥러닝의 발전을 가져왔으며 인공지능이 적용 가능한 영역을 획기적으로 확장시키고 있다. 하지만 이러한 대규모의 학습 데이터에 기반한 딥러닝은 학습시간이 증가되고, 더 나아가 학습 데이터의 구축을 위한 레이블링(labeling) 비용이 데이터 수에 비례하여 증가하게 된다. 따라서 본 논문에서는, 딥러닝의 정확도를 최대한 유지하면서 필요한 학습 데이터의 수를 줄일수 있도록, 전체 데이터에서 제한된 수의 학습 데이터의 후보를 선별하는 기법에 대하여 연구한다. 특히 기존의 능동 학습에 기반한 기법들이 학습 데이터의 수가 매우 적어질 경우 딥러닝의 정확도가 급격히 떨어지는 문제를 해결할 수 있는 새로운 기법을 제안하였으며, 제안된 기법으로 선별된 학습 데이터는 그 수의 변화에 대해 딥러닝이 일관되게 경쟁력있는 정확도를 가짐을 다양한 실험을 통하여 검증하였다.The increase of big data, parallel hardware processing and enhanced learning algorithms have led to the development of deep learning and are dramatically expanding the domain of applications based on artificial intelligence. However, such large-scale training data extends the learning time, and furthermore, the labeling cost for building the training set increases in proportion to the volume of data. Therefore, in this paper, we study how to select a limited number of candidates for training set from given data so as to reduce the number of training set while maintaining the accuracy of deep learning. Especially, we found that, with the training set selected by the previous work, the accuracy of deep learning drastically decreases when the number of training set is very small and so proposed new methods to resolve this fall-off in accuracy. Through experiments, we showed that deep learning consistently has the competitive accuracy with the training set selected by the proposed method regardless of the size of training set.Deep Learning, Train Set Selection, Active Learning, Gamma distribution"
288,딥러닝을 이용한 범용적 스테그아날리시스,김현재 /Hyunjae Kim,2017,한국정보과학회,"스테그아날리시스(Steganalysis)란 이미지 등 일반적인 자료에 암호화된 정보를 은닉하는 스테가노그래피(Steganography)에 대한 검출 및 분석 방법으로, 기계학습 기반 방법론을 포함한다. 기존 기계학습 기반 스테그아날리시스는 영상(Image)의 특징(Feature) 추출 및 모델링에 기반하며, 최근 딥러닝(Deep Learning)의 적용으로 검출 정확도가 큰 폭으로 향상되었다. 하지만 현존하는 스테그아날리시스 모델은 단일 스테가노그래피 기법에 대해 국한되어 있어 학습에 사용되지 않은 스테고(Stego) 이미지의 경우 검출이 불가능한 결정적 한계를 가진다. 본 연구에서는 다양한 스테가노그래피 기법으로 생성된 스테고이미지에 딥러닝을 적용하여 스테그아날리시스를 학습하는 범용적 모델을 제안한다. 다양한 실험을 통해 제안 기법의 효용성 및 가능성을 확인하고, 범용적 스테그아날리시스 모델이 각각에 특화된 검출 기법과 유사한 정확도로 스테고 이미지를 검출할 수 있음을 보인다.Steganalysis is to detect information hidden by steganography inside general data such as images. There are stegoanalysis techniques that use machine learning (ML). Existing ML approaches to steganalysis are based on extracting features from stego images and modeling them.
Recently deep learning-based methodologies have shown significant improvements in detection accuracy. However, all the existing methods, including deep learning-based ones, have a critical limitation in that they can only detect stego images that are created by a specific steganography method. In this paper, we propose a generalized steganalysis method that can model multiple types of stego images using deep learning. Through various experiments, we confirm the effectiveness of our approach and envision directions for future research. In particular, we show that our method can detect each type of steganography with the same level of accuracy as that of a steganalysis method dedicated to that type of steganography, thereby demonstrating the general applicability of our approach to multiple types of stego images.steganalysis, steganography, deep learning, generalized steganalysis model"
289,"딥러닝 프레임워크의 비교: 티아노, 텐서플로, CNTK를 중심으로",정여진 /Yeojin Chung,2017,한국지능정보시스템학회,"딥러닝 프레임워크의 대표적인 기능으로는 ‘자동미분’과 ‘GPU의 활용’ 등을 들 수 있다. 본 논문은 파이썬의 라이브러리 형태로 사용 가능한 프레임워크 중에서 구글의 텐서플로와 마이크로소프트의 CNTK, 그리고 텐서 플로의 원조라고 할 수 있는 티아노를 비교하였다. 본문에서는 자동미분의 개념과 GPU의 활용형태를 간단히 설명하고, 그 다음에 logistic regression을 실행하는 예를 통하여 각 프레임워크의 문법을 알아본 뒤에, 마지막으로 대표적인 딥러닝 응용인 CNN의 예제를 실행시켜보고 코딩의 편의성과 실행속도 등을 확인해 보았다. 그 결과, 편의성의 관점에서 보면 티아노가 가장 코딩 하기가 어렵고, CNTK와 텐서플로는 많은 부분이 비슷하게 추상화 되어 있어서 코딩이 비슷하지만 가중치와 편향을 직접 정의하느냐의 여부에서 차이를 보였다. 그리고 각프레임워크의 실행속도에 대한 평가는 ‘큰 차이는 없다’는 것이다. 텐서플로는 티아노에 비하여 속도가 느리다는 평가가 있어왔는데, 본 연구의 실험에 의하면, 비록 CNN 모형에 국한되었지만, 텐서플로가 아주 조금이지만빠른 것으로 나타났다. CNTK의 경우에도, 비록 실험환경이 달랐지만, 실험환경의 차이에 의한 속도의 차이의 편차범위 이내에 있는 것으로 판단이 되었다. 본 연구에서는 세 종류의 딥러닝 프레임워크만을 살펴보았는데,위키피디아에 따르면 딥러닝 프레임워크의 종류는 12가지가 있으며, 각 프레임워크의 특징을 15가지 속성으로구분하여 차이를 특정하고 있다. 그 많은 속성 중에서 사용자의 입장에서 볼 때 중요한 속성은 어떤 언어(파이썬, C++, Java, 등)로 사용가능한지, 어떤 딥러닝 모형에 대한 라이브러리가 잘 구현되어 있는지 등일 것이다. 그리고 사용자가 대규모의 딥러닝 모형을 구축한다면, 다중 GPU 혹은 다중 서버를 지원하는지의 여부도 중요할 것이다. 또한 딥러닝 모형을 처음 학습하는 경우에는 사용설명서가 많은지 예제 프로그램이 많은지 여부도 중요한 기준이 될 것이다.The deep learning framework is software designed to help develop deep learning models. Some of its important functions include “automatic differentiation” and “utilization of GPU”. The list of popular deep learning framework includes Caffe (BVLC) and Theano (University of Montreal). And recently, Microsoft's deep learning framework, Microsoft Cognitive Toolkit, was released as open-source license, following Google’s Tensorflow a year earlier. The early deep learning frameworks have been developed mainly for research at universities. Beginning with the inception of Tensorflow, however, it seems that companies such as Microsoft and Facebook have started to join the competition of framework development. Given the trend, Google and other companies are expected to continue investing in the deep learning framework to bring forward the initiative in the artificial intelligence business. From this point of view, we think it is a good time to compare some of deep learning frameworks. So we compare three deep learning frameworks which can be used as a Python library. Those are Google's Tensorflow, Microsoft’s CNTK, and Theano which is sort of a predecessor of the preceding two.
The most common and important function of deep learning frameworks is the ability to perform automatic differentiation. Basically all the mathematical expressions of deep learning models can be represented as computational graphs, which consist of nodes and edges. Partial derivatives on each edge of a computational graph can then be obtained. With the partial derivatives, we can let software compute differentiation of any node with respect to any variable by utilizing chain rule of Calculus.
First of all, the convenience of coding is in the order of CNTK, Tensorflow, and Theano. The criterion is simply based on the lengths of the codes and the learning curve and the ease of coding are not the main concern. According to the criteria, Theano was the most difficult to implement with, and CNTK and Tensorflow were somewhat easier. With Tensorflow, we need to define weight variables and biases explicitly. The reason that CNTK and Tensorflow are easier to implement with is that those frameworks provide us with more abstraction than Theano. We, however, need to mention that low-level coding is not always bad. It gives us flexibility of coding. With the low-level coding such as in Theano, we can implement and test any new deep learning models or any new search methods that we can think of.
The assessment of the execution speed of each framework is that there is not meaningful difference. According to the experiment, execution speeds of Theano and Tensorflow are very similar, although the experiment was limited to a CNN model. In the case of CNTK, the experimental environment was not maintained as the same. The code written in CNTK has to be run in PC environment without GPU where codes execute as much as 50 times slower than with GPU. But we concluded that the difference of execution speed was within the range of variation caused by the different hardware setup.
In this study, we compared three types of deep learning framework: Theano, Tensorflow, and CNTK. According to Wikipedia, there are 12 available deep learning frameworks. And 15 different attributes differentiate each framework. Some of the important attributes would include interface language (Python, C ++, Java, etc.) and the availability of libraries on various deep learning models such as CNN, RNN, DBN, and etc. And if a user implements a large scale deep learning model, it will also be important to support multiple GPU or multiple servers. Also, if you are learning the deep learning model, it would also be important if there are enough examples and references.deep learning framework, Theano, TensorFlow, CNTK, computational graph, CIFAR-10"
290,딥러닝을 이용한 주변 무선단말 파악방안,이웅섭 /Lee， Woong-sup,2018,한국정보통신학회,"최근 단말-대-단말(Device-to-device, D2D) 통신기술이 차세대 무선통신시스템의 핵심기술로 큰 관심을 받고 있다. 이러한 단말간 통신에서는 자신의 주변에 어떠한 단말이 있는지 파악하는 주변단말 탐색(Neighbor discovery)이매우 중요하다. 본 논문에서는 최근 큰 관심을 받고 있는 딥러닝(Deep learning) 기술을 활용하여 단말간 통신에서 주변단말을 파악하는 방안에 대해서 제안한다. 제안 방안은 기존의 방안과 달리 무선채널의 공간적 연관성을 이용하여단말간의 신호 전송 없이 단말이 기지국으로 전송하는 상향링크 파일럿 신호를 기반으로 주변 단말을 찾고 따라서기존의 방식에 비해 신호전송 복잡도(signaling complexity)를 크게 줄일 수 있다. 또한 제안 방안에서는 떨어져 있는거리에 따라서 주변 단말을 분류 가능하여 기존 방안에 비해서 좀 더 세밀한 단말 탐색이 가능하다. 마지막으로 본 논문에서는 tensorflow를 이용한 컴퓨터 시뮬레이션을 통해 제안 방안의 성능을 검증하였다.Recently, the device-to-device (D2D) communication has been conceived as the key technology for the next-generationmobile communication systems. The neighbor discovery in which the nearby users are found, is essential for the properoperation of the D2D communication. In this paper, we propose new neighbor discovery scheme based on deep learningtechnology which has gained a lot of attention recently. In the proposed scheme, the neighboring users can be foundusing the uplink pilot transmission of users only, unlike conventional neighbor discovery schemes in which direct pilotcommunication among users is required, such that the signaling overhead can be greatly reduced in our proposed scheme.
Moreover, the neighbors with different proximity can also be classified accordingly which enables more accurate neighbordiscovery compared to the conventional schemes. The performance of our proposed scheme is verified through thetensorflow-based computer simulations.
키워드 : 딥러닝, 주변단말 탐색, 단말-대-단말 통신Deep learning, Neighbor discovery, Device-to-device communication, Wireless channel correlation"
291,신뢰성있는 딥러닝 기반 분석 모델을 참조하기 위한 딥러닝 기술 언어,문종혁 /Jong Hyeok Mun,2021,한국정보처리학회,"최근 딥러닝은 하드웨어 성능이 향상됨에 따라 자연어 처리, 영상 인식 등의 다양한 기술에 접목되어 활용되고 있다. 이러한 기술들을 활용해 지능형교통 시스템(ITS), 스마트홈, 헬스케어 등의 산업분야에서 데이터를 분석하여 고속도로 속도위반 차량 검출, 에너지 사용량 제어, 응급상황 등과 같은고품질의 서비스를 제공하며, 고품질의 서비스를 제공하기 위해서는 정확도가 향상된 딥러닝 모델이 적용되어야 한다. 이를 위해 서비스 환경의 데이터를분석하기 위한 딥러닝 모델을 개발할 때, 개발자는 신뢰성이 검증된 최신의 딥러닝 모델을 적용할 수 있어야 한다. 이는 개발자가 참조하는 딥러닝 모델에적용된 학습 데이터셋의 정확도를 측정하여 검증할 수 있다. 이러한 검증을 위해서 개발자는 학습 데이터셋, 딥러닝의 계층구조 및 개발 환경 등과 같은내용을 포함하는 딥러닝 모델을 문서화하여 적용하기 위한 구조적인 정보가 필요하다. 본 논문에서는 신뢰성있는 딥러닝 기반 데이터 분석 모델을 참조하기위한 딥러닝 기술 언어를 제안한다. 제안하는 기술 언어는 신뢰성 있는 딥러닝 모델을 개발하는데 필요한 학습데이터셋, 개발 환경 및 설정 등의 정보와더불어 딥러닝 모델의 계층구조를 표현할 수 있다. 제안하는 딥러닝 기술 언어를 이용하여 개발자는 지능형 교통 시스템에서 참조하는 분석 모델의정확도를 검증할 수 있다. 실험에서는 제안하는 언어의 유효성을 검증하기 위해, 번호판 인식 모델을 중심으로 딥러닝 기술 문서의 적용과정을 보인다With the recent advancements of deep learning, companies such as smart home, healthcare, and intelligent transportation systems areutilizing its functionality to provide high-quality services for vehicle detection, emergency situation detection, and controlling energyconsumption. To provide reliable services in such sensitive systems, deep learning models are required to have high accuracy. In orderto develop a deep learning model for analyzing previously mentioned services, developers should utilize the state of the art deep learningmodels that have already been verified for higher accuracy. The developers can verify the accuracy of the referenced model by validatingthe model on the dataset. For this validation, the developer needs structural information to document and apply deep learning models,including metadata such as learning dataset, network architecture, and development environments. In this paper, we propose a descriptionlanguage that represents the network architecture of the deep learning model along with its metadata that are necessary to develop adeep learning model. Through the proposed description language, developers can easily verify the accuracy of the referenced deep learningmodel. Our experiments demonstrate the application scenario of a deep learning description document that focuses on the license platerecognition for the detection of illegally parked vehicles.Trusted Deep Learning, Model Reference, Deep Learning Description Language, Traffic Situation Analysis Model"
292,딥러닝 분석을 이용한 중국 역내·외 위안화 변동성 예측,이우식 /Lee Woosik,2016,한국데이터정보과학회,"2008년 글로벌 금융위기 이후 중국은 위안화 국제화의 점진적 추진을 진행하면서 중국상하이 외환시장과 중국홍콩 외환시장에서 거래되는 통화인 역내위안화와 역외위안화를 형성시켰다. 본 연구는 위안화 국제화와 점진적인 중국 자본계정 개방에 따라 급변하는 외환시장상황의 변동성을 정확하게 파악하기 위해서 GARCH모형 (일반화된 자기회귀 조건부이분산성모형)에 다단계인공신경망을 결합한 MLP-GARCH 모형과 GARCH모형과 기계학습의 일종인 딥러닝 (deep learning)을 통합한 DL-GARCH을 가지고 위안화 변동성예측을 비교 실험과 분석을 하였다. 비교분석 결과 DLGARCH 모형은 MLP-GARCH보다 모형 위안화 역내·외 환율변동성 예측 면에서 더욱 더 개선된예측값을 제공하였다. 그래서 이분산시계열모형을 딥러닝과 결합한 DL-GARCH 모형은 시계열의환율 변동성 예측 문제에 딥러닝을 응용할 수 있음을 확인하였다. 향후 이분산시계열과 결합된 딥러닝 모형은 다른 금융시계열 데이터에 응용하여 그 일반화 가능성을 높일 수 있을 것이다.The People’s Republic of China has vigorously been pursuing the internationalization of the Chinese Yuan or Renminbi after the financial crisis of 2008. In this view, an abrupt increase of use of the Chinese Yuan in the onshore and offshore markets are important milestones to be one of important currencies. One of the most frequently used methods to forecast volatility is GARCH model. Since a prediction error of the GARCH model has been reported quite high, a lot of efforts have been made to improve forecasting capability of the GARCH model. In this paper, we have proposed MLPGARCH and a DL-GARCH by employing Artificial Neural Network to the GARCH. In an application to forecasting Chinese Yuan volatility, we have successfully shown their overall outperformance in forecasting over the GARCH.Deep learning, DL-GARCH, GARCH, The Chinese Yuan, volatility."
293,딥러닝 기법을 이용한 내일강수 예측,하지훈,2016,한국지능시스템학회,"정확한 강수예측을 위해서는 예측인자 선정과 예측방법에 대한 선택이 매우 중요하다. 최근에는 강수예측 방법으로기계학습 기법이 많이 사용되고 있으며, 그 중에서도 특히 인공신경망을 사용한 강수예측 방법은 좋은 성능을 보였다. 본논문에서는 딥러닝 기법 중 하나인 DBN(deep belief network)를 이용한 새로운 강수예측 방법을 제안한다. DBN는 비지도사전 학습을 통해 초기 가중치를 설정하여 기존 인공신경망의 문제점을 보완한다. 예측인자로는 기온, 전일-전주 강수일, 태양과 달 궤도 관련 자료를 선정하였다. 기온과 전일-전주 강수일은 서울에서의 1974년부터 2013년까지 총 40년간의AWS(automatic weather system) 관측 자료를 사용하였고, 태양과 달의 궤도 관련 자료는 서울을 중심으로 계산한 결과를사용하였다. 전체 기간에서 일부는 학습 자료로 사용하여 예측모델을 생성하였고, 나머지를 생성한 모델의 검증 자료로사용하였다. 모델 검증 결과로 나온 예측값들은 확률값을 가지며 임계치를 이용하여 강수유무를 판별하였다. 강수 정확도의척도로 양분예보기법 중 CSI(critical successive index)와 Bias(frequency bias)를 계산하였다. 이를 통해 DBN와 MLP(multilayer perceptron)의 성능을 비교한 결과 DBN의 강수 예측 정확도가 높았고, 수행속도 또한 2배 이상 빨랐다.For accurate precipitation forecasts the choice of weather faoctrs and prediction method is very important. Recently, machinele arning has been widely used for forecasting precipitation, and artificial neural network, one of machine learning techniques, showed good performance. In this paper, we suggest a new method for forecasting precipitation using DBN, one of deep learning techniques.
DBN has an advantage that initial weights are set by unsupervised learning, so this compensates for the defects of artificial neural networks. We used past precipitation, temperature, and the parameters of the sun and moon's motion as features for forecasting precipitation. The dataset consists of observation data which had been measured for 40 years from AWS in Seoul. Experiments were based on 8-fold cross validation. As a result of estimation, we got probabilities of test dataset, so threshold was used for the decision of precipitation. CSI and Bias were used for indicating the precision of precipitation. Our experimental results showed that DBN performed better than MLP.Deep learning, Deep belief network, Precipitation, Forecast"
294,딥러닝 신경망모형을 이용한 실시간 교통정보수집 연구,김대현 /KIM DAE HYON,2016,사단법인 인문사회과학기술융합학회,"비디오 기반의 실시간 교통정보수집 시스템은 자율주행을 포함한 지능형교통체계의 주요 서브시스템중 하나이며, 우수한 예측(추정)력 때문에 핵심 알고리즘으로 인공신경망 모형이 주로 사용되고 있다. 특히, 최근 과학 학술지 네이처에 구글 딥마인드의 알파고가 공개되면서 딥러닝과 인공지능에 대한 전 세계적인 관심이 집중되고 있다. 최근까지 가장 널리 사용된 인공신경망모형은 역전파 모형이었으나 최근 컴퓨터 하드웨어 및 알고리즘의 새로운 개발과 더불어 딥러닝 알고리즘을 이용한 딥인공신경망 모형에 대한 관심이 높아지고 있다. 그러나 아직 딥러닝 인공신경망에 대한 연구는 초기단계에 있는 실정이며 적용분야도 극히 한정되어 있다. 본 연구에서는 실시간 도로교통정보 수집을 위한 영상처리분야에서 딥러닝 알고리즘을 활용하고 실험결과를 통하여 향후 교통정보수집‧분석 분야에서의 딥러닝 인공신경망 모형의 적용가능성을 살펴보고자 한다. 특히, 본 연구에서 도출한 딥러닝 인공신경망모형의 실험결과를 기존 역전파 모형에 의한 실험결과와 비교하여 모형의 우수성을 비교‧평가하고자 한다.Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Normally, artificial neural networks have been used in real-time applications for collecting traffic information because of their remarkable performance in terms of predictive accuracy and real-time computing. Especially, the recent publication of a paper in the journal Nature and the success of Google DeepMind’s world-beating Go program AlphaGo caused significant increases of the public interest in artificial intelligence. Even though various types of learning models have been proposed, the Backpropagation has been the most popular neural network model. However, the Deep learning neural network model has attracted much interest recently. In this study, the Deep leaning neural network will be applied in image processing techniques for the real-time traffic information collection to investigate the performance of Deep learning algorithm in traffic engineering fields. In addition, the experimental results from Deep learing algorithm will be compared with the results from Backpropagation model to evaluate and propose better performing model.Intelligent Transportation Systems (ITS), artificial neural networks, Deep learning, traffic information, image processing"
295,딥러닝의 모형과 응용사례,안성만 /SungMahn Ahn,2016,한국지능정보시스템학회,"딥러닝은 인공신경망(neural network)이라는 인공지능분야의 모형이 발전된 형태로서, 계층구조로 이루어진 인공신경망의 내부계층(hidden layer)이 여러 단계로 이루어진 구조이다. 딥러닝에서의 주요 모형은 합성곱신경망(convolutional neural network), 순환신경망(recurrent neural network), 그리고 심층신뢰신경망(deep belief network)의 세가지라고 할 수있다. 그 중에서 현재 흥미로운 연구가 많이 발표되어서 관심이 집중되고 있는 모형은 지도학습(supervised learning)모형인 처음 두 개의 모형이다. 따라서 본 논문에서는 지도학습모형의 가중치를 최적화하는 기본적인 방법인 오류역전파 알고리즘을 살펴본 뒤에 합성곱신경망과 순환신경망의 구조와 응용사례 등을 살펴보고자 한다. 본문에서 다루지 않은 모형인 심층신뢰신경망은 아직까지는 합성곱신경망 이나 순환신경망보다는 상대적으로 주목을 덜 받고 있다. 그러나 심층신뢰신경망은 CNN이나 RNN과는 달리 비지도학습(unsupervised learning)모형이며, 사람이나 동물은 관찰을 통해서 스스로 학습한다는 점에서 궁극적으로는 비지도학습모형이 더 많이 연구되어야 할 주제가 될 것이다.Deep learning model is a kind of neural networks that allows multiple hidden layers. There are various deep learning architectures such as convolutional neural networks, deep belief networks and recurrent neural networks. Those have been applied to fields like computer vision, automatic speech recognition, natural language processing, audio recognition and bioinformatics where they have been shown to produce state-of-the-art results on various tasks. Among those architectures, convolutional neural networks and recurrent neural networks are classified as the supervised learning model. And in recent years, those supervised learning models have gained more popularity than unsupervised learning models such as deep belief networks, because supervised learning models have shown fashionable applications in such fields mentioned above.
Deep learning models can be trained with backpropagation algorithm. Backpropagation is an abbreviation for “backward propagation of errors” and a common method of training artificial neural networks used in conjunction with an optimization method such as gradient descent. The method calculates the gradient of an error function with respect to all the weights in the network. The gradient is fed to the optimization method which in turn uses it to update the weights, in an attempt to minimize the error function.
Convolutional neural networks use a special architecture which is particularly well-adapted to classify images. Using this architecture makes convolutional networks fast to train. This, in turn, helps us train deep, muti-layer networks, which are very good at classifying images. These days, deep convolutional networks are used in most neural networks for image recognition. Convolutional neural networks use three basic ideas: local receptive fields, shared weights, and pooling. By local receptive fields, we mean that each neuron in the first(or any) hidden layer will be connected to a small region of the input(or previous layer’s) neurons. Shared weights mean that we’re going to use the same weights and bias for each of the local receptive field. This means that all the neurons in the hidden layer detect exactly the same feature, just at different locations in the input image. In addition to the convolutional layers just described, convolutional neural networks also contain pooling layers. Pooling layers are usually used immediately after convolutional layers. What the pooling layers do is to simplify the information in the output from the convolutional layer.
Recent convolutional network architectures have 10 to 20 hidden layers and billions of connections between units. Training deep learning networks has taken weeks several years ago, but thanks to progress in GPU and algorithm enhancement, training time has reduced to several hours.
Neural networks with time-varying behavior are known as recurrent neural networks or RNNs. A recurrent neural network is a class of artificial neural network where connections between units form a directed cycle. This creates an internal state of the network which allows it to exhibit dynamic temporal behavior. Unlike feedforward neural networks, RNNs can use their internal memory to process arbitrary sequences of inputs. Early RNN models turned out to be very difficult to train, harder even than deep feedforward networks. The reason is the unstable gradient problem such as vanishing gradient and exploding gradient. The gradient can get smaller and smaller as it is propagated back through layers. This makes learning in early layers extremely slow. The problem actually gets worse in RNNs, since gradients aren’t just propagated backward through layers, they’re propagated backward through time. If the network runs for a long time, that can make the gradient extremely unstable and hard to learn from. It has been possible to incorporate an idea known as long short-term memory units (LSTMs) into RNNs. LSTMs make it much easier to get good results when training RNNs, and many recent papers make use of LSTMs or related ideas.Deep learning, Convolutional neural networks, Recurrent neural networks, Error backpropagation algorithm"
296,딥러닝분석과 기술적 분석 지표를 이용한 한국 코스피주가지수 방향성 예측,이우식 /Lee Woosik,2017,한국데이터정보과학회,"2016년 3월 구글 (Google)의 바둑인공지능 알파고 (AlphaGo)가 이세돌 9단과의 바둑대결에서승리한 이후 다양한 분야에서 인공지능 사용에 대한 관심이 높아지고 있는 가운데 금융투자 분야에서도 인공지능과 투자자문 전문가의 합성어인 로보어드바이저 (Robo-Advisor)에 대한 관심이 높아지고 있다. 인공지능 (artificial intelligence)기반의 의사결정은 비용 절감은 물론 효과적인 의사결정을가능하게 한다는 점에서 큰 장점이 있다. 본 연구에서는 기술적 분석 (technical analysis) 지표와 딥러닝 (deep learning) 모형을 결합하여 한국 코스피 지수를 예측하는 모형을 개발하고 제시한 모형들의 예측력을 비교, 분석한다. 분석 결과 기술적 분석 지표에 딥러닝 알고리즘을 결합한 모형이 주가지수 방향성 예측 문제에 응용될 수 있음을 확인하였다. 향후 본 연구에서 제안된 기술적 분석 지표와딥러닝모형을 결합한 기법은 로보어드바이저서비스에 응용할 수 있는 일반화 가능성을 보여준다Since Google’s AlphaGo defeated a world champion of Go players in 2016, there have been many interests in the deep learning. In the financial sector, a Robo-Advisor using deep learning gains a significant attention, which builds and manages portfolios of financial instruments for investors.In this paper, we have proposed the a deep learning algorithm geared toward identification and forecast of the KOSPI index direction,and we also have compared the accuracy of the prediction.In an application of forecasting the financial market index direction, we have shown that the Robo-Advisor using deep learning has a significant effect on finance industry. The Robo-Advisor collects a massive data such as earnings statements, news reports and regulatory filings, analyzes those and recommends investors how to view market trends and identify the best time to purchase financial assets. On the other hand, the Robo-Advisor allows businesses to learn more about their customers, develop better marketing strategies, increase sales and decrease costs.Artificial intelligence, deep learning, FinTech, Robo-Advisor, technical analysis."
297,딥러닝의 효율적인 학습을 위한 학습데이터 선별 기법에 관한 연구,주경돈 /Gyoung-don Joo,2017,한국정보과학회,"빅데이터의 증가와 병렬 하드웨어 처리, 학습 알고리즘의 개선은 딥러닝의 발전을 가져왔으며 인공지능이 적용 가능한 영역을 획기적으로 확장시키고 있다. 하지만 이러한 대규모의 학습 데이터에 기반한 딥러닝은 학습시간이 증가되고, 더 나아가 학습 데이터의 구축을 위한 레이블링(labeling) 비용이 데이터 수에 비례하여 증가하게 된다. 따라서 본 논문에서는, 딥러닝의 정확도를 최대한 유지하면서 필요한 학습 데이터의 수를 줄일수 있도록, 전체 데이터에서 제한된 수의 학습 데이터의 후보를 선별하는 기법에 대하여 연구한다. 특히 기존의 능동 학습에 기반한 기법들이 학습 데이터의 수가 매우 적어질 경우 딥러닝의 정확도가 급격히 떨어지는 문제를 해결할 수 있는 새로운 기법을 제안하였으며, 제안된 기법으로 선별된 학습 데이터는 그 수의 변화에 대해 딥러닝이 일관되게 경쟁력있는 정확도를 가짐을 다양한 실험을 통하여 검증하였다.The increase of big data, parallel hardware processing and enhanced learning algorithms have led to the development of deep learning and are dramatically expanding the domain of applications based on artificial intelligence. However, such large-scale training data extends the learning time, and furthermore, the labeling cost for building the training set increases in proportion to the volume of data. Therefore, in this paper, we study how to select a limited number of candidates for training set from given data so as to reduce the number of training set while maintaining the accuracy of deep learning. Especially, we found that, with the training set selected by the previous work, the accuracy of deep learning drastically decreases when the number of training set is very small and so proposed new methods to resolve this fall-off in accuracy. Through experiments, we showed that deep learning consistently has the competitive accuracy with the training set selected by the proposed method regardless of the size of training set.Deep Learning, Train Set Selection, Active Learning, Gamma distribution"
298,딥러닝을 이용한 범용적 스테그아날리시스,김현재 /Hyunjae Kim,2017,한국정보과학회,"스테그아날리시스(Steganalysis)란 이미지 등 일반적인 자료에 암호화된 정보를 은닉하는 스테가노그래피(Steganography)에 대한 검출 및 분석 방법으로, 기계학습 기반 방법론을 포함한다. 기존 기계학습 기반 스테그아날리시스는 영상(Image)의 특징(Feature) 추출 및 모델링에 기반하며, 최근 딥러닝(Deep Learning)의 적용으로 검출 정확도가 큰 폭으로 향상되었다. 하지만 현존하는 스테그아날리시스 모델은 단일 스테가노그래피 기법에 대해 국한되어 있어 학습에 사용되지 않은 스테고(Stego) 이미지의 경우 검출이 불가능한 결정적 한계를 가진다. 본 연구에서는 다양한 스테가노그래피 기법으로 생성된 스테고이미지에 딥러닝을 적용하여 스테그아날리시스를 학습하는 범용적 모델을 제안한다. 다양한 실험을 통해 제안 기법의 효용성 및 가능성을 확인하고, 범용적 스테그아날리시스 모델이 각각에 특화된 검출 기법과 유사한 정확도로 스테고 이미지를 검출할 수 있음을 보인다.Steganalysis is to detect information hidden by steganography inside general data such as images. There are stegoanalysis techniques that use machine learning (ML). Existing ML approaches to steganalysis are based on extracting features from stego images and modeling them.
Recently deep learning-based methodologies have shown significant improvements in detection accuracy. However, all the existing methods, including deep learning-based ones, have a critical limitation in that they can only detect stego images that are created by a specific steganography method. In this paper, we propose a generalized steganalysis method that can model multiple types of stego images using deep learning. Through various experiments, we confirm the effectiveness of our approach and envision directions for future research. In particular, we show that our method can detect each type of steganography with the same level of accuracy as that of a steganalysis method dedicated to that type of steganography, thereby demonstrating the general applicability of our approach to multiple types of stego images.steganalysis, steganography, deep learning, generalized steganalysis model"
299,"딥러닝 프레임워크의 비교: 티아노, 텐서플로, CNTK를 중심으로",정여진 /Yeojin Chung,2017,한국지능정보시스템학회,"딥러닝 프레임워크의 대표적인 기능으로는 ‘자동미분’과 ‘GPU의 활용’ 등을 들 수 있다. 본 논문은 파이썬의 라이브러리 형태로 사용 가능한 프레임워크 중에서 구글의 텐서플로와 마이크로소프트의 CNTK, 그리고 텐서 플로의 원조라고 할 수 있는 티아노를 비교하였다. 본문에서는 자동미분의 개념과 GPU의 활용형태를 간단히 설명하고, 그 다음에 logistic regression을 실행하는 예를 통하여 각 프레임워크의 문법을 알아본 뒤에, 마지막으로 대표적인 딥러닝 응용인 CNN의 예제를 실행시켜보고 코딩의 편의성과 실행속도 등을 확인해 보았다. 그 결과, 편의성의 관점에서 보면 티아노가 가장 코딩 하기가 어렵고, CNTK와 텐서플로는 많은 부분이 비슷하게 추상화 되어 있어서 코딩이 비슷하지만 가중치와 편향을 직접 정의하느냐의 여부에서 차이를 보였다. 그리고 각프레임워크의 실행속도에 대한 평가는 ‘큰 차이는 없다’는 것이다. 텐서플로는 티아노에 비하여 속도가 느리다는 평가가 있어왔는데, 본 연구의 실험에 의하면, 비록 CNN 모형에 국한되었지만, 텐서플로가 아주 조금이지만빠른 것으로 나타났다. CNTK의 경우에도, 비록 실험환경이 달랐지만, 실험환경의 차이에 의한 속도의 차이의 편차범위 이내에 있는 것으로 판단이 되었다. 본 연구에서는 세 종류의 딥러닝 프레임워크만을 살펴보았는데,위키피디아에 따르면 딥러닝 프레임워크의 종류는 12가지가 있으며, 각 프레임워크의 특징을 15가지 속성으로구분하여 차이를 특정하고 있다. 그 많은 속성 중에서 사용자의 입장에서 볼 때 중요한 속성은 어떤 언어(파이썬, C++, Java, 등)로 사용가능한지, 어떤 딥러닝 모형에 대한 라이브러리가 잘 구현되어 있는지 등일 것이다. 그리고 사용자가 대규모의 딥러닝 모형을 구축한다면, 다중 GPU 혹은 다중 서버를 지원하는지의 여부도 중요할 것이다. 또한 딥러닝 모형을 처음 학습하는 경우에는 사용설명서가 많은지 예제 프로그램이 많은지 여부도 중요한 기준이 될 것이다.The deep learning framework is software designed to help develop deep learning models. Some of its important functions include “automatic differentiation” and “utilization of GPU”. The list of popular deep learning framework includes Caffe (BVLC) and Theano (University of Montreal). And recently, Microsoft's deep learning framework, Microsoft Cognitive Toolkit, was released as open-source license, following Google’s Tensorflow a year earlier. The early deep learning frameworks have been developed mainly for research at universities. Beginning with the inception of Tensorflow, however, it seems that companies such as Microsoft and Facebook have started to join the competition of framework development. Given the trend, Google and other companies are expected to continue investing in the deep learning framework to bring forward the initiative in the artificial intelligence business. From this point of view, we think it is a good time to compare some of deep learning frameworks. So we compare three deep learning frameworks which can be used as a Python library. Those are Google's Tensorflow, Microsoft’s CNTK, and Theano which is sort of a predecessor of the preceding two.
The most common and important function of deep learning frameworks is the ability to perform automatic differentiation. Basically all the mathematical expressions of deep learning models can be represented as computational graphs, which consist of nodes and edges. Partial derivatives on each edge of a computational graph can then be obtained. With the partial derivatives, we can let software compute differentiation of any node with respect to any variable by utilizing chain rule of Calculus.
First of all, the convenience of coding is in the order of CNTK, Tensorflow, and Theano. The criterion is simply based on the lengths of the codes and the learning curve and the ease of coding are not the main concern. According to the criteria, Theano was the most difficult to implement with, and CNTK and Tensorflow were somewhat easier. With Tensorflow, we need to define weight variables and biases explicitly. The reason that CNTK and Tensorflow are easier to implement with is that those frameworks provide us with more abstraction than Theano. We, however, need to mention that low-level coding is not always bad. It gives us flexibility of coding. With the low-level coding such as in Theano, we can implement and test any new deep learning models or any new search methods that we can think of.
The assessment of the execution speed of each framework is that there is not meaningful difference. According to the experiment, execution speeds of Theano and Tensorflow are very similar, although the experiment was limited to a CNN model. In the case of CNTK, the experimental environment was not maintained as the same. The code written in CNTK has to be run in PC environment without GPU where codes execute as much as 50 times slower than with GPU. But we concluded that the difference of execution speed was within the range of variation caused by the different hardware setup.
In this study, we compared three types of deep learning framework: Theano, Tensorflow, and CNTK. According to Wikipedia, there are 12 available deep learning frameworks. And 15 different attributes differentiate each framework. Some of the important attributes would include interface language (Python, C ++, Java, etc.) and the availability of libraries on various deep learning models such as CNN, RNN, DBN, and etc. And if a user implements a large scale deep learning model, it will also be important to support multiple GPU or multiple servers. Also, if you are learning the deep learning model, it would also be important if there are enough examples and references.deep learning framework, Theano, TensorFlow, CNTK, computational graph, CIFAR-10"
300,딥러닝을 이용한 주변 무선단말 파악방안,이웅섭 /Lee， Woong-sup,2018,한국정보통신학회,"최근 단말-대-단말(Device-to-device, D2D) 통신기술이 차세대 무선통신시스템의 핵심기술로 큰 관심을 받고 있다. 이러한 단말간 통신에서는 자신의 주변에 어떠한 단말이 있는지 파악하는 주변단말 탐색(Neighbor discovery)이매우 중요하다. 본 논문에서는 최근 큰 관심을 받고 있는 딥러닝(Deep learning) 기술을 활용하여 단말간 통신에서 주변단말을 파악하는 방안에 대해서 제안한다. 제안 방안은 기존의 방안과 달리 무선채널의 공간적 연관성을 이용하여단말간의 신호 전송 없이 단말이 기지국으로 전송하는 상향링크 파일럿 신호를 기반으로 주변 단말을 찾고 따라서기존의 방식에 비해 신호전송 복잡도(signaling complexity)를 크게 줄일 수 있다. 또한 제안 방안에서는 떨어져 있는거리에 따라서 주변 단말을 분류 가능하여 기존 방안에 비해서 좀 더 세밀한 단말 탐색이 가능하다. 마지막으로 본 논문에서는 tensorflow를 이용한 컴퓨터 시뮬레이션을 통해 제안 방안의 성능을 검증하였다.Recently, the device-to-device (D2D) communication has been conceived as the key technology for the next-generationmobile communication systems. The neighbor discovery in which the nearby users are found, is essential for the properoperation of the D2D communication. In this paper, we propose new neighbor discovery scheme based on deep learningtechnology which has gained a lot of attention recently. In the proposed scheme, the neighboring users can be foundusing the uplink pilot transmission of users only, unlike conventional neighbor discovery schemes in which direct pilotcommunication among users is required, such that the signaling overhead can be greatly reduced in our proposed scheme.
Moreover, the neighbors with different proximity can also be classified accordingly which enables more accurate neighbordiscovery compared to the conventional schemes. The performance of our proposed scheme is verified through thetensorflow-based computer simulations.
키워드 : 딥러닝, 주변단말 탐색, 단말-대-단말 통신Deep learning, Neighbor discovery, Device-to-device communication, Wireless channel correlation"
301,신뢰성있는 딥러닝 기반 분석 모델을 참조하기 위한 딥러닝 기술 언어,문종혁 /Jong Hyeok Mun,2021,한국정보처리학회,"최근 딥러닝은 하드웨어 성능이 향상됨에 따라 자연어 처리, 영상 인식 등의 다양한 기술에 접목되어 활용되고 있다. 이러한 기술들을 활용해 지능형교통 시스템(ITS), 스마트홈, 헬스케어 등의 산업분야에서 데이터를 분석하여 고속도로 속도위반 차량 검출, 에너지 사용량 제어, 응급상황 등과 같은고품질의 서비스를 제공하며, 고품질의 서비스를 제공하기 위해서는 정확도가 향상된 딥러닝 모델이 적용되어야 한다. 이를 위해 서비스 환경의 데이터를분석하기 위한 딥러닝 모델을 개발할 때, 개발자는 신뢰성이 검증된 최신의 딥러닝 모델을 적용할 수 있어야 한다. 이는 개발자가 참조하는 딥러닝 모델에적용된 학습 데이터셋의 정확도를 측정하여 검증할 수 있다. 이러한 검증을 위해서 개발자는 학습 데이터셋, 딥러닝의 계층구조 및 개발 환경 등과 같은내용을 포함하는 딥러닝 모델을 문서화하여 적용하기 위한 구조적인 정보가 필요하다. 본 논문에서는 신뢰성있는 딥러닝 기반 데이터 분석 모델을 참조하기위한 딥러닝 기술 언어를 제안한다. 제안하는 기술 언어는 신뢰성 있는 딥러닝 모델을 개발하는데 필요한 학습데이터셋, 개발 환경 및 설정 등의 정보와더불어 딥러닝 모델의 계층구조를 표현할 수 있다. 제안하는 딥러닝 기술 언어를 이용하여 개발자는 지능형 교통 시스템에서 참조하는 분석 모델의정확도를 검증할 수 있다. 실험에서는 제안하는 언어의 유효성을 검증하기 위해, 번호판 인식 모델을 중심으로 딥러닝 기술 문서의 적용과정을 보인다With the recent advancements of deep learning, companies such as smart home, healthcare, and intelligent transportation systems areutilizing its functionality to provide high-quality services for vehicle detection, emergency situation detection, and controlling energyconsumption. To provide reliable services in such sensitive systems, deep learning models are required to have high accuracy. In orderto develop a deep learning model for analyzing previously mentioned services, developers should utilize the state of the art deep learningmodels that have already been verified for higher accuracy. The developers can verify the accuracy of the referenced model by validatingthe model on the dataset. For this validation, the developer needs structural information to document and apply deep learning models,including metadata such as learning dataset, network architecture, and development environments. In this paper, we propose a descriptionlanguage that represents the network architecture of the deep learning model along with its metadata that are necessary to develop adeep learning model. Through the proposed description language, developers can easily verify the accuracy of the referenced deep learningmodel. Our experiments demonstrate the application scenario of a deep learning description document that focuses on the license platerecognition for the detection of illegally parked vehicles.Trusted Deep Learning, Model Reference, Deep Learning Description Language, Traffic Situation Analysis Model"
302,딥러닝 분석을 이용한 중국 역내·외 위안화 변동성 예측,이우식 /Lee Woosik,2016,한국데이터정보과학회,"2008년 글로벌 금융위기 이후 중국은 위안화 국제화의 점진적 추진을 진행하면서 중국상하이 외환시장과 중국홍콩 외환시장에서 거래되는 통화인 역내위안화와 역외위안화를 형성시켰다. 본 연구는 위안화 국제화와 점진적인 중국 자본계정 개방에 따라 급변하는 외환시장상황의 변동성을 정확하게 파악하기 위해서 GARCH모형 (일반화된 자기회귀 조건부이분산성모형)에 다단계인공신경망을 결합한 MLP-GARCH 모형과 GARCH모형과 기계학습의 일종인 딥러닝 (deep learning)을 통합한 DL-GARCH을 가지고 위안화 변동성예측을 비교 실험과 분석을 하였다. 비교분석 결과 DLGARCH 모형은 MLP-GARCH보다 모형 위안화 역내·외 환율변동성 예측 면에서 더욱 더 개선된예측값을 제공하였다. 그래서 이분산시계열모형을 딥러닝과 결합한 DL-GARCH 모형은 시계열의환율 변동성 예측 문제에 딥러닝을 응용할 수 있음을 확인하였다. 향후 이분산시계열과 결합된 딥러닝 모형은 다른 금융시계열 데이터에 응용하여 그 일반화 가능성을 높일 수 있을 것이다.The People’s Republic of China has vigorously been pursuing the internationalization of the Chinese Yuan or Renminbi after the financial crisis of 2008. In this view, an abrupt increase of use of the Chinese Yuan in the onshore and offshore markets are important milestones to be one of important currencies. One of the most frequently used methods to forecast volatility is GARCH model. Since a prediction error of the GARCH model has been reported quite high, a lot of efforts have been made to improve forecasting capability of the GARCH model. In this paper, we have proposed MLPGARCH and a DL-GARCH by employing Artificial Neural Network to the GARCH. In an application to forecasting Chinese Yuan volatility, we have successfully shown their overall outperformance in forecasting over the GARCH.Deep learning, DL-GARCH, GARCH, The Chinese Yuan, volatility."
303,딥러닝 기법을 이용한 내일강수 예측,하지훈,2016,한국지능시스템학회,"정확한 강수예측을 위해서는 예측인자 선정과 예측방법에 대한 선택이 매우 중요하다. 최근에는 강수예측 방법으로기계학습 기법이 많이 사용되고 있으며, 그 중에서도 특히 인공신경망을 사용한 강수예측 방법은 좋은 성능을 보였다. 본논문에서는 딥러닝 기법 중 하나인 DBN(deep belief network)를 이용한 새로운 강수예측 방법을 제안한다. DBN는 비지도사전 학습을 통해 초기 가중치를 설정하여 기존 인공신경망의 문제점을 보완한다. 예측인자로는 기온, 전일-전주 강수일, 태양과 달 궤도 관련 자료를 선정하였다. 기온과 전일-전주 강수일은 서울에서의 1974년부터 2013년까지 총 40년간의AWS(automatic weather system) 관측 자료를 사용하였고, 태양과 달의 궤도 관련 자료는 서울을 중심으로 계산한 결과를사용하였다. 전체 기간에서 일부는 학습 자료로 사용하여 예측모델을 생성하였고, 나머지를 생성한 모델의 검증 자료로사용하였다. 모델 검증 결과로 나온 예측값들은 확률값을 가지며 임계치를 이용하여 강수유무를 판별하였다. 강수 정확도의척도로 양분예보기법 중 CSI(critical successive index)와 Bias(frequency bias)를 계산하였다. 이를 통해 DBN와 MLP(multilayer perceptron)의 성능을 비교한 결과 DBN의 강수 예측 정확도가 높았고, 수행속도 또한 2배 이상 빨랐다.For accurate precipitation forecasts the choice of weather faoctrs and prediction method is very important. Recently, machinele arning has been widely used for forecasting precipitation, and artificial neural network, one of machine learning techniques, showed good performance. In this paper, we suggest a new method for forecasting precipitation using DBN, one of deep learning techniques.
DBN has an advantage that initial weights are set by unsupervised learning, so this compensates for the defects of artificial neural networks. We used past precipitation, temperature, and the parameters of the sun and moon's motion as features for forecasting precipitation. The dataset consists of observation data which had been measured for 40 years from AWS in Seoul. Experiments were based on 8-fold cross validation. As a result of estimation, we got probabilities of test dataset, so threshold was used for the decision of precipitation. CSI and Bias were used for indicating the precision of precipitation. Our experimental results showed that DBN performed better than MLP.Deep learning, Deep belief network, Precipitation, Forecast"
304,딥러닝 신경망모형을 이용한 실시간 교통정보수집 연구,김대현 /KIM DAE HYON,2016,사단법인 인문사회과학기술융합학회,"비디오 기반의 실시간 교통정보수집 시스템은 자율주행을 포함한 지능형교통체계의 주요 서브시스템중 하나이며, 우수한 예측(추정)력 때문에 핵심 알고리즘으로 인공신경망 모형이 주로 사용되고 있다. 특히, 최근 과학 학술지 네이처에 구글 딥마인드의 알파고가 공개되면서 딥러닝과 인공지능에 대한 전 세계적인 관심이 집중되고 있다. 최근까지 가장 널리 사용된 인공신경망모형은 역전파 모형이었으나 최근 컴퓨터 하드웨어 및 알고리즘의 새로운 개발과 더불어 딥러닝 알고리즘을 이용한 딥인공신경망 모형에 대한 관심이 높아지고 있다. 그러나 아직 딥러닝 인공신경망에 대한 연구는 초기단계에 있는 실정이며 적용분야도 극히 한정되어 있다. 본 연구에서는 실시간 도로교통정보 수집을 위한 영상처리분야에서 딥러닝 알고리즘을 활용하고 실험결과를 통하여 향후 교통정보수집‧분석 분야에서의 딥러닝 인공신경망 모형의 적용가능성을 살펴보고자 한다. 특히, 본 연구에서 도출한 딥러닝 인공신경망모형의 실험결과를 기존 역전파 모형에 의한 실험결과와 비교하여 모형의 우수성을 비교‧평가하고자 한다.Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Normally, artificial neural networks have been used in real-time applications for collecting traffic information because of their remarkable performance in terms of predictive accuracy and real-time computing. Especially, the recent publication of a paper in the journal Nature and the success of Google DeepMind’s world-beating Go program AlphaGo caused significant increases of the public interest in artificial intelligence. Even though various types of learning models have been proposed, the Backpropagation has been the most popular neural network model. However, the Deep learning neural network model has attracted much interest recently. In this study, the Deep leaning neural network will be applied in image processing techniques for the real-time traffic information collection to investigate the performance of Deep learning algorithm in traffic engineering fields. In addition, the experimental results from Deep learing algorithm will be compared with the results from Backpropagation model to evaluate and propose better performing model.Intelligent Transportation Systems (ITS), artificial neural networks, Deep learning, traffic information, image processing"
305,딥러닝의 모형과 응용사례,안성만 /SungMahn Ahn,2016,한국지능정보시스템학회,"딥러닝은 인공신경망(neural network)이라는 인공지능분야의 모형이 발전된 형태로서, 계층구조로 이루어진 인공신경망의 내부계층(hidden layer)이 여러 단계로 이루어진 구조이다. 딥러닝에서의 주요 모형은 합성곱신경망(convolutional neural network), 순환신경망(recurrent neural network), 그리고 심층신뢰신경망(deep belief network)의 세가지라고 할 수있다. 그 중에서 현재 흥미로운 연구가 많이 발표되어서 관심이 집중되고 있는 모형은 지도학습(supervised learning)모형인 처음 두 개의 모형이다. 따라서 본 논문에서는 지도학습모형의 가중치를 최적화하는 기본적인 방법인 오류역전파 알고리즘을 살펴본 뒤에 합성곱신경망과 순환신경망의 구조와 응용사례 등을 살펴보고자 한다. 본문에서 다루지 않은 모형인 심층신뢰신경망은 아직까지는 합성곱신경망 이나 순환신경망보다는 상대적으로 주목을 덜 받고 있다. 그러나 심층신뢰신경망은 CNN이나 RNN과는 달리 비지도학습(unsupervised learning)모형이며, 사람이나 동물은 관찰을 통해서 스스로 학습한다는 점에서 궁극적으로는 비지도학습모형이 더 많이 연구되어야 할 주제가 될 것이다.Deep learning model is a kind of neural networks that allows multiple hidden layers. There are various deep learning architectures such as convolutional neural networks, deep belief networks and recurrent neural networks. Those have been applied to fields like computer vision, automatic speech recognition, natural language processing, audio recognition and bioinformatics where they have been shown to produce state-of-the-art results on various tasks. Among those architectures, convolutional neural networks and recurrent neural networks are classified as the supervised learning model. And in recent years, those supervised learning models have gained more popularity than unsupervised learning models such as deep belief networks, because supervised learning models have shown fashionable applications in such fields mentioned above.
Deep learning models can be trained with backpropagation algorithm. Backpropagation is an abbreviation for “backward propagation of errors” and a common method of training artificial neural networks used in conjunction with an optimization method such as gradient descent. The method calculates the gradient of an error function with respect to all the weights in the network. The gradient is fed to the optimization method which in turn uses it to update the weights, in an attempt to minimize the error function.
Convolutional neural networks use a special architecture which is particularly well-adapted to classify images. Using this architecture makes convolutional networks fast to train. This, in turn, helps us train deep, muti-layer networks, which are very good at classifying images. These days, deep convolutional networks are used in most neural networks for image recognition. Convolutional neural networks use three basic ideas: local receptive fields, shared weights, and pooling. By local receptive fields, we mean that each neuron in the first(or any) hidden layer will be connected to a small region of the input(or previous layer’s) neurons. Shared weights mean that we’re going to use the same weights and bias for each of the local receptive field. This means that all the neurons in the hidden layer detect exactly the same feature, just at different locations in the input image. In addition to the convolutional layers just described, convolutional neural networks also contain pooling layers. Pooling layers are usually used immediately after convolutional layers. What the pooling layers do is to simplify the information in the output from the convolutional layer.
Recent convolutional network architectures have 10 to 20 hidden layers and billions of connections between units. Training deep learning networks has taken weeks several years ago, but thanks to progress in GPU and algorithm enhancement, training time has reduced to several hours.
Neural networks with time-varying behavior are known as recurrent neural networks or RNNs. A recurrent neural network is a class of artificial neural network where connections between units form a directed cycle. This creates an internal state of the network which allows it to exhibit dynamic temporal behavior. Unlike feedforward neural networks, RNNs can use their internal memory to process arbitrary sequences of inputs. Early RNN models turned out to be very difficult to train, harder even than deep feedforward networks. The reason is the unstable gradient problem such as vanishing gradient and exploding gradient. The gradient can get smaller and smaller as it is propagated back through layers. This makes learning in early layers extremely slow. The problem actually gets worse in RNNs, since gradients aren’t just propagated backward through layers, they’re propagated backward through time. If the network runs for a long time, that can make the gradient extremely unstable and hard to learn from. It has been possible to incorporate an idea known as long short-term memory units (LSTMs) into RNNs. LSTMs make it much easier to get good results when training RNNs, and many recent papers make use of LSTMs or related ideas.Deep learning, Convolutional neural networks, Recurrent neural networks, Error backpropagation algorithm"
306,딥러닝분석과 기술적 분석 지표를 이용한 한국 코스피주가지수 방향성 예측,이우식 /Lee Woosik,2017,한국데이터정보과학회,"2016년 3월 구글 (Google)의 바둑인공지능 알파고 (AlphaGo)가 이세돌 9단과의 바둑대결에서승리한 이후 다양한 분야에서 인공지능 사용에 대한 관심이 높아지고 있는 가운데 금융투자 분야에서도 인공지능과 투자자문 전문가의 합성어인 로보어드바이저 (Robo-Advisor)에 대한 관심이 높아지고 있다. 인공지능 (artificial intelligence)기반의 의사결정은 비용 절감은 물론 효과적인 의사결정을가능하게 한다는 점에서 큰 장점이 있다. 본 연구에서는 기술적 분석 (technical analysis) 지표와 딥러닝 (deep learning) 모형을 결합하여 한국 코스피 지수를 예측하는 모형을 개발하고 제시한 모형들의 예측력을 비교, 분석한다. 분석 결과 기술적 분석 지표에 딥러닝 알고리즘을 결합한 모형이 주가지수 방향성 예측 문제에 응용될 수 있음을 확인하였다. 향후 본 연구에서 제안된 기술적 분석 지표와딥러닝모형을 결합한 기법은 로보어드바이저서비스에 응용할 수 있는 일반화 가능성을 보여준다Since Google’s AlphaGo defeated a world champion of Go players in 2016, there have been many interests in the deep learning. In the financial sector, a Robo-Advisor using deep learning gains a significant attention, which builds and manages portfolios of financial instruments for investors.In this paper, we have proposed the a deep learning algorithm geared toward identification and forecast of the KOSPI index direction,and we also have compared the accuracy of the prediction.In an application of forecasting the financial market index direction, we have shown that the Robo-Advisor using deep learning has a significant effect on finance industry. The Robo-Advisor collects a massive data such as earnings statements, news reports and regulatory filings, analyzes those and recommends investors how to view market trends and identify the best time to purchase financial assets. On the other hand, the Robo-Advisor allows businesses to learn more about their customers, develop better marketing strategies, increase sales and decrease costs.Artificial intelligence, deep learning, FinTech, Robo-Advisor, technical analysis."
307,딥러닝의 효율적인 학습을 위한 학습데이터 선별 기법에 관한 연구,주경돈 /Gyoung-don Joo,2017,한국정보과학회,"빅데이터의 증가와 병렬 하드웨어 처리, 학습 알고리즘의 개선은 딥러닝의 발전을 가져왔으며 인공지능이 적용 가능한 영역을 획기적으로 확장시키고 있다. 하지만 이러한 대규모의 학습 데이터에 기반한 딥러닝은 학습시간이 증가되고, 더 나아가 학습 데이터의 구축을 위한 레이블링(labeling) 비용이 데이터 수에 비례하여 증가하게 된다. 따라서 본 논문에서는, 딥러닝의 정확도를 최대한 유지하면서 필요한 학습 데이터의 수를 줄일수 있도록, 전체 데이터에서 제한된 수의 학습 데이터의 후보를 선별하는 기법에 대하여 연구한다. 특히 기존의 능동 학습에 기반한 기법들이 학습 데이터의 수가 매우 적어질 경우 딥러닝의 정확도가 급격히 떨어지는 문제를 해결할 수 있는 새로운 기법을 제안하였으며, 제안된 기법으로 선별된 학습 데이터는 그 수의 변화에 대해 딥러닝이 일관되게 경쟁력있는 정확도를 가짐을 다양한 실험을 통하여 검증하였다.The increase of big data, parallel hardware processing and enhanced learning algorithms have led to the development of deep learning and are dramatically expanding the domain of applications based on artificial intelligence. However, such large-scale training data extends the learning time, and furthermore, the labeling cost for building the training set increases in proportion to the volume of data. Therefore, in this paper, we study how to select a limited number of candidates for training set from given data so as to reduce the number of training set while maintaining the accuracy of deep learning. Especially, we found that, with the training set selected by the previous work, the accuracy of deep learning drastically decreases when the number of training set is very small and so proposed new methods to resolve this fall-off in accuracy. Through experiments, we showed that deep learning consistently has the competitive accuracy with the training set selected by the proposed method regardless of the size of training set.Deep Learning, Train Set Selection, Active Learning, Gamma distribution"
308,딥러닝을 이용한 범용적 스테그아날리시스,김현재 /Hyunjae Kim,2017,한국정보과학회,"스테그아날리시스(Steganalysis)란 이미지 등 일반적인 자료에 암호화된 정보를 은닉하는 스테가노그래피(Steganography)에 대한 검출 및 분석 방법으로, 기계학습 기반 방법론을 포함한다. 기존 기계학습 기반 스테그아날리시스는 영상(Image)의 특징(Feature) 추출 및 모델링에 기반하며, 최근 딥러닝(Deep Learning)의 적용으로 검출 정확도가 큰 폭으로 향상되었다. 하지만 현존하는 스테그아날리시스 모델은 단일 스테가노그래피 기법에 대해 국한되어 있어 학습에 사용되지 않은 스테고(Stego) 이미지의 경우 검출이 불가능한 결정적 한계를 가진다. 본 연구에서는 다양한 스테가노그래피 기법으로 생성된 스테고이미지에 딥러닝을 적용하여 스테그아날리시스를 학습하는 범용적 모델을 제안한다. 다양한 실험을 통해 제안 기법의 효용성 및 가능성을 확인하고, 범용적 스테그아날리시스 모델이 각각에 특화된 검출 기법과 유사한 정확도로 스테고 이미지를 검출할 수 있음을 보인다.Steganalysis is to detect information hidden by steganography inside general data such as images. There are stegoanalysis techniques that use machine learning (ML). Existing ML approaches to steganalysis are based on extracting features from stego images and modeling them.
Recently deep learning-based methodologies have shown significant improvements in detection accuracy. However, all the existing methods, including deep learning-based ones, have a critical limitation in that they can only detect stego images that are created by a specific steganography method. In this paper, we propose a generalized steganalysis method that can model multiple types of stego images using deep learning. Through various experiments, we confirm the effectiveness of our approach and envision directions for future research. In particular, we show that our method can detect each type of steganography with the same level of accuracy as that of a steganalysis method dedicated to that type of steganography, thereby demonstrating the general applicability of our approach to multiple types of stego images.steganalysis, steganography, deep learning, generalized steganalysis model"
309,"딥러닝 프레임워크의 비교: 티아노, 텐서플로, CNTK를 중심으로",정여진 /Yeojin Chung,2017,한국지능정보시스템학회,"딥러닝 프레임워크의 대표적인 기능으로는 ‘자동미분’과 ‘GPU의 활용’ 등을 들 수 있다. 본 논문은 파이썬의 라이브러리 형태로 사용 가능한 프레임워크 중에서 구글의 텐서플로와 마이크로소프트의 CNTK, 그리고 텐서 플로의 원조라고 할 수 있는 티아노를 비교하였다. 본문에서는 자동미분의 개념과 GPU의 활용형태를 간단히 설명하고, 그 다음에 logistic regression을 실행하는 예를 통하여 각 프레임워크의 문법을 알아본 뒤에, 마지막으로 대표적인 딥러닝 응용인 CNN의 예제를 실행시켜보고 코딩의 편의성과 실행속도 등을 확인해 보았다. 그 결과, 편의성의 관점에서 보면 티아노가 가장 코딩 하기가 어렵고, CNTK와 텐서플로는 많은 부분이 비슷하게 추상화 되어 있어서 코딩이 비슷하지만 가중치와 편향을 직접 정의하느냐의 여부에서 차이를 보였다. 그리고 각프레임워크의 실행속도에 대한 평가는 ‘큰 차이는 없다’는 것이다. 텐서플로는 티아노에 비하여 속도가 느리다는 평가가 있어왔는데, 본 연구의 실험에 의하면, 비록 CNN 모형에 국한되었지만, 텐서플로가 아주 조금이지만빠른 것으로 나타났다. CNTK의 경우에도, 비록 실험환경이 달랐지만, 실험환경의 차이에 의한 속도의 차이의 편차범위 이내에 있는 것으로 판단이 되었다. 본 연구에서는 세 종류의 딥러닝 프레임워크만을 살펴보았는데,위키피디아에 따르면 딥러닝 프레임워크의 종류는 12가지가 있으며, 각 프레임워크의 특징을 15가지 속성으로구분하여 차이를 특정하고 있다. 그 많은 속성 중에서 사용자의 입장에서 볼 때 중요한 속성은 어떤 언어(파이썬, C++, Java, 등)로 사용가능한지, 어떤 딥러닝 모형에 대한 라이브러리가 잘 구현되어 있는지 등일 것이다. 그리고 사용자가 대규모의 딥러닝 모형을 구축한다면, 다중 GPU 혹은 다중 서버를 지원하는지의 여부도 중요할 것이다. 또한 딥러닝 모형을 처음 학습하는 경우에는 사용설명서가 많은지 예제 프로그램이 많은지 여부도 중요한 기준이 될 것이다.The deep learning framework is software designed to help develop deep learning models. Some of its important functions include “automatic differentiation” and “utilization of GPU”. The list of popular deep learning framework includes Caffe (BVLC) and Theano (University of Montreal). And recently, Microsoft's deep learning framework, Microsoft Cognitive Toolkit, was released as open-source license, following Google’s Tensorflow a year earlier. The early deep learning frameworks have been developed mainly for research at universities. Beginning with the inception of Tensorflow, however, it seems that companies such as Microsoft and Facebook have started to join the competition of framework development. Given the trend, Google and other companies are expected to continue investing in the deep learning framework to bring forward the initiative in the artificial intelligence business. From this point of view, we think it is a good time to compare some of deep learning frameworks. So we compare three deep learning frameworks which can be used as a Python library. Those are Google's Tensorflow, Microsoft’s CNTK, and Theano which is sort of a predecessor of the preceding two.
The most common and important function of deep learning frameworks is the ability to perform automatic differentiation. Basically all the mathematical expressions of deep learning models can be represented as computational graphs, which consist of nodes and edges. Partial derivatives on each edge of a computational graph can then be obtained. With the partial derivatives, we can let software compute differentiation of any node with respect to any variable by utilizing chain rule of Calculus.
First of all, the convenience of coding is in the order of CNTK, Tensorflow, and Theano. The criterion is simply based on the lengths of the codes and the learning curve and the ease of coding are not the main concern. According to the criteria, Theano was the most difficult to implement with, and CNTK and Tensorflow were somewhat easier. With Tensorflow, we need to define weight variables and biases explicitly. The reason that CNTK and Tensorflow are easier to implement with is that those frameworks provide us with more abstraction than Theano. We, however, need to mention that low-level coding is not always bad. It gives us flexibility of coding. With the low-level coding such as in Theano, we can implement and test any new deep learning models or any new search methods that we can think of.
The assessment of the execution speed of each framework is that there is not meaningful difference. According to the experiment, execution speeds of Theano and Tensorflow are very similar, although the experiment was limited to a CNN model. In the case of CNTK, the experimental environment was not maintained as the same. The code written in CNTK has to be run in PC environment without GPU where codes execute as much as 50 times slower than with GPU. But we concluded that the difference of execution speed was within the range of variation caused by the different hardware setup.
In this study, we compared three types of deep learning framework: Theano, Tensorflow, and CNTK. According to Wikipedia, there are 12 available deep learning frameworks. And 15 different attributes differentiate each framework. Some of the important attributes would include interface language (Python, C ++, Java, etc.) and the availability of libraries on various deep learning models such as CNN, RNN, DBN, and etc. And if a user implements a large scale deep learning model, it will also be important to support multiple GPU or multiple servers. Also, if you are learning the deep learning model, it would also be important if there are enough examples and references.deep learning framework, Theano, TensorFlow, CNTK, computational graph, CIFAR-10"
310,딥러닝을 이용한 주변 무선단말 파악방안,이웅섭 /Lee， Woong-sup,2018,한국정보통신학회,"최근 단말-대-단말(Device-to-device, D2D) 통신기술이 차세대 무선통신시스템의 핵심기술로 큰 관심을 받고 있다. 이러한 단말간 통신에서는 자신의 주변에 어떠한 단말이 있는지 파악하는 주변단말 탐색(Neighbor discovery)이매우 중요하다. 본 논문에서는 최근 큰 관심을 받고 있는 딥러닝(Deep learning) 기술을 활용하여 단말간 통신에서 주변단말을 파악하는 방안에 대해서 제안한다. 제안 방안은 기존의 방안과 달리 무선채널의 공간적 연관성을 이용하여단말간의 신호 전송 없이 단말이 기지국으로 전송하는 상향링크 파일럿 신호를 기반으로 주변 단말을 찾고 따라서기존의 방식에 비해 신호전송 복잡도(signaling complexity)를 크게 줄일 수 있다. 또한 제안 방안에서는 떨어져 있는거리에 따라서 주변 단말을 분류 가능하여 기존 방안에 비해서 좀 더 세밀한 단말 탐색이 가능하다. 마지막으로 본 논문에서는 tensorflow를 이용한 컴퓨터 시뮬레이션을 통해 제안 방안의 성능을 검증하였다.Recently, the device-to-device (D2D) communication has been conceived as the key technology for the next-generationmobile communication systems. The neighbor discovery in which the nearby users are found, is essential for the properoperation of the D2D communication. In this paper, we propose new neighbor discovery scheme based on deep learningtechnology which has gained a lot of attention recently. In the proposed scheme, the neighboring users can be foundusing the uplink pilot transmission of users only, unlike conventional neighbor discovery schemes in which direct pilotcommunication among users is required, such that the signaling overhead can be greatly reduced in our proposed scheme.
Moreover, the neighbors with different proximity can also be classified accordingly which enables more accurate neighbordiscovery compared to the conventional schemes. The performance of our proposed scheme is verified through thetensorflow-based computer simulations.
키워드 : 딥러닝, 주변단말 탐색, 단말-대-단말 통신Deep learning, Neighbor discovery, Device-to-device communication, Wireless channel correlation"
311,신뢰성있는 딥러닝 기반 분석 모델을 참조하기 위한 딥러닝 기술 언어,문종혁 /Jong Hyeok Mun,2021,한국정보처리학회,"최근 딥러닝은 하드웨어 성능이 향상됨에 따라 자연어 처리, 영상 인식 등의 다양한 기술에 접목되어 활용되고 있다. 이러한 기술들을 활용해 지능형교통 시스템(ITS), 스마트홈, 헬스케어 등의 산업분야에서 데이터를 분석하여 고속도로 속도위반 차량 검출, 에너지 사용량 제어, 응급상황 등과 같은고품질의 서비스를 제공하며, 고품질의 서비스를 제공하기 위해서는 정확도가 향상된 딥러닝 모델이 적용되어야 한다. 이를 위해 서비스 환경의 데이터를분석하기 위한 딥러닝 모델을 개발할 때, 개발자는 신뢰성이 검증된 최신의 딥러닝 모델을 적용할 수 있어야 한다. 이는 개발자가 참조하는 딥러닝 모델에적용된 학습 데이터셋의 정확도를 측정하여 검증할 수 있다. 이러한 검증을 위해서 개발자는 학습 데이터셋, 딥러닝의 계층구조 및 개발 환경 등과 같은내용을 포함하는 딥러닝 모델을 문서화하여 적용하기 위한 구조적인 정보가 필요하다. 본 논문에서는 신뢰성있는 딥러닝 기반 데이터 분석 모델을 참조하기위한 딥러닝 기술 언어를 제안한다. 제안하는 기술 언어는 신뢰성 있는 딥러닝 모델을 개발하는데 필요한 학습데이터셋, 개발 환경 및 설정 등의 정보와더불어 딥러닝 모델의 계층구조를 표현할 수 있다. 제안하는 딥러닝 기술 언어를 이용하여 개발자는 지능형 교통 시스템에서 참조하는 분석 모델의정확도를 검증할 수 있다. 실험에서는 제안하는 언어의 유효성을 검증하기 위해, 번호판 인식 모델을 중심으로 딥러닝 기술 문서의 적용과정을 보인다With the recent advancements of deep learning, companies such as smart home, healthcare, and intelligent transportation systems areutilizing its functionality to provide high-quality services for vehicle detection, emergency situation detection, and controlling energyconsumption. To provide reliable services in such sensitive systems, deep learning models are required to have high accuracy. In orderto develop a deep learning model for analyzing previously mentioned services, developers should utilize the state of the art deep learningmodels that have already been verified for higher accuracy. The developers can verify the accuracy of the referenced model by validatingthe model on the dataset. For this validation, the developer needs structural information to document and apply deep learning models,including metadata such as learning dataset, network architecture, and development environments. In this paper, we propose a descriptionlanguage that represents the network architecture of the deep learning model along with its metadata that are necessary to develop adeep learning model. Through the proposed description language, developers can easily verify the accuracy of the referenced deep learningmodel. Our experiments demonstrate the application scenario of a deep learning description document that focuses on the license platerecognition for the detection of illegally parked vehicles.Trusted Deep Learning, Model Reference, Deep Learning Description Language, Traffic Situation Analysis Model"
312,딥러닝 분석을 이용한 중국 역내·외 위안화 변동성 예측,이우식 /Lee Woosik,2016,한국데이터정보과학회,"2008년 글로벌 금융위기 이후 중국은 위안화 국제화의 점진적 추진을 진행하면서 중국상하이 외환시장과 중국홍콩 외환시장에서 거래되는 통화인 역내위안화와 역외위안화를 형성시켰다. 본 연구는 위안화 국제화와 점진적인 중국 자본계정 개방에 따라 급변하는 외환시장상황의 변동성을 정확하게 파악하기 위해서 GARCH모형 (일반화된 자기회귀 조건부이분산성모형)에 다단계인공신경망을 결합한 MLP-GARCH 모형과 GARCH모형과 기계학습의 일종인 딥러닝 (deep learning)을 통합한 DL-GARCH을 가지고 위안화 변동성예측을 비교 실험과 분석을 하였다. 비교분석 결과 DLGARCH 모형은 MLP-GARCH보다 모형 위안화 역내·외 환율변동성 예측 면에서 더욱 더 개선된예측값을 제공하였다. 그래서 이분산시계열모형을 딥러닝과 결합한 DL-GARCH 모형은 시계열의환율 변동성 예측 문제에 딥러닝을 응용할 수 있음을 확인하였다. 향후 이분산시계열과 결합된 딥러닝 모형은 다른 금융시계열 데이터에 응용하여 그 일반화 가능성을 높일 수 있을 것이다.The People’s Republic of China has vigorously been pursuing the internationalization of the Chinese Yuan or Renminbi after the financial crisis of 2008. In this view, an abrupt increase of use of the Chinese Yuan in the onshore and offshore markets are important milestones to be one of important currencies. One of the most frequently used methods to forecast volatility is GARCH model. Since a prediction error of the GARCH model has been reported quite high, a lot of efforts have been made to improve forecasting capability of the GARCH model. In this paper, we have proposed MLPGARCH and a DL-GARCH by employing Artificial Neural Network to the GARCH. In an application to forecasting Chinese Yuan volatility, we have successfully shown their overall outperformance in forecasting over the GARCH.Deep learning, DL-GARCH, GARCH, The Chinese Yuan, volatility."
313,딥러닝 기법을 이용한 내일강수 예측,하지훈,2016,한국지능시스템학회,"정확한 강수예측을 위해서는 예측인자 선정과 예측방법에 대한 선택이 매우 중요하다. 최근에는 강수예측 방법으로기계학습 기법이 많이 사용되고 있으며, 그 중에서도 특히 인공신경망을 사용한 강수예측 방법은 좋은 성능을 보였다. 본논문에서는 딥러닝 기법 중 하나인 DBN(deep belief network)를 이용한 새로운 강수예측 방법을 제안한다. DBN는 비지도사전 학습을 통해 초기 가중치를 설정하여 기존 인공신경망의 문제점을 보완한다. 예측인자로는 기온, 전일-전주 강수일, 태양과 달 궤도 관련 자료를 선정하였다. 기온과 전일-전주 강수일은 서울에서의 1974년부터 2013년까지 총 40년간의AWS(automatic weather system) 관측 자료를 사용하였고, 태양과 달의 궤도 관련 자료는 서울을 중심으로 계산한 결과를사용하였다. 전체 기간에서 일부는 학습 자료로 사용하여 예측모델을 생성하였고, 나머지를 생성한 모델의 검증 자료로사용하였다. 모델 검증 결과로 나온 예측값들은 확률값을 가지며 임계치를 이용하여 강수유무를 판별하였다. 강수 정확도의척도로 양분예보기법 중 CSI(critical successive index)와 Bias(frequency bias)를 계산하였다. 이를 통해 DBN와 MLP(multilayer perceptron)의 성능을 비교한 결과 DBN의 강수 예측 정확도가 높았고, 수행속도 또한 2배 이상 빨랐다.For accurate precipitation forecasts the choice of weather faoctrs and prediction method is very important. Recently, machinele arning has been widely used for forecasting precipitation, and artificial neural network, one of machine learning techniques, showed good performance. In this paper, we suggest a new method for forecasting precipitation using DBN, one of deep learning techniques.
DBN has an advantage that initial weights are set by unsupervised learning, so this compensates for the defects of artificial neural networks. We used past precipitation, temperature, and the parameters of the sun and moon's motion as features for forecasting precipitation. The dataset consists of observation data which had been measured for 40 years from AWS in Seoul. Experiments were based on 8-fold cross validation. As a result of estimation, we got probabilities of test dataset, so threshold was used for the decision of precipitation. CSI and Bias were used for indicating the precision of precipitation. Our experimental results showed that DBN performed better than MLP.Deep learning, Deep belief network, Precipitation, Forecast"
314,딥러닝 신경망모형을 이용한 실시간 교통정보수집 연구,김대현 /KIM DAE HYON,2016,사단법인 인문사회과학기술융합학회,"비디오 기반의 실시간 교통정보수집 시스템은 자율주행을 포함한 지능형교통체계의 주요 서브시스템중 하나이며, 우수한 예측(추정)력 때문에 핵심 알고리즘으로 인공신경망 모형이 주로 사용되고 있다. 특히, 최근 과학 학술지 네이처에 구글 딥마인드의 알파고가 공개되면서 딥러닝과 인공지능에 대한 전 세계적인 관심이 집중되고 있다. 최근까지 가장 널리 사용된 인공신경망모형은 역전파 모형이었으나 최근 컴퓨터 하드웨어 및 알고리즘의 새로운 개발과 더불어 딥러닝 알고리즘을 이용한 딥인공신경망 모형에 대한 관심이 높아지고 있다. 그러나 아직 딥러닝 인공신경망에 대한 연구는 초기단계에 있는 실정이며 적용분야도 극히 한정되어 있다. 본 연구에서는 실시간 도로교통정보 수집을 위한 영상처리분야에서 딥러닝 알고리즘을 활용하고 실험결과를 통하여 향후 교통정보수집‧분석 분야에서의 딥러닝 인공신경망 모형의 적용가능성을 살펴보고자 한다. 특히, 본 연구에서 도출한 딥러닝 인공신경망모형의 실험결과를 기존 역전파 모형에 의한 실험결과와 비교하여 모형의 우수성을 비교‧평가하고자 한다.Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Normally, artificial neural networks have been used in real-time applications for collecting traffic information because of their remarkable performance in terms of predictive accuracy and real-time computing. Especially, the recent publication of a paper in the journal Nature and the success of Google DeepMind’s world-beating Go program AlphaGo caused significant increases of the public interest in artificial intelligence. Even though various types of learning models have been proposed, the Backpropagation has been the most popular neural network model. However, the Deep learning neural network model has attracted much interest recently. In this study, the Deep leaning neural network will be applied in image processing techniques for the real-time traffic information collection to investigate the performance of Deep learning algorithm in traffic engineering fields. In addition, the experimental results from Deep learing algorithm will be compared with the results from Backpropagation model to evaluate and propose better performing model.Intelligent Transportation Systems (ITS), artificial neural networks, Deep learning, traffic information, image processing"
315,딥러닝의 모형과 응용사례,안성만 /SungMahn Ahn,2016,한국지능정보시스템학회,"딥러닝은 인공신경망(neural network)이라는 인공지능분야의 모형이 발전된 형태로서, 계층구조로 이루어진 인공신경망의 내부계층(hidden layer)이 여러 단계로 이루어진 구조이다. 딥러닝에서의 주요 모형은 합성곱신경망(convolutional neural network), 순환신경망(recurrent neural network), 그리고 심층신뢰신경망(deep belief network)의 세가지라고 할 수있다. 그 중에서 현재 흥미로운 연구가 많이 발표되어서 관심이 집중되고 있는 모형은 지도학습(supervised learning)모형인 처음 두 개의 모형이다. 따라서 본 논문에서는 지도학습모형의 가중치를 최적화하는 기본적인 방법인 오류역전파 알고리즘을 살펴본 뒤에 합성곱신경망과 순환신경망의 구조와 응용사례 등을 살펴보고자 한다. 본문에서 다루지 않은 모형인 심층신뢰신경망은 아직까지는 합성곱신경망 이나 순환신경망보다는 상대적으로 주목을 덜 받고 있다. 그러나 심층신뢰신경망은 CNN이나 RNN과는 달리 비지도학습(unsupervised learning)모형이며, 사람이나 동물은 관찰을 통해서 스스로 학습한다는 점에서 궁극적으로는 비지도학습모형이 더 많이 연구되어야 할 주제가 될 것이다.Deep learning model is a kind of neural networks that allows multiple hidden layers. There are various deep learning architectures such as convolutional neural networks, deep belief networks and recurrent neural networks. Those have been applied to fields like computer vision, automatic speech recognition, natural language processing, audio recognition and bioinformatics where they have been shown to produce state-of-the-art results on various tasks. Among those architectures, convolutional neural networks and recurrent neural networks are classified as the supervised learning model. And in recent years, those supervised learning models have gained more popularity than unsupervised learning models such as deep belief networks, because supervised learning models have shown fashionable applications in such fields mentioned above.
Deep learning models can be trained with backpropagation algorithm. Backpropagation is an abbreviation for “backward propagation of errors” and a common method of training artificial neural networks used in conjunction with an optimization method such as gradient descent. The method calculates the gradient of an error function with respect to all the weights in the network. The gradient is fed to the optimization method which in turn uses it to update the weights, in an attempt to minimize the error function.
Convolutional neural networks use a special architecture which is particularly well-adapted to classify images. Using this architecture makes convolutional networks fast to train. This, in turn, helps us train deep, muti-layer networks, which are very good at classifying images. These days, deep convolutional networks are used in most neural networks for image recognition. Convolutional neural networks use three basic ideas: local receptive fields, shared weights, and pooling. By local receptive fields, we mean that each neuron in the first(or any) hidden layer will be connected to a small region of the input(or previous layer’s) neurons. Shared weights mean that we’re going to use the same weights and bias for each of the local receptive field. This means that all the neurons in the hidden layer detect exactly the same feature, just at different locations in the input image. In addition to the convolutional layers just described, convolutional neural networks also contain pooling layers. Pooling layers are usually used immediately after convolutional layers. What the pooling layers do is to simplify the information in the output from the convolutional layer.
Recent convolutional network architectures have 10 to 20 hidden layers and billions of connections between units. Training deep learning networks has taken weeks several years ago, but thanks to progress in GPU and algorithm enhancement, training time has reduced to several hours.
Neural networks with time-varying behavior are known as recurrent neural networks or RNNs. A recurrent neural network is a class of artificial neural network where connections between units form a directed cycle. This creates an internal state of the network which allows it to exhibit dynamic temporal behavior. Unlike feedforward neural networks, RNNs can use their internal memory to process arbitrary sequences of inputs. Early RNN models turned out to be very difficult to train, harder even than deep feedforward networks. The reason is the unstable gradient problem such as vanishing gradient and exploding gradient. The gradient can get smaller and smaller as it is propagated back through layers. This makes learning in early layers extremely slow. The problem actually gets worse in RNNs, since gradients aren’t just propagated backward through layers, they’re propagated backward through time. If the network runs for a long time, that can make the gradient extremely unstable and hard to learn from. It has been possible to incorporate an idea known as long short-term memory units (LSTMs) into RNNs. LSTMs make it much easier to get good results when training RNNs, and many recent papers make use of LSTMs or related ideas.Deep learning, Convolutional neural networks, Recurrent neural networks, Error backpropagation algorithm"
316,딥러닝분석과 기술적 분석 지표를 이용한 한국 코스피주가지수 방향성 예측,이우식 /Lee Woosik,2017,한국데이터정보과학회,"2016년 3월 구글 (Google)의 바둑인공지능 알파고 (AlphaGo)가 이세돌 9단과의 바둑대결에서승리한 이후 다양한 분야에서 인공지능 사용에 대한 관심이 높아지고 있는 가운데 금융투자 분야에서도 인공지능과 투자자문 전문가의 합성어인 로보어드바이저 (Robo-Advisor)에 대한 관심이 높아지고 있다. 인공지능 (artificial intelligence)기반의 의사결정은 비용 절감은 물론 효과적인 의사결정을가능하게 한다는 점에서 큰 장점이 있다. 본 연구에서는 기술적 분석 (technical analysis) 지표와 딥러닝 (deep learning) 모형을 결합하여 한국 코스피 지수를 예측하는 모형을 개발하고 제시한 모형들의 예측력을 비교, 분석한다. 분석 결과 기술적 분석 지표에 딥러닝 알고리즘을 결합한 모형이 주가지수 방향성 예측 문제에 응용될 수 있음을 확인하였다. 향후 본 연구에서 제안된 기술적 분석 지표와딥러닝모형을 결합한 기법은 로보어드바이저서비스에 응용할 수 있는 일반화 가능성을 보여준다Since Google’s AlphaGo defeated a world champion of Go players in 2016, there have been many interests in the deep learning. In the financial sector, a Robo-Advisor using deep learning gains a significant attention, which builds and manages portfolios of financial instruments for investors.In this paper, we have proposed the a deep learning algorithm geared toward identification and forecast of the KOSPI index direction,and we also have compared the accuracy of the prediction.In an application of forecasting the financial market index direction, we have shown that the Robo-Advisor using deep learning has a significant effect on finance industry. The Robo-Advisor collects a massive data such as earnings statements, news reports and regulatory filings, analyzes those and recommends investors how to view market trends and identify the best time to purchase financial assets. On the other hand, the Robo-Advisor allows businesses to learn more about their customers, develop better marketing strategies, increase sales and decrease costs.Artificial intelligence, deep learning, FinTech, Robo-Advisor, technical analysis."
317,딥러닝의 효율적인 학습을 위한 학습데이터 선별 기법에 관한 연구,주경돈 /Gyoung-don Joo,2017,한국정보과학회,"빅데이터의 증가와 병렬 하드웨어 처리, 학습 알고리즘의 개선은 딥러닝의 발전을 가져왔으며 인공지능이 적용 가능한 영역을 획기적으로 확장시키고 있다. 하지만 이러한 대규모의 학습 데이터에 기반한 딥러닝은 학습시간이 증가되고, 더 나아가 학습 데이터의 구축을 위한 레이블링(labeling) 비용이 데이터 수에 비례하여 증가하게 된다. 따라서 본 논문에서는, 딥러닝의 정확도를 최대한 유지하면서 필요한 학습 데이터의 수를 줄일수 있도록, 전체 데이터에서 제한된 수의 학습 데이터의 후보를 선별하는 기법에 대하여 연구한다. 특히 기존의 능동 학습에 기반한 기법들이 학습 데이터의 수가 매우 적어질 경우 딥러닝의 정확도가 급격히 떨어지는 문제를 해결할 수 있는 새로운 기법을 제안하였으며, 제안된 기법으로 선별된 학습 데이터는 그 수의 변화에 대해 딥러닝이 일관되게 경쟁력있는 정확도를 가짐을 다양한 실험을 통하여 검증하였다.The increase of big data, parallel hardware processing and enhanced learning algorithms have led to the development of deep learning and are dramatically expanding the domain of applications based on artificial intelligence. However, such large-scale training data extends the learning time, and furthermore, the labeling cost for building the training set increases in proportion to the volume of data. Therefore, in this paper, we study how to select a limited number of candidates for training set from given data so as to reduce the number of training set while maintaining the accuracy of deep learning. Especially, we found that, with the training set selected by the previous work, the accuracy of deep learning drastically decreases when the number of training set is very small and so proposed new methods to resolve this fall-off in accuracy. Through experiments, we showed that deep learning consistently has the competitive accuracy with the training set selected by the proposed method regardless of the size of training set.Deep Learning, Train Set Selection, Active Learning, Gamma distribution"
318,딥러닝을 이용한 범용적 스테그아날리시스,김현재 /Hyunjae Kim,2017,한국정보과학회,"스테그아날리시스(Steganalysis)란 이미지 등 일반적인 자료에 암호화된 정보를 은닉하는 스테가노그래피(Steganography)에 대한 검출 및 분석 방법으로, 기계학습 기반 방법론을 포함한다. 기존 기계학습 기반 스테그아날리시스는 영상(Image)의 특징(Feature) 추출 및 모델링에 기반하며, 최근 딥러닝(Deep Learning)의 적용으로 검출 정확도가 큰 폭으로 향상되었다. 하지만 현존하는 스테그아날리시스 모델은 단일 스테가노그래피 기법에 대해 국한되어 있어 학습에 사용되지 않은 스테고(Stego) 이미지의 경우 검출이 불가능한 결정적 한계를 가진다. 본 연구에서는 다양한 스테가노그래피 기법으로 생성된 스테고이미지에 딥러닝을 적용하여 스테그아날리시스를 학습하는 범용적 모델을 제안한다. 다양한 실험을 통해 제안 기법의 효용성 및 가능성을 확인하고, 범용적 스테그아날리시스 모델이 각각에 특화된 검출 기법과 유사한 정확도로 스테고 이미지를 검출할 수 있음을 보인다.Steganalysis is to detect information hidden by steganography inside general data such as images. There are stegoanalysis techniques that use machine learning (ML). Existing ML approaches to steganalysis are based on extracting features from stego images and modeling them.
Recently deep learning-based methodologies have shown significant improvements in detection accuracy. However, all the existing methods, including deep learning-based ones, have a critical limitation in that they can only detect stego images that are created by a specific steganography method. In this paper, we propose a generalized steganalysis method that can model multiple types of stego images using deep learning. Through various experiments, we confirm the effectiveness of our approach and envision directions for future research. In particular, we show that our method can detect each type of steganography with the same level of accuracy as that of a steganalysis method dedicated to that type of steganography, thereby demonstrating the general applicability of our approach to multiple types of stego images.steganalysis, steganography, deep learning, generalized steganalysis model"
319,"딥러닝 프레임워크의 비교: 티아노, 텐서플로, CNTK를 중심으로",정여진 /Yeojin Chung,2017,한국지능정보시스템학회,"딥러닝 프레임워크의 대표적인 기능으로는 ‘자동미분’과 ‘GPU의 활용’ 등을 들 수 있다. 본 논문은 파이썬의 라이브러리 형태로 사용 가능한 프레임워크 중에서 구글의 텐서플로와 마이크로소프트의 CNTK, 그리고 텐서 플로의 원조라고 할 수 있는 티아노를 비교하였다. 본문에서는 자동미분의 개념과 GPU의 활용형태를 간단히 설명하고, 그 다음에 logistic regression을 실행하는 예를 통하여 각 프레임워크의 문법을 알아본 뒤에, 마지막으로 대표적인 딥러닝 응용인 CNN의 예제를 실행시켜보고 코딩의 편의성과 실행속도 등을 확인해 보았다. 그 결과, 편의성의 관점에서 보면 티아노가 가장 코딩 하기가 어렵고, CNTK와 텐서플로는 많은 부분이 비슷하게 추상화 되어 있어서 코딩이 비슷하지만 가중치와 편향을 직접 정의하느냐의 여부에서 차이를 보였다. 그리고 각프레임워크의 실행속도에 대한 평가는 ‘큰 차이는 없다’는 것이다. 텐서플로는 티아노에 비하여 속도가 느리다는 평가가 있어왔는데, 본 연구의 실험에 의하면, 비록 CNN 모형에 국한되었지만, 텐서플로가 아주 조금이지만빠른 것으로 나타났다. CNTK의 경우에도, 비록 실험환경이 달랐지만, 실험환경의 차이에 의한 속도의 차이의 편차범위 이내에 있는 것으로 판단이 되었다. 본 연구에서는 세 종류의 딥러닝 프레임워크만을 살펴보았는데,위키피디아에 따르면 딥러닝 프레임워크의 종류는 12가지가 있으며, 각 프레임워크의 특징을 15가지 속성으로구분하여 차이를 특정하고 있다. 그 많은 속성 중에서 사용자의 입장에서 볼 때 중요한 속성은 어떤 언어(파이썬, C++, Java, 등)로 사용가능한지, 어떤 딥러닝 모형에 대한 라이브러리가 잘 구현되어 있는지 등일 것이다. 그리고 사용자가 대규모의 딥러닝 모형을 구축한다면, 다중 GPU 혹은 다중 서버를 지원하는지의 여부도 중요할 것이다. 또한 딥러닝 모형을 처음 학습하는 경우에는 사용설명서가 많은지 예제 프로그램이 많은지 여부도 중요한 기준이 될 것이다.The deep learning framework is software designed to help develop deep learning models. Some of its important functions include “automatic differentiation” and “utilization of GPU”. The list of popular deep learning framework includes Caffe (BVLC) and Theano (University of Montreal). And recently, Microsoft's deep learning framework, Microsoft Cognitive Toolkit, was released as open-source license, following Google’s Tensorflow a year earlier. The early deep learning frameworks have been developed mainly for research at universities. Beginning with the inception of Tensorflow, however, it seems that companies such as Microsoft and Facebook have started to join the competition of framework development. Given the trend, Google and other companies are expected to continue investing in the deep learning framework to bring forward the initiative in the artificial intelligence business. From this point of view, we think it is a good time to compare some of deep learning frameworks. So we compare three deep learning frameworks which can be used as a Python library. Those are Google's Tensorflow, Microsoft’s CNTK, and Theano which is sort of a predecessor of the preceding two.
The most common and important function of deep learning frameworks is the ability to perform automatic differentiation. Basically all the mathematical expressions of deep learning models can be represented as computational graphs, which consist of nodes and edges. Partial derivatives on each edge of a computational graph can then be obtained. With the partial derivatives, we can let software compute differentiation of any node with respect to any variable by utilizing chain rule of Calculus.
First of all, the convenience of coding is in the order of CNTK, Tensorflow, and Theano. The criterion is simply based on the lengths of the codes and the learning curve and the ease of coding are not the main concern. According to the criteria, Theano was the most difficult to implement with, and CNTK and Tensorflow were somewhat easier. With Tensorflow, we need to define weight variables and biases explicitly. The reason that CNTK and Tensorflow are easier to implement with is that those frameworks provide us with more abstraction than Theano. We, however, need to mention that low-level coding is not always bad. It gives us flexibility of coding. With the low-level coding such as in Theano, we can implement and test any new deep learning models or any new search methods that we can think of.
The assessment of the execution speed of each framework is that there is not meaningful difference. According to the experiment, execution speeds of Theano and Tensorflow are very similar, although the experiment was limited to a CNN model. In the case of CNTK, the experimental environment was not maintained as the same. The code written in CNTK has to be run in PC environment without GPU where codes execute as much as 50 times slower than with GPU. But we concluded that the difference of execution speed was within the range of variation caused by the different hardware setup.
In this study, we compared three types of deep learning framework: Theano, Tensorflow, and CNTK. According to Wikipedia, there are 12 available deep learning frameworks. And 15 different attributes differentiate each framework. Some of the important attributes would include interface language (Python, C ++, Java, etc.) and the availability of libraries on various deep learning models such as CNN, RNN, DBN, and etc. And if a user implements a large scale deep learning model, it will also be important to support multiple GPU or multiple servers. Also, if you are learning the deep learning model, it would also be important if there are enough examples and references.deep learning framework, Theano, TensorFlow, CNTK, computational graph, CIFAR-10"
320,딥러닝을 이용한 주변 무선단말 파악방안,이웅섭 /Lee， Woong-sup,2018,한국정보통신학회,"최근 단말-대-단말(Device-to-device, D2D) 통신기술이 차세대 무선통신시스템의 핵심기술로 큰 관심을 받고 있다. 이러한 단말간 통신에서는 자신의 주변에 어떠한 단말이 있는지 파악하는 주변단말 탐색(Neighbor discovery)이매우 중요하다. 본 논문에서는 최근 큰 관심을 받고 있는 딥러닝(Deep learning) 기술을 활용하여 단말간 통신에서 주변단말을 파악하는 방안에 대해서 제안한다. 제안 방안은 기존의 방안과 달리 무선채널의 공간적 연관성을 이용하여단말간의 신호 전송 없이 단말이 기지국으로 전송하는 상향링크 파일럿 신호를 기반으로 주변 단말을 찾고 따라서기존의 방식에 비해 신호전송 복잡도(signaling complexity)를 크게 줄일 수 있다. 또한 제안 방안에서는 떨어져 있는거리에 따라서 주변 단말을 분류 가능하여 기존 방안에 비해서 좀 더 세밀한 단말 탐색이 가능하다. 마지막으로 본 논문에서는 tensorflow를 이용한 컴퓨터 시뮬레이션을 통해 제안 방안의 성능을 검증하였다.Recently, the device-to-device (D2D) communication has been conceived as the key technology for the next-generationmobile communication systems. The neighbor discovery in which the nearby users are found, is essential for the properoperation of the D2D communication. In this paper, we propose new neighbor discovery scheme based on deep learningtechnology which has gained a lot of attention recently. In the proposed scheme, the neighboring users can be foundusing the uplink pilot transmission of users only, unlike conventional neighbor discovery schemes in which direct pilotcommunication among users is required, such that the signaling overhead can be greatly reduced in our proposed scheme.
Moreover, the neighbors with different proximity can also be classified accordingly which enables more accurate neighbordiscovery compared to the conventional schemes. The performance of our proposed scheme is verified through thetensorflow-based computer simulations.
키워드 : 딥러닝, 주변단말 탐색, 단말-대-단말 통신Deep learning, Neighbor discovery, Device-to-device communication, Wireless channel correlation"
321,신뢰성있는 딥러닝 기반 분석 모델을 참조하기 위한 딥러닝 기술 언어,문종혁 /Jong Hyeok Mun,2021,한국정보처리학회,"최근 딥러닝은 하드웨어 성능이 향상됨에 따라 자연어 처리, 영상 인식 등의 다양한 기술에 접목되어 활용되고 있다. 이러한 기술들을 활용해 지능형교통 시스템(ITS), 스마트홈, 헬스케어 등의 산업분야에서 데이터를 분석하여 고속도로 속도위반 차량 검출, 에너지 사용량 제어, 응급상황 등과 같은고품질의 서비스를 제공하며, 고품질의 서비스를 제공하기 위해서는 정확도가 향상된 딥러닝 모델이 적용되어야 한다. 이를 위해 서비스 환경의 데이터를분석하기 위한 딥러닝 모델을 개발할 때, 개발자는 신뢰성이 검증된 최신의 딥러닝 모델을 적용할 수 있어야 한다. 이는 개발자가 참조하는 딥러닝 모델에적용된 학습 데이터셋의 정확도를 측정하여 검증할 수 있다. 이러한 검증을 위해서 개발자는 학습 데이터셋, 딥러닝의 계층구조 및 개발 환경 등과 같은내용을 포함하는 딥러닝 모델을 문서화하여 적용하기 위한 구조적인 정보가 필요하다. 본 논문에서는 신뢰성있는 딥러닝 기반 데이터 분석 모델을 참조하기위한 딥러닝 기술 언어를 제안한다. 제안하는 기술 언어는 신뢰성 있는 딥러닝 모델을 개발하는데 필요한 학습데이터셋, 개발 환경 및 설정 등의 정보와더불어 딥러닝 모델의 계층구조를 표현할 수 있다. 제안하는 딥러닝 기술 언어를 이용하여 개발자는 지능형 교통 시스템에서 참조하는 분석 모델의정확도를 검증할 수 있다. 실험에서는 제안하는 언어의 유효성을 검증하기 위해, 번호판 인식 모델을 중심으로 딥러닝 기술 문서의 적용과정을 보인다With the recent advancements of deep learning, companies such as smart home, healthcare, and intelligent transportation systems areutilizing its functionality to provide high-quality services for vehicle detection, emergency situation detection, and controlling energyconsumption. To provide reliable services in such sensitive systems, deep learning models are required to have high accuracy. In orderto develop a deep learning model for analyzing previously mentioned services, developers should utilize the state of the art deep learningmodels that have already been verified for higher accuracy. The developers can verify the accuracy of the referenced model by validatingthe model on the dataset. For this validation, the developer needs structural information to document and apply deep learning models,including metadata such as learning dataset, network architecture, and development environments. In this paper, we propose a descriptionlanguage that represents the network architecture of the deep learning model along with its metadata that are necessary to develop adeep learning model. Through the proposed description language, developers can easily verify the accuracy of the referenced deep learningmodel. Our experiments demonstrate the application scenario of a deep learning description document that focuses on the license platerecognition for the detection of illegally parked vehicles.Trusted Deep Learning, Model Reference, Deep Learning Description Language, Traffic Situation Analysis Model"
322,딥러닝 분석을 이용한 중국 역내·외 위안화 변동성 예측,이우식 /Lee Woosik,2016,한국데이터정보과학회,"2008년 글로벌 금융위기 이후 중국은 위안화 국제화의 점진적 추진을 진행하면서 중국상하이 외환시장과 중국홍콩 외환시장에서 거래되는 통화인 역내위안화와 역외위안화를 형성시켰다. 본 연구는 위안화 국제화와 점진적인 중국 자본계정 개방에 따라 급변하는 외환시장상황의 변동성을 정확하게 파악하기 위해서 GARCH모형 (일반화된 자기회귀 조건부이분산성모형)에 다단계인공신경망을 결합한 MLP-GARCH 모형과 GARCH모형과 기계학습의 일종인 딥러닝 (deep learning)을 통합한 DL-GARCH을 가지고 위안화 변동성예측을 비교 실험과 분석을 하였다. 비교분석 결과 DLGARCH 모형은 MLP-GARCH보다 모형 위안화 역내·외 환율변동성 예측 면에서 더욱 더 개선된예측값을 제공하였다. 그래서 이분산시계열모형을 딥러닝과 결합한 DL-GARCH 모형은 시계열의환율 변동성 예측 문제에 딥러닝을 응용할 수 있음을 확인하였다. 향후 이분산시계열과 결합된 딥러닝 모형은 다른 금융시계열 데이터에 응용하여 그 일반화 가능성을 높일 수 있을 것이다.The People’s Republic of China has vigorously been pursuing the internationalization of the Chinese Yuan or Renminbi after the financial crisis of 2008. In this view, an abrupt increase of use of the Chinese Yuan in the onshore and offshore markets are important milestones to be one of important currencies. One of the most frequently used methods to forecast volatility is GARCH model. Since a prediction error of the GARCH model has been reported quite high, a lot of efforts have been made to improve forecasting capability of the GARCH model. In this paper, we have proposed MLPGARCH and a DL-GARCH by employing Artificial Neural Network to the GARCH. In an application to forecasting Chinese Yuan volatility, we have successfully shown their overall outperformance in forecasting over the GARCH.Deep learning, DL-GARCH, GARCH, The Chinese Yuan, volatility."
323,딥러닝 기법을 이용한 내일강수 예측,하지훈,2016,한국지능시스템학회,"정확한 강수예측을 위해서는 예측인자 선정과 예측방법에 대한 선택이 매우 중요하다. 최근에는 강수예측 방법으로기계학습 기법이 많이 사용되고 있으며, 그 중에서도 특히 인공신경망을 사용한 강수예측 방법은 좋은 성능을 보였다. 본논문에서는 딥러닝 기법 중 하나인 DBN(deep belief network)를 이용한 새로운 강수예측 방법을 제안한다. DBN는 비지도사전 학습을 통해 초기 가중치를 설정하여 기존 인공신경망의 문제점을 보완한다. 예측인자로는 기온, 전일-전주 강수일, 태양과 달 궤도 관련 자료를 선정하였다. 기온과 전일-전주 강수일은 서울에서의 1974년부터 2013년까지 총 40년간의AWS(automatic weather system) 관측 자료를 사용하였고, 태양과 달의 궤도 관련 자료는 서울을 중심으로 계산한 결과를사용하였다. 전체 기간에서 일부는 학습 자료로 사용하여 예측모델을 생성하였고, 나머지를 생성한 모델의 검증 자료로사용하였다. 모델 검증 결과로 나온 예측값들은 확률값을 가지며 임계치를 이용하여 강수유무를 판별하였다. 강수 정확도의척도로 양분예보기법 중 CSI(critical successive index)와 Bias(frequency bias)를 계산하였다. 이를 통해 DBN와 MLP(multilayer perceptron)의 성능을 비교한 결과 DBN의 강수 예측 정확도가 높았고, 수행속도 또한 2배 이상 빨랐다.For accurate precipitation forecasts the choice of weather faoctrs and prediction method is very important. Recently, machinele arning has been widely used for forecasting precipitation, and artificial neural network, one of machine learning techniques, showed good performance. In this paper, we suggest a new method for forecasting precipitation using DBN, one of deep learning techniques.
DBN has an advantage that initial weights are set by unsupervised learning, so this compensates for the defects of artificial neural networks. We used past precipitation, temperature, and the parameters of the sun and moon's motion as features for forecasting precipitation. The dataset consists of observation data which had been measured for 40 years from AWS in Seoul. Experiments were based on 8-fold cross validation. As a result of estimation, we got probabilities of test dataset, so threshold was used for the decision of precipitation. CSI and Bias were used for indicating the precision of precipitation. Our experimental results showed that DBN performed better than MLP.Deep learning, Deep belief network, Precipitation, Forecast"
324,딥러닝 신경망모형을 이용한 실시간 교통정보수집 연구,김대현 /KIM DAE HYON,2016,사단법인 인문사회과학기술융합학회,"비디오 기반의 실시간 교통정보수집 시스템은 자율주행을 포함한 지능형교통체계의 주요 서브시스템중 하나이며, 우수한 예측(추정)력 때문에 핵심 알고리즘으로 인공신경망 모형이 주로 사용되고 있다. 특히, 최근 과학 학술지 네이처에 구글 딥마인드의 알파고가 공개되면서 딥러닝과 인공지능에 대한 전 세계적인 관심이 집중되고 있다. 최근까지 가장 널리 사용된 인공신경망모형은 역전파 모형이었으나 최근 컴퓨터 하드웨어 및 알고리즘의 새로운 개발과 더불어 딥러닝 알고리즘을 이용한 딥인공신경망 모형에 대한 관심이 높아지고 있다. 그러나 아직 딥러닝 인공신경망에 대한 연구는 초기단계에 있는 실정이며 적용분야도 극히 한정되어 있다. 본 연구에서는 실시간 도로교통정보 수집을 위한 영상처리분야에서 딥러닝 알고리즘을 활용하고 실험결과를 통하여 향후 교통정보수집‧분석 분야에서의 딥러닝 인공신경망 모형의 적용가능성을 살펴보고자 한다. 특히, 본 연구에서 도출한 딥러닝 인공신경망모형의 실험결과를 기존 역전파 모형에 의한 실험결과와 비교하여 모형의 우수성을 비교‧평가하고자 한다.Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Normally, artificial neural networks have been used in real-time applications for collecting traffic information because of their remarkable performance in terms of predictive accuracy and real-time computing. Especially, the recent publication of a paper in the journal Nature and the success of Google DeepMind’s world-beating Go program AlphaGo caused significant increases of the public interest in artificial intelligence. Even though various types of learning models have been proposed, the Backpropagation has been the most popular neural network model. However, the Deep learning neural network model has attracted much interest recently. In this study, the Deep leaning neural network will be applied in image processing techniques for the real-time traffic information collection to investigate the performance of Deep learning algorithm in traffic engineering fields. In addition, the experimental results from Deep learing algorithm will be compared with the results from Backpropagation model to evaluate and propose better performing model.Intelligent Transportation Systems (ITS), artificial neural networks, Deep learning, traffic information, image processing"
325,딥러닝의 모형과 응용사례,안성만 /SungMahn Ahn,2016,한국지능정보시스템학회,"딥러닝은 인공신경망(neural network)이라는 인공지능분야의 모형이 발전된 형태로서, 계층구조로 이루어진 인공신경망의 내부계층(hidden layer)이 여러 단계로 이루어진 구조이다. 딥러닝에서의 주요 모형은 합성곱신경망(convolutional neural network), 순환신경망(recurrent neural network), 그리고 심층신뢰신경망(deep belief network)의 세가지라고 할 수있다. 그 중에서 현재 흥미로운 연구가 많이 발표되어서 관심이 집중되고 있는 모형은 지도학습(supervised learning)모형인 처음 두 개의 모형이다. 따라서 본 논문에서는 지도학습모형의 가중치를 최적화하는 기본적인 방법인 오류역전파 알고리즘을 살펴본 뒤에 합성곱신경망과 순환신경망의 구조와 응용사례 등을 살펴보고자 한다. 본문에서 다루지 않은 모형인 심층신뢰신경망은 아직까지는 합성곱신경망 이나 순환신경망보다는 상대적으로 주목을 덜 받고 있다. 그러나 심층신뢰신경망은 CNN이나 RNN과는 달리 비지도학습(unsupervised learning)모형이며, 사람이나 동물은 관찰을 통해서 스스로 학습한다는 점에서 궁극적으로는 비지도학습모형이 더 많이 연구되어야 할 주제가 될 것이다.Deep learning model is a kind of neural networks that allows multiple hidden layers. There are various deep learning architectures such as convolutional neural networks, deep belief networks and recurrent neural networks. Those have been applied to fields like computer vision, automatic speech recognition, natural language processing, audio recognition and bioinformatics where they have been shown to produce state-of-the-art results on various tasks. Among those architectures, convolutional neural networks and recurrent neural networks are classified as the supervised learning model. And in recent years, those supervised learning models have gained more popularity than unsupervised learning models such as deep belief networks, because supervised learning models have shown fashionable applications in such fields mentioned above.
Deep learning models can be trained with backpropagation algorithm. Backpropagation is an abbreviation for “backward propagation of errors” and a common method of training artificial neural networks used in conjunction with an optimization method such as gradient descent. The method calculates the gradient of an error function with respect to all the weights in the network. The gradient is fed to the optimization method which in turn uses it to update the weights, in an attempt to minimize the error function.
Convolutional neural networks use a special architecture which is particularly well-adapted to classify images. Using this architecture makes convolutional networks fast to train. This, in turn, helps us train deep, muti-layer networks, which are very good at classifying images. These days, deep convolutional networks are used in most neural networks for image recognition. Convolutional neural networks use three basic ideas: local receptive fields, shared weights, and pooling. By local receptive fields, we mean that each neuron in the first(or any) hidden layer will be connected to a small region of the input(or previous layer’s) neurons. Shared weights mean that we’re going to use the same weights and bias for each of the local receptive field. This means that all the neurons in the hidden layer detect exactly the same feature, just at different locations in the input image. In addition to the convolutional layers just described, convolutional neural networks also contain pooling layers. Pooling layers are usually used immediately after convolutional layers. What the pooling layers do is to simplify the information in the output from the convolutional layer.
Recent convolutional network architectures have 10 to 20 hidden layers and billions of connections between units. Training deep learning networks has taken weeks several years ago, but thanks to progress in GPU and algorithm enhancement, training time has reduced to several hours.
Neural networks with time-varying behavior are known as recurrent neural networks or RNNs. A recurrent neural network is a class of artificial neural network where connections between units form a directed cycle. This creates an internal state of the network which allows it to exhibit dynamic temporal behavior. Unlike feedforward neural networks, RNNs can use their internal memory to process arbitrary sequences of inputs. Early RNN models turned out to be very difficult to train, harder even than deep feedforward networks. The reason is the unstable gradient problem such as vanishing gradient and exploding gradient. The gradient can get smaller and smaller as it is propagated back through layers. This makes learning in early layers extremely slow. The problem actually gets worse in RNNs, since gradients aren’t just propagated backward through layers, they’re propagated backward through time. If the network runs for a long time, that can make the gradient extremely unstable and hard to learn from. It has been possible to incorporate an idea known as long short-term memory units (LSTMs) into RNNs. LSTMs make it much easier to get good results when training RNNs, and many recent papers make use of LSTMs or related ideas.Deep learning, Convolutional neural networks, Recurrent neural networks, Error backpropagation algorithm"
326,딥러닝분석과 기술적 분석 지표를 이용한 한국 코스피주가지수 방향성 예측,이우식 /Lee Woosik,2017,한국데이터정보과학회,"2016년 3월 구글 (Google)의 바둑인공지능 알파고 (AlphaGo)가 이세돌 9단과의 바둑대결에서승리한 이후 다양한 분야에서 인공지능 사용에 대한 관심이 높아지고 있는 가운데 금융투자 분야에서도 인공지능과 투자자문 전문가의 합성어인 로보어드바이저 (Robo-Advisor)에 대한 관심이 높아지고 있다. 인공지능 (artificial intelligence)기반의 의사결정은 비용 절감은 물론 효과적인 의사결정을가능하게 한다는 점에서 큰 장점이 있다. 본 연구에서는 기술적 분석 (technical analysis) 지표와 딥러닝 (deep learning) 모형을 결합하여 한국 코스피 지수를 예측하는 모형을 개발하고 제시한 모형들의 예측력을 비교, 분석한다. 분석 결과 기술적 분석 지표에 딥러닝 알고리즘을 결합한 모형이 주가지수 방향성 예측 문제에 응용될 수 있음을 확인하였다. 향후 본 연구에서 제안된 기술적 분석 지표와딥러닝모형을 결합한 기법은 로보어드바이저서비스에 응용할 수 있는 일반화 가능성을 보여준다Since Google’s AlphaGo defeated a world champion of Go players in 2016, there have been many interests in the deep learning. In the financial sector, a Robo-Advisor using deep learning gains a significant attention, which builds and manages portfolios of financial instruments for investors.In this paper, we have proposed the a deep learning algorithm geared toward identification and forecast of the KOSPI index direction,and we also have compared the accuracy of the prediction.In an application of forecasting the financial market index direction, we have shown that the Robo-Advisor using deep learning has a significant effect on finance industry. The Robo-Advisor collects a massive data such as earnings statements, news reports and regulatory filings, analyzes those and recommends investors how to view market trends and identify the best time to purchase financial assets. On the other hand, the Robo-Advisor allows businesses to learn more about their customers, develop better marketing strategies, increase sales and decrease costs.Artificial intelligence, deep learning, FinTech, Robo-Advisor, technical analysis."
327,딥러닝의 효율적인 학습을 위한 학습데이터 선별 기법에 관한 연구,주경돈 /Gyoung-don Joo,2017,한국정보과학회,"빅데이터의 증가와 병렬 하드웨어 처리, 학습 알고리즘의 개선은 딥러닝의 발전을 가져왔으며 인공지능이 적용 가능한 영역을 획기적으로 확장시키고 있다. 하지만 이러한 대규모의 학습 데이터에 기반한 딥러닝은 학습시간이 증가되고, 더 나아가 학습 데이터의 구축을 위한 레이블링(labeling) 비용이 데이터 수에 비례하여 증가하게 된다. 따라서 본 논문에서는, 딥러닝의 정확도를 최대한 유지하면서 필요한 학습 데이터의 수를 줄일수 있도록, 전체 데이터에서 제한된 수의 학습 데이터의 후보를 선별하는 기법에 대하여 연구한다. 특히 기존의 능동 학습에 기반한 기법들이 학습 데이터의 수가 매우 적어질 경우 딥러닝의 정확도가 급격히 떨어지는 문제를 해결할 수 있는 새로운 기법을 제안하였으며, 제안된 기법으로 선별된 학습 데이터는 그 수의 변화에 대해 딥러닝이 일관되게 경쟁력있는 정확도를 가짐을 다양한 실험을 통하여 검증하였다.The increase of big data, parallel hardware processing and enhanced learning algorithms have led to the development of deep learning and are dramatically expanding the domain of applications based on artificial intelligence. However, such large-scale training data extends the learning time, and furthermore, the labeling cost for building the training set increases in proportion to the volume of data. Therefore, in this paper, we study how to select a limited number of candidates for training set from given data so as to reduce the number of training set while maintaining the accuracy of deep learning. Especially, we found that, with the training set selected by the previous work, the accuracy of deep learning drastically decreases when the number of training set is very small and so proposed new methods to resolve this fall-off in accuracy. Through experiments, we showed that deep learning consistently has the competitive accuracy with the training set selected by the proposed method regardless of the size of training set.Deep Learning, Train Set Selection, Active Learning, Gamma distribution"
328,딥러닝을 이용한 범용적 스테그아날리시스,김현재 /Hyunjae Kim,2017,한국정보과학회,"스테그아날리시스(Steganalysis)란 이미지 등 일반적인 자료에 암호화된 정보를 은닉하는 스테가노그래피(Steganography)에 대한 검출 및 분석 방법으로, 기계학습 기반 방법론을 포함한다. 기존 기계학습 기반 스테그아날리시스는 영상(Image)의 특징(Feature) 추출 및 모델링에 기반하며, 최근 딥러닝(Deep Learning)의 적용으로 검출 정확도가 큰 폭으로 향상되었다. 하지만 현존하는 스테그아날리시스 모델은 단일 스테가노그래피 기법에 대해 국한되어 있어 학습에 사용되지 않은 스테고(Stego) 이미지의 경우 검출이 불가능한 결정적 한계를 가진다. 본 연구에서는 다양한 스테가노그래피 기법으로 생성된 스테고이미지에 딥러닝을 적용하여 스테그아날리시스를 학습하는 범용적 모델을 제안한다. 다양한 실험을 통해 제안 기법의 효용성 및 가능성을 확인하고, 범용적 스테그아날리시스 모델이 각각에 특화된 검출 기법과 유사한 정확도로 스테고 이미지를 검출할 수 있음을 보인다.Steganalysis is to detect information hidden by steganography inside general data such as images. There are stegoanalysis techniques that use machine learning (ML). Existing ML approaches to steganalysis are based on extracting features from stego images and modeling them.
Recently deep learning-based methodologies have shown significant improvements in detection accuracy. However, all the existing methods, including deep learning-based ones, have a critical limitation in that they can only detect stego images that are created by a specific steganography method. In this paper, we propose a generalized steganalysis method that can model multiple types of stego images using deep learning. Through various experiments, we confirm the effectiveness of our approach and envision directions for future research. In particular, we show that our method can detect each type of steganography with the same level of accuracy as that of a steganalysis method dedicated to that type of steganography, thereby demonstrating the general applicability of our approach to multiple types of stego images.steganalysis, steganography, deep learning, generalized steganalysis model"
329,"딥러닝 프레임워크의 비교: 티아노, 텐서플로, CNTK를 중심으로",정여진 /Yeojin Chung,2017,한국지능정보시스템학회,"딥러닝 프레임워크의 대표적인 기능으로는 ‘자동미분’과 ‘GPU의 활용’ 등을 들 수 있다. 본 논문은 파이썬의 라이브러리 형태로 사용 가능한 프레임워크 중에서 구글의 텐서플로와 마이크로소프트의 CNTK, 그리고 텐서 플로의 원조라고 할 수 있는 티아노를 비교하였다. 본문에서는 자동미분의 개념과 GPU의 활용형태를 간단히 설명하고, 그 다음에 logistic regression을 실행하는 예를 통하여 각 프레임워크의 문법을 알아본 뒤에, 마지막으로 대표적인 딥러닝 응용인 CNN의 예제를 실행시켜보고 코딩의 편의성과 실행속도 등을 확인해 보았다. 그 결과, 편의성의 관점에서 보면 티아노가 가장 코딩 하기가 어렵고, CNTK와 텐서플로는 많은 부분이 비슷하게 추상화 되어 있어서 코딩이 비슷하지만 가중치와 편향을 직접 정의하느냐의 여부에서 차이를 보였다. 그리고 각프레임워크의 실행속도에 대한 평가는 ‘큰 차이는 없다’는 것이다. 텐서플로는 티아노에 비하여 속도가 느리다는 평가가 있어왔는데, 본 연구의 실험에 의하면, 비록 CNN 모형에 국한되었지만, 텐서플로가 아주 조금이지만빠른 것으로 나타났다. CNTK의 경우에도, 비록 실험환경이 달랐지만, 실험환경의 차이에 의한 속도의 차이의 편차범위 이내에 있는 것으로 판단이 되었다. 본 연구에서는 세 종류의 딥러닝 프레임워크만을 살펴보았는데,위키피디아에 따르면 딥러닝 프레임워크의 종류는 12가지가 있으며, 각 프레임워크의 특징을 15가지 속성으로구분하여 차이를 특정하고 있다. 그 많은 속성 중에서 사용자의 입장에서 볼 때 중요한 속성은 어떤 언어(파이썬, C++, Java, 등)로 사용가능한지, 어떤 딥러닝 모형에 대한 라이브러리가 잘 구현되어 있는지 등일 것이다. 그리고 사용자가 대규모의 딥러닝 모형을 구축한다면, 다중 GPU 혹은 다중 서버를 지원하는지의 여부도 중요할 것이다. 또한 딥러닝 모형을 처음 학습하는 경우에는 사용설명서가 많은지 예제 프로그램이 많은지 여부도 중요한 기준이 될 것이다.The deep learning framework is software designed to help develop deep learning models. Some of its important functions include “automatic differentiation” and “utilization of GPU”. The list of popular deep learning framework includes Caffe (BVLC) and Theano (University of Montreal). And recently, Microsoft's deep learning framework, Microsoft Cognitive Toolkit, was released as open-source license, following Google’s Tensorflow a year earlier. The early deep learning frameworks have been developed mainly for research at universities. Beginning with the inception of Tensorflow, however, it seems that companies such as Microsoft and Facebook have started to join the competition of framework development. Given the trend, Google and other companies are expected to continue investing in the deep learning framework to bring forward the initiative in the artificial intelligence business. From this point of view, we think it is a good time to compare some of deep learning frameworks. So we compare three deep learning frameworks which can be used as a Python library. Those are Google's Tensorflow, Microsoft’s CNTK, and Theano which is sort of a predecessor of the preceding two.
The most common and important function of deep learning frameworks is the ability to perform automatic differentiation. Basically all the mathematical expressions of deep learning models can be represented as computational graphs, which consist of nodes and edges. Partial derivatives on each edge of a computational graph can then be obtained. With the partial derivatives, we can let software compute differentiation of any node with respect to any variable by utilizing chain rule of Calculus.
First of all, the convenience of coding is in the order of CNTK, Tensorflow, and Theano. The criterion is simply based on the lengths of the codes and the learning curve and the ease of coding are not the main concern. According to the criteria, Theano was the most difficult to implement with, and CNTK and Tensorflow were somewhat easier. With Tensorflow, we need to define weight variables and biases explicitly. The reason that CNTK and Tensorflow are easier to implement with is that those frameworks provide us with more abstraction than Theano. We, however, need to mention that low-level coding is not always bad. It gives us flexibility of coding. With the low-level coding such as in Theano, we can implement and test any new deep learning models or any new search methods that we can think of.
The assessment of the execution speed of each framework is that there is not meaningful difference. According to the experiment, execution speeds of Theano and Tensorflow are very similar, although the experiment was limited to a CNN model. In the case of CNTK, the experimental environment was not maintained as the same. The code written in CNTK has to be run in PC environment without GPU where codes execute as much as 50 times slower than with GPU. But we concluded that the difference of execution speed was within the range of variation caused by the different hardware setup.
In this study, we compared three types of deep learning framework: Theano, Tensorflow, and CNTK. According to Wikipedia, there are 12 available deep learning frameworks. And 15 different attributes differentiate each framework. Some of the important attributes would include interface language (Python, C ++, Java, etc.) and the availability of libraries on various deep learning models such as CNN, RNN, DBN, and etc. And if a user implements a large scale deep learning model, it will also be important to support multiple GPU or multiple servers. Also, if you are learning the deep learning model, it would also be important if there are enough examples and references.deep learning framework, Theano, TensorFlow, CNTK, computational graph, CIFAR-10"
330,딥러닝을 이용한 주변 무선단말 파악방안,이웅섭 /Lee， Woong-sup,2018,한국정보통신학회,"최근 단말-대-단말(Device-to-device, D2D) 통신기술이 차세대 무선통신시스템의 핵심기술로 큰 관심을 받고 있다. 이러한 단말간 통신에서는 자신의 주변에 어떠한 단말이 있는지 파악하는 주변단말 탐색(Neighbor discovery)이매우 중요하다. 본 논문에서는 최근 큰 관심을 받고 있는 딥러닝(Deep learning) 기술을 활용하여 단말간 통신에서 주변단말을 파악하는 방안에 대해서 제안한다. 제안 방안은 기존의 방안과 달리 무선채널의 공간적 연관성을 이용하여단말간의 신호 전송 없이 단말이 기지국으로 전송하는 상향링크 파일럿 신호를 기반으로 주변 단말을 찾고 따라서기존의 방식에 비해 신호전송 복잡도(signaling complexity)를 크게 줄일 수 있다. 또한 제안 방안에서는 떨어져 있는거리에 따라서 주변 단말을 분류 가능하여 기존 방안에 비해서 좀 더 세밀한 단말 탐색이 가능하다. 마지막으로 본 논문에서는 tensorflow를 이용한 컴퓨터 시뮬레이션을 통해 제안 방안의 성능을 검증하였다.Recently, the device-to-device (D2D) communication has been conceived as the key technology for the next-generationmobile communication systems. The neighbor discovery in which the nearby users are found, is essential for the properoperation of the D2D communication. In this paper, we propose new neighbor discovery scheme based on deep learningtechnology which has gained a lot of attention recently. In the proposed scheme, the neighboring users can be foundusing the uplink pilot transmission of users only, unlike conventional neighbor discovery schemes in which direct pilotcommunication among users is required, such that the signaling overhead can be greatly reduced in our proposed scheme.
Moreover, the neighbors with different proximity can also be classified accordingly which enables more accurate neighbordiscovery compared to the conventional schemes. The performance of our proposed scheme is verified through thetensorflow-based computer simulations.
키워드 : 딥러닝, 주변단말 탐색, 단말-대-단말 통신Deep learning, Neighbor discovery, Device-to-device communication, Wireless channel correlation"
331,신뢰성있는 딥러닝 기반 분석 모델을 참조하기 위한 딥러닝 기술 언어,문종혁 /Jong Hyeok Mun,2021,한국정보처리학회,"최근 딥러닝은 하드웨어 성능이 향상됨에 따라 자연어 처리, 영상 인식 등의 다양한 기술에 접목되어 활용되고 있다. 이러한 기술들을 활용해 지능형교통 시스템(ITS), 스마트홈, 헬스케어 등의 산업분야에서 데이터를 분석하여 고속도로 속도위반 차량 검출, 에너지 사용량 제어, 응급상황 등과 같은고품질의 서비스를 제공하며, 고품질의 서비스를 제공하기 위해서는 정확도가 향상된 딥러닝 모델이 적용되어야 한다. 이를 위해 서비스 환경의 데이터를분석하기 위한 딥러닝 모델을 개발할 때, 개발자는 신뢰성이 검증된 최신의 딥러닝 모델을 적용할 수 있어야 한다. 이는 개발자가 참조하는 딥러닝 모델에적용된 학습 데이터셋의 정확도를 측정하여 검증할 수 있다. 이러한 검증을 위해서 개발자는 학습 데이터셋, 딥러닝의 계층구조 및 개발 환경 등과 같은내용을 포함하는 딥러닝 모델을 문서화하여 적용하기 위한 구조적인 정보가 필요하다. 본 논문에서는 신뢰성있는 딥러닝 기반 데이터 분석 모델을 참조하기위한 딥러닝 기술 언어를 제안한다. 제안하는 기술 언어는 신뢰성 있는 딥러닝 모델을 개발하는데 필요한 학습데이터셋, 개발 환경 및 설정 등의 정보와더불어 딥러닝 모델의 계층구조를 표현할 수 있다. 제안하는 딥러닝 기술 언어를 이용하여 개발자는 지능형 교통 시스템에서 참조하는 분석 모델의정확도를 검증할 수 있다. 실험에서는 제안하는 언어의 유효성을 검증하기 위해, 번호판 인식 모델을 중심으로 딥러닝 기술 문서의 적용과정을 보인다With the recent advancements of deep learning, companies such as smart home, healthcare, and intelligent transportation systems areutilizing its functionality to provide high-quality services for vehicle detection, emergency situation detection, and controlling energyconsumption. To provide reliable services in such sensitive systems, deep learning models are required to have high accuracy. In orderto develop a deep learning model for analyzing previously mentioned services, developers should utilize the state of the art deep learningmodels that have already been verified for higher accuracy. The developers can verify the accuracy of the referenced model by validatingthe model on the dataset. For this validation, the developer needs structural information to document and apply deep learning models,including metadata such as learning dataset, network architecture, and development environments. In this paper, we propose a descriptionlanguage that represents the network architecture of the deep learning model along with its metadata that are necessary to develop adeep learning model. Through the proposed description language, developers can easily verify the accuracy of the referenced deep learningmodel. Our experiments demonstrate the application scenario of a deep learning description document that focuses on the license platerecognition for the detection of illegally parked vehicles.Trusted Deep Learning, Model Reference, Deep Learning Description Language, Traffic Situation Analysis Model"
332,딥러닝 분석을 이용한 중국 역내·외 위안화 변동성 예측,이우식 /Lee Woosik,2016,한국데이터정보과학회,"2008년 글로벌 금융위기 이후 중국은 위안화 국제화의 점진적 추진을 진행하면서 중국상하이 외환시장과 중국홍콩 외환시장에서 거래되는 통화인 역내위안화와 역외위안화를 형성시켰다. 본 연구는 위안화 국제화와 점진적인 중국 자본계정 개방에 따라 급변하는 외환시장상황의 변동성을 정확하게 파악하기 위해서 GARCH모형 (일반화된 자기회귀 조건부이분산성모형)에 다단계인공신경망을 결합한 MLP-GARCH 모형과 GARCH모형과 기계학습의 일종인 딥러닝 (deep learning)을 통합한 DL-GARCH을 가지고 위안화 변동성예측을 비교 실험과 분석을 하였다. 비교분석 결과 DLGARCH 모형은 MLP-GARCH보다 모형 위안화 역내·외 환율변동성 예측 면에서 더욱 더 개선된예측값을 제공하였다. 그래서 이분산시계열모형을 딥러닝과 결합한 DL-GARCH 모형은 시계열의환율 변동성 예측 문제에 딥러닝을 응용할 수 있음을 확인하였다. 향후 이분산시계열과 결합된 딥러닝 모형은 다른 금융시계열 데이터에 응용하여 그 일반화 가능성을 높일 수 있을 것이다.The People’s Republic of China has vigorously been pursuing the internationalization of the Chinese Yuan or Renminbi after the financial crisis of 2008. In this view, an abrupt increase of use of the Chinese Yuan in the onshore and offshore markets are important milestones to be one of important currencies. One of the most frequently used methods to forecast volatility is GARCH model. Since a prediction error of the GARCH model has been reported quite high, a lot of efforts have been made to improve forecasting capability of the GARCH model. In this paper, we have proposed MLPGARCH and a DL-GARCH by employing Artificial Neural Network to the GARCH. In an application to forecasting Chinese Yuan volatility, we have successfully shown their overall outperformance in forecasting over the GARCH.Deep learning, DL-GARCH, GARCH, The Chinese Yuan, volatility."
333,딥러닝 기법을 이용한 내일강수 예측,하지훈,2016,한국지능시스템학회,"정확한 강수예측을 위해서는 예측인자 선정과 예측방법에 대한 선택이 매우 중요하다. 최근에는 강수예측 방법으로기계학습 기법이 많이 사용되고 있으며, 그 중에서도 특히 인공신경망을 사용한 강수예측 방법은 좋은 성능을 보였다. 본논문에서는 딥러닝 기법 중 하나인 DBN(deep belief network)를 이용한 새로운 강수예측 방법을 제안한다. DBN는 비지도사전 학습을 통해 초기 가중치를 설정하여 기존 인공신경망의 문제점을 보완한다. 예측인자로는 기온, 전일-전주 강수일, 태양과 달 궤도 관련 자료를 선정하였다. 기온과 전일-전주 강수일은 서울에서의 1974년부터 2013년까지 총 40년간의AWS(automatic weather system) 관측 자료를 사용하였고, 태양과 달의 궤도 관련 자료는 서울을 중심으로 계산한 결과를사용하였다. 전체 기간에서 일부는 학습 자료로 사용하여 예측모델을 생성하였고, 나머지를 생성한 모델의 검증 자료로사용하였다. 모델 검증 결과로 나온 예측값들은 확률값을 가지며 임계치를 이용하여 강수유무를 판별하였다. 강수 정확도의척도로 양분예보기법 중 CSI(critical successive index)와 Bias(frequency bias)를 계산하였다. 이를 통해 DBN와 MLP(multilayer perceptron)의 성능을 비교한 결과 DBN의 강수 예측 정확도가 높았고, 수행속도 또한 2배 이상 빨랐다.For accurate precipitation forecasts the choice of weather faoctrs and prediction method is very important. Recently, machinele arning has been widely used for forecasting precipitation, and artificial neural network, one of machine learning techniques, showed good performance. In this paper, we suggest a new method for forecasting precipitation using DBN, one of deep learning techniques.
DBN has an advantage that initial weights are set by unsupervised learning, so this compensates for the defects of artificial neural networks. We used past precipitation, temperature, and the parameters of the sun and moon's motion as features for forecasting precipitation. The dataset consists of observation data which had been measured for 40 years from AWS in Seoul. Experiments were based on 8-fold cross validation. As a result of estimation, we got probabilities of test dataset, so threshold was used for the decision of precipitation. CSI and Bias were used for indicating the precision of precipitation. Our experimental results showed that DBN performed better than MLP.Deep learning, Deep belief network, Precipitation, Forecast"
334,딥러닝 신경망모형을 이용한 실시간 교통정보수집 연구,김대현 /KIM DAE HYON,2016,사단법인 인문사회과학기술융합학회,"비디오 기반의 실시간 교통정보수집 시스템은 자율주행을 포함한 지능형교통체계의 주요 서브시스템중 하나이며, 우수한 예측(추정)력 때문에 핵심 알고리즘으로 인공신경망 모형이 주로 사용되고 있다. 특히, 최근 과학 학술지 네이처에 구글 딥마인드의 알파고가 공개되면서 딥러닝과 인공지능에 대한 전 세계적인 관심이 집중되고 있다. 최근까지 가장 널리 사용된 인공신경망모형은 역전파 모형이었으나 최근 컴퓨터 하드웨어 및 알고리즘의 새로운 개발과 더불어 딥러닝 알고리즘을 이용한 딥인공신경망 모형에 대한 관심이 높아지고 있다. 그러나 아직 딥러닝 인공신경망에 대한 연구는 초기단계에 있는 실정이며 적용분야도 극히 한정되어 있다. 본 연구에서는 실시간 도로교통정보 수집을 위한 영상처리분야에서 딥러닝 알고리즘을 활용하고 실험결과를 통하여 향후 교통정보수집‧분석 분야에서의 딥러닝 인공신경망 모형의 적용가능성을 살펴보고자 한다. 특히, 본 연구에서 도출한 딥러닝 인공신경망모형의 실험결과를 기존 역전파 모형에 의한 실험결과와 비교하여 모형의 우수성을 비교‧평가하고자 한다.Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Normally, artificial neural networks have been used in real-time applications for collecting traffic information because of their remarkable performance in terms of predictive accuracy and real-time computing. Especially, the recent publication of a paper in the journal Nature and the success of Google DeepMind’s world-beating Go program AlphaGo caused significant increases of the public interest in artificial intelligence. Even though various types of learning models have been proposed, the Backpropagation has been the most popular neural network model. However, the Deep learning neural network model has attracted much interest recently. In this study, the Deep leaning neural network will be applied in image processing techniques for the real-time traffic information collection to investigate the performance of Deep learning algorithm in traffic engineering fields. In addition, the experimental results from Deep learing algorithm will be compared with the results from Backpropagation model to evaluate and propose better performing model.Intelligent Transportation Systems (ITS), artificial neural networks, Deep learning, traffic information, image processing"
335,딥러닝의 모형과 응용사례,안성만 /SungMahn Ahn,2016,한국지능정보시스템학회,"딥러닝은 인공신경망(neural network)이라는 인공지능분야의 모형이 발전된 형태로서, 계층구조로 이루어진 인공신경망의 내부계층(hidden layer)이 여러 단계로 이루어진 구조이다. 딥러닝에서의 주요 모형은 합성곱신경망(convolutional neural network), 순환신경망(recurrent neural network), 그리고 심층신뢰신경망(deep belief network)의 세가지라고 할 수있다. 그 중에서 현재 흥미로운 연구가 많이 발표되어서 관심이 집중되고 있는 모형은 지도학습(supervised learning)모형인 처음 두 개의 모형이다. 따라서 본 논문에서는 지도학습모형의 가중치를 최적화하는 기본적인 방법인 오류역전파 알고리즘을 살펴본 뒤에 합성곱신경망과 순환신경망의 구조와 응용사례 등을 살펴보고자 한다. 본문에서 다루지 않은 모형인 심층신뢰신경망은 아직까지는 합성곱신경망 이나 순환신경망보다는 상대적으로 주목을 덜 받고 있다. 그러나 심층신뢰신경망은 CNN이나 RNN과는 달리 비지도학습(unsupervised learning)모형이며, 사람이나 동물은 관찰을 통해서 스스로 학습한다는 점에서 궁극적으로는 비지도학습모형이 더 많이 연구되어야 할 주제가 될 것이다.Deep learning model is a kind of neural networks that allows multiple hidden layers. There are various deep learning architectures such as convolutional neural networks, deep belief networks and recurrent neural networks. Those have been applied to fields like computer vision, automatic speech recognition, natural language processing, audio recognition and bioinformatics where they have been shown to produce state-of-the-art results on various tasks. Among those architectures, convolutional neural networks and recurrent neural networks are classified as the supervised learning model. And in recent years, those supervised learning models have gained more popularity than unsupervised learning models such as deep belief networks, because supervised learning models have shown fashionable applications in such fields mentioned above.
Deep learning models can be trained with backpropagation algorithm. Backpropagation is an abbreviation for “backward propagation of errors” and a common method of training artificial neural networks used in conjunction with an optimization method such as gradient descent. The method calculates the gradient of an error function with respect to all the weights in the network. The gradient is fed to the optimization method which in turn uses it to update the weights, in an attempt to minimize the error function.
Convolutional neural networks use a special architecture which is particularly well-adapted to classify images. Using this architecture makes convolutional networks fast to train. This, in turn, helps us train deep, muti-layer networks, which are very good at classifying images. These days, deep convolutional networks are used in most neural networks for image recognition. Convolutional neural networks use three basic ideas: local receptive fields, shared weights, and pooling. By local receptive fields, we mean that each neuron in the first(or any) hidden layer will be connected to a small region of the input(or previous layer’s) neurons. Shared weights mean that we’re going to use the same weights and bias for each of the local receptive field. This means that all the neurons in the hidden layer detect exactly the same feature, just at different locations in the input image. In addition to the convolutional layers just described, convolutional neural networks also contain pooling layers. Pooling layers are usually used immediately after convolutional layers. What the pooling layers do is to simplify the information in the output from the convolutional layer.
Recent convolutional network architectures have 10 to 20 hidden layers and billions of connections between units. Training deep learning networks has taken weeks several years ago, but thanks to progress in GPU and algorithm enhancement, training time has reduced to several hours.
Neural networks with time-varying behavior are known as recurrent neural networks or RNNs. A recurrent neural network is a class of artificial neural network where connections between units form a directed cycle. This creates an internal state of the network which allows it to exhibit dynamic temporal behavior. Unlike feedforward neural networks, RNNs can use their internal memory to process arbitrary sequences of inputs. Early RNN models turned out to be very difficult to train, harder even than deep feedforward networks. The reason is the unstable gradient problem such as vanishing gradient and exploding gradient. The gradient can get smaller and smaller as it is propagated back through layers. This makes learning in early layers extremely slow. The problem actually gets worse in RNNs, since gradients aren’t just propagated backward through layers, they’re propagated backward through time. If the network runs for a long time, that can make the gradient extremely unstable and hard to learn from. It has been possible to incorporate an idea known as long short-term memory units (LSTMs) into RNNs. LSTMs make it much easier to get good results when training RNNs, and many recent papers make use of LSTMs or related ideas.Deep learning, Convolutional neural networks, Recurrent neural networks, Error backpropagation algorithm"
336,딥러닝분석과 기술적 분석 지표를 이용한 한국 코스피주가지수 방향성 예측,이우식 /Lee Woosik,2017,한국데이터정보과학회,"2016년 3월 구글 (Google)의 바둑인공지능 알파고 (AlphaGo)가 이세돌 9단과의 바둑대결에서승리한 이후 다양한 분야에서 인공지능 사용에 대한 관심이 높아지고 있는 가운데 금융투자 분야에서도 인공지능과 투자자문 전문가의 합성어인 로보어드바이저 (Robo-Advisor)에 대한 관심이 높아지고 있다. 인공지능 (artificial intelligence)기반의 의사결정은 비용 절감은 물론 효과적인 의사결정을가능하게 한다는 점에서 큰 장점이 있다. 본 연구에서는 기술적 분석 (technical analysis) 지표와 딥러닝 (deep learning) 모형을 결합하여 한국 코스피 지수를 예측하는 모형을 개발하고 제시한 모형들의 예측력을 비교, 분석한다. 분석 결과 기술적 분석 지표에 딥러닝 알고리즘을 결합한 모형이 주가지수 방향성 예측 문제에 응용될 수 있음을 확인하였다. 향후 본 연구에서 제안된 기술적 분석 지표와딥러닝모형을 결합한 기법은 로보어드바이저서비스에 응용할 수 있는 일반화 가능성을 보여준다Since Google’s AlphaGo defeated a world champion of Go players in 2016, there have been many interests in the deep learning. In the financial sector, a Robo-Advisor using deep learning gains a significant attention, which builds and manages portfolios of financial instruments for investors.In this paper, we have proposed the a deep learning algorithm geared toward identification and forecast of the KOSPI index direction,and we also have compared the accuracy of the prediction.In an application of forecasting the financial market index direction, we have shown that the Robo-Advisor using deep learning has a significant effect on finance industry. The Robo-Advisor collects a massive data such as earnings statements, news reports and regulatory filings, analyzes those and recommends investors how to view market trends and identify the best time to purchase financial assets. On the other hand, the Robo-Advisor allows businesses to learn more about their customers, develop better marketing strategies, increase sales and decrease costs.Artificial intelligence, deep learning, FinTech, Robo-Advisor, technical analysis."
337,딥러닝의 효율적인 학습을 위한 학습데이터 선별 기법에 관한 연구,주경돈 /Gyoung-don Joo,2017,한국정보과학회,"빅데이터의 증가와 병렬 하드웨어 처리, 학습 알고리즘의 개선은 딥러닝의 발전을 가져왔으며 인공지능이 적용 가능한 영역을 획기적으로 확장시키고 있다. 하지만 이러한 대규모의 학습 데이터에 기반한 딥러닝은 학습시간이 증가되고, 더 나아가 학습 데이터의 구축을 위한 레이블링(labeling) 비용이 데이터 수에 비례하여 증가하게 된다. 따라서 본 논문에서는, 딥러닝의 정확도를 최대한 유지하면서 필요한 학습 데이터의 수를 줄일수 있도록, 전체 데이터에서 제한된 수의 학습 데이터의 후보를 선별하는 기법에 대하여 연구한다. 특히 기존의 능동 학습에 기반한 기법들이 학습 데이터의 수가 매우 적어질 경우 딥러닝의 정확도가 급격히 떨어지는 문제를 해결할 수 있는 새로운 기법을 제안하였으며, 제안된 기법으로 선별된 학습 데이터는 그 수의 변화에 대해 딥러닝이 일관되게 경쟁력있는 정확도를 가짐을 다양한 실험을 통하여 검증하였다.The increase of big data, parallel hardware processing and enhanced learning algorithms have led to the development of deep learning and are dramatically expanding the domain of applications based on artificial intelligence. However, such large-scale training data extends the learning time, and furthermore, the labeling cost for building the training set increases in proportion to the volume of data. Therefore, in this paper, we study how to select a limited number of candidates for training set from given data so as to reduce the number of training set while maintaining the accuracy of deep learning. Especially, we found that, with the training set selected by the previous work, the accuracy of deep learning drastically decreases when the number of training set is very small and so proposed new methods to resolve this fall-off in accuracy. Through experiments, we showed that deep learning consistently has the competitive accuracy with the training set selected by the proposed method regardless of the size of training set.Deep Learning, Train Set Selection, Active Learning, Gamma distribution"
338,딥러닝을 이용한 범용적 스테그아날리시스,김현재 /Hyunjae Kim,2017,한국정보과학회,"스테그아날리시스(Steganalysis)란 이미지 등 일반적인 자료에 암호화된 정보를 은닉하는 스테가노그래피(Steganography)에 대한 검출 및 분석 방법으로, 기계학습 기반 방법론을 포함한다. 기존 기계학습 기반 스테그아날리시스는 영상(Image)의 특징(Feature) 추출 및 모델링에 기반하며, 최근 딥러닝(Deep Learning)의 적용으로 검출 정확도가 큰 폭으로 향상되었다. 하지만 현존하는 스테그아날리시스 모델은 단일 스테가노그래피 기법에 대해 국한되어 있어 학습에 사용되지 않은 스테고(Stego) 이미지의 경우 검출이 불가능한 결정적 한계를 가진다. 본 연구에서는 다양한 스테가노그래피 기법으로 생성된 스테고이미지에 딥러닝을 적용하여 스테그아날리시스를 학습하는 범용적 모델을 제안한다. 다양한 실험을 통해 제안 기법의 효용성 및 가능성을 확인하고, 범용적 스테그아날리시스 모델이 각각에 특화된 검출 기법과 유사한 정확도로 스테고 이미지를 검출할 수 있음을 보인다.Steganalysis is to detect information hidden by steganography inside general data such as images. There are stegoanalysis techniques that use machine learning (ML). Existing ML approaches to steganalysis are based on extracting features from stego images and modeling them.
Recently deep learning-based methodologies have shown significant improvements in detection accuracy. However, all the existing methods, including deep learning-based ones, have a critical limitation in that they can only detect stego images that are created by a specific steganography method. In this paper, we propose a generalized steganalysis method that can model multiple types of stego images using deep learning. Through various experiments, we confirm the effectiveness of our approach and envision directions for future research. In particular, we show that our method can detect each type of steganography with the same level of accuracy as that of a steganalysis method dedicated to that type of steganography, thereby demonstrating the general applicability of our approach to multiple types of stego images.steganalysis, steganography, deep learning, generalized steganalysis model"
339,"딥러닝 프레임워크의 비교: 티아노, 텐서플로, CNTK를 중심으로",정여진 /Yeojin Chung,2017,한국지능정보시스템학회,"딥러닝 프레임워크의 대표적인 기능으로는 ‘자동미분’과 ‘GPU의 활용’ 등을 들 수 있다. 본 논문은 파이썬의 라이브러리 형태로 사용 가능한 프레임워크 중에서 구글의 텐서플로와 마이크로소프트의 CNTK, 그리고 텐서 플로의 원조라고 할 수 있는 티아노를 비교하였다. 본문에서는 자동미분의 개념과 GPU의 활용형태를 간단히 설명하고, 그 다음에 logistic regression을 실행하는 예를 통하여 각 프레임워크의 문법을 알아본 뒤에, 마지막으로 대표적인 딥러닝 응용인 CNN의 예제를 실행시켜보고 코딩의 편의성과 실행속도 등을 확인해 보았다. 그 결과, 편의성의 관점에서 보면 티아노가 가장 코딩 하기가 어렵고, CNTK와 텐서플로는 많은 부분이 비슷하게 추상화 되어 있어서 코딩이 비슷하지만 가중치와 편향을 직접 정의하느냐의 여부에서 차이를 보였다. 그리고 각프레임워크의 실행속도에 대한 평가는 ‘큰 차이는 없다’는 것이다. 텐서플로는 티아노에 비하여 속도가 느리다는 평가가 있어왔는데, 본 연구의 실험에 의하면, 비록 CNN 모형에 국한되었지만, 텐서플로가 아주 조금이지만빠른 것으로 나타났다. CNTK의 경우에도, 비록 실험환경이 달랐지만, 실험환경의 차이에 의한 속도의 차이의 편차범위 이내에 있는 것으로 판단이 되었다. 본 연구에서는 세 종류의 딥러닝 프레임워크만을 살펴보았는데,위키피디아에 따르면 딥러닝 프레임워크의 종류는 12가지가 있으며, 각 프레임워크의 특징을 15가지 속성으로구분하여 차이를 특정하고 있다. 그 많은 속성 중에서 사용자의 입장에서 볼 때 중요한 속성은 어떤 언어(파이썬, C++, Java, 등)로 사용가능한지, 어떤 딥러닝 모형에 대한 라이브러리가 잘 구현되어 있는지 등일 것이다. 그리고 사용자가 대규모의 딥러닝 모형을 구축한다면, 다중 GPU 혹은 다중 서버를 지원하는지의 여부도 중요할 것이다. 또한 딥러닝 모형을 처음 학습하는 경우에는 사용설명서가 많은지 예제 프로그램이 많은지 여부도 중요한 기준이 될 것이다.The deep learning framework is software designed to help develop deep learning models. Some of its important functions include “automatic differentiation” and “utilization of GPU”. The list of popular deep learning framework includes Caffe (BVLC) and Theano (University of Montreal). And recently, Microsoft's deep learning framework, Microsoft Cognitive Toolkit, was released as open-source license, following Google’s Tensorflow a year earlier. The early deep learning frameworks have been developed mainly for research at universities. Beginning with the inception of Tensorflow, however, it seems that companies such as Microsoft and Facebook have started to join the competition of framework development. Given the trend, Google and other companies are expected to continue investing in the deep learning framework to bring forward the initiative in the artificial intelligence business. From this point of view, we think it is a good time to compare some of deep learning frameworks. So we compare three deep learning frameworks which can be used as a Python library. Those are Google's Tensorflow, Microsoft’s CNTK, and Theano which is sort of a predecessor of the preceding two.
The most common and important function of deep learning frameworks is the ability to perform automatic differentiation. Basically all the mathematical expressions of deep learning models can be represented as computational graphs, which consist of nodes and edges. Partial derivatives on each edge of a computational graph can then be obtained. With the partial derivatives, we can let software compute differentiation of any node with respect to any variable by utilizing chain rule of Calculus.
First of all, the convenience of coding is in the order of CNTK, Tensorflow, and Theano. The criterion is simply based on the lengths of the codes and the learning curve and the ease of coding are not the main concern. According to the criteria, Theano was the most difficult to implement with, and CNTK and Tensorflow were somewhat easier. With Tensorflow, we need to define weight variables and biases explicitly. The reason that CNTK and Tensorflow are easier to implement with is that those frameworks provide us with more abstraction than Theano. We, however, need to mention that low-level coding is not always bad. It gives us flexibility of coding. With the low-level coding such as in Theano, we can implement and test any new deep learning models or any new search methods that we can think of.
The assessment of the execution speed of each framework is that there is not meaningful difference. According to the experiment, execution speeds of Theano and Tensorflow are very similar, although the experiment was limited to a CNN model. In the case of CNTK, the experimental environment was not maintained as the same. The code written in CNTK has to be run in PC environment without GPU where codes execute as much as 50 times slower than with GPU. But we concluded that the difference of execution speed was within the range of variation caused by the different hardware setup.
In this study, we compared three types of deep learning framework: Theano, Tensorflow, and CNTK. According to Wikipedia, there are 12 available deep learning frameworks. And 15 different attributes differentiate each framework. Some of the important attributes would include interface language (Python, C ++, Java, etc.) and the availability of libraries on various deep learning models such as CNN, RNN, DBN, and etc. And if a user implements a large scale deep learning model, it will also be important to support multiple GPU or multiple servers. Also, if you are learning the deep learning model, it would also be important if there are enough examples and references.deep learning framework, Theano, TensorFlow, CNTK, computational graph, CIFAR-10"
340,딥러닝을 이용한 주변 무선단말 파악방안,이웅섭 /Lee， Woong-sup,2018,한국정보통신학회,"최근 단말-대-단말(Device-to-device, D2D) 통신기술이 차세대 무선통신시스템의 핵심기술로 큰 관심을 받고 있다. 이러한 단말간 통신에서는 자신의 주변에 어떠한 단말이 있는지 파악하는 주변단말 탐색(Neighbor discovery)이매우 중요하다. 본 논문에서는 최근 큰 관심을 받고 있는 딥러닝(Deep learning) 기술을 활용하여 단말간 통신에서 주변단말을 파악하는 방안에 대해서 제안한다. 제안 방안은 기존의 방안과 달리 무선채널의 공간적 연관성을 이용하여단말간의 신호 전송 없이 단말이 기지국으로 전송하는 상향링크 파일럿 신호를 기반으로 주변 단말을 찾고 따라서기존의 방식에 비해 신호전송 복잡도(signaling complexity)를 크게 줄일 수 있다. 또한 제안 방안에서는 떨어져 있는거리에 따라서 주변 단말을 분류 가능하여 기존 방안에 비해서 좀 더 세밀한 단말 탐색이 가능하다. 마지막으로 본 논문에서는 tensorflow를 이용한 컴퓨터 시뮬레이션을 통해 제안 방안의 성능을 검증하였다.Recently, the device-to-device (D2D) communication has been conceived as the key technology for the next-generationmobile communication systems. The neighbor discovery in which the nearby users are found, is essential for the properoperation of the D2D communication. In this paper, we propose new neighbor discovery scheme based on deep learningtechnology which has gained a lot of attention recently. In the proposed scheme, the neighboring users can be foundusing the uplink pilot transmission of users only, unlike conventional neighbor discovery schemes in which direct pilotcommunication among users is required, such that the signaling overhead can be greatly reduced in our proposed scheme.
Moreover, the neighbors with different proximity can also be classified accordingly which enables more accurate neighbordiscovery compared to the conventional schemes. The performance of our proposed scheme is verified through thetensorflow-based computer simulations.
키워드 : 딥러닝, 주변단말 탐색, 단말-대-단말 통신Deep learning, Neighbor discovery, Device-to-device communication, Wireless channel correlation"
341,신뢰성있는 딥러닝 기반 분석 모델을 참조하기 위한 딥러닝 기술 언어,문종혁 /Jong Hyeok Mun,2021,한국정보처리학회,"최근 딥러닝은 하드웨어 성능이 향상됨에 따라 자연어 처리, 영상 인식 등의 다양한 기술에 접목되어 활용되고 있다. 이러한 기술들을 활용해 지능형교통 시스템(ITS), 스마트홈, 헬스케어 등의 산업분야에서 데이터를 분석하여 고속도로 속도위반 차량 검출, 에너지 사용량 제어, 응급상황 등과 같은고품질의 서비스를 제공하며, 고품질의 서비스를 제공하기 위해서는 정확도가 향상된 딥러닝 모델이 적용되어야 한다. 이를 위해 서비스 환경의 데이터를분석하기 위한 딥러닝 모델을 개발할 때, 개발자는 신뢰성이 검증된 최신의 딥러닝 모델을 적용할 수 있어야 한다. 이는 개발자가 참조하는 딥러닝 모델에적용된 학습 데이터셋의 정확도를 측정하여 검증할 수 있다. 이러한 검증을 위해서 개발자는 학습 데이터셋, 딥러닝의 계층구조 및 개발 환경 등과 같은내용을 포함하는 딥러닝 모델을 문서화하여 적용하기 위한 구조적인 정보가 필요하다. 본 논문에서는 신뢰성있는 딥러닝 기반 데이터 분석 모델을 참조하기위한 딥러닝 기술 언어를 제안한다. 제안하는 기술 언어는 신뢰성 있는 딥러닝 모델을 개발하는데 필요한 학습데이터셋, 개발 환경 및 설정 등의 정보와더불어 딥러닝 모델의 계층구조를 표현할 수 있다. 제안하는 딥러닝 기술 언어를 이용하여 개발자는 지능형 교통 시스템에서 참조하는 분석 모델의정확도를 검증할 수 있다. 실험에서는 제안하는 언어의 유효성을 검증하기 위해, 번호판 인식 모델을 중심으로 딥러닝 기술 문서의 적용과정을 보인다With the recent advancements of deep learning, companies such as smart home, healthcare, and intelligent transportation systems areutilizing its functionality to provide high-quality services for vehicle detection, emergency situation detection, and controlling energyconsumption. To provide reliable services in such sensitive systems, deep learning models are required to have high accuracy. In orderto develop a deep learning model for analyzing previously mentioned services, developers should utilize the state of the art deep learningmodels that have already been verified for higher accuracy. The developers can verify the accuracy of the referenced model by validatingthe model on the dataset. For this validation, the developer needs structural information to document and apply deep learning models,including metadata such as learning dataset, network architecture, and development environments. In this paper, we propose a descriptionlanguage that represents the network architecture of the deep learning model along with its metadata that are necessary to develop adeep learning model. Through the proposed description language, developers can easily verify the accuracy of the referenced deep learningmodel. Our experiments demonstrate the application scenario of a deep learning description document that focuses on the license platerecognition for the detection of illegally parked vehicles.Trusted Deep Learning, Model Reference, Deep Learning Description Language, Traffic Situation Analysis Model"
342,딥러닝 분석을 이용한 중국 역내·외 위안화 변동성 예측,이우식 /Lee Woosik,2016,한국데이터정보과학회,"2008년 글로벌 금융위기 이후 중국은 위안화 국제화의 점진적 추진을 진행하면서 중국상하이 외환시장과 중국홍콩 외환시장에서 거래되는 통화인 역내위안화와 역외위안화를 형성시켰다. 본 연구는 위안화 국제화와 점진적인 중국 자본계정 개방에 따라 급변하는 외환시장상황의 변동성을 정확하게 파악하기 위해서 GARCH모형 (일반화된 자기회귀 조건부이분산성모형)에 다단계인공신경망을 결합한 MLP-GARCH 모형과 GARCH모형과 기계학습의 일종인 딥러닝 (deep learning)을 통합한 DL-GARCH을 가지고 위안화 변동성예측을 비교 실험과 분석을 하였다. 비교분석 결과 DLGARCH 모형은 MLP-GARCH보다 모형 위안화 역내·외 환율변동성 예측 면에서 더욱 더 개선된예측값을 제공하였다. 그래서 이분산시계열모형을 딥러닝과 결합한 DL-GARCH 모형은 시계열의환율 변동성 예측 문제에 딥러닝을 응용할 수 있음을 확인하였다. 향후 이분산시계열과 결합된 딥러닝 모형은 다른 금융시계열 데이터에 응용하여 그 일반화 가능성을 높일 수 있을 것이다.The People’s Republic of China has vigorously been pursuing the internationalization of the Chinese Yuan or Renminbi after the financial crisis of 2008. In this view, an abrupt increase of use of the Chinese Yuan in the onshore and offshore markets are important milestones to be one of important currencies. One of the most frequently used methods to forecast volatility is GARCH model. Since a prediction error of the GARCH model has been reported quite high, a lot of efforts have been made to improve forecasting capability of the GARCH model. In this paper, we have proposed MLPGARCH and a DL-GARCH by employing Artificial Neural Network to the GARCH. In an application to forecasting Chinese Yuan volatility, we have successfully shown their overall outperformance in forecasting over the GARCH.Deep learning, DL-GARCH, GARCH, The Chinese Yuan, volatility."
343,딥러닝 기법을 이용한 내일강수 예측,하지훈,2016,한국지능시스템학회,"정확한 강수예측을 위해서는 예측인자 선정과 예측방법에 대한 선택이 매우 중요하다. 최근에는 강수예측 방법으로기계학습 기법이 많이 사용되고 있으며, 그 중에서도 특히 인공신경망을 사용한 강수예측 방법은 좋은 성능을 보였다. 본논문에서는 딥러닝 기법 중 하나인 DBN(deep belief network)를 이용한 새로운 강수예측 방법을 제안한다. DBN는 비지도사전 학습을 통해 초기 가중치를 설정하여 기존 인공신경망의 문제점을 보완한다. 예측인자로는 기온, 전일-전주 강수일, 태양과 달 궤도 관련 자료를 선정하였다. 기온과 전일-전주 강수일은 서울에서의 1974년부터 2013년까지 총 40년간의AWS(automatic weather system) 관측 자료를 사용하였고, 태양과 달의 궤도 관련 자료는 서울을 중심으로 계산한 결과를사용하였다. 전체 기간에서 일부는 학습 자료로 사용하여 예측모델을 생성하였고, 나머지를 생성한 모델의 검증 자료로사용하였다. 모델 검증 결과로 나온 예측값들은 확률값을 가지며 임계치를 이용하여 강수유무를 판별하였다. 강수 정확도의척도로 양분예보기법 중 CSI(critical successive index)와 Bias(frequency bias)를 계산하였다. 이를 통해 DBN와 MLP(multilayer perceptron)의 성능을 비교한 결과 DBN의 강수 예측 정확도가 높았고, 수행속도 또한 2배 이상 빨랐다.For accurate precipitation forecasts the choice of weather faoctrs and prediction method is very important. Recently, machinele arning has been widely used for forecasting precipitation, and artificial neural network, one of machine learning techniques, showed good performance. In this paper, we suggest a new method for forecasting precipitation using DBN, one of deep learning techniques.
DBN has an advantage that initial weights are set by unsupervised learning, so this compensates for the defects of artificial neural networks. We used past precipitation, temperature, and the parameters of the sun and moon's motion as features for forecasting precipitation. The dataset consists of observation data which had been measured for 40 years from AWS in Seoul. Experiments were based on 8-fold cross validation. As a result of estimation, we got probabilities of test dataset, so threshold was used for the decision of precipitation. CSI and Bias were used for indicating the precision of precipitation. Our experimental results showed that DBN performed better than MLP.Deep learning, Deep belief network, Precipitation, Forecast"
344,딥러닝 신경망모형을 이용한 실시간 교통정보수집 연구,김대현 /KIM DAE HYON,2016,사단법인 인문사회과학기술융합학회,"비디오 기반의 실시간 교통정보수집 시스템은 자율주행을 포함한 지능형교통체계의 주요 서브시스템중 하나이며, 우수한 예측(추정)력 때문에 핵심 알고리즘으로 인공신경망 모형이 주로 사용되고 있다. 특히, 최근 과학 학술지 네이처에 구글 딥마인드의 알파고가 공개되면서 딥러닝과 인공지능에 대한 전 세계적인 관심이 집중되고 있다. 최근까지 가장 널리 사용된 인공신경망모형은 역전파 모형이었으나 최근 컴퓨터 하드웨어 및 알고리즘의 새로운 개발과 더불어 딥러닝 알고리즘을 이용한 딥인공신경망 모형에 대한 관심이 높아지고 있다. 그러나 아직 딥러닝 인공신경망에 대한 연구는 초기단계에 있는 실정이며 적용분야도 극히 한정되어 있다. 본 연구에서는 실시간 도로교통정보 수집을 위한 영상처리분야에서 딥러닝 알고리즘을 활용하고 실험결과를 통하여 향후 교통정보수집‧분석 분야에서의 딥러닝 인공신경망 모형의 적용가능성을 살펴보고자 한다. 특히, 본 연구에서 도출한 딥러닝 인공신경망모형의 실험결과를 기존 역전파 모형에 의한 실험결과와 비교하여 모형의 우수성을 비교‧평가하고자 한다.Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Normally, artificial neural networks have been used in real-time applications for collecting traffic information because of their remarkable performance in terms of predictive accuracy and real-time computing. Especially, the recent publication of a paper in the journal Nature and the success of Google DeepMind’s world-beating Go program AlphaGo caused significant increases of the public interest in artificial intelligence. Even though various types of learning models have been proposed, the Backpropagation has been the most popular neural network model. However, the Deep learning neural network model has attracted much interest recently. In this study, the Deep leaning neural network will be applied in image processing techniques for the real-time traffic information collection to investigate the performance of Deep learning algorithm in traffic engineering fields. In addition, the experimental results from Deep learing algorithm will be compared with the results from Backpropagation model to evaluate and propose better performing model.Intelligent Transportation Systems (ITS), artificial neural networks, Deep learning, traffic information, image processing"
345,딥러닝의 모형과 응용사례,안성만 /SungMahn Ahn,2016,한국지능정보시스템학회,"딥러닝은 인공신경망(neural network)이라는 인공지능분야의 모형이 발전된 형태로서, 계층구조로 이루어진 인공신경망의 내부계층(hidden layer)이 여러 단계로 이루어진 구조이다. 딥러닝에서의 주요 모형은 합성곱신경망(convolutional neural network), 순환신경망(recurrent neural network), 그리고 심층신뢰신경망(deep belief network)의 세가지라고 할 수있다. 그 중에서 현재 흥미로운 연구가 많이 발표되어서 관심이 집중되고 있는 모형은 지도학습(supervised learning)모형인 처음 두 개의 모형이다. 따라서 본 논문에서는 지도학습모형의 가중치를 최적화하는 기본적인 방법인 오류역전파 알고리즘을 살펴본 뒤에 합성곱신경망과 순환신경망의 구조와 응용사례 등을 살펴보고자 한다. 본문에서 다루지 않은 모형인 심층신뢰신경망은 아직까지는 합성곱신경망 이나 순환신경망보다는 상대적으로 주목을 덜 받고 있다. 그러나 심층신뢰신경망은 CNN이나 RNN과는 달리 비지도학습(unsupervised learning)모형이며, 사람이나 동물은 관찰을 통해서 스스로 학습한다는 점에서 궁극적으로는 비지도학습모형이 더 많이 연구되어야 할 주제가 될 것이다.Deep learning model is a kind of neural networks that allows multiple hidden layers. There are various deep learning architectures such as convolutional neural networks, deep belief networks and recurrent neural networks. Those have been applied to fields like computer vision, automatic speech recognition, natural language processing, audio recognition and bioinformatics where they have been shown to produce state-of-the-art results on various tasks. Among those architectures, convolutional neural networks and recurrent neural networks are classified as the supervised learning model. And in recent years, those supervised learning models have gained more popularity than unsupervised learning models such as deep belief networks, because supervised learning models have shown fashionable applications in such fields mentioned above.
Deep learning models can be trained with backpropagation algorithm. Backpropagation is an abbreviation for “backward propagation of errors” and a common method of training artificial neural networks used in conjunction with an optimization method such as gradient descent. The method calculates the gradient of an error function with respect to all the weights in the network. The gradient is fed to the optimization method which in turn uses it to update the weights, in an attempt to minimize the error function.
Convolutional neural networks use a special architecture which is particularly well-adapted to classify images. Using this architecture makes convolutional networks fast to train. This, in turn, helps us train deep, muti-layer networks, which are very good at classifying images. These days, deep convolutional networks are used in most neural networks for image recognition. Convolutional neural networks use three basic ideas: local receptive fields, shared weights, and pooling. By local receptive fields, we mean that each neuron in the first(or any) hidden layer will be connected to a small region of the input(or previous layer’s) neurons. Shared weights mean that we’re going to use the same weights and bias for each of the local receptive field. This means that all the neurons in the hidden layer detect exactly the same feature, just at different locations in the input image. In addition to the convolutional layers just described, convolutional neural networks also contain pooling layers. Pooling layers are usually used immediately after convolutional layers. What the pooling layers do is to simplify the information in the output from the convolutional layer.
Recent convolutional network architectures have 10 to 20 hidden layers and billions of connections between units. Training deep learning networks has taken weeks several years ago, but thanks to progress in GPU and algorithm enhancement, training time has reduced to several hours.
Neural networks with time-varying behavior are known as recurrent neural networks or RNNs. A recurrent neural network is a class of artificial neural network where connections between units form a directed cycle. This creates an internal state of the network which allows it to exhibit dynamic temporal behavior. Unlike feedforward neural networks, RNNs can use their internal memory to process arbitrary sequences of inputs. Early RNN models turned out to be very difficult to train, harder even than deep feedforward networks. The reason is the unstable gradient problem such as vanishing gradient and exploding gradient. The gradient can get smaller and smaller as it is propagated back through layers. This makes learning in early layers extremely slow. The problem actually gets worse in RNNs, since gradients aren’t just propagated backward through layers, they’re propagated backward through time. If the network runs for a long time, that can make the gradient extremely unstable and hard to learn from. It has been possible to incorporate an idea known as long short-term memory units (LSTMs) into RNNs. LSTMs make it much easier to get good results when training RNNs, and many recent papers make use of LSTMs or related ideas.Deep learning, Convolutional neural networks, Recurrent neural networks, Error backpropagation algorithm"
346,딥러닝분석과 기술적 분석 지표를 이용한 한국 코스피주가지수 방향성 예측,이우식 /Lee Woosik,2017,한국데이터정보과학회,"2016년 3월 구글 (Google)의 바둑인공지능 알파고 (AlphaGo)가 이세돌 9단과의 바둑대결에서승리한 이후 다양한 분야에서 인공지능 사용에 대한 관심이 높아지고 있는 가운데 금융투자 분야에서도 인공지능과 투자자문 전문가의 합성어인 로보어드바이저 (Robo-Advisor)에 대한 관심이 높아지고 있다. 인공지능 (artificial intelligence)기반의 의사결정은 비용 절감은 물론 효과적인 의사결정을가능하게 한다는 점에서 큰 장점이 있다. 본 연구에서는 기술적 분석 (technical analysis) 지표와 딥러닝 (deep learning) 모형을 결합하여 한국 코스피 지수를 예측하는 모형을 개발하고 제시한 모형들의 예측력을 비교, 분석한다. 분석 결과 기술적 분석 지표에 딥러닝 알고리즘을 결합한 모형이 주가지수 방향성 예측 문제에 응용될 수 있음을 확인하였다. 향후 본 연구에서 제안된 기술적 분석 지표와딥러닝모형을 결합한 기법은 로보어드바이저서비스에 응용할 수 있는 일반화 가능성을 보여준다Since Google’s AlphaGo defeated a world champion of Go players in 2016, there have been many interests in the deep learning. In the financial sector, a Robo-Advisor using deep learning gains a significant attention, which builds and manages portfolios of financial instruments for investors.In this paper, we have proposed the a deep learning algorithm geared toward identification and forecast of the KOSPI index direction,and we also have compared the accuracy of the prediction.In an application of forecasting the financial market index direction, we have shown that the Robo-Advisor using deep learning has a significant effect on finance industry. The Robo-Advisor collects a massive data such as earnings statements, news reports and regulatory filings, analyzes those and recommends investors how to view market trends and identify the best time to purchase financial assets. On the other hand, the Robo-Advisor allows businesses to learn more about their customers, develop better marketing strategies, increase sales and decrease costs.Artificial intelligence, deep learning, FinTech, Robo-Advisor, technical analysis."
347,딥러닝의 효율적인 학습을 위한 학습데이터 선별 기법에 관한 연구,주경돈 /Gyoung-don Joo,2017,한국정보과학회,"빅데이터의 증가와 병렬 하드웨어 처리, 학습 알고리즘의 개선은 딥러닝의 발전을 가져왔으며 인공지능이 적용 가능한 영역을 획기적으로 확장시키고 있다. 하지만 이러한 대규모의 학습 데이터에 기반한 딥러닝은 학습시간이 증가되고, 더 나아가 학습 데이터의 구축을 위한 레이블링(labeling) 비용이 데이터 수에 비례하여 증가하게 된다. 따라서 본 논문에서는, 딥러닝의 정확도를 최대한 유지하면서 필요한 학습 데이터의 수를 줄일수 있도록, 전체 데이터에서 제한된 수의 학습 데이터의 후보를 선별하는 기법에 대하여 연구한다. 특히 기존의 능동 학습에 기반한 기법들이 학습 데이터의 수가 매우 적어질 경우 딥러닝의 정확도가 급격히 떨어지는 문제를 해결할 수 있는 새로운 기법을 제안하였으며, 제안된 기법으로 선별된 학습 데이터는 그 수의 변화에 대해 딥러닝이 일관되게 경쟁력있는 정확도를 가짐을 다양한 실험을 통하여 검증하였다.The increase of big data, parallel hardware processing and enhanced learning algorithms have led to the development of deep learning and are dramatically expanding the domain of applications based on artificial intelligence. However, such large-scale training data extends the learning time, and furthermore, the labeling cost for building the training set increases in proportion to the volume of data. Therefore, in this paper, we study how to select a limited number of candidates for training set from given data so as to reduce the number of training set while maintaining the accuracy of deep learning. Especially, we found that, with the training set selected by the previous work, the accuracy of deep learning drastically decreases when the number of training set is very small and so proposed new methods to resolve this fall-off in accuracy. Through experiments, we showed that deep learning consistently has the competitive accuracy with the training set selected by the proposed method regardless of the size of training set.Deep Learning, Train Set Selection, Active Learning, Gamma distribution"
348,딥러닝을 이용한 범용적 스테그아날리시스,김현재 /Hyunjae Kim,2017,한국정보과학회,"스테그아날리시스(Steganalysis)란 이미지 등 일반적인 자료에 암호화된 정보를 은닉하는 스테가노그래피(Steganography)에 대한 검출 및 분석 방법으로, 기계학습 기반 방법론을 포함한다. 기존 기계학습 기반 스테그아날리시스는 영상(Image)의 특징(Feature) 추출 및 모델링에 기반하며, 최근 딥러닝(Deep Learning)의 적용으로 검출 정확도가 큰 폭으로 향상되었다. 하지만 현존하는 스테그아날리시스 모델은 단일 스테가노그래피 기법에 대해 국한되어 있어 학습에 사용되지 않은 스테고(Stego) 이미지의 경우 검출이 불가능한 결정적 한계를 가진다. 본 연구에서는 다양한 스테가노그래피 기법으로 생성된 스테고이미지에 딥러닝을 적용하여 스테그아날리시스를 학습하는 범용적 모델을 제안한다. 다양한 실험을 통해 제안 기법의 효용성 및 가능성을 확인하고, 범용적 스테그아날리시스 모델이 각각에 특화된 검출 기법과 유사한 정확도로 스테고 이미지를 검출할 수 있음을 보인다.Steganalysis is to detect information hidden by steganography inside general data such as images. There are stegoanalysis techniques that use machine learning (ML). Existing ML approaches to steganalysis are based on extracting features from stego images and modeling them.
Recently deep learning-based methodologies have shown significant improvements in detection accuracy. However, all the existing methods, including deep learning-based ones, have a critical limitation in that they can only detect stego images that are created by a specific steganography method. In this paper, we propose a generalized steganalysis method that can model multiple types of stego images using deep learning. Through various experiments, we confirm the effectiveness of our approach and envision directions for future research. In particular, we show that our method can detect each type of steganography with the same level of accuracy as that of a steganalysis method dedicated to that type of steganography, thereby demonstrating the general applicability of our approach to multiple types of stego images.steganalysis, steganography, deep learning, generalized steganalysis model"
349,"딥러닝 프레임워크의 비교: 티아노, 텐서플로, CNTK를 중심으로",정여진 /Yeojin Chung,2017,한국지능정보시스템학회,"딥러닝 프레임워크의 대표적인 기능으로는 ‘자동미분’과 ‘GPU의 활용’ 등을 들 수 있다. 본 논문은 파이썬의 라이브러리 형태로 사용 가능한 프레임워크 중에서 구글의 텐서플로와 마이크로소프트의 CNTK, 그리고 텐서 플로의 원조라고 할 수 있는 티아노를 비교하였다. 본문에서는 자동미분의 개념과 GPU의 활용형태를 간단히 설명하고, 그 다음에 logistic regression을 실행하는 예를 통하여 각 프레임워크의 문법을 알아본 뒤에, 마지막으로 대표적인 딥러닝 응용인 CNN의 예제를 실행시켜보고 코딩의 편의성과 실행속도 등을 확인해 보았다. 그 결과, 편의성의 관점에서 보면 티아노가 가장 코딩 하기가 어렵고, CNTK와 텐서플로는 많은 부분이 비슷하게 추상화 되어 있어서 코딩이 비슷하지만 가중치와 편향을 직접 정의하느냐의 여부에서 차이를 보였다. 그리고 각프레임워크의 실행속도에 대한 평가는 ‘큰 차이는 없다’는 것이다. 텐서플로는 티아노에 비하여 속도가 느리다는 평가가 있어왔는데, 본 연구의 실험에 의하면, 비록 CNN 모형에 국한되었지만, 텐서플로가 아주 조금이지만빠른 것으로 나타났다. CNTK의 경우에도, 비록 실험환경이 달랐지만, 실험환경의 차이에 의한 속도의 차이의 편차범위 이내에 있는 것으로 판단이 되었다. 본 연구에서는 세 종류의 딥러닝 프레임워크만을 살펴보았는데,위키피디아에 따르면 딥러닝 프레임워크의 종류는 12가지가 있으며, 각 프레임워크의 특징을 15가지 속성으로구분하여 차이를 특정하고 있다. 그 많은 속성 중에서 사용자의 입장에서 볼 때 중요한 속성은 어떤 언어(파이썬, C++, Java, 등)로 사용가능한지, 어떤 딥러닝 모형에 대한 라이브러리가 잘 구현되어 있는지 등일 것이다. 그리고 사용자가 대규모의 딥러닝 모형을 구축한다면, 다중 GPU 혹은 다중 서버를 지원하는지의 여부도 중요할 것이다. 또한 딥러닝 모형을 처음 학습하는 경우에는 사용설명서가 많은지 예제 프로그램이 많은지 여부도 중요한 기준이 될 것이다.The deep learning framework is software designed to help develop deep learning models. Some of its important functions include “automatic differentiation” and “utilization of GPU”. The list of popular deep learning framework includes Caffe (BVLC) and Theano (University of Montreal). And recently, Microsoft's deep learning framework, Microsoft Cognitive Toolkit, was released as open-source license, following Google’s Tensorflow a year earlier. The early deep learning frameworks have been developed mainly for research at universities. Beginning with the inception of Tensorflow, however, it seems that companies such as Microsoft and Facebook have started to join the competition of framework development. Given the trend, Google and other companies are expected to continue investing in the deep learning framework to bring forward the initiative in the artificial intelligence business. From this point of view, we think it is a good time to compare some of deep learning frameworks. So we compare three deep learning frameworks which can be used as a Python library. Those are Google's Tensorflow, Microsoft’s CNTK, and Theano which is sort of a predecessor of the preceding two.
The most common and important function of deep learning frameworks is the ability to perform automatic differentiation. Basically all the mathematical expressions of deep learning models can be represented as computational graphs, which consist of nodes and edges. Partial derivatives on each edge of a computational graph can then be obtained. With the partial derivatives, we can let software compute differentiation of any node with respect to any variable by utilizing chain rule of Calculus.
First of all, the convenience of coding is in the order of CNTK, Tensorflow, and Theano. The criterion is simply based on the lengths of the codes and the learning curve and the ease of coding are not the main concern. According to the criteria, Theano was the most difficult to implement with, and CNTK and Tensorflow were somewhat easier. With Tensorflow, we need to define weight variables and biases explicitly. The reason that CNTK and Tensorflow are easier to implement with is that those frameworks provide us with more abstraction than Theano. We, however, need to mention that low-level coding is not always bad. It gives us flexibility of coding. With the low-level coding such as in Theano, we can implement and test any new deep learning models or any new search methods that we can think of.
The assessment of the execution speed of each framework is that there is not meaningful difference. According to the experiment, execution speeds of Theano and Tensorflow are very similar, although the experiment was limited to a CNN model. In the case of CNTK, the experimental environment was not maintained as the same. The code written in CNTK has to be run in PC environment without GPU where codes execute as much as 50 times slower than with GPU. But we concluded that the difference of execution speed was within the range of variation caused by the different hardware setup.
In this study, we compared three types of deep learning framework: Theano, Tensorflow, and CNTK. According to Wikipedia, there are 12 available deep learning frameworks. And 15 different attributes differentiate each framework. Some of the important attributes would include interface language (Python, C ++, Java, etc.) and the availability of libraries on various deep learning models such as CNN, RNN, DBN, and etc. And if a user implements a large scale deep learning model, it will also be important to support multiple GPU or multiple servers. Also, if you are learning the deep learning model, it would also be important if there are enough examples and references.deep learning framework, Theano, TensorFlow, CNTK, computational graph, CIFAR-10"
350,딥러닝을 이용한 주변 무선단말 파악방안,이웅섭 /Lee， Woong-sup,2018,한국정보통신학회,"최근 단말-대-단말(Device-to-device, D2D) 통신기술이 차세대 무선통신시스템의 핵심기술로 큰 관심을 받고 있다. 이러한 단말간 통신에서는 자신의 주변에 어떠한 단말이 있는지 파악하는 주변단말 탐색(Neighbor discovery)이매우 중요하다. 본 논문에서는 최근 큰 관심을 받고 있는 딥러닝(Deep learning) 기술을 활용하여 단말간 통신에서 주변단말을 파악하는 방안에 대해서 제안한다. 제안 방안은 기존의 방안과 달리 무선채널의 공간적 연관성을 이용하여단말간의 신호 전송 없이 단말이 기지국으로 전송하는 상향링크 파일럿 신호를 기반으로 주변 단말을 찾고 따라서기존의 방식에 비해 신호전송 복잡도(signaling complexity)를 크게 줄일 수 있다. 또한 제안 방안에서는 떨어져 있는거리에 따라서 주변 단말을 분류 가능하여 기존 방안에 비해서 좀 더 세밀한 단말 탐색이 가능하다. 마지막으로 본 논문에서는 tensorflow를 이용한 컴퓨터 시뮬레이션을 통해 제안 방안의 성능을 검증하였다.Recently, the device-to-device (D2D) communication has been conceived as the key technology for the next-generationmobile communication systems. The neighbor discovery in which the nearby users are found, is essential for the properoperation of the D2D communication. In this paper, we propose new neighbor discovery scheme based on deep learningtechnology which has gained a lot of attention recently. In the proposed scheme, the neighboring users can be foundusing the uplink pilot transmission of users only, unlike conventional neighbor discovery schemes in which direct pilotcommunication among users is required, such that the signaling overhead can be greatly reduced in our proposed scheme.
Moreover, the neighbors with different proximity can also be classified accordingly which enables more accurate neighbordiscovery compared to the conventional schemes. The performance of our proposed scheme is verified through thetensorflow-based computer simulations.
키워드 : 딥러닝, 주변단말 탐색, 단말-대-단말 통신Deep learning, Neighbor discovery, Device-to-device communication, Wireless channel correlation"
351,신뢰성있는 딥러닝 기반 분석 모델을 참조하기 위한 딥러닝 기술 언어,문종혁 /Jong Hyeok Mun,2021,한국정보처리학회,"최근 딥러닝은 하드웨어 성능이 향상됨에 따라 자연어 처리, 영상 인식 등의 다양한 기술에 접목되어 활용되고 있다. 이러한 기술들을 활용해 지능형교통 시스템(ITS), 스마트홈, 헬스케어 등의 산업분야에서 데이터를 분석하여 고속도로 속도위반 차량 검출, 에너지 사용량 제어, 응급상황 등과 같은고품질의 서비스를 제공하며, 고품질의 서비스를 제공하기 위해서는 정확도가 향상된 딥러닝 모델이 적용되어야 한다. 이를 위해 서비스 환경의 데이터를분석하기 위한 딥러닝 모델을 개발할 때, 개발자는 신뢰성이 검증된 최신의 딥러닝 모델을 적용할 수 있어야 한다. 이는 개발자가 참조하는 딥러닝 모델에적용된 학습 데이터셋의 정확도를 측정하여 검증할 수 있다. 이러한 검증을 위해서 개발자는 학습 데이터셋, 딥러닝의 계층구조 및 개발 환경 등과 같은내용을 포함하는 딥러닝 모델을 문서화하여 적용하기 위한 구조적인 정보가 필요하다. 본 논문에서는 신뢰성있는 딥러닝 기반 데이터 분석 모델을 참조하기위한 딥러닝 기술 언어를 제안한다. 제안하는 기술 언어는 신뢰성 있는 딥러닝 모델을 개발하는데 필요한 학습데이터셋, 개발 환경 및 설정 등의 정보와더불어 딥러닝 모델의 계층구조를 표현할 수 있다. 제안하는 딥러닝 기술 언어를 이용하여 개발자는 지능형 교통 시스템에서 참조하는 분석 모델의정확도를 검증할 수 있다. 실험에서는 제안하는 언어의 유효성을 검증하기 위해, 번호판 인식 모델을 중심으로 딥러닝 기술 문서의 적용과정을 보인다With the recent advancements of deep learning, companies such as smart home, healthcare, and intelligent transportation systems areutilizing its functionality to provide high-quality services for vehicle detection, emergency situation detection, and controlling energyconsumption. To provide reliable services in such sensitive systems, deep learning models are required to have high accuracy. In orderto develop a deep learning model for analyzing previously mentioned services, developers should utilize the state of the art deep learningmodels that have already been verified for higher accuracy. The developers can verify the accuracy of the referenced model by validatingthe model on the dataset. For this validation, the developer needs structural information to document and apply deep learning models,including metadata such as learning dataset, network architecture, and development environments. In this paper, we propose a descriptionlanguage that represents the network architecture of the deep learning model along with its metadata that are necessary to develop adeep learning model. Through the proposed description language, developers can easily verify the accuracy of the referenced deep learningmodel. Our experiments demonstrate the application scenario of a deep learning description document that focuses on the license platerecognition for the detection of illegally parked vehicles.Trusted Deep Learning, Model Reference, Deep Learning Description Language, Traffic Situation Analysis Model"
352,딥러닝 분석을 이용한 중국 역내·외 위안화 변동성 예측,이우식 /Lee Woosik,2016,한국데이터정보과학회,"2008년 글로벌 금융위기 이후 중국은 위안화 국제화의 점진적 추진을 진행하면서 중국상하이 외환시장과 중국홍콩 외환시장에서 거래되는 통화인 역내위안화와 역외위안화를 형성시켰다. 본 연구는 위안화 국제화와 점진적인 중국 자본계정 개방에 따라 급변하는 외환시장상황의 변동성을 정확하게 파악하기 위해서 GARCH모형 (일반화된 자기회귀 조건부이분산성모형)에 다단계인공신경망을 결합한 MLP-GARCH 모형과 GARCH모형과 기계학습의 일종인 딥러닝 (deep learning)을 통합한 DL-GARCH을 가지고 위안화 변동성예측을 비교 실험과 분석을 하였다. 비교분석 결과 DLGARCH 모형은 MLP-GARCH보다 모형 위안화 역내·외 환율변동성 예측 면에서 더욱 더 개선된예측값을 제공하였다. 그래서 이분산시계열모형을 딥러닝과 결합한 DL-GARCH 모형은 시계열의환율 변동성 예측 문제에 딥러닝을 응용할 수 있음을 확인하였다. 향후 이분산시계열과 결합된 딥러닝 모형은 다른 금융시계열 데이터에 응용하여 그 일반화 가능성을 높일 수 있을 것이다.The People’s Republic of China has vigorously been pursuing the internationalization of the Chinese Yuan or Renminbi after the financial crisis of 2008. In this view, an abrupt increase of use of the Chinese Yuan in the onshore and offshore markets are important milestones to be one of important currencies. One of the most frequently used methods to forecast volatility is GARCH model. Since a prediction error of the GARCH model has been reported quite high, a lot of efforts have been made to improve forecasting capability of the GARCH model. In this paper, we have proposed MLPGARCH and a DL-GARCH by employing Artificial Neural Network to the GARCH. In an application to forecasting Chinese Yuan volatility, we have successfully shown their overall outperformance in forecasting over the GARCH.Deep learning, DL-GARCH, GARCH, The Chinese Yuan, volatility."
353,딥러닝 기법을 이용한 내일강수 예측,하지훈,2016,한국지능시스템학회,"정확한 강수예측을 위해서는 예측인자 선정과 예측방법에 대한 선택이 매우 중요하다. 최근에는 강수예측 방법으로기계학습 기법이 많이 사용되고 있으며, 그 중에서도 특히 인공신경망을 사용한 강수예측 방법은 좋은 성능을 보였다. 본논문에서는 딥러닝 기법 중 하나인 DBN(deep belief network)를 이용한 새로운 강수예측 방법을 제안한다. DBN는 비지도사전 학습을 통해 초기 가중치를 설정하여 기존 인공신경망의 문제점을 보완한다. 예측인자로는 기온, 전일-전주 강수일, 태양과 달 궤도 관련 자료를 선정하였다. 기온과 전일-전주 강수일은 서울에서의 1974년부터 2013년까지 총 40년간의AWS(automatic weather system) 관측 자료를 사용하였고, 태양과 달의 궤도 관련 자료는 서울을 중심으로 계산한 결과를사용하였다. 전체 기간에서 일부는 학습 자료로 사용하여 예측모델을 생성하였고, 나머지를 생성한 모델의 검증 자료로사용하였다. 모델 검증 결과로 나온 예측값들은 확률값을 가지며 임계치를 이용하여 강수유무를 판별하였다. 강수 정확도의척도로 양분예보기법 중 CSI(critical successive index)와 Bias(frequency bias)를 계산하였다. 이를 통해 DBN와 MLP(multilayer perceptron)의 성능을 비교한 결과 DBN의 강수 예측 정확도가 높았고, 수행속도 또한 2배 이상 빨랐다.For accurate precipitation forecasts the choice of weather faoctrs and prediction method is very important. Recently, machinele arning has been widely used for forecasting precipitation, and artificial neural network, one of machine learning techniques, showed good performance. In this paper, we suggest a new method for forecasting precipitation using DBN, one of deep learning techniques.
DBN has an advantage that initial weights are set by unsupervised learning, so this compensates for the defects of artificial neural networks. We used past precipitation, temperature, and the parameters of the sun and moon's motion as features for forecasting precipitation. The dataset consists of observation data which had been measured for 40 years from AWS in Seoul. Experiments were based on 8-fold cross validation. As a result of estimation, we got probabilities of test dataset, so threshold was used for the decision of precipitation. CSI and Bias were used for indicating the precision of precipitation. Our experimental results showed that DBN performed better than MLP.Deep learning, Deep belief network, Precipitation, Forecast"
354,딥러닝 신경망모형을 이용한 실시간 교통정보수집 연구,김대현 /KIM DAE HYON,2016,사단법인 인문사회과학기술융합학회,"비디오 기반의 실시간 교통정보수집 시스템은 자율주행을 포함한 지능형교통체계의 주요 서브시스템중 하나이며, 우수한 예측(추정)력 때문에 핵심 알고리즘으로 인공신경망 모형이 주로 사용되고 있다. 특히, 최근 과학 학술지 네이처에 구글 딥마인드의 알파고가 공개되면서 딥러닝과 인공지능에 대한 전 세계적인 관심이 집중되고 있다. 최근까지 가장 널리 사용된 인공신경망모형은 역전파 모형이었으나 최근 컴퓨터 하드웨어 및 알고리즘의 새로운 개발과 더불어 딥러닝 알고리즘을 이용한 딥인공신경망 모형에 대한 관심이 높아지고 있다. 그러나 아직 딥러닝 인공신경망에 대한 연구는 초기단계에 있는 실정이며 적용분야도 극히 한정되어 있다. 본 연구에서는 실시간 도로교통정보 수집을 위한 영상처리분야에서 딥러닝 알고리즘을 활용하고 실험결과를 통하여 향후 교통정보수집‧분석 분야에서의 딥러닝 인공신경망 모형의 적용가능성을 살펴보고자 한다. 특히, 본 연구에서 도출한 딥러닝 인공신경망모형의 실험결과를 기존 역전파 모형에 의한 실험결과와 비교하여 모형의 우수성을 비교‧평가하고자 한다.Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Normally, artificial neural networks have been used in real-time applications for collecting traffic information because of their remarkable performance in terms of predictive accuracy and real-time computing. Especially, the recent publication of a paper in the journal Nature and the success of Google DeepMind’s world-beating Go program AlphaGo caused significant increases of the public interest in artificial intelligence. Even though various types of learning models have been proposed, the Backpropagation has been the most popular neural network model. However, the Deep learning neural network model has attracted much interest recently. In this study, the Deep leaning neural network will be applied in image processing techniques for the real-time traffic information collection to investigate the performance of Deep learning algorithm in traffic engineering fields. In addition, the experimental results from Deep learing algorithm will be compared with the results from Backpropagation model to evaluate and propose better performing model.Intelligent Transportation Systems (ITS), artificial neural networks, Deep learning, traffic information, image processing"
355,딥러닝의 모형과 응용사례,안성만 /SungMahn Ahn,2016,한국지능정보시스템학회,"딥러닝은 인공신경망(neural network)이라는 인공지능분야의 모형이 발전된 형태로서, 계층구조로 이루어진 인공신경망의 내부계층(hidden layer)이 여러 단계로 이루어진 구조이다. 딥러닝에서의 주요 모형은 합성곱신경망(convolutional neural network), 순환신경망(recurrent neural network), 그리고 심층신뢰신경망(deep belief network)의 세가지라고 할 수있다. 그 중에서 현재 흥미로운 연구가 많이 발표되어서 관심이 집중되고 있는 모형은 지도학습(supervised learning)모형인 처음 두 개의 모형이다. 따라서 본 논문에서는 지도학습모형의 가중치를 최적화하는 기본적인 방법인 오류역전파 알고리즘을 살펴본 뒤에 합성곱신경망과 순환신경망의 구조와 응용사례 등을 살펴보고자 한다. 본문에서 다루지 않은 모형인 심층신뢰신경망은 아직까지는 합성곱신경망 이나 순환신경망보다는 상대적으로 주목을 덜 받고 있다. 그러나 심층신뢰신경망은 CNN이나 RNN과는 달리 비지도학습(unsupervised learning)모형이며, 사람이나 동물은 관찰을 통해서 스스로 학습한다는 점에서 궁극적으로는 비지도학습모형이 더 많이 연구되어야 할 주제가 될 것이다.Deep learning model is a kind of neural networks that allows multiple hidden layers. There are various deep learning architectures such as convolutional neural networks, deep belief networks and recurrent neural networks. Those have been applied to fields like computer vision, automatic speech recognition, natural language processing, audio recognition and bioinformatics where they have been shown to produce state-of-the-art results on various tasks. Among those architectures, convolutional neural networks and recurrent neural networks are classified as the supervised learning model. And in recent years, those supervised learning models have gained more popularity than unsupervised learning models such as deep belief networks, because supervised learning models have shown fashionable applications in such fields mentioned above.
Deep learning models can be trained with backpropagation algorithm. Backpropagation is an abbreviation for “backward propagation of errors” and a common method of training artificial neural networks used in conjunction with an optimization method such as gradient descent. The method calculates the gradient of an error function with respect to all the weights in the network. The gradient is fed to the optimization method which in turn uses it to update the weights, in an attempt to minimize the error function.
Convolutional neural networks use a special architecture which is particularly well-adapted to classify images. Using this architecture makes convolutional networks fast to train. This, in turn, helps us train deep, muti-layer networks, which are very good at classifying images. These days, deep convolutional networks are used in most neural networks for image recognition. Convolutional neural networks use three basic ideas: local receptive fields, shared weights, and pooling. By local receptive fields, we mean that each neuron in the first(or any) hidden layer will be connected to a small region of the input(or previous layer’s) neurons. Shared weights mean that we’re going to use the same weights and bias for each of the local receptive field. This means that all the neurons in the hidden layer detect exactly the same feature, just at different locations in the input image. In addition to the convolutional layers just described, convolutional neural networks also contain pooling layers. Pooling layers are usually used immediately after convolutional layers. What the pooling layers do is to simplify the information in the output from the convolutional layer.
Recent convolutional network architectures have 10 to 20 hidden layers and billions of connections between units. Training deep learning networks has taken weeks several years ago, but thanks to progress in GPU and algorithm enhancement, training time has reduced to several hours.
Neural networks with time-varying behavior are known as recurrent neural networks or RNNs. A recurrent neural network is a class of artificial neural network where connections between units form a directed cycle. This creates an internal state of the network which allows it to exhibit dynamic temporal behavior. Unlike feedforward neural networks, RNNs can use their internal memory to process arbitrary sequences of inputs. Early RNN models turned out to be very difficult to train, harder even than deep feedforward networks. The reason is the unstable gradient problem such as vanishing gradient and exploding gradient. The gradient can get smaller and smaller as it is propagated back through layers. This makes learning in early layers extremely slow. The problem actually gets worse in RNNs, since gradients aren’t just propagated backward through layers, they’re propagated backward through time. If the network runs for a long time, that can make the gradient extremely unstable and hard to learn from. It has been possible to incorporate an idea known as long short-term memory units (LSTMs) into RNNs. LSTMs make it much easier to get good results when training RNNs, and many recent papers make use of LSTMs or related ideas.Deep learning, Convolutional neural networks, Recurrent neural networks, Error backpropagation algorithm"
356,딥러닝분석과 기술적 분석 지표를 이용한 한국 코스피주가지수 방향성 예측,이우식 /Lee Woosik,2017,한국데이터정보과학회,"2016년 3월 구글 (Google)의 바둑인공지능 알파고 (AlphaGo)가 이세돌 9단과의 바둑대결에서승리한 이후 다양한 분야에서 인공지능 사용에 대한 관심이 높아지고 있는 가운데 금융투자 분야에서도 인공지능과 투자자문 전문가의 합성어인 로보어드바이저 (Robo-Advisor)에 대한 관심이 높아지고 있다. 인공지능 (artificial intelligence)기반의 의사결정은 비용 절감은 물론 효과적인 의사결정을가능하게 한다는 점에서 큰 장점이 있다. 본 연구에서는 기술적 분석 (technical analysis) 지표와 딥러닝 (deep learning) 모형을 결합하여 한국 코스피 지수를 예측하는 모형을 개발하고 제시한 모형들의 예측력을 비교, 분석한다. 분석 결과 기술적 분석 지표에 딥러닝 알고리즘을 결합한 모형이 주가지수 방향성 예측 문제에 응용될 수 있음을 확인하였다. 향후 본 연구에서 제안된 기술적 분석 지표와딥러닝모형을 결합한 기법은 로보어드바이저서비스에 응용할 수 있는 일반화 가능성을 보여준다Since Google’s AlphaGo defeated a world champion of Go players in 2016, there have been many interests in the deep learning. In the financial sector, a Robo-Advisor using deep learning gains a significant attention, which builds and manages portfolios of financial instruments for investors.In this paper, we have proposed the a deep learning algorithm geared toward identification and forecast of the KOSPI index direction,and we also have compared the accuracy of the prediction.In an application of forecasting the financial market index direction, we have shown that the Robo-Advisor using deep learning has a significant effect on finance industry. The Robo-Advisor collects a massive data such as earnings statements, news reports and regulatory filings, analyzes those and recommends investors how to view market trends and identify the best time to purchase financial assets. On the other hand, the Robo-Advisor allows businesses to learn more about their customers, develop better marketing strategies, increase sales and decrease costs.Artificial intelligence, deep learning, FinTech, Robo-Advisor, technical analysis."
357,딥러닝의 효율적인 학습을 위한 학습데이터 선별 기법에 관한 연구,주경돈 /Gyoung-don Joo,2017,한국정보과학회,"빅데이터의 증가와 병렬 하드웨어 처리, 학습 알고리즘의 개선은 딥러닝의 발전을 가져왔으며 인공지능이 적용 가능한 영역을 획기적으로 확장시키고 있다. 하지만 이러한 대규모의 학습 데이터에 기반한 딥러닝은 학습시간이 증가되고, 더 나아가 학습 데이터의 구축을 위한 레이블링(labeling) 비용이 데이터 수에 비례하여 증가하게 된다. 따라서 본 논문에서는, 딥러닝의 정확도를 최대한 유지하면서 필요한 학습 데이터의 수를 줄일수 있도록, 전체 데이터에서 제한된 수의 학습 데이터의 후보를 선별하는 기법에 대하여 연구한다. 특히 기존의 능동 학습에 기반한 기법들이 학습 데이터의 수가 매우 적어질 경우 딥러닝의 정확도가 급격히 떨어지는 문제를 해결할 수 있는 새로운 기법을 제안하였으며, 제안된 기법으로 선별된 학습 데이터는 그 수의 변화에 대해 딥러닝이 일관되게 경쟁력있는 정확도를 가짐을 다양한 실험을 통하여 검증하였다.The increase of big data, parallel hardware processing and enhanced learning algorithms have led to the development of deep learning and are dramatically expanding the domain of applications based on artificial intelligence. However, such large-scale training data extends the learning time, and furthermore, the labeling cost for building the training set increases in proportion to the volume of data. Therefore, in this paper, we study how to select a limited number of candidates for training set from given data so as to reduce the number of training set while maintaining the accuracy of deep learning. Especially, we found that, with the training set selected by the previous work, the accuracy of deep learning drastically decreases when the number of training set is very small and so proposed new methods to resolve this fall-off in accuracy. Through experiments, we showed that deep learning consistently has the competitive accuracy with the training set selected by the proposed method regardless of the size of training set.Deep Learning, Train Set Selection, Active Learning, Gamma distribution"
358,딥러닝을 이용한 범용적 스테그아날리시스,김현재 /Hyunjae Kim,2017,한국정보과학회,"스테그아날리시스(Steganalysis)란 이미지 등 일반적인 자료에 암호화된 정보를 은닉하는 스테가노그래피(Steganography)에 대한 검출 및 분석 방법으로, 기계학습 기반 방법론을 포함한다. 기존 기계학습 기반 스테그아날리시스는 영상(Image)의 특징(Feature) 추출 및 모델링에 기반하며, 최근 딥러닝(Deep Learning)의 적용으로 검출 정확도가 큰 폭으로 향상되었다. 하지만 현존하는 스테그아날리시스 모델은 단일 스테가노그래피 기법에 대해 국한되어 있어 학습에 사용되지 않은 스테고(Stego) 이미지의 경우 검출이 불가능한 결정적 한계를 가진다. 본 연구에서는 다양한 스테가노그래피 기법으로 생성된 스테고이미지에 딥러닝을 적용하여 스테그아날리시스를 학습하는 범용적 모델을 제안한다. 다양한 실험을 통해 제안 기법의 효용성 및 가능성을 확인하고, 범용적 스테그아날리시스 모델이 각각에 특화된 검출 기법과 유사한 정확도로 스테고 이미지를 검출할 수 있음을 보인다.Steganalysis is to detect information hidden by steganography inside general data such as images. There are stegoanalysis techniques that use machine learning (ML). Existing ML approaches to steganalysis are based on extracting features from stego images and modeling them.
Recently deep learning-based methodologies have shown significant improvements in detection accuracy. However, all the existing methods, including deep learning-based ones, have a critical limitation in that they can only detect stego images that are created by a specific steganography method. In this paper, we propose a generalized steganalysis method that can model multiple types of stego images using deep learning. Through various experiments, we confirm the effectiveness of our approach and envision directions for future research. In particular, we show that our method can detect each type of steganography with the same level of accuracy as that of a steganalysis method dedicated to that type of steganography, thereby demonstrating the general applicability of our approach to multiple types of stego images.steganalysis, steganography, deep learning, generalized steganalysis model"
359,"딥러닝 프레임워크의 비교: 티아노, 텐서플로, CNTK를 중심으로",정여진 /Yeojin Chung,2017,한국지능정보시스템학회,"딥러닝 프레임워크의 대표적인 기능으로는 ‘자동미분’과 ‘GPU의 활용’ 등을 들 수 있다. 본 논문은 파이썬의 라이브러리 형태로 사용 가능한 프레임워크 중에서 구글의 텐서플로와 마이크로소프트의 CNTK, 그리고 텐서 플로의 원조라고 할 수 있는 티아노를 비교하였다. 본문에서는 자동미분의 개념과 GPU의 활용형태를 간단히 설명하고, 그 다음에 logistic regression을 실행하는 예를 통하여 각 프레임워크의 문법을 알아본 뒤에, 마지막으로 대표적인 딥러닝 응용인 CNN의 예제를 실행시켜보고 코딩의 편의성과 실행속도 등을 확인해 보았다. 그 결과, 편의성의 관점에서 보면 티아노가 가장 코딩 하기가 어렵고, CNTK와 텐서플로는 많은 부분이 비슷하게 추상화 되어 있어서 코딩이 비슷하지만 가중치와 편향을 직접 정의하느냐의 여부에서 차이를 보였다. 그리고 각프레임워크의 실행속도에 대한 평가는 ‘큰 차이는 없다’는 것이다. 텐서플로는 티아노에 비하여 속도가 느리다는 평가가 있어왔는데, 본 연구의 실험에 의하면, 비록 CNN 모형에 국한되었지만, 텐서플로가 아주 조금이지만빠른 것으로 나타났다. CNTK의 경우에도, 비록 실험환경이 달랐지만, 실험환경의 차이에 의한 속도의 차이의 편차범위 이내에 있는 것으로 판단이 되었다. 본 연구에서는 세 종류의 딥러닝 프레임워크만을 살펴보았는데,위키피디아에 따르면 딥러닝 프레임워크의 종류는 12가지가 있으며, 각 프레임워크의 특징을 15가지 속성으로구분하여 차이를 특정하고 있다. 그 많은 속성 중에서 사용자의 입장에서 볼 때 중요한 속성은 어떤 언어(파이썬, C++, Java, 등)로 사용가능한지, 어떤 딥러닝 모형에 대한 라이브러리가 잘 구현되어 있는지 등일 것이다. 그리고 사용자가 대규모의 딥러닝 모형을 구축한다면, 다중 GPU 혹은 다중 서버를 지원하는지의 여부도 중요할 것이다. 또한 딥러닝 모형을 처음 학습하는 경우에는 사용설명서가 많은지 예제 프로그램이 많은지 여부도 중요한 기준이 될 것이다.The deep learning framework is software designed to help develop deep learning models. Some of its important functions include “automatic differentiation” and “utilization of GPU”. The list of popular deep learning framework includes Caffe (BVLC) and Theano (University of Montreal). And recently, Microsoft's deep learning framework, Microsoft Cognitive Toolkit, was released as open-source license, following Google’s Tensorflow a year earlier. The early deep learning frameworks have been developed mainly for research at universities. Beginning with the inception of Tensorflow, however, it seems that companies such as Microsoft and Facebook have started to join the competition of framework development. Given the trend, Google and other companies are expected to continue investing in the deep learning framework to bring forward the initiative in the artificial intelligence business. From this point of view, we think it is a good time to compare some of deep learning frameworks. So we compare three deep learning frameworks which can be used as a Python library. Those are Google's Tensorflow, Microsoft’s CNTK, and Theano which is sort of a predecessor of the preceding two.
The most common and important function of deep learning frameworks is the ability to perform automatic differentiation. Basically all the mathematical expressions of deep learning models can be represented as computational graphs, which consist of nodes and edges. Partial derivatives on each edge of a computational graph can then be obtained. With the partial derivatives, we can let software compute differentiation of any node with respect to any variable by utilizing chain rule of Calculus.
First of all, the convenience of coding is in the order of CNTK, Tensorflow, and Theano. The criterion is simply based on the lengths of the codes and the learning curve and the ease of coding are not the main concern. According to the criteria, Theano was the most difficult to implement with, and CNTK and Tensorflow were somewhat easier. With Tensorflow, we need to define weight variables and biases explicitly. The reason that CNTK and Tensorflow are easier to implement with is that those frameworks provide us with more abstraction than Theano. We, however, need to mention that low-level coding is not always bad. It gives us flexibility of coding. With the low-level coding such as in Theano, we can implement and test any new deep learning models or any new search methods that we can think of.
The assessment of the execution speed of each framework is that there is not meaningful difference. According to the experiment, execution speeds of Theano and Tensorflow are very similar, although the experiment was limited to a CNN model. In the case of CNTK, the experimental environment was not maintained as the same. The code written in CNTK has to be run in PC environment without GPU where codes execute as much as 50 times slower than with GPU. But we concluded that the difference of execution speed was within the range of variation caused by the different hardware setup.
In this study, we compared three types of deep learning framework: Theano, Tensorflow, and CNTK. According to Wikipedia, there are 12 available deep learning frameworks. And 15 different attributes differentiate each framework. Some of the important attributes would include interface language (Python, C ++, Java, etc.) and the availability of libraries on various deep learning models such as CNN, RNN, DBN, and etc. And if a user implements a large scale deep learning model, it will also be important to support multiple GPU or multiple servers. Also, if you are learning the deep learning model, it would also be important if there are enough examples and references.deep learning framework, Theano, TensorFlow, CNTK, computational graph, CIFAR-10"
360,딥러닝을 이용한 주변 무선단말 파악방안,이웅섭 /Lee， Woong-sup,2018,한국정보통신학회,"최근 단말-대-단말(Device-to-device, D2D) 통신기술이 차세대 무선통신시스템의 핵심기술로 큰 관심을 받고 있다. 이러한 단말간 통신에서는 자신의 주변에 어떠한 단말이 있는지 파악하는 주변단말 탐색(Neighbor discovery)이매우 중요하다. 본 논문에서는 최근 큰 관심을 받고 있는 딥러닝(Deep learning) 기술을 활용하여 단말간 통신에서 주변단말을 파악하는 방안에 대해서 제안한다. 제안 방안은 기존의 방안과 달리 무선채널의 공간적 연관성을 이용하여단말간의 신호 전송 없이 단말이 기지국으로 전송하는 상향링크 파일럿 신호를 기반으로 주변 단말을 찾고 따라서기존의 방식에 비해 신호전송 복잡도(signaling complexity)를 크게 줄일 수 있다. 또한 제안 방안에서는 떨어져 있는거리에 따라서 주변 단말을 분류 가능하여 기존 방안에 비해서 좀 더 세밀한 단말 탐색이 가능하다. 마지막으로 본 논문에서는 tensorflow를 이용한 컴퓨터 시뮬레이션을 통해 제안 방안의 성능을 검증하였다.Recently, the device-to-device (D2D) communication has been conceived as the key technology for the next-generationmobile communication systems. The neighbor discovery in which the nearby users are found, is essential for the properoperation of the D2D communication. In this paper, we propose new neighbor discovery scheme based on deep learningtechnology which has gained a lot of attention recently. In the proposed scheme, the neighboring users can be foundusing the uplink pilot transmission of users only, unlike conventional neighbor discovery schemes in which direct pilotcommunication among users is required, such that the signaling overhead can be greatly reduced in our proposed scheme.
Moreover, the neighbors with different proximity can also be classified accordingly which enables more accurate neighbordiscovery compared to the conventional schemes. The performance of our proposed scheme is verified through thetensorflow-based computer simulations.
키워드 : 딥러닝, 주변단말 탐색, 단말-대-단말 통신Deep learning, Neighbor discovery, Device-to-device communication, Wireless channel correlation"
361,신뢰성있는 딥러닝 기반 분석 모델을 참조하기 위한 딥러닝 기술 언어,문종혁 /Jong Hyeok Mun,2021,한국정보처리학회,"최근 딥러닝은 하드웨어 성능이 향상됨에 따라 자연어 처리, 영상 인식 등의 다양한 기술에 접목되어 활용되고 있다. 이러한 기술들을 활용해 지능형교통 시스템(ITS), 스마트홈, 헬스케어 등의 산업분야에서 데이터를 분석하여 고속도로 속도위반 차량 검출, 에너지 사용량 제어, 응급상황 등과 같은고품질의 서비스를 제공하며, 고품질의 서비스를 제공하기 위해서는 정확도가 향상된 딥러닝 모델이 적용되어야 한다. 이를 위해 서비스 환경의 데이터를분석하기 위한 딥러닝 모델을 개발할 때, 개발자는 신뢰성이 검증된 최신의 딥러닝 모델을 적용할 수 있어야 한다. 이는 개발자가 참조하는 딥러닝 모델에적용된 학습 데이터셋의 정확도를 측정하여 검증할 수 있다. 이러한 검증을 위해서 개발자는 학습 데이터셋, 딥러닝의 계층구조 및 개발 환경 등과 같은내용을 포함하는 딥러닝 모델을 문서화하여 적용하기 위한 구조적인 정보가 필요하다. 본 논문에서는 신뢰성있는 딥러닝 기반 데이터 분석 모델을 참조하기위한 딥러닝 기술 언어를 제안한다. 제안하는 기술 언어는 신뢰성 있는 딥러닝 모델을 개발하는데 필요한 학습데이터셋, 개발 환경 및 설정 등의 정보와더불어 딥러닝 모델의 계층구조를 표현할 수 있다. 제안하는 딥러닝 기술 언어를 이용하여 개발자는 지능형 교통 시스템에서 참조하는 분석 모델의정확도를 검증할 수 있다. 실험에서는 제안하는 언어의 유효성을 검증하기 위해, 번호판 인식 모델을 중심으로 딥러닝 기술 문서의 적용과정을 보인다With the recent advancements of deep learning, companies such as smart home, healthcare, and intelligent transportation systems areutilizing its functionality to provide high-quality services for vehicle detection, emergency situation detection, and controlling energyconsumption. To provide reliable services in such sensitive systems, deep learning models are required to have high accuracy. In orderto develop a deep learning model for analyzing previously mentioned services, developers should utilize the state of the art deep learningmodels that have already been verified for higher accuracy. The developers can verify the accuracy of the referenced model by validatingthe model on the dataset. For this validation, the developer needs structural information to document and apply deep learning models,including metadata such as learning dataset, network architecture, and development environments. In this paper, we propose a descriptionlanguage that represents the network architecture of the deep learning model along with its metadata that are necessary to develop adeep learning model. Through the proposed description language, developers can easily verify the accuracy of the referenced deep learningmodel. Our experiments demonstrate the application scenario of a deep learning description document that focuses on the license platerecognition for the detection of illegally parked vehicles.Trusted Deep Learning, Model Reference, Deep Learning Description Language, Traffic Situation Analysis Model"
362,딥러닝 분석을 이용한 중국 역내·외 위안화 변동성 예측,이우식 /Lee Woosik,2016,한국데이터정보과학회,"2008년 글로벌 금융위기 이후 중국은 위안화 국제화의 점진적 추진을 진행하면서 중국상하이 외환시장과 중국홍콩 외환시장에서 거래되는 통화인 역내위안화와 역외위안화를 형성시켰다. 본 연구는 위안화 국제화와 점진적인 중국 자본계정 개방에 따라 급변하는 외환시장상황의 변동성을 정확하게 파악하기 위해서 GARCH모형 (일반화된 자기회귀 조건부이분산성모형)에 다단계인공신경망을 결합한 MLP-GARCH 모형과 GARCH모형과 기계학습의 일종인 딥러닝 (deep learning)을 통합한 DL-GARCH을 가지고 위안화 변동성예측을 비교 실험과 분석을 하였다. 비교분석 결과 DLGARCH 모형은 MLP-GARCH보다 모형 위안화 역내·외 환율변동성 예측 면에서 더욱 더 개선된예측값을 제공하였다. 그래서 이분산시계열모형을 딥러닝과 결합한 DL-GARCH 모형은 시계열의환율 변동성 예측 문제에 딥러닝을 응용할 수 있음을 확인하였다. 향후 이분산시계열과 결합된 딥러닝 모형은 다른 금융시계열 데이터에 응용하여 그 일반화 가능성을 높일 수 있을 것이다.The People’s Republic of China has vigorously been pursuing the internationalization of the Chinese Yuan or Renminbi after the financial crisis of 2008. In this view, an abrupt increase of use of the Chinese Yuan in the onshore and offshore markets are important milestones to be one of important currencies. One of the most frequently used methods to forecast volatility is GARCH model. Since a prediction error of the GARCH model has been reported quite high, a lot of efforts have been made to improve forecasting capability of the GARCH model. In this paper, we have proposed MLPGARCH and a DL-GARCH by employing Artificial Neural Network to the GARCH. In an application to forecasting Chinese Yuan volatility, we have successfully shown their overall outperformance in forecasting over the GARCH.Deep learning, DL-GARCH, GARCH, The Chinese Yuan, volatility."
363,딥러닝 기법을 이용한 내일강수 예측,하지훈,2016,한국지능시스템학회,"정확한 강수예측을 위해서는 예측인자 선정과 예측방법에 대한 선택이 매우 중요하다. 최근에는 강수예측 방법으로기계학습 기법이 많이 사용되고 있으며, 그 중에서도 특히 인공신경망을 사용한 강수예측 방법은 좋은 성능을 보였다. 본논문에서는 딥러닝 기법 중 하나인 DBN(deep belief network)를 이용한 새로운 강수예측 방법을 제안한다. DBN는 비지도사전 학습을 통해 초기 가중치를 설정하여 기존 인공신경망의 문제점을 보완한다. 예측인자로는 기온, 전일-전주 강수일, 태양과 달 궤도 관련 자료를 선정하였다. 기온과 전일-전주 강수일은 서울에서의 1974년부터 2013년까지 총 40년간의AWS(automatic weather system) 관측 자료를 사용하였고, 태양과 달의 궤도 관련 자료는 서울을 중심으로 계산한 결과를사용하였다. 전체 기간에서 일부는 학습 자료로 사용하여 예측모델을 생성하였고, 나머지를 생성한 모델의 검증 자료로사용하였다. 모델 검증 결과로 나온 예측값들은 확률값을 가지며 임계치를 이용하여 강수유무를 판별하였다. 강수 정확도의척도로 양분예보기법 중 CSI(critical successive index)와 Bias(frequency bias)를 계산하였다. 이를 통해 DBN와 MLP(multilayer perceptron)의 성능을 비교한 결과 DBN의 강수 예측 정확도가 높았고, 수행속도 또한 2배 이상 빨랐다.For accurate precipitation forecasts the choice of weather faoctrs and prediction method is very important. Recently, machinele arning has been widely used for forecasting precipitation, and artificial neural network, one of machine learning techniques, showed good performance. In this paper, we suggest a new method for forecasting precipitation using DBN, one of deep learning techniques.
DBN has an advantage that initial weights are set by unsupervised learning, so this compensates for the defects of artificial neural networks. We used past precipitation, temperature, and the parameters of the sun and moon's motion as features for forecasting precipitation. The dataset consists of observation data which had been measured for 40 years from AWS in Seoul. Experiments were based on 8-fold cross validation. As a result of estimation, we got probabilities of test dataset, so threshold was used for the decision of precipitation. CSI and Bias were used for indicating the precision of precipitation. Our experimental results showed that DBN performed better than MLP.Deep learning, Deep belief network, Precipitation, Forecast"
364,딥러닝 신경망모형을 이용한 실시간 교통정보수집 연구,김대현 /KIM DAE HYON,2016,사단법인 인문사회과학기술융합학회,"비디오 기반의 실시간 교통정보수집 시스템은 자율주행을 포함한 지능형교통체계의 주요 서브시스템중 하나이며, 우수한 예측(추정)력 때문에 핵심 알고리즘으로 인공신경망 모형이 주로 사용되고 있다. 특히, 최근 과학 학술지 네이처에 구글 딥마인드의 알파고가 공개되면서 딥러닝과 인공지능에 대한 전 세계적인 관심이 집중되고 있다. 최근까지 가장 널리 사용된 인공신경망모형은 역전파 모형이었으나 최근 컴퓨터 하드웨어 및 알고리즘의 새로운 개발과 더불어 딥러닝 알고리즘을 이용한 딥인공신경망 모형에 대한 관심이 높아지고 있다. 그러나 아직 딥러닝 인공신경망에 대한 연구는 초기단계에 있는 실정이며 적용분야도 극히 한정되어 있다. 본 연구에서는 실시간 도로교통정보 수집을 위한 영상처리분야에서 딥러닝 알고리즘을 활용하고 실험결과를 통하여 향후 교통정보수집‧분석 분야에서의 딥러닝 인공신경망 모형의 적용가능성을 살펴보고자 한다. 특히, 본 연구에서 도출한 딥러닝 인공신경망모형의 실험결과를 기존 역전파 모형에 의한 실험결과와 비교하여 모형의 우수성을 비교‧평가하고자 한다.Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Normally, artificial neural networks have been used in real-time applications for collecting traffic information because of their remarkable performance in terms of predictive accuracy and real-time computing. Especially, the recent publication of a paper in the journal Nature and the success of Google DeepMind’s world-beating Go program AlphaGo caused significant increases of the public interest in artificial intelligence. Even though various types of learning models have been proposed, the Backpropagation has been the most popular neural network model. However, the Deep learning neural network model has attracted much interest recently. In this study, the Deep leaning neural network will be applied in image processing techniques for the real-time traffic information collection to investigate the performance of Deep learning algorithm in traffic engineering fields. In addition, the experimental results from Deep learing algorithm will be compared with the results from Backpropagation model to evaluate and propose better performing model.Intelligent Transportation Systems (ITS), artificial neural networks, Deep learning, traffic information, image processing"
365,딥러닝의 모형과 응용사례,안성만 /SungMahn Ahn,2016,한국지능정보시스템학회,"딥러닝은 인공신경망(neural network)이라는 인공지능분야의 모형이 발전된 형태로서, 계층구조로 이루어진 인공신경망의 내부계층(hidden layer)이 여러 단계로 이루어진 구조이다. 딥러닝에서의 주요 모형은 합성곱신경망(convolutional neural network), 순환신경망(recurrent neural network), 그리고 심층신뢰신경망(deep belief network)의 세가지라고 할 수있다. 그 중에서 현재 흥미로운 연구가 많이 발표되어서 관심이 집중되고 있는 모형은 지도학습(supervised learning)모형인 처음 두 개의 모형이다. 따라서 본 논문에서는 지도학습모형의 가중치를 최적화하는 기본적인 방법인 오류역전파 알고리즘을 살펴본 뒤에 합성곱신경망과 순환신경망의 구조와 응용사례 등을 살펴보고자 한다. 본문에서 다루지 않은 모형인 심층신뢰신경망은 아직까지는 합성곱신경망 이나 순환신경망보다는 상대적으로 주목을 덜 받고 있다. 그러나 심층신뢰신경망은 CNN이나 RNN과는 달리 비지도학습(unsupervised learning)모형이며, 사람이나 동물은 관찰을 통해서 스스로 학습한다는 점에서 궁극적으로는 비지도학습모형이 더 많이 연구되어야 할 주제가 될 것이다.Deep learning model is a kind of neural networks that allows multiple hidden layers. There are various deep learning architectures such as convolutional neural networks, deep belief networks and recurrent neural networks. Those have been applied to fields like computer vision, automatic speech recognition, natural language processing, audio recognition and bioinformatics where they have been shown to produce state-of-the-art results on various tasks. Among those architectures, convolutional neural networks and recurrent neural networks are classified as the supervised learning model. And in recent years, those supervised learning models have gained more popularity than unsupervised learning models such as deep belief networks, because supervised learning models have shown fashionable applications in such fields mentioned above.
Deep learning models can be trained with backpropagation algorithm. Backpropagation is an abbreviation for “backward propagation of errors” and a common method of training artificial neural networks used in conjunction with an optimization method such as gradient descent. The method calculates the gradient of an error function with respect to all the weights in the network. The gradient is fed to the optimization method which in turn uses it to update the weights, in an attempt to minimize the error function.
Convolutional neural networks use a special architecture which is particularly well-adapted to classify images. Using this architecture makes convolutional networks fast to train. This, in turn, helps us train deep, muti-layer networks, which are very good at classifying images. These days, deep convolutional networks are used in most neural networks for image recognition. Convolutional neural networks use three basic ideas: local receptive fields, shared weights, and pooling. By local receptive fields, we mean that each neuron in the first(or any) hidden layer will be connected to a small region of the input(or previous layer’s) neurons. Shared weights mean that we’re going to use the same weights and bias for each of the local receptive field. This means that all the neurons in the hidden layer detect exactly the same feature, just at different locations in the input image. In addition to the convolutional layers just described, convolutional neural networks also contain pooling layers. Pooling layers are usually used immediately after convolutional layers. What the pooling layers do is to simplify the information in the output from the convolutional layer.
Recent convolutional network architectures have 10 to 20 hidden layers and billions of connections between units. Training deep learning networks has taken weeks several years ago, but thanks to progress in GPU and algorithm enhancement, training time has reduced to several hours.
Neural networks with time-varying behavior are known as recurrent neural networks or RNNs. A recurrent neural network is a class of artificial neural network where connections between units form a directed cycle. This creates an internal state of the network which allows it to exhibit dynamic temporal behavior. Unlike feedforward neural networks, RNNs can use their internal memory to process arbitrary sequences of inputs. Early RNN models turned out to be very difficult to train, harder even than deep feedforward networks. The reason is the unstable gradient problem such as vanishing gradient and exploding gradient. The gradient can get smaller and smaller as it is propagated back through layers. This makes learning in early layers extremely slow. The problem actually gets worse in RNNs, since gradients aren’t just propagated backward through layers, they’re propagated backward through time. If the network runs for a long time, that can make the gradient extremely unstable and hard to learn from. It has been possible to incorporate an idea known as long short-term memory units (LSTMs) into RNNs. LSTMs make it much easier to get good results when training RNNs, and many recent papers make use of LSTMs or related ideas.Deep learning, Convolutional neural networks, Recurrent neural networks, Error backpropagation algorithm"
366,딥러닝분석과 기술적 분석 지표를 이용한 한국 코스피주가지수 방향성 예측,이우식 /Lee Woosik,2017,한국데이터정보과학회,"2016년 3월 구글 (Google)의 바둑인공지능 알파고 (AlphaGo)가 이세돌 9단과의 바둑대결에서승리한 이후 다양한 분야에서 인공지능 사용에 대한 관심이 높아지고 있는 가운데 금융투자 분야에서도 인공지능과 투자자문 전문가의 합성어인 로보어드바이저 (Robo-Advisor)에 대한 관심이 높아지고 있다. 인공지능 (artificial intelligence)기반의 의사결정은 비용 절감은 물론 효과적인 의사결정을가능하게 한다는 점에서 큰 장점이 있다. 본 연구에서는 기술적 분석 (technical analysis) 지표와 딥러닝 (deep learning) 모형을 결합하여 한국 코스피 지수를 예측하는 모형을 개발하고 제시한 모형들의 예측력을 비교, 분석한다. 분석 결과 기술적 분석 지표에 딥러닝 알고리즘을 결합한 모형이 주가지수 방향성 예측 문제에 응용될 수 있음을 확인하였다. 향후 본 연구에서 제안된 기술적 분석 지표와딥러닝모형을 결합한 기법은 로보어드바이저서비스에 응용할 수 있는 일반화 가능성을 보여준다Since Google’s AlphaGo defeated a world champion of Go players in 2016, there have been many interests in the deep learning. In the financial sector, a Robo-Advisor using deep learning gains a significant attention, which builds and manages portfolios of financial instruments for investors.In this paper, we have proposed the a deep learning algorithm geared toward identification and forecast of the KOSPI index direction,and we also have compared the accuracy of the prediction.In an application of forecasting the financial market index direction, we have shown that the Robo-Advisor using deep learning has a significant effect on finance industry. The Robo-Advisor collects a massive data such as earnings statements, news reports and regulatory filings, analyzes those and recommends investors how to view market trends and identify the best time to purchase financial assets. On the other hand, the Robo-Advisor allows businesses to learn more about their customers, develop better marketing strategies, increase sales and decrease costs.Artificial intelligence, deep learning, FinTech, Robo-Advisor, technical analysis."
367,딥러닝의 효율적인 학습을 위한 학습데이터 선별 기법에 관한 연구,주경돈 /Gyoung-don Joo,2017,한국정보과학회,"빅데이터의 증가와 병렬 하드웨어 처리, 학습 알고리즘의 개선은 딥러닝의 발전을 가져왔으며 인공지능이 적용 가능한 영역을 획기적으로 확장시키고 있다. 하지만 이러한 대규모의 학습 데이터에 기반한 딥러닝은 학습시간이 증가되고, 더 나아가 학습 데이터의 구축을 위한 레이블링(labeling) 비용이 데이터 수에 비례하여 증가하게 된다. 따라서 본 논문에서는, 딥러닝의 정확도를 최대한 유지하면서 필요한 학습 데이터의 수를 줄일수 있도록, 전체 데이터에서 제한된 수의 학습 데이터의 후보를 선별하는 기법에 대하여 연구한다. 특히 기존의 능동 학습에 기반한 기법들이 학습 데이터의 수가 매우 적어질 경우 딥러닝의 정확도가 급격히 떨어지는 문제를 해결할 수 있는 새로운 기법을 제안하였으며, 제안된 기법으로 선별된 학습 데이터는 그 수의 변화에 대해 딥러닝이 일관되게 경쟁력있는 정확도를 가짐을 다양한 실험을 통하여 검증하였다.The increase of big data, parallel hardware processing and enhanced learning algorithms have led to the development of deep learning and are dramatically expanding the domain of applications based on artificial intelligence. However, such large-scale training data extends the learning time, and furthermore, the labeling cost for building the training set increases in proportion to the volume of data. Therefore, in this paper, we study how to select a limited number of candidates for training set from given data so as to reduce the number of training set while maintaining the accuracy of deep learning. Especially, we found that, with the training set selected by the previous work, the accuracy of deep learning drastically decreases when the number of training set is very small and so proposed new methods to resolve this fall-off in accuracy. Through experiments, we showed that deep learning consistently has the competitive accuracy with the training set selected by the proposed method regardless of the size of training set.Deep Learning, Train Set Selection, Active Learning, Gamma distribution"
368,딥러닝을 이용한 범용적 스테그아날리시스,김현재 /Hyunjae Kim,2017,한국정보과학회,"스테그아날리시스(Steganalysis)란 이미지 등 일반적인 자료에 암호화된 정보를 은닉하는 스테가노그래피(Steganography)에 대한 검출 및 분석 방법으로, 기계학습 기반 방법론을 포함한다. 기존 기계학습 기반 스테그아날리시스는 영상(Image)의 특징(Feature) 추출 및 모델링에 기반하며, 최근 딥러닝(Deep Learning)의 적용으로 검출 정확도가 큰 폭으로 향상되었다. 하지만 현존하는 스테그아날리시스 모델은 단일 스테가노그래피 기법에 대해 국한되어 있어 학습에 사용되지 않은 스테고(Stego) 이미지의 경우 검출이 불가능한 결정적 한계를 가진다. 본 연구에서는 다양한 스테가노그래피 기법으로 생성된 스테고이미지에 딥러닝을 적용하여 스테그아날리시스를 학습하는 범용적 모델을 제안한다. 다양한 실험을 통해 제안 기법의 효용성 및 가능성을 확인하고, 범용적 스테그아날리시스 모델이 각각에 특화된 검출 기법과 유사한 정확도로 스테고 이미지를 검출할 수 있음을 보인다.Steganalysis is to detect information hidden by steganography inside general data such as images. There are stegoanalysis techniques that use machine learning (ML). Existing ML approaches to steganalysis are based on extracting features from stego images and modeling them.
Recently deep learning-based methodologies have shown significant improvements in detection accuracy. However, all the existing methods, including deep learning-based ones, have a critical limitation in that they can only detect stego images that are created by a specific steganography method. In this paper, we propose a generalized steganalysis method that can model multiple types of stego images using deep learning. Through various experiments, we confirm the effectiveness of our approach and envision directions for future research. In particular, we show that our method can detect each type of steganography with the same level of accuracy as that of a steganalysis method dedicated to that type of steganography, thereby demonstrating the general applicability of our approach to multiple types of stego images.steganalysis, steganography, deep learning, generalized steganalysis model"
369,"딥러닝 프레임워크의 비교: 티아노, 텐서플로, CNTK를 중심으로",정여진 /Yeojin Chung,2017,한국지능정보시스템학회,"딥러닝 프레임워크의 대표적인 기능으로는 ‘자동미분’과 ‘GPU의 활용’ 등을 들 수 있다. 본 논문은 파이썬의 라이브러리 형태로 사용 가능한 프레임워크 중에서 구글의 텐서플로와 마이크로소프트의 CNTK, 그리고 텐서 플로의 원조라고 할 수 있는 티아노를 비교하였다. 본문에서는 자동미분의 개념과 GPU의 활용형태를 간단히 설명하고, 그 다음에 logistic regression을 실행하는 예를 통하여 각 프레임워크의 문법을 알아본 뒤에, 마지막으로 대표적인 딥러닝 응용인 CNN의 예제를 실행시켜보고 코딩의 편의성과 실행속도 등을 확인해 보았다. 그 결과, 편의성의 관점에서 보면 티아노가 가장 코딩 하기가 어렵고, CNTK와 텐서플로는 많은 부분이 비슷하게 추상화 되어 있어서 코딩이 비슷하지만 가중치와 편향을 직접 정의하느냐의 여부에서 차이를 보였다. 그리고 각프레임워크의 실행속도에 대한 평가는 ‘큰 차이는 없다’는 것이다. 텐서플로는 티아노에 비하여 속도가 느리다는 평가가 있어왔는데, 본 연구의 실험에 의하면, 비록 CNN 모형에 국한되었지만, 텐서플로가 아주 조금이지만빠른 것으로 나타났다. CNTK의 경우에도, 비록 실험환경이 달랐지만, 실험환경의 차이에 의한 속도의 차이의 편차범위 이내에 있는 것으로 판단이 되었다. 본 연구에서는 세 종류의 딥러닝 프레임워크만을 살펴보았는데,위키피디아에 따르면 딥러닝 프레임워크의 종류는 12가지가 있으며, 각 프레임워크의 특징을 15가지 속성으로구분하여 차이를 특정하고 있다. 그 많은 속성 중에서 사용자의 입장에서 볼 때 중요한 속성은 어떤 언어(파이썬, C++, Java, 등)로 사용가능한지, 어떤 딥러닝 모형에 대한 라이브러리가 잘 구현되어 있는지 등일 것이다. 그리고 사용자가 대규모의 딥러닝 모형을 구축한다면, 다중 GPU 혹은 다중 서버를 지원하는지의 여부도 중요할 것이다. 또한 딥러닝 모형을 처음 학습하는 경우에는 사용설명서가 많은지 예제 프로그램이 많은지 여부도 중요한 기준이 될 것이다.The deep learning framework is software designed to help develop deep learning models. Some of its important functions include “automatic differentiation” and “utilization of GPU”. The list of popular deep learning framework includes Caffe (BVLC) and Theano (University of Montreal). And recently, Microsoft's deep learning framework, Microsoft Cognitive Toolkit, was released as open-source license, following Google’s Tensorflow a year earlier. The early deep learning frameworks have been developed mainly for research at universities. Beginning with the inception of Tensorflow, however, it seems that companies such as Microsoft and Facebook have started to join the competition of framework development. Given the trend, Google and other companies are expected to continue investing in the deep learning framework to bring forward the initiative in the artificial intelligence business. From this point of view, we think it is a good time to compare some of deep learning frameworks. So we compare three deep learning frameworks which can be used as a Python library. Those are Google's Tensorflow, Microsoft’s CNTK, and Theano which is sort of a predecessor of the preceding two.
The most common and important function of deep learning frameworks is the ability to perform automatic differentiation. Basically all the mathematical expressions of deep learning models can be represented as computational graphs, which consist of nodes and edges. Partial derivatives on each edge of a computational graph can then be obtained. With the partial derivatives, we can let software compute differentiation of any node with respect to any variable by utilizing chain rule of Calculus.
First of all, the convenience of coding is in the order of CNTK, Tensorflow, and Theano. The criterion is simply based on the lengths of the codes and the learning curve and the ease of coding are not the main concern. According to the criteria, Theano was the most difficult to implement with, and CNTK and Tensorflow were somewhat easier. With Tensorflow, we need to define weight variables and biases explicitly. The reason that CNTK and Tensorflow are easier to implement with is that those frameworks provide us with more abstraction than Theano. We, however, need to mention that low-level coding is not always bad. It gives us flexibility of coding. With the low-level coding such as in Theano, we can implement and test any new deep learning models or any new search methods that we can think of.
The assessment of the execution speed of each framework is that there is not meaningful difference. According to the experiment, execution speeds of Theano and Tensorflow are very similar, although the experiment was limited to a CNN model. In the case of CNTK, the experimental environment was not maintained as the same. The code written in CNTK has to be run in PC environment without GPU where codes execute as much as 50 times slower than with GPU. But we concluded that the difference of execution speed was within the range of variation caused by the different hardware setup.
In this study, we compared three types of deep learning framework: Theano, Tensorflow, and CNTK. According to Wikipedia, there are 12 available deep learning frameworks. And 15 different attributes differentiate each framework. Some of the important attributes would include interface language (Python, C ++, Java, etc.) and the availability of libraries on various deep learning models such as CNN, RNN, DBN, and etc. And if a user implements a large scale deep learning model, it will also be important to support multiple GPU or multiple servers. Also, if you are learning the deep learning model, it would also be important if there are enough examples and references.deep learning framework, Theano, TensorFlow, CNTK, computational graph, CIFAR-10"
370,딥러닝을 이용한 주변 무선단말 파악방안,이웅섭 /Lee， Woong-sup,2018,한국정보통신학회,"최근 단말-대-단말(Device-to-device, D2D) 통신기술이 차세대 무선통신시스템의 핵심기술로 큰 관심을 받고 있다. 이러한 단말간 통신에서는 자신의 주변에 어떠한 단말이 있는지 파악하는 주변단말 탐색(Neighbor discovery)이매우 중요하다. 본 논문에서는 최근 큰 관심을 받고 있는 딥러닝(Deep learning) 기술을 활용하여 단말간 통신에서 주변단말을 파악하는 방안에 대해서 제안한다. 제안 방안은 기존의 방안과 달리 무선채널의 공간적 연관성을 이용하여단말간의 신호 전송 없이 단말이 기지국으로 전송하는 상향링크 파일럿 신호를 기반으로 주변 단말을 찾고 따라서기존의 방식에 비해 신호전송 복잡도(signaling complexity)를 크게 줄일 수 있다. 또한 제안 방안에서는 떨어져 있는거리에 따라서 주변 단말을 분류 가능하여 기존 방안에 비해서 좀 더 세밀한 단말 탐색이 가능하다. 마지막으로 본 논문에서는 tensorflow를 이용한 컴퓨터 시뮬레이션을 통해 제안 방안의 성능을 검증하였다.Recently, the device-to-device (D2D) communication has been conceived as the key technology for the next-generationmobile communication systems. The neighbor discovery in which the nearby users are found, is essential for the properoperation of the D2D communication. In this paper, we propose new neighbor discovery scheme based on deep learningtechnology which has gained a lot of attention recently. In the proposed scheme, the neighboring users can be foundusing the uplink pilot transmission of users only, unlike conventional neighbor discovery schemes in which direct pilotcommunication among users is required, such that the signaling overhead can be greatly reduced in our proposed scheme.
Moreover, the neighbors with different proximity can also be classified accordingly which enables more accurate neighbordiscovery compared to the conventional schemes. The performance of our proposed scheme is verified through thetensorflow-based computer simulations.
키워드 : 딥러닝, 주변단말 탐색, 단말-대-단말 통신Deep learning, Neighbor discovery, Device-to-device communication, Wireless channel correlation"
371,신뢰성있는 딥러닝 기반 분석 모델을 참조하기 위한 딥러닝 기술 언어,문종혁 /Jong Hyeok Mun,2021,한국정보처리학회,"최근 딥러닝은 하드웨어 성능이 향상됨에 따라 자연어 처리, 영상 인식 등의 다양한 기술에 접목되어 활용되고 있다. 이러한 기술들을 활용해 지능형교통 시스템(ITS), 스마트홈, 헬스케어 등의 산업분야에서 데이터를 분석하여 고속도로 속도위반 차량 검출, 에너지 사용량 제어, 응급상황 등과 같은고품질의 서비스를 제공하며, 고품질의 서비스를 제공하기 위해서는 정확도가 향상된 딥러닝 모델이 적용되어야 한다. 이를 위해 서비스 환경의 데이터를분석하기 위한 딥러닝 모델을 개발할 때, 개발자는 신뢰성이 검증된 최신의 딥러닝 모델을 적용할 수 있어야 한다. 이는 개발자가 참조하는 딥러닝 모델에적용된 학습 데이터셋의 정확도를 측정하여 검증할 수 있다. 이러한 검증을 위해서 개발자는 학습 데이터셋, 딥러닝의 계층구조 및 개발 환경 등과 같은내용을 포함하는 딥러닝 모델을 문서화하여 적용하기 위한 구조적인 정보가 필요하다. 본 논문에서는 신뢰성있는 딥러닝 기반 데이터 분석 모델을 참조하기위한 딥러닝 기술 언어를 제안한다. 제안하는 기술 언어는 신뢰성 있는 딥러닝 모델을 개발하는데 필요한 학습데이터셋, 개발 환경 및 설정 등의 정보와더불어 딥러닝 모델의 계층구조를 표현할 수 있다. 제안하는 딥러닝 기술 언어를 이용하여 개발자는 지능형 교통 시스템에서 참조하는 분석 모델의정확도를 검증할 수 있다. 실험에서는 제안하는 언어의 유효성을 검증하기 위해, 번호판 인식 모델을 중심으로 딥러닝 기술 문서의 적용과정을 보인다With the recent advancements of deep learning, companies such as smart home, healthcare, and intelligent transportation systems areutilizing its functionality to provide high-quality services for vehicle detection, emergency situation detection, and controlling energyconsumption. To provide reliable services in such sensitive systems, deep learning models are required to have high accuracy. In orderto develop a deep learning model for analyzing previously mentioned services, developers should utilize the state of the art deep learningmodels that have already been verified for higher accuracy. The developers can verify the accuracy of the referenced model by validatingthe model on the dataset. For this validation, the developer needs structural information to document and apply deep learning models,including metadata such as learning dataset, network architecture, and development environments. In this paper, we propose a descriptionlanguage that represents the network architecture of the deep learning model along with its metadata that are necessary to develop adeep learning model. Through the proposed description language, developers can easily verify the accuracy of the referenced deep learningmodel. Our experiments demonstrate the application scenario of a deep learning description document that focuses on the license platerecognition for the detection of illegally parked vehicles.Trusted Deep Learning, Model Reference, Deep Learning Description Language, Traffic Situation Analysis Model"
372,딥러닝 분석을 이용한 중국 역내·외 위안화 변동성 예측,이우식 /Lee Woosik,2016,한국데이터정보과학회,"2008년 글로벌 금융위기 이후 중국은 위안화 국제화의 점진적 추진을 진행하면서 중국상하이 외환시장과 중국홍콩 외환시장에서 거래되는 통화인 역내위안화와 역외위안화를 형성시켰다. 본 연구는 위안화 국제화와 점진적인 중국 자본계정 개방에 따라 급변하는 외환시장상황의 변동성을 정확하게 파악하기 위해서 GARCH모형 (일반화된 자기회귀 조건부이분산성모형)에 다단계인공신경망을 결합한 MLP-GARCH 모형과 GARCH모형과 기계학습의 일종인 딥러닝 (deep learning)을 통합한 DL-GARCH을 가지고 위안화 변동성예측을 비교 실험과 분석을 하였다. 비교분석 결과 DLGARCH 모형은 MLP-GARCH보다 모형 위안화 역내·외 환율변동성 예측 면에서 더욱 더 개선된예측값을 제공하였다. 그래서 이분산시계열모형을 딥러닝과 결합한 DL-GARCH 모형은 시계열의환율 변동성 예측 문제에 딥러닝을 응용할 수 있음을 확인하였다. 향후 이분산시계열과 결합된 딥러닝 모형은 다른 금융시계열 데이터에 응용하여 그 일반화 가능성을 높일 수 있을 것이다.The People’s Republic of China has vigorously been pursuing the internationalization of the Chinese Yuan or Renminbi after the financial crisis of 2008. In this view, an abrupt increase of use of the Chinese Yuan in the onshore and offshore markets are important milestones to be one of important currencies. One of the most frequently used methods to forecast volatility is GARCH model. Since a prediction error of the GARCH model has been reported quite high, a lot of efforts have been made to improve forecasting capability of the GARCH model. In this paper, we have proposed MLPGARCH and a DL-GARCH by employing Artificial Neural Network to the GARCH. In an application to forecasting Chinese Yuan volatility, we have successfully shown their overall outperformance in forecasting over the GARCH.Deep learning, DL-GARCH, GARCH, The Chinese Yuan, volatility."
373,딥러닝 기법을 이용한 내일강수 예측,하지훈,2016,한국지능시스템학회,"정확한 강수예측을 위해서는 예측인자 선정과 예측방법에 대한 선택이 매우 중요하다. 최근에는 강수예측 방법으로기계학습 기법이 많이 사용되고 있으며, 그 중에서도 특히 인공신경망을 사용한 강수예측 방법은 좋은 성능을 보였다. 본논문에서는 딥러닝 기법 중 하나인 DBN(deep belief network)를 이용한 새로운 강수예측 방법을 제안한다. DBN는 비지도사전 학습을 통해 초기 가중치를 설정하여 기존 인공신경망의 문제점을 보완한다. 예측인자로는 기온, 전일-전주 강수일, 태양과 달 궤도 관련 자료를 선정하였다. 기온과 전일-전주 강수일은 서울에서의 1974년부터 2013년까지 총 40년간의AWS(automatic weather system) 관측 자료를 사용하였고, 태양과 달의 궤도 관련 자료는 서울을 중심으로 계산한 결과를사용하였다. 전체 기간에서 일부는 학습 자료로 사용하여 예측모델을 생성하였고, 나머지를 생성한 모델의 검증 자료로사용하였다. 모델 검증 결과로 나온 예측값들은 확률값을 가지며 임계치를 이용하여 강수유무를 판별하였다. 강수 정확도의척도로 양분예보기법 중 CSI(critical successive index)와 Bias(frequency bias)를 계산하였다. 이를 통해 DBN와 MLP(multilayer perceptron)의 성능을 비교한 결과 DBN의 강수 예측 정확도가 높았고, 수행속도 또한 2배 이상 빨랐다.For accurate precipitation forecasts the choice of weather faoctrs and prediction method is very important. Recently, machinele arning has been widely used for forecasting precipitation, and artificial neural network, one of machine learning techniques, showed good performance. In this paper, we suggest a new method for forecasting precipitation using DBN, one of deep learning techniques.
DBN has an advantage that initial weights are set by unsupervised learning, so this compensates for the defects of artificial neural networks. We used past precipitation, temperature, and the parameters of the sun and moon's motion as features for forecasting precipitation. The dataset consists of observation data which had been measured for 40 years from AWS in Seoul. Experiments were based on 8-fold cross validation. As a result of estimation, we got probabilities of test dataset, so threshold was used for the decision of precipitation. CSI and Bias were used for indicating the precision of precipitation. Our experimental results showed that DBN performed better than MLP.Deep learning, Deep belief network, Precipitation, Forecast"
374,딥러닝 신경망모형을 이용한 실시간 교통정보수집 연구,김대현 /KIM DAE HYON,2016,사단법인 인문사회과학기술융합학회,"비디오 기반의 실시간 교통정보수집 시스템은 자율주행을 포함한 지능형교통체계의 주요 서브시스템중 하나이며, 우수한 예측(추정)력 때문에 핵심 알고리즘으로 인공신경망 모형이 주로 사용되고 있다. 특히, 최근 과학 학술지 네이처에 구글 딥마인드의 알파고가 공개되면서 딥러닝과 인공지능에 대한 전 세계적인 관심이 집중되고 있다. 최근까지 가장 널리 사용된 인공신경망모형은 역전파 모형이었으나 최근 컴퓨터 하드웨어 및 알고리즘의 새로운 개발과 더불어 딥러닝 알고리즘을 이용한 딥인공신경망 모형에 대한 관심이 높아지고 있다. 그러나 아직 딥러닝 인공신경망에 대한 연구는 초기단계에 있는 실정이며 적용분야도 극히 한정되어 있다. 본 연구에서는 실시간 도로교통정보 수집을 위한 영상처리분야에서 딥러닝 알고리즘을 활용하고 실험결과를 통하여 향후 교통정보수집‧분석 분야에서의 딥러닝 인공신경망 모형의 적용가능성을 살펴보고자 한다. 특히, 본 연구에서 도출한 딥러닝 인공신경망모형의 실험결과를 기존 역전파 모형에 의한 실험결과와 비교하여 모형의 우수성을 비교‧평가하고자 한다.Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Normally, artificial neural networks have been used in real-time applications for collecting traffic information because of their remarkable performance in terms of predictive accuracy and real-time computing. Especially, the recent publication of a paper in the journal Nature and the success of Google DeepMind’s world-beating Go program AlphaGo caused significant increases of the public interest in artificial intelligence. Even though various types of learning models have been proposed, the Backpropagation has been the most popular neural network model. However, the Deep learning neural network model has attracted much interest recently. In this study, the Deep leaning neural network will be applied in image processing techniques for the real-time traffic information collection to investigate the performance of Deep learning algorithm in traffic engineering fields. In addition, the experimental results from Deep learing algorithm will be compared with the results from Backpropagation model to evaluate and propose better performing model.Intelligent Transportation Systems (ITS), artificial neural networks, Deep learning, traffic information, image processing"
375,딥러닝의 모형과 응용사례,안성만 /SungMahn Ahn,2016,한국지능정보시스템학회,"딥러닝은 인공신경망(neural network)이라는 인공지능분야의 모형이 발전된 형태로서, 계층구조로 이루어진 인공신경망의 내부계층(hidden layer)이 여러 단계로 이루어진 구조이다. 딥러닝에서의 주요 모형은 합성곱신경망(convolutional neural network), 순환신경망(recurrent neural network), 그리고 심층신뢰신경망(deep belief network)의 세가지라고 할 수있다. 그 중에서 현재 흥미로운 연구가 많이 발표되어서 관심이 집중되고 있는 모형은 지도학습(supervised learning)모형인 처음 두 개의 모형이다. 따라서 본 논문에서는 지도학습모형의 가중치를 최적화하는 기본적인 방법인 오류역전파 알고리즘을 살펴본 뒤에 합성곱신경망과 순환신경망의 구조와 응용사례 등을 살펴보고자 한다. 본문에서 다루지 않은 모형인 심층신뢰신경망은 아직까지는 합성곱신경망 이나 순환신경망보다는 상대적으로 주목을 덜 받고 있다. 그러나 심층신뢰신경망은 CNN이나 RNN과는 달리 비지도학습(unsupervised learning)모형이며, 사람이나 동물은 관찰을 통해서 스스로 학습한다는 점에서 궁극적으로는 비지도학습모형이 더 많이 연구되어야 할 주제가 될 것이다.Deep learning model is a kind of neural networks that allows multiple hidden layers. There are various deep learning architectures such as convolutional neural networks, deep belief networks and recurrent neural networks. Those have been applied to fields like computer vision, automatic speech recognition, natural language processing, audio recognition and bioinformatics where they have been shown to produce state-of-the-art results on various tasks. Among those architectures, convolutional neural networks and recurrent neural networks are classified as the supervised learning model. And in recent years, those supervised learning models have gained more popularity than unsupervised learning models such as deep belief networks, because supervised learning models have shown fashionable applications in such fields mentioned above.
Deep learning models can be trained with backpropagation algorithm. Backpropagation is an abbreviation for “backward propagation of errors” and a common method of training artificial neural networks used in conjunction with an optimization method such as gradient descent. The method calculates the gradient of an error function with respect to all the weights in the network. The gradient is fed to the optimization method which in turn uses it to update the weights, in an attempt to minimize the error function.
Convolutional neural networks use a special architecture which is particularly well-adapted to classify images. Using this architecture makes convolutional networks fast to train. This, in turn, helps us train deep, muti-layer networks, which are very good at classifying images. These days, deep convolutional networks are used in most neural networks for image recognition. Convolutional neural networks use three basic ideas: local receptive fields, shared weights, and pooling. By local receptive fields, we mean that each neuron in the first(or any) hidden layer will be connected to a small region of the input(or previous layer’s) neurons. Shared weights mean that we’re going to use the same weights and bias for each of the local receptive field. This means that all the neurons in the hidden layer detect exactly the same feature, just at different locations in the input image. In addition to the convolutional layers just described, convolutional neural networks also contain pooling layers. Pooling layers are usually used immediately after convolutional layers. What the pooling layers do is to simplify the information in the output from the convolutional layer.
Recent convolutional network architectures have 10 to 20 hidden layers and billions of connections between units. Training deep learning networks has taken weeks several years ago, but thanks to progress in GPU and algorithm enhancement, training time has reduced to several hours.
Neural networks with time-varying behavior are known as recurrent neural networks or RNNs. A recurrent neural network is a class of artificial neural network where connections between units form a directed cycle. This creates an internal state of the network which allows it to exhibit dynamic temporal behavior. Unlike feedforward neural networks, RNNs can use their internal memory to process arbitrary sequences of inputs. Early RNN models turned out to be very difficult to train, harder even than deep feedforward networks. The reason is the unstable gradient problem such as vanishing gradient and exploding gradient. The gradient can get smaller and smaller as it is propagated back through layers. This makes learning in early layers extremely slow. The problem actually gets worse in RNNs, since gradients aren’t just propagated backward through layers, they’re propagated backward through time. If the network runs for a long time, that can make the gradient extremely unstable and hard to learn from. It has been possible to incorporate an idea known as long short-term memory units (LSTMs) into RNNs. LSTMs make it much easier to get good results when training RNNs, and many recent papers make use of LSTMs or related ideas.Deep learning, Convolutional neural networks, Recurrent neural networks, Error backpropagation algorithm"
376,딥러닝분석과 기술적 분석 지표를 이용한 한국 코스피주가지수 방향성 예측,이우식 /Lee Woosik,2017,한국데이터정보과학회,"2016년 3월 구글 (Google)의 바둑인공지능 알파고 (AlphaGo)가 이세돌 9단과의 바둑대결에서승리한 이후 다양한 분야에서 인공지능 사용에 대한 관심이 높아지고 있는 가운데 금융투자 분야에서도 인공지능과 투자자문 전문가의 합성어인 로보어드바이저 (Robo-Advisor)에 대한 관심이 높아지고 있다. 인공지능 (artificial intelligence)기반의 의사결정은 비용 절감은 물론 효과적인 의사결정을가능하게 한다는 점에서 큰 장점이 있다. 본 연구에서는 기술적 분석 (technical analysis) 지표와 딥러닝 (deep learning) 모형을 결합하여 한국 코스피 지수를 예측하는 모형을 개발하고 제시한 모형들의 예측력을 비교, 분석한다. 분석 결과 기술적 분석 지표에 딥러닝 알고리즘을 결합한 모형이 주가지수 방향성 예측 문제에 응용될 수 있음을 확인하였다. 향후 본 연구에서 제안된 기술적 분석 지표와딥러닝모형을 결합한 기법은 로보어드바이저서비스에 응용할 수 있는 일반화 가능성을 보여준다Since Google’s AlphaGo defeated a world champion of Go players in 2016, there have been many interests in the deep learning. In the financial sector, a Robo-Advisor using deep learning gains a significant attention, which builds and manages portfolios of financial instruments for investors.In this paper, we have proposed the a deep learning algorithm geared toward identification and forecast of the KOSPI index direction,and we also have compared the accuracy of the prediction.In an application of forecasting the financial market index direction, we have shown that the Robo-Advisor using deep learning has a significant effect on finance industry. The Robo-Advisor collects a massive data such as earnings statements, news reports and regulatory filings, analyzes those and recommends investors how to view market trends and identify the best time to purchase financial assets. On the other hand, the Robo-Advisor allows businesses to learn more about their customers, develop better marketing strategies, increase sales and decrease costs.Artificial intelligence, deep learning, FinTech, Robo-Advisor, technical analysis."
377,딥러닝의 효율적인 학습을 위한 학습데이터 선별 기법에 관한 연구,주경돈 /Gyoung-don Joo,2017,한국정보과학회,"빅데이터의 증가와 병렬 하드웨어 처리, 학습 알고리즘의 개선은 딥러닝의 발전을 가져왔으며 인공지능이 적용 가능한 영역을 획기적으로 확장시키고 있다. 하지만 이러한 대규모의 학습 데이터에 기반한 딥러닝은 학습시간이 증가되고, 더 나아가 학습 데이터의 구축을 위한 레이블링(labeling) 비용이 데이터 수에 비례하여 증가하게 된다. 따라서 본 논문에서는, 딥러닝의 정확도를 최대한 유지하면서 필요한 학습 데이터의 수를 줄일수 있도록, 전체 데이터에서 제한된 수의 학습 데이터의 후보를 선별하는 기법에 대하여 연구한다. 특히 기존의 능동 학습에 기반한 기법들이 학습 데이터의 수가 매우 적어질 경우 딥러닝의 정확도가 급격히 떨어지는 문제를 해결할 수 있는 새로운 기법을 제안하였으며, 제안된 기법으로 선별된 학습 데이터는 그 수의 변화에 대해 딥러닝이 일관되게 경쟁력있는 정확도를 가짐을 다양한 실험을 통하여 검증하였다.The increase of big data, parallel hardware processing and enhanced learning algorithms have led to the development of deep learning and are dramatically expanding the domain of applications based on artificial intelligence. However, such large-scale training data extends the learning time, and furthermore, the labeling cost for building the training set increases in proportion to the volume of data. Therefore, in this paper, we study how to select a limited number of candidates for training set from given data so as to reduce the number of training set while maintaining the accuracy of deep learning. Especially, we found that, with the training set selected by the previous work, the accuracy of deep learning drastically decreases when the number of training set is very small and so proposed new methods to resolve this fall-off in accuracy. Through experiments, we showed that deep learning consistently has the competitive accuracy with the training set selected by the proposed method regardless of the size of training set.Deep Learning, Train Set Selection, Active Learning, Gamma distribution"
378,딥러닝을 이용한 범용적 스테그아날리시스,김현재 /Hyunjae Kim,2017,한국정보과학회,"스테그아날리시스(Steganalysis)란 이미지 등 일반적인 자료에 암호화된 정보를 은닉하는 스테가노그래피(Steganography)에 대한 검출 및 분석 방법으로, 기계학습 기반 방법론을 포함한다. 기존 기계학습 기반 스테그아날리시스는 영상(Image)의 특징(Feature) 추출 및 모델링에 기반하며, 최근 딥러닝(Deep Learning)의 적용으로 검출 정확도가 큰 폭으로 향상되었다. 하지만 현존하는 스테그아날리시스 모델은 단일 스테가노그래피 기법에 대해 국한되어 있어 학습에 사용되지 않은 스테고(Stego) 이미지의 경우 검출이 불가능한 결정적 한계를 가진다. 본 연구에서는 다양한 스테가노그래피 기법으로 생성된 스테고이미지에 딥러닝을 적용하여 스테그아날리시스를 학습하는 범용적 모델을 제안한다. 다양한 실험을 통해 제안 기법의 효용성 및 가능성을 확인하고, 범용적 스테그아날리시스 모델이 각각에 특화된 검출 기법과 유사한 정확도로 스테고 이미지를 검출할 수 있음을 보인다.Steganalysis is to detect information hidden by steganography inside general data such as images. There are stegoanalysis techniques that use machine learning (ML). Existing ML approaches to steganalysis are based on extracting features from stego images and modeling them.
Recently deep learning-based methodologies have shown significant improvements in detection accuracy. However, all the existing methods, including deep learning-based ones, have a critical limitation in that they can only detect stego images that are created by a specific steganography method. In this paper, we propose a generalized steganalysis method that can model multiple types of stego images using deep learning. Through various experiments, we confirm the effectiveness of our approach and envision directions for future research. In particular, we show that our method can detect each type of steganography with the same level of accuracy as that of a steganalysis method dedicated to that type of steganography, thereby demonstrating the general applicability of our approach to multiple types of stego images.steganalysis, steganography, deep learning, generalized steganalysis model"
379,"딥러닝 프레임워크의 비교: 티아노, 텐서플로, CNTK를 중심으로",정여진 /Yeojin Chung,2017,한국지능정보시스템학회,"딥러닝 프레임워크의 대표적인 기능으로는 ‘자동미분’과 ‘GPU의 활용’ 등을 들 수 있다. 본 논문은 파이썬의 라이브러리 형태로 사용 가능한 프레임워크 중에서 구글의 텐서플로와 마이크로소프트의 CNTK, 그리고 텐서 플로의 원조라고 할 수 있는 티아노를 비교하였다. 본문에서는 자동미분의 개념과 GPU의 활용형태를 간단히 설명하고, 그 다음에 logistic regression을 실행하는 예를 통하여 각 프레임워크의 문법을 알아본 뒤에, 마지막으로 대표적인 딥러닝 응용인 CNN의 예제를 실행시켜보고 코딩의 편의성과 실행속도 등을 확인해 보았다. 그 결과, 편의성의 관점에서 보면 티아노가 가장 코딩 하기가 어렵고, CNTK와 텐서플로는 많은 부분이 비슷하게 추상화 되어 있어서 코딩이 비슷하지만 가중치와 편향을 직접 정의하느냐의 여부에서 차이를 보였다. 그리고 각프레임워크의 실행속도에 대한 평가는 ‘큰 차이는 없다’는 것이다. 텐서플로는 티아노에 비하여 속도가 느리다는 평가가 있어왔는데, 본 연구의 실험에 의하면, 비록 CNN 모형에 국한되었지만, 텐서플로가 아주 조금이지만빠른 것으로 나타났다. CNTK의 경우에도, 비록 실험환경이 달랐지만, 실험환경의 차이에 의한 속도의 차이의 편차범위 이내에 있는 것으로 판단이 되었다. 본 연구에서는 세 종류의 딥러닝 프레임워크만을 살펴보았는데,위키피디아에 따르면 딥러닝 프레임워크의 종류는 12가지가 있으며, 각 프레임워크의 특징을 15가지 속성으로구분하여 차이를 특정하고 있다. 그 많은 속성 중에서 사용자의 입장에서 볼 때 중요한 속성은 어떤 언어(파이썬, C++, Java, 등)로 사용가능한지, 어떤 딥러닝 모형에 대한 라이브러리가 잘 구현되어 있는지 등일 것이다. 그리고 사용자가 대규모의 딥러닝 모형을 구축한다면, 다중 GPU 혹은 다중 서버를 지원하는지의 여부도 중요할 것이다. 또한 딥러닝 모형을 처음 학습하는 경우에는 사용설명서가 많은지 예제 프로그램이 많은지 여부도 중요한 기준이 될 것이다.The deep learning framework is software designed to help develop deep learning models. Some of its important functions include “automatic differentiation” and “utilization of GPU”. The list of popular deep learning framework includes Caffe (BVLC) and Theano (University of Montreal). And recently, Microsoft's deep learning framework, Microsoft Cognitive Toolkit, was released as open-source license, following Google’s Tensorflow a year earlier. The early deep learning frameworks have been developed mainly for research at universities. Beginning with the inception of Tensorflow, however, it seems that companies such as Microsoft and Facebook have started to join the competition of framework development. Given the trend, Google and other companies are expected to continue investing in the deep learning framework to bring forward the initiative in the artificial intelligence business. From this point of view, we think it is a good time to compare some of deep learning frameworks. So we compare three deep learning frameworks which can be used as a Python library. Those are Google's Tensorflow, Microsoft’s CNTK, and Theano which is sort of a predecessor of the preceding two.
The most common and important function of deep learning frameworks is the ability to perform automatic differentiation. Basically all the mathematical expressions of deep learning models can be represented as computational graphs, which consist of nodes and edges. Partial derivatives on each edge of a computational graph can then be obtained. With the partial derivatives, we can let software compute differentiation of any node with respect to any variable by utilizing chain rule of Calculus.
First of all, the convenience of coding is in the order of CNTK, Tensorflow, and Theano. The criterion is simply based on the lengths of the codes and the learning curve and the ease of coding are not the main concern. According to the criteria, Theano was the most difficult to implement with, and CNTK and Tensorflow were somewhat easier. With Tensorflow, we need to define weight variables and biases explicitly. The reason that CNTK and Tensorflow are easier to implement with is that those frameworks provide us with more abstraction than Theano. We, however, need to mention that low-level coding is not always bad. It gives us flexibility of coding. With the low-level coding such as in Theano, we can implement and test any new deep learning models or any new search methods that we can think of.
The assessment of the execution speed of each framework is that there is not meaningful difference. According to the experiment, execution speeds of Theano and Tensorflow are very similar, although the experiment was limited to a CNN model. In the case of CNTK, the experimental environment was not maintained as the same. The code written in CNTK has to be run in PC environment without GPU where codes execute as much as 50 times slower than with GPU. But we concluded that the difference of execution speed was within the range of variation caused by the different hardware setup.
In this study, we compared three types of deep learning framework: Theano, Tensorflow, and CNTK. According to Wikipedia, there are 12 available deep learning frameworks. And 15 different attributes differentiate each framework. Some of the important attributes would include interface language (Python, C ++, Java, etc.) and the availability of libraries on various deep learning models such as CNN, RNN, DBN, and etc. And if a user implements a large scale deep learning model, it will also be important to support multiple GPU or multiple servers. Also, if you are learning the deep learning model, it would also be important if there are enough examples and references.deep learning framework, Theano, TensorFlow, CNTK, computational graph, CIFAR-10"
380,딥러닝을 이용한 주변 무선단말 파악방안,이웅섭 /Lee， Woong-sup,2018,한국정보통신학회,"최근 단말-대-단말(Device-to-device, D2D) 통신기술이 차세대 무선통신시스템의 핵심기술로 큰 관심을 받고 있다. 이러한 단말간 통신에서는 자신의 주변에 어떠한 단말이 있는지 파악하는 주변단말 탐색(Neighbor discovery)이매우 중요하다. 본 논문에서는 최근 큰 관심을 받고 있는 딥러닝(Deep learning) 기술을 활용하여 단말간 통신에서 주변단말을 파악하는 방안에 대해서 제안한다. 제안 방안은 기존의 방안과 달리 무선채널의 공간적 연관성을 이용하여단말간의 신호 전송 없이 단말이 기지국으로 전송하는 상향링크 파일럿 신호를 기반으로 주변 단말을 찾고 따라서기존의 방식에 비해 신호전송 복잡도(signaling complexity)를 크게 줄일 수 있다. 또한 제안 방안에서는 떨어져 있는거리에 따라서 주변 단말을 분류 가능하여 기존 방안에 비해서 좀 더 세밀한 단말 탐색이 가능하다. 마지막으로 본 논문에서는 tensorflow를 이용한 컴퓨터 시뮬레이션을 통해 제안 방안의 성능을 검증하였다.Recently, the device-to-device (D2D) communication has been conceived as the key technology for the next-generationmobile communication systems. The neighbor discovery in which the nearby users are found, is essential for the properoperation of the D2D communication. In this paper, we propose new neighbor discovery scheme based on deep learningtechnology which has gained a lot of attention recently. In the proposed scheme, the neighboring users can be foundusing the uplink pilot transmission of users only, unlike conventional neighbor discovery schemes in which direct pilotcommunication among users is required, such that the signaling overhead can be greatly reduced in our proposed scheme.
Moreover, the neighbors with different proximity can also be classified accordingly which enables more accurate neighbordiscovery compared to the conventional schemes. The performance of our proposed scheme is verified through thetensorflow-based computer simulations.
키워드 : 딥러닝, 주변단말 탐색, 단말-대-단말 통신Deep learning, Neighbor discovery, Device-to-device communication, Wireless channel correlation"
381,신뢰성있는 딥러닝 기반 분석 모델을 참조하기 위한 딥러닝 기술 언어,문종혁 /Jong Hyeok Mun,2021,한국정보처리학회,"최근 딥러닝은 하드웨어 성능이 향상됨에 따라 자연어 처리, 영상 인식 등의 다양한 기술에 접목되어 활용되고 있다. 이러한 기술들을 활용해 지능형교통 시스템(ITS), 스마트홈, 헬스케어 등의 산업분야에서 데이터를 분석하여 고속도로 속도위반 차량 검출, 에너지 사용량 제어, 응급상황 등과 같은고품질의 서비스를 제공하며, 고품질의 서비스를 제공하기 위해서는 정확도가 향상된 딥러닝 모델이 적용되어야 한다. 이를 위해 서비스 환경의 데이터를분석하기 위한 딥러닝 모델을 개발할 때, 개발자는 신뢰성이 검증된 최신의 딥러닝 모델을 적용할 수 있어야 한다. 이는 개발자가 참조하는 딥러닝 모델에적용된 학습 데이터셋의 정확도를 측정하여 검증할 수 있다. 이러한 검증을 위해서 개발자는 학습 데이터셋, 딥러닝의 계층구조 및 개발 환경 등과 같은내용을 포함하는 딥러닝 모델을 문서화하여 적용하기 위한 구조적인 정보가 필요하다. 본 논문에서는 신뢰성있는 딥러닝 기반 데이터 분석 모델을 참조하기위한 딥러닝 기술 언어를 제안한다. 제안하는 기술 언어는 신뢰성 있는 딥러닝 모델을 개발하는데 필요한 학습데이터셋, 개발 환경 및 설정 등의 정보와더불어 딥러닝 모델의 계층구조를 표현할 수 있다. 제안하는 딥러닝 기술 언어를 이용하여 개발자는 지능형 교통 시스템에서 참조하는 분석 모델의정확도를 검증할 수 있다. 실험에서는 제안하는 언어의 유효성을 검증하기 위해, 번호판 인식 모델을 중심으로 딥러닝 기술 문서의 적용과정을 보인다With the recent advancements of deep learning, companies such as smart home, healthcare, and intelligent transportation systems areutilizing its functionality to provide high-quality services for vehicle detection, emergency situation detection, and controlling energyconsumption. To provide reliable services in such sensitive systems, deep learning models are required to have high accuracy. In orderto develop a deep learning model for analyzing previously mentioned services, developers should utilize the state of the art deep learningmodels that have already been verified for higher accuracy. The developers can verify the accuracy of the referenced model by validatingthe model on the dataset. For this validation, the developer needs structural information to document and apply deep learning models,including metadata such as learning dataset, network architecture, and development environments. In this paper, we propose a descriptionlanguage that represents the network architecture of the deep learning model along with its metadata that are necessary to develop adeep learning model. Through the proposed description language, developers can easily verify the accuracy of the referenced deep learningmodel. Our experiments demonstrate the application scenario of a deep learning description document that focuses on the license platerecognition for the detection of illegally parked vehicles.Trusted Deep Learning, Model Reference, Deep Learning Description Language, Traffic Situation Analysis Model"
382,딥러닝 분석을 이용한 중국 역내·외 위안화 변동성 예측,이우식 /Lee Woosik,2016,한국데이터정보과학회,"2008년 글로벌 금융위기 이후 중국은 위안화 국제화의 점진적 추진을 진행하면서 중국상하이 외환시장과 중국홍콩 외환시장에서 거래되는 통화인 역내위안화와 역외위안화를 형성시켰다. 본 연구는 위안화 국제화와 점진적인 중국 자본계정 개방에 따라 급변하는 외환시장상황의 변동성을 정확하게 파악하기 위해서 GARCH모형 (일반화된 자기회귀 조건부이분산성모형)에 다단계인공신경망을 결합한 MLP-GARCH 모형과 GARCH모형과 기계학습의 일종인 딥러닝 (deep learning)을 통합한 DL-GARCH을 가지고 위안화 변동성예측을 비교 실험과 분석을 하였다. 비교분석 결과 DLGARCH 모형은 MLP-GARCH보다 모형 위안화 역내·외 환율변동성 예측 면에서 더욱 더 개선된예측값을 제공하였다. 그래서 이분산시계열모형을 딥러닝과 결합한 DL-GARCH 모형은 시계열의환율 변동성 예측 문제에 딥러닝을 응용할 수 있음을 확인하였다. 향후 이분산시계열과 결합된 딥러닝 모형은 다른 금융시계열 데이터에 응용하여 그 일반화 가능성을 높일 수 있을 것이다.The People’s Republic of China has vigorously been pursuing the internationalization of the Chinese Yuan or Renminbi after the financial crisis of 2008. In this view, an abrupt increase of use of the Chinese Yuan in the onshore and offshore markets are important milestones to be one of important currencies. One of the most frequently used methods to forecast volatility is GARCH model. Since a prediction error of the GARCH model has been reported quite high, a lot of efforts have been made to improve forecasting capability of the GARCH model. In this paper, we have proposed MLPGARCH and a DL-GARCH by employing Artificial Neural Network to the GARCH. In an application to forecasting Chinese Yuan volatility, we have successfully shown their overall outperformance in forecasting over the GARCH.Deep learning, DL-GARCH, GARCH, The Chinese Yuan, volatility."
383,딥러닝 기법을 이용한 내일강수 예측,하지훈,2016,한국지능시스템학회,"정확한 강수예측을 위해서는 예측인자 선정과 예측방법에 대한 선택이 매우 중요하다. 최근에는 강수예측 방법으로기계학습 기법이 많이 사용되고 있으며, 그 중에서도 특히 인공신경망을 사용한 강수예측 방법은 좋은 성능을 보였다. 본논문에서는 딥러닝 기법 중 하나인 DBN(deep belief network)를 이용한 새로운 강수예측 방법을 제안한다. DBN는 비지도사전 학습을 통해 초기 가중치를 설정하여 기존 인공신경망의 문제점을 보완한다. 예측인자로는 기온, 전일-전주 강수일, 태양과 달 궤도 관련 자료를 선정하였다. 기온과 전일-전주 강수일은 서울에서의 1974년부터 2013년까지 총 40년간의AWS(automatic weather system) 관측 자료를 사용하였고, 태양과 달의 궤도 관련 자료는 서울을 중심으로 계산한 결과를사용하였다. 전체 기간에서 일부는 학습 자료로 사용하여 예측모델을 생성하였고, 나머지를 생성한 모델의 검증 자료로사용하였다. 모델 검증 결과로 나온 예측값들은 확률값을 가지며 임계치를 이용하여 강수유무를 판별하였다. 강수 정확도의척도로 양분예보기법 중 CSI(critical successive index)와 Bias(frequency bias)를 계산하였다. 이를 통해 DBN와 MLP(multilayer perceptron)의 성능을 비교한 결과 DBN의 강수 예측 정확도가 높았고, 수행속도 또한 2배 이상 빨랐다.For accurate precipitation forecasts the choice of weather faoctrs and prediction method is very important. Recently, machinele arning has been widely used for forecasting precipitation, and artificial neural network, one of machine learning techniques, showed good performance. In this paper, we suggest a new method for forecasting precipitation using DBN, one of deep learning techniques.
DBN has an advantage that initial weights are set by unsupervised learning, so this compensates for the defects of artificial neural networks. We used past precipitation, temperature, and the parameters of the sun and moon's motion as features for forecasting precipitation. The dataset consists of observation data which had been measured for 40 years from AWS in Seoul. Experiments were based on 8-fold cross validation. As a result of estimation, we got probabilities of test dataset, so threshold was used for the decision of precipitation. CSI and Bias were used for indicating the precision of precipitation. Our experimental results showed that DBN performed better than MLP.Deep learning, Deep belief network, Precipitation, Forecast"
384,딥러닝 신경망모형을 이용한 실시간 교통정보수집 연구,김대현 /KIM DAE HYON,2016,사단법인 인문사회과학기술융합학회,"비디오 기반의 실시간 교통정보수집 시스템은 자율주행을 포함한 지능형교통체계의 주요 서브시스템중 하나이며, 우수한 예측(추정)력 때문에 핵심 알고리즘으로 인공신경망 모형이 주로 사용되고 있다. 특히, 최근 과학 학술지 네이처에 구글 딥마인드의 알파고가 공개되면서 딥러닝과 인공지능에 대한 전 세계적인 관심이 집중되고 있다. 최근까지 가장 널리 사용된 인공신경망모형은 역전파 모형이었으나 최근 컴퓨터 하드웨어 및 알고리즘의 새로운 개발과 더불어 딥러닝 알고리즘을 이용한 딥인공신경망 모형에 대한 관심이 높아지고 있다. 그러나 아직 딥러닝 인공신경망에 대한 연구는 초기단계에 있는 실정이며 적용분야도 극히 한정되어 있다. 본 연구에서는 실시간 도로교통정보 수집을 위한 영상처리분야에서 딥러닝 알고리즘을 활용하고 실험결과를 통하여 향후 교통정보수집‧분석 분야에서의 딥러닝 인공신경망 모형의 적용가능성을 살펴보고자 한다. 특히, 본 연구에서 도출한 딥러닝 인공신경망모형의 실험결과를 기존 역전파 모형에 의한 실험결과와 비교하여 모형의 우수성을 비교‧평가하고자 한다.Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Normally, artificial neural networks have been used in real-time applications for collecting traffic information because of their remarkable performance in terms of predictive accuracy and real-time computing. Especially, the recent publication of a paper in the journal Nature and the success of Google DeepMind’s world-beating Go program AlphaGo caused significant increases of the public interest in artificial intelligence. Even though various types of learning models have been proposed, the Backpropagation has been the most popular neural network model. However, the Deep learning neural network model has attracted much interest recently. In this study, the Deep leaning neural network will be applied in image processing techniques for the real-time traffic information collection to investigate the performance of Deep learning algorithm in traffic engineering fields. In addition, the experimental results from Deep learing algorithm will be compared with the results from Backpropagation model to evaluate and propose better performing model.Intelligent Transportation Systems (ITS), artificial neural networks, Deep learning, traffic information, image processing"
385,딥러닝의 모형과 응용사례,안성만 /SungMahn Ahn,2016,한국지능정보시스템학회,"딥러닝은 인공신경망(neural network)이라는 인공지능분야의 모형이 발전된 형태로서, 계층구조로 이루어진 인공신경망의 내부계층(hidden layer)이 여러 단계로 이루어진 구조이다. 딥러닝에서의 주요 모형은 합성곱신경망(convolutional neural network), 순환신경망(recurrent neural network), 그리고 심층신뢰신경망(deep belief network)의 세가지라고 할 수있다. 그 중에서 현재 흥미로운 연구가 많이 발표되어서 관심이 집중되고 있는 모형은 지도학습(supervised learning)모형인 처음 두 개의 모형이다. 따라서 본 논문에서는 지도학습모형의 가중치를 최적화하는 기본적인 방법인 오류역전파 알고리즘을 살펴본 뒤에 합성곱신경망과 순환신경망의 구조와 응용사례 등을 살펴보고자 한다. 본문에서 다루지 않은 모형인 심층신뢰신경망은 아직까지는 합성곱신경망 이나 순환신경망보다는 상대적으로 주목을 덜 받고 있다. 그러나 심층신뢰신경망은 CNN이나 RNN과는 달리 비지도학습(unsupervised learning)모형이며, 사람이나 동물은 관찰을 통해서 스스로 학습한다는 점에서 궁극적으로는 비지도학습모형이 더 많이 연구되어야 할 주제가 될 것이다.Deep learning model is a kind of neural networks that allows multiple hidden layers. There are various deep learning architectures such as convolutional neural networks, deep belief networks and recurrent neural networks. Those have been applied to fields like computer vision, automatic speech recognition, natural language processing, audio recognition and bioinformatics where they have been shown to produce state-of-the-art results on various tasks. Among those architectures, convolutional neural networks and recurrent neural networks are classified as the supervised learning model. And in recent years, those supervised learning models have gained more popularity than unsupervised learning models such as deep belief networks, because supervised learning models have shown fashionable applications in such fields mentioned above.
Deep learning models can be trained with backpropagation algorithm. Backpropagation is an abbreviation for “backward propagation of errors” and a common method of training artificial neural networks used in conjunction with an optimization method such as gradient descent. The method calculates the gradient of an error function with respect to all the weights in the network. The gradient is fed to the optimization method which in turn uses it to update the weights, in an attempt to minimize the error function.
Convolutional neural networks use a special architecture which is particularly well-adapted to classify images. Using this architecture makes convolutional networks fast to train. This, in turn, helps us train deep, muti-layer networks, which are very good at classifying images. These days, deep convolutional networks are used in most neural networks for image recognition. Convolutional neural networks use three basic ideas: local receptive fields, shared weights, and pooling. By local receptive fields, we mean that each neuron in the first(or any) hidden layer will be connected to a small region of the input(or previous layer’s) neurons. Shared weights mean that we’re going to use the same weights and bias for each of the local receptive field. This means that all the neurons in the hidden layer detect exactly the same feature, just at different locations in the input image. In addition to the convolutional layers just described, convolutional neural networks also contain pooling layers. Pooling layers are usually used immediately after convolutional layers. What the pooling layers do is to simplify the information in the output from the convolutional layer.
Recent convolutional network architectures have 10 to 20 hidden layers and billions of connections between units. Training deep learning networks has taken weeks several years ago, but thanks to progress in GPU and algorithm enhancement, training time has reduced to several hours.
Neural networks with time-varying behavior are known as recurrent neural networks or RNNs. A recurrent neural network is a class of artificial neural network where connections between units form a directed cycle. This creates an internal state of the network which allows it to exhibit dynamic temporal behavior. Unlike feedforward neural networks, RNNs can use their internal memory to process arbitrary sequences of inputs. Early RNN models turned out to be very difficult to train, harder even than deep feedforward networks. The reason is the unstable gradient problem such as vanishing gradient and exploding gradient. The gradient can get smaller and smaller as it is propagated back through layers. This makes learning in early layers extremely slow. The problem actually gets worse in RNNs, since gradients aren’t just propagated backward through layers, they’re propagated backward through time. If the network runs for a long time, that can make the gradient extremely unstable and hard to learn from. It has been possible to incorporate an idea known as long short-term memory units (LSTMs) into RNNs. LSTMs make it much easier to get good results when training RNNs, and many recent papers make use of LSTMs or related ideas.Deep learning, Convolutional neural networks, Recurrent neural networks, Error backpropagation algorithm"
386,딥러닝분석과 기술적 분석 지표를 이용한 한국 코스피주가지수 방향성 예측,이우식 /Lee Woosik,2017,한국데이터정보과학회,"2016년 3월 구글 (Google)의 바둑인공지능 알파고 (AlphaGo)가 이세돌 9단과의 바둑대결에서승리한 이후 다양한 분야에서 인공지능 사용에 대한 관심이 높아지고 있는 가운데 금융투자 분야에서도 인공지능과 투자자문 전문가의 합성어인 로보어드바이저 (Robo-Advisor)에 대한 관심이 높아지고 있다. 인공지능 (artificial intelligence)기반의 의사결정은 비용 절감은 물론 효과적인 의사결정을가능하게 한다는 점에서 큰 장점이 있다. 본 연구에서는 기술적 분석 (technical analysis) 지표와 딥러닝 (deep learning) 모형을 결합하여 한국 코스피 지수를 예측하는 모형을 개발하고 제시한 모형들의 예측력을 비교, 분석한다. 분석 결과 기술적 분석 지표에 딥러닝 알고리즘을 결합한 모형이 주가지수 방향성 예측 문제에 응용될 수 있음을 확인하였다. 향후 본 연구에서 제안된 기술적 분석 지표와딥러닝모형을 결합한 기법은 로보어드바이저서비스에 응용할 수 있는 일반화 가능성을 보여준다Since Google’s AlphaGo defeated a world champion of Go players in 2016, there have been many interests in the deep learning. In the financial sector, a Robo-Advisor using deep learning gains a significant attention, which builds and manages portfolios of financial instruments for investors.In this paper, we have proposed the a deep learning algorithm geared toward identification and forecast of the KOSPI index direction,and we also have compared the accuracy of the prediction.In an application of forecasting the financial market index direction, we have shown that the Robo-Advisor using deep learning has a significant effect on finance industry. The Robo-Advisor collects a massive data such as earnings statements, news reports and regulatory filings, analyzes those and recommends investors how to view market trends and identify the best time to purchase financial assets. On the other hand, the Robo-Advisor allows businesses to learn more about their customers, develop better marketing strategies, increase sales and decrease costs.Artificial intelligence, deep learning, FinTech, Robo-Advisor, technical analysis."
387,딥러닝의 효율적인 학습을 위한 학습데이터 선별 기법에 관한 연구,주경돈 /Gyoung-don Joo,2017,한국정보과학회,"빅데이터의 증가와 병렬 하드웨어 처리, 학습 알고리즘의 개선은 딥러닝의 발전을 가져왔으며 인공지능이 적용 가능한 영역을 획기적으로 확장시키고 있다. 하지만 이러한 대규모의 학습 데이터에 기반한 딥러닝은 학습시간이 증가되고, 더 나아가 학습 데이터의 구축을 위한 레이블링(labeling) 비용이 데이터 수에 비례하여 증가하게 된다. 따라서 본 논문에서는, 딥러닝의 정확도를 최대한 유지하면서 필요한 학습 데이터의 수를 줄일수 있도록, 전체 데이터에서 제한된 수의 학습 데이터의 후보를 선별하는 기법에 대하여 연구한다. 특히 기존의 능동 학습에 기반한 기법들이 학습 데이터의 수가 매우 적어질 경우 딥러닝의 정확도가 급격히 떨어지는 문제를 해결할 수 있는 새로운 기법을 제안하였으며, 제안된 기법으로 선별된 학습 데이터는 그 수의 변화에 대해 딥러닝이 일관되게 경쟁력있는 정확도를 가짐을 다양한 실험을 통하여 검증하였다.The increase of big data, parallel hardware processing and enhanced learning algorithms have led to the development of deep learning and are dramatically expanding the domain of applications based on artificial intelligence. However, such large-scale training data extends the learning time, and furthermore, the labeling cost for building the training set increases in proportion to the volume of data. Therefore, in this paper, we study how to select a limited number of candidates for training set from given data so as to reduce the number of training set while maintaining the accuracy of deep learning. Especially, we found that, with the training set selected by the previous work, the accuracy of deep learning drastically decreases when the number of training set is very small and so proposed new methods to resolve this fall-off in accuracy. Through experiments, we showed that deep learning consistently has the competitive accuracy with the training set selected by the proposed method regardless of the size of training set.Deep Learning, Train Set Selection, Active Learning, Gamma distribution"
388,딥러닝을 이용한 범용적 스테그아날리시스,김현재 /Hyunjae Kim,2017,한국정보과학회,"스테그아날리시스(Steganalysis)란 이미지 등 일반적인 자료에 암호화된 정보를 은닉하는 스테가노그래피(Steganography)에 대한 검출 및 분석 방법으로, 기계학습 기반 방법론을 포함한다. 기존 기계학습 기반 스테그아날리시스는 영상(Image)의 특징(Feature) 추출 및 모델링에 기반하며, 최근 딥러닝(Deep Learning)의 적용으로 검출 정확도가 큰 폭으로 향상되었다. 하지만 현존하는 스테그아날리시스 모델은 단일 스테가노그래피 기법에 대해 국한되어 있어 학습에 사용되지 않은 스테고(Stego) 이미지의 경우 검출이 불가능한 결정적 한계를 가진다. 본 연구에서는 다양한 스테가노그래피 기법으로 생성된 스테고이미지에 딥러닝을 적용하여 스테그아날리시스를 학습하는 범용적 모델을 제안한다. 다양한 실험을 통해 제안 기법의 효용성 및 가능성을 확인하고, 범용적 스테그아날리시스 모델이 각각에 특화된 검출 기법과 유사한 정확도로 스테고 이미지를 검출할 수 있음을 보인다.Steganalysis is to detect information hidden by steganography inside general data such as images. There are stegoanalysis techniques that use machine learning (ML). Existing ML approaches to steganalysis are based on extracting features from stego images and modeling them.
Recently deep learning-based methodologies have shown significant improvements in detection accuracy. However, all the existing methods, including deep learning-based ones, have a critical limitation in that they can only detect stego images that are created by a specific steganography method. In this paper, we propose a generalized steganalysis method that can model multiple types of stego images using deep learning. Through various experiments, we confirm the effectiveness of our approach and envision directions for future research. In particular, we show that our method can detect each type of steganography with the same level of accuracy as that of a steganalysis method dedicated to that type of steganography, thereby demonstrating the general applicability of our approach to multiple types of stego images.steganalysis, steganography, deep learning, generalized steganalysis model"
389,"딥러닝 프레임워크의 비교: 티아노, 텐서플로, CNTK를 중심으로",정여진 /Yeojin Chung,2017,한국지능정보시스템학회,"딥러닝 프레임워크의 대표적인 기능으로는 ‘자동미분’과 ‘GPU의 활용’ 등을 들 수 있다. 본 논문은 파이썬의 라이브러리 형태로 사용 가능한 프레임워크 중에서 구글의 텐서플로와 마이크로소프트의 CNTK, 그리고 텐서 플로의 원조라고 할 수 있는 티아노를 비교하였다. 본문에서는 자동미분의 개념과 GPU의 활용형태를 간단히 설명하고, 그 다음에 logistic regression을 실행하는 예를 통하여 각 프레임워크의 문법을 알아본 뒤에, 마지막으로 대표적인 딥러닝 응용인 CNN의 예제를 실행시켜보고 코딩의 편의성과 실행속도 등을 확인해 보았다. 그 결과, 편의성의 관점에서 보면 티아노가 가장 코딩 하기가 어렵고, CNTK와 텐서플로는 많은 부분이 비슷하게 추상화 되어 있어서 코딩이 비슷하지만 가중치와 편향을 직접 정의하느냐의 여부에서 차이를 보였다. 그리고 각프레임워크의 실행속도에 대한 평가는 ‘큰 차이는 없다’는 것이다. 텐서플로는 티아노에 비하여 속도가 느리다는 평가가 있어왔는데, 본 연구의 실험에 의하면, 비록 CNN 모형에 국한되었지만, 텐서플로가 아주 조금이지만빠른 것으로 나타났다. CNTK의 경우에도, 비록 실험환경이 달랐지만, 실험환경의 차이에 의한 속도의 차이의 편차범위 이내에 있는 것으로 판단이 되었다. 본 연구에서는 세 종류의 딥러닝 프레임워크만을 살펴보았는데,위키피디아에 따르면 딥러닝 프레임워크의 종류는 12가지가 있으며, 각 프레임워크의 특징을 15가지 속성으로구분하여 차이를 특정하고 있다. 그 많은 속성 중에서 사용자의 입장에서 볼 때 중요한 속성은 어떤 언어(파이썬, C++, Java, 등)로 사용가능한지, 어떤 딥러닝 모형에 대한 라이브러리가 잘 구현되어 있는지 등일 것이다. 그리고 사용자가 대규모의 딥러닝 모형을 구축한다면, 다중 GPU 혹은 다중 서버를 지원하는지의 여부도 중요할 것이다. 또한 딥러닝 모형을 처음 학습하는 경우에는 사용설명서가 많은지 예제 프로그램이 많은지 여부도 중요한 기준이 될 것이다.The deep learning framework is software designed to help develop deep learning models. Some of its important functions include “automatic differentiation” and “utilization of GPU”. The list of popular deep learning framework includes Caffe (BVLC) and Theano (University of Montreal). And recently, Microsoft's deep learning framework, Microsoft Cognitive Toolkit, was released as open-source license, following Google’s Tensorflow a year earlier. The early deep learning frameworks have been developed mainly for research at universities. Beginning with the inception of Tensorflow, however, it seems that companies such as Microsoft and Facebook have started to join the competition of framework development. Given the trend, Google and other companies are expected to continue investing in the deep learning framework to bring forward the initiative in the artificial intelligence business. From this point of view, we think it is a good time to compare some of deep learning frameworks. So we compare three deep learning frameworks which can be used as a Python library. Those are Google's Tensorflow, Microsoft’s CNTK, and Theano which is sort of a predecessor of the preceding two.
The most common and important function of deep learning frameworks is the ability to perform automatic differentiation. Basically all the mathematical expressions of deep learning models can be represented as computational graphs, which consist of nodes and edges. Partial derivatives on each edge of a computational graph can then be obtained. With the partial derivatives, we can let software compute differentiation of any node with respect to any variable by utilizing chain rule of Calculus.
First of all, the convenience of coding is in the order of CNTK, Tensorflow, and Theano. The criterion is simply based on the lengths of the codes and the learning curve and the ease of coding are not the main concern. According to the criteria, Theano was the most difficult to implement with, and CNTK and Tensorflow were somewhat easier. With Tensorflow, we need to define weight variables and biases explicitly. The reason that CNTK and Tensorflow are easier to implement with is that those frameworks provide us with more abstraction than Theano. We, however, need to mention that low-level coding is not always bad. It gives us flexibility of coding. With the low-level coding such as in Theano, we can implement and test any new deep learning models or any new search methods that we can think of.
The assessment of the execution speed of each framework is that there is not meaningful difference. According to the experiment, execution speeds of Theano and Tensorflow are very similar, although the experiment was limited to a CNN model. In the case of CNTK, the experimental environment was not maintained as the same. The code written in CNTK has to be run in PC environment without GPU where codes execute as much as 50 times slower than with GPU. But we concluded that the difference of execution speed was within the range of variation caused by the different hardware setup.
In this study, we compared three types of deep learning framework: Theano, Tensorflow, and CNTK. According to Wikipedia, there are 12 available deep learning frameworks. And 15 different attributes differentiate each framework. Some of the important attributes would include interface language (Python, C ++, Java, etc.) and the availability of libraries on various deep learning models such as CNN, RNN, DBN, and etc. And if a user implements a large scale deep learning model, it will also be important to support multiple GPU or multiple servers. Also, if you are learning the deep learning model, it would also be important if there are enough examples and references.deep learning framework, Theano, TensorFlow, CNTK, computational graph, CIFAR-10"
390,딥러닝을 이용한 주변 무선단말 파악방안,이웅섭 /Lee， Woong-sup,2018,한국정보통신학회,"최근 단말-대-단말(Device-to-device, D2D) 통신기술이 차세대 무선통신시스템의 핵심기술로 큰 관심을 받고 있다. 이러한 단말간 통신에서는 자신의 주변에 어떠한 단말이 있는지 파악하는 주변단말 탐색(Neighbor discovery)이매우 중요하다. 본 논문에서는 최근 큰 관심을 받고 있는 딥러닝(Deep learning) 기술을 활용하여 단말간 통신에서 주변단말을 파악하는 방안에 대해서 제안한다. 제안 방안은 기존의 방안과 달리 무선채널의 공간적 연관성을 이용하여단말간의 신호 전송 없이 단말이 기지국으로 전송하는 상향링크 파일럿 신호를 기반으로 주변 단말을 찾고 따라서기존의 방식에 비해 신호전송 복잡도(signaling complexity)를 크게 줄일 수 있다. 또한 제안 방안에서는 떨어져 있는거리에 따라서 주변 단말을 분류 가능하여 기존 방안에 비해서 좀 더 세밀한 단말 탐색이 가능하다. 마지막으로 본 논문에서는 tensorflow를 이용한 컴퓨터 시뮬레이션을 통해 제안 방안의 성능을 검증하였다.Recently, the device-to-device (D2D) communication has been conceived as the key technology for the next-generationmobile communication systems. The neighbor discovery in which the nearby users are found, is essential for the properoperation of the D2D communication. In this paper, we propose new neighbor discovery scheme based on deep learningtechnology which has gained a lot of attention recently. In the proposed scheme, the neighboring users can be foundusing the uplink pilot transmission of users only, unlike conventional neighbor discovery schemes in which direct pilotcommunication among users is required, such that the signaling overhead can be greatly reduced in our proposed scheme.
Moreover, the neighbors with different proximity can also be classified accordingly which enables more accurate neighbordiscovery compared to the conventional schemes. The performance of our proposed scheme is verified through thetensorflow-based computer simulations.
키워드 : 딥러닝, 주변단말 탐색, 단말-대-단말 통신Deep learning, Neighbor discovery, Device-to-device communication, Wireless channel correlation"
391,신뢰성있는 딥러닝 기반 분석 모델을 참조하기 위한 딥러닝 기술 언어,문종혁 /Jong Hyeok Mun,2021,한국정보처리학회,"최근 딥러닝은 하드웨어 성능이 향상됨에 따라 자연어 처리, 영상 인식 등의 다양한 기술에 접목되어 활용되고 있다. 이러한 기술들을 활용해 지능형교통 시스템(ITS), 스마트홈, 헬스케어 등의 산업분야에서 데이터를 분석하여 고속도로 속도위반 차량 검출, 에너지 사용량 제어, 응급상황 등과 같은고품질의 서비스를 제공하며, 고품질의 서비스를 제공하기 위해서는 정확도가 향상된 딥러닝 모델이 적용되어야 한다. 이를 위해 서비스 환경의 데이터를분석하기 위한 딥러닝 모델을 개발할 때, 개발자는 신뢰성이 검증된 최신의 딥러닝 모델을 적용할 수 있어야 한다. 이는 개발자가 참조하는 딥러닝 모델에적용된 학습 데이터셋의 정확도를 측정하여 검증할 수 있다. 이러한 검증을 위해서 개발자는 학습 데이터셋, 딥러닝의 계층구조 및 개발 환경 등과 같은내용을 포함하는 딥러닝 모델을 문서화하여 적용하기 위한 구조적인 정보가 필요하다. 본 논문에서는 신뢰성있는 딥러닝 기반 데이터 분석 모델을 참조하기위한 딥러닝 기술 언어를 제안한다. 제안하는 기술 언어는 신뢰성 있는 딥러닝 모델을 개발하는데 필요한 학습데이터셋, 개발 환경 및 설정 등의 정보와더불어 딥러닝 모델의 계층구조를 표현할 수 있다. 제안하는 딥러닝 기술 언어를 이용하여 개발자는 지능형 교통 시스템에서 참조하는 분석 모델의정확도를 검증할 수 있다. 실험에서는 제안하는 언어의 유효성을 검증하기 위해, 번호판 인식 모델을 중심으로 딥러닝 기술 문서의 적용과정을 보인다With the recent advancements of deep learning, companies such as smart home, healthcare, and intelligent transportation systems areutilizing its functionality to provide high-quality services for vehicle detection, emergency situation detection, and controlling energyconsumption. To provide reliable services in such sensitive systems, deep learning models are required to have high accuracy. In orderto develop a deep learning model for analyzing previously mentioned services, developers should utilize the state of the art deep learningmodels that have already been verified for higher accuracy. The developers can verify the accuracy of the referenced model by validatingthe model on the dataset. For this validation, the developer needs structural information to document and apply deep learning models,including metadata such as learning dataset, network architecture, and development environments. In this paper, we propose a descriptionlanguage that represents the network architecture of the deep learning model along with its metadata that are necessary to develop adeep learning model. Through the proposed description language, developers can easily verify the accuracy of the referenced deep learningmodel. Our experiments demonstrate the application scenario of a deep learning description document that focuses on the license platerecognition for the detection of illegally parked vehicles.Trusted Deep Learning, Model Reference, Deep Learning Description Language, Traffic Situation Analysis Model"
392,딥러닝 분석을 이용한 중국 역내·외 위안화 변동성 예측,이우식 /Lee Woosik,2016,한국데이터정보과학회,"2008년 글로벌 금융위기 이후 중국은 위안화 국제화의 점진적 추진을 진행하면서 중국상하이 외환시장과 중국홍콩 외환시장에서 거래되는 통화인 역내위안화와 역외위안화를 형성시켰다. 본 연구는 위안화 국제화와 점진적인 중국 자본계정 개방에 따라 급변하는 외환시장상황의 변동성을 정확하게 파악하기 위해서 GARCH모형 (일반화된 자기회귀 조건부이분산성모형)에 다단계인공신경망을 결합한 MLP-GARCH 모형과 GARCH모형과 기계학습의 일종인 딥러닝 (deep learning)을 통합한 DL-GARCH을 가지고 위안화 변동성예측을 비교 실험과 분석을 하였다. 비교분석 결과 DLGARCH 모형은 MLP-GARCH보다 모형 위안화 역내·외 환율변동성 예측 면에서 더욱 더 개선된예측값을 제공하였다. 그래서 이분산시계열모형을 딥러닝과 결합한 DL-GARCH 모형은 시계열의환율 변동성 예측 문제에 딥러닝을 응용할 수 있음을 확인하였다. 향후 이분산시계열과 결합된 딥러닝 모형은 다른 금융시계열 데이터에 응용하여 그 일반화 가능성을 높일 수 있을 것이다.The People’s Republic of China has vigorously been pursuing the internationalization of the Chinese Yuan or Renminbi after the financial crisis of 2008. In this view, an abrupt increase of use of the Chinese Yuan in the onshore and offshore markets are important milestones to be one of important currencies. One of the most frequently used methods to forecast volatility is GARCH model. Since a prediction error of the GARCH model has been reported quite high, a lot of efforts have been made to improve forecasting capability of the GARCH model. In this paper, we have proposed MLPGARCH and a DL-GARCH by employing Artificial Neural Network to the GARCH. In an application to forecasting Chinese Yuan volatility, we have successfully shown their overall outperformance in forecasting over the GARCH.Deep learning, DL-GARCH, GARCH, The Chinese Yuan, volatility."
393,딥러닝 기법을 이용한 내일강수 예측,하지훈,2016,한국지능시스템학회,"정확한 강수예측을 위해서는 예측인자 선정과 예측방법에 대한 선택이 매우 중요하다. 최근에는 강수예측 방법으로기계학습 기법이 많이 사용되고 있으며, 그 중에서도 특히 인공신경망을 사용한 강수예측 방법은 좋은 성능을 보였다. 본논문에서는 딥러닝 기법 중 하나인 DBN(deep belief network)를 이용한 새로운 강수예측 방법을 제안한다. DBN는 비지도사전 학습을 통해 초기 가중치를 설정하여 기존 인공신경망의 문제점을 보완한다. 예측인자로는 기온, 전일-전주 강수일, 태양과 달 궤도 관련 자료를 선정하였다. 기온과 전일-전주 강수일은 서울에서의 1974년부터 2013년까지 총 40년간의AWS(automatic weather system) 관측 자료를 사용하였고, 태양과 달의 궤도 관련 자료는 서울을 중심으로 계산한 결과를사용하였다. 전체 기간에서 일부는 학습 자료로 사용하여 예측모델을 생성하였고, 나머지를 생성한 모델의 검증 자료로사용하였다. 모델 검증 결과로 나온 예측값들은 확률값을 가지며 임계치를 이용하여 강수유무를 판별하였다. 강수 정확도의척도로 양분예보기법 중 CSI(critical successive index)와 Bias(frequency bias)를 계산하였다. 이를 통해 DBN와 MLP(multilayer perceptron)의 성능을 비교한 결과 DBN의 강수 예측 정확도가 높았고, 수행속도 또한 2배 이상 빨랐다.For accurate precipitation forecasts the choice of weather faoctrs and prediction method is very important. Recently, machinele arning has been widely used for forecasting precipitation, and artificial neural network, one of machine learning techniques, showed good performance. In this paper, we suggest a new method for forecasting precipitation using DBN, one of deep learning techniques.
DBN has an advantage that initial weights are set by unsupervised learning, so this compensates for the defects of artificial neural networks. We used past precipitation, temperature, and the parameters of the sun and moon's motion as features for forecasting precipitation. The dataset consists of observation data which had been measured for 40 years from AWS in Seoul. Experiments were based on 8-fold cross validation. As a result of estimation, we got probabilities of test dataset, so threshold was used for the decision of precipitation. CSI and Bias were used for indicating the precision of precipitation. Our experimental results showed that DBN performed better than MLP.Deep learning, Deep belief network, Precipitation, Forecast"
394,딥러닝 신경망모형을 이용한 실시간 교통정보수집 연구,김대현 /KIM DAE HYON,2016,사단법인 인문사회과학기술융합학회,"비디오 기반의 실시간 교통정보수집 시스템은 자율주행을 포함한 지능형교통체계의 주요 서브시스템중 하나이며, 우수한 예측(추정)력 때문에 핵심 알고리즘으로 인공신경망 모형이 주로 사용되고 있다. 특히, 최근 과학 학술지 네이처에 구글 딥마인드의 알파고가 공개되면서 딥러닝과 인공지능에 대한 전 세계적인 관심이 집중되고 있다. 최근까지 가장 널리 사용된 인공신경망모형은 역전파 모형이었으나 최근 컴퓨터 하드웨어 및 알고리즘의 새로운 개발과 더불어 딥러닝 알고리즘을 이용한 딥인공신경망 모형에 대한 관심이 높아지고 있다. 그러나 아직 딥러닝 인공신경망에 대한 연구는 초기단계에 있는 실정이며 적용분야도 극히 한정되어 있다. 본 연구에서는 실시간 도로교통정보 수집을 위한 영상처리분야에서 딥러닝 알고리즘을 활용하고 실험결과를 통하여 향후 교통정보수집‧분석 분야에서의 딥러닝 인공신경망 모형의 적용가능성을 살펴보고자 한다. 특히, 본 연구에서 도출한 딥러닝 인공신경망모형의 실험결과를 기존 역전파 모형에 의한 실험결과와 비교하여 모형의 우수성을 비교‧평가하고자 한다.Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Normally, artificial neural networks have been used in real-time applications for collecting traffic information because of their remarkable performance in terms of predictive accuracy and real-time computing. Especially, the recent publication of a paper in the journal Nature and the success of Google DeepMind’s world-beating Go program AlphaGo caused significant increases of the public interest in artificial intelligence. Even though various types of learning models have been proposed, the Backpropagation has been the most popular neural network model. However, the Deep learning neural network model has attracted much interest recently. In this study, the Deep leaning neural network will be applied in image processing techniques for the real-time traffic information collection to investigate the performance of Deep learning algorithm in traffic engineering fields. In addition, the experimental results from Deep learing algorithm will be compared with the results from Backpropagation model to evaluate and propose better performing model.Intelligent Transportation Systems (ITS), artificial neural networks, Deep learning, traffic information, image processing"
395,딥러닝의 모형과 응용사례,안성만 /SungMahn Ahn,2016,한국지능정보시스템학회,"딥러닝은 인공신경망(neural network)이라는 인공지능분야의 모형이 발전된 형태로서, 계층구조로 이루어진 인공신경망의 내부계층(hidden layer)이 여러 단계로 이루어진 구조이다. 딥러닝에서의 주요 모형은 합성곱신경망(convolutional neural network), 순환신경망(recurrent neural network), 그리고 심층신뢰신경망(deep belief network)의 세가지라고 할 수있다. 그 중에서 현재 흥미로운 연구가 많이 발표되어서 관심이 집중되고 있는 모형은 지도학습(supervised learning)모형인 처음 두 개의 모형이다. 따라서 본 논문에서는 지도학습모형의 가중치를 최적화하는 기본적인 방법인 오류역전파 알고리즘을 살펴본 뒤에 합성곱신경망과 순환신경망의 구조와 응용사례 등을 살펴보고자 한다. 본문에서 다루지 않은 모형인 심층신뢰신경망은 아직까지는 합성곱신경망 이나 순환신경망보다는 상대적으로 주목을 덜 받고 있다. 그러나 심층신뢰신경망은 CNN이나 RNN과는 달리 비지도학습(unsupervised learning)모형이며, 사람이나 동물은 관찰을 통해서 스스로 학습한다는 점에서 궁극적으로는 비지도학습모형이 더 많이 연구되어야 할 주제가 될 것이다.Deep learning model is a kind of neural networks that allows multiple hidden layers. There are various deep learning architectures such as convolutional neural networks, deep belief networks and recurrent neural networks. Those have been applied to fields like computer vision, automatic speech recognition, natural language processing, audio recognition and bioinformatics where they have been shown to produce state-of-the-art results on various tasks. Among those architectures, convolutional neural networks and recurrent neural networks are classified as the supervised learning model. And in recent years, those supervised learning models have gained more popularity than unsupervised learning models such as deep belief networks, because supervised learning models have shown fashionable applications in such fields mentioned above.
Deep learning models can be trained with backpropagation algorithm. Backpropagation is an abbreviation for “backward propagation of errors” and a common method of training artificial neural networks used in conjunction with an optimization method such as gradient descent. The method calculates the gradient of an error function with respect to all the weights in the network. The gradient is fed to the optimization method which in turn uses it to update the weights, in an attempt to minimize the error function.
Convolutional neural networks use a special architecture which is particularly well-adapted to classify images. Using this architecture makes convolutional networks fast to train. This, in turn, helps us train deep, muti-layer networks, which are very good at classifying images. These days, deep convolutional networks are used in most neural networks for image recognition. Convolutional neural networks use three basic ideas: local receptive fields, shared weights, and pooling. By local receptive fields, we mean that each neuron in the first(or any) hidden layer will be connected to a small region of the input(or previous layer’s) neurons. Shared weights mean that we’re going to use the same weights and bias for each of the local receptive field. This means that all the neurons in the hidden layer detect exactly the same feature, just at different locations in the input image. In addition to the convolutional layers just described, convolutional neural networks also contain pooling layers. Pooling layers are usually used immediately after convolutional layers. What the pooling layers do is to simplify the information in the output from the convolutional layer.
Recent convolutional network architectures have 10 to 20 hidden layers and billions of connections between units. Training deep learning networks has taken weeks several years ago, but thanks to progress in GPU and algorithm enhancement, training time has reduced to several hours.
Neural networks with time-varying behavior are known as recurrent neural networks or RNNs. A recurrent neural network is a class of artificial neural network where connections between units form a directed cycle. This creates an internal state of the network which allows it to exhibit dynamic temporal behavior. Unlike feedforward neural networks, RNNs can use their internal memory to process arbitrary sequences of inputs. Early RNN models turned out to be very difficult to train, harder even than deep feedforward networks. The reason is the unstable gradient problem such as vanishing gradient and exploding gradient. The gradient can get smaller and smaller as it is propagated back through layers. This makes learning in early layers extremely slow. The problem actually gets worse in RNNs, since gradients aren’t just propagated backward through layers, they’re propagated backward through time. If the network runs for a long time, that can make the gradient extremely unstable and hard to learn from. It has been possible to incorporate an idea known as long short-term memory units (LSTMs) into RNNs. LSTMs make it much easier to get good results when training RNNs, and many recent papers make use of LSTMs or related ideas.Deep learning, Convolutional neural networks, Recurrent neural networks, Error backpropagation algorithm"
396,딥러닝분석과 기술적 분석 지표를 이용한 한국 코스피주가지수 방향성 예측,이우식 /Lee Woosik,2017,한국데이터정보과학회,"2016년 3월 구글 (Google)의 바둑인공지능 알파고 (AlphaGo)가 이세돌 9단과의 바둑대결에서승리한 이후 다양한 분야에서 인공지능 사용에 대한 관심이 높아지고 있는 가운데 금융투자 분야에서도 인공지능과 투자자문 전문가의 합성어인 로보어드바이저 (Robo-Advisor)에 대한 관심이 높아지고 있다. 인공지능 (artificial intelligence)기반의 의사결정은 비용 절감은 물론 효과적인 의사결정을가능하게 한다는 점에서 큰 장점이 있다. 본 연구에서는 기술적 분석 (technical analysis) 지표와 딥러닝 (deep learning) 모형을 결합하여 한국 코스피 지수를 예측하는 모형을 개발하고 제시한 모형들의 예측력을 비교, 분석한다. 분석 결과 기술적 분석 지표에 딥러닝 알고리즘을 결합한 모형이 주가지수 방향성 예측 문제에 응용될 수 있음을 확인하였다. 향후 본 연구에서 제안된 기술적 분석 지표와딥러닝모형을 결합한 기법은 로보어드바이저서비스에 응용할 수 있는 일반화 가능성을 보여준다Since Google’s AlphaGo defeated a world champion of Go players in 2016, there have been many interests in the deep learning. In the financial sector, a Robo-Advisor using deep learning gains a significant attention, which builds and manages portfolios of financial instruments for investors.In this paper, we have proposed the a deep learning algorithm geared toward identification and forecast of the KOSPI index direction,and we also have compared the accuracy of the prediction.In an application of forecasting the financial market index direction, we have shown that the Robo-Advisor using deep learning has a significant effect on finance industry. The Robo-Advisor collects a massive data such as earnings statements, news reports and regulatory filings, analyzes those and recommends investors how to view market trends and identify the best time to purchase financial assets. On the other hand, the Robo-Advisor allows businesses to learn more about their customers, develop better marketing strategies, increase sales and decrease costs.Artificial intelligence, deep learning, FinTech, Robo-Advisor, technical analysis."
397,딥러닝의 효율적인 학습을 위한 학습데이터 선별 기법에 관한 연구,주경돈 /Gyoung-don Joo,2017,한국정보과학회,"빅데이터의 증가와 병렬 하드웨어 처리, 학습 알고리즘의 개선은 딥러닝의 발전을 가져왔으며 인공지능이 적용 가능한 영역을 획기적으로 확장시키고 있다. 하지만 이러한 대규모의 학습 데이터에 기반한 딥러닝은 학습시간이 증가되고, 더 나아가 학습 데이터의 구축을 위한 레이블링(labeling) 비용이 데이터 수에 비례하여 증가하게 된다. 따라서 본 논문에서는, 딥러닝의 정확도를 최대한 유지하면서 필요한 학습 데이터의 수를 줄일수 있도록, 전체 데이터에서 제한된 수의 학습 데이터의 후보를 선별하는 기법에 대하여 연구한다. 특히 기존의 능동 학습에 기반한 기법들이 학습 데이터의 수가 매우 적어질 경우 딥러닝의 정확도가 급격히 떨어지는 문제를 해결할 수 있는 새로운 기법을 제안하였으며, 제안된 기법으로 선별된 학습 데이터는 그 수의 변화에 대해 딥러닝이 일관되게 경쟁력있는 정확도를 가짐을 다양한 실험을 통하여 검증하였다.The increase of big data, parallel hardware processing and enhanced learning algorithms have led to the development of deep learning and are dramatically expanding the domain of applications based on artificial intelligence. However, such large-scale training data extends the learning time, and furthermore, the labeling cost for building the training set increases in proportion to the volume of data. Therefore, in this paper, we study how to select a limited number of candidates for training set from given data so as to reduce the number of training set while maintaining the accuracy of deep learning. Especially, we found that, with the training set selected by the previous work, the accuracy of deep learning drastically decreases when the number of training set is very small and so proposed new methods to resolve this fall-off in accuracy. Through experiments, we showed that deep learning consistently has the competitive accuracy with the training set selected by the proposed method regardless of the size of training set.Deep Learning, Train Set Selection, Active Learning, Gamma distribution"
398,딥러닝을 이용한 범용적 스테그아날리시스,김현재 /Hyunjae Kim,2017,한국정보과학회,"스테그아날리시스(Steganalysis)란 이미지 등 일반적인 자료에 암호화된 정보를 은닉하는 스테가노그래피(Steganography)에 대한 검출 및 분석 방법으로, 기계학습 기반 방법론을 포함한다. 기존 기계학습 기반 스테그아날리시스는 영상(Image)의 특징(Feature) 추출 및 모델링에 기반하며, 최근 딥러닝(Deep Learning)의 적용으로 검출 정확도가 큰 폭으로 향상되었다. 하지만 현존하는 스테그아날리시스 모델은 단일 스테가노그래피 기법에 대해 국한되어 있어 학습에 사용되지 않은 스테고(Stego) 이미지의 경우 검출이 불가능한 결정적 한계를 가진다. 본 연구에서는 다양한 스테가노그래피 기법으로 생성된 스테고이미지에 딥러닝을 적용하여 스테그아날리시스를 학습하는 범용적 모델을 제안한다. 다양한 실험을 통해 제안 기법의 효용성 및 가능성을 확인하고, 범용적 스테그아날리시스 모델이 각각에 특화된 검출 기법과 유사한 정확도로 스테고 이미지를 검출할 수 있음을 보인다.Steganalysis is to detect information hidden by steganography inside general data such as images. There are stegoanalysis techniques that use machine learning (ML). Existing ML approaches to steganalysis are based on extracting features from stego images and modeling them.
Recently deep learning-based methodologies have shown significant improvements in detection accuracy. However, all the existing methods, including deep learning-based ones, have a critical limitation in that they can only detect stego images that are created by a specific steganography method. In this paper, we propose a generalized steganalysis method that can model multiple types of stego images using deep learning. Through various experiments, we confirm the effectiveness of our approach and envision directions for future research. In particular, we show that our method can detect each type of steganography with the same level of accuracy as that of a steganalysis method dedicated to that type of steganography, thereby demonstrating the general applicability of our approach to multiple types of stego images.steganalysis, steganography, deep learning, generalized steganalysis model"
399,"딥러닝 프레임워크의 비교: 티아노, 텐서플로, CNTK를 중심으로",정여진 /Yeojin Chung,2017,한국지능정보시스템학회,"딥러닝 프레임워크의 대표적인 기능으로는 ‘자동미분’과 ‘GPU의 활용’ 등을 들 수 있다. 본 논문은 파이썬의 라이브러리 형태로 사용 가능한 프레임워크 중에서 구글의 텐서플로와 마이크로소프트의 CNTK, 그리고 텐서 플로의 원조라고 할 수 있는 티아노를 비교하였다. 본문에서는 자동미분의 개념과 GPU의 활용형태를 간단히 설명하고, 그 다음에 logistic regression을 실행하는 예를 통하여 각 프레임워크의 문법을 알아본 뒤에, 마지막으로 대표적인 딥러닝 응용인 CNN의 예제를 실행시켜보고 코딩의 편의성과 실행속도 등을 확인해 보았다. 그 결과, 편의성의 관점에서 보면 티아노가 가장 코딩 하기가 어렵고, CNTK와 텐서플로는 많은 부분이 비슷하게 추상화 되어 있어서 코딩이 비슷하지만 가중치와 편향을 직접 정의하느냐의 여부에서 차이를 보였다. 그리고 각프레임워크의 실행속도에 대한 평가는 ‘큰 차이는 없다’는 것이다. 텐서플로는 티아노에 비하여 속도가 느리다는 평가가 있어왔는데, 본 연구의 실험에 의하면, 비록 CNN 모형에 국한되었지만, 텐서플로가 아주 조금이지만빠른 것으로 나타났다. CNTK의 경우에도, 비록 실험환경이 달랐지만, 실험환경의 차이에 의한 속도의 차이의 편차범위 이내에 있는 것으로 판단이 되었다. 본 연구에서는 세 종류의 딥러닝 프레임워크만을 살펴보았는데,위키피디아에 따르면 딥러닝 프레임워크의 종류는 12가지가 있으며, 각 프레임워크의 특징을 15가지 속성으로구분하여 차이를 특정하고 있다. 그 많은 속성 중에서 사용자의 입장에서 볼 때 중요한 속성은 어떤 언어(파이썬, C++, Java, 등)로 사용가능한지, 어떤 딥러닝 모형에 대한 라이브러리가 잘 구현되어 있는지 등일 것이다. 그리고 사용자가 대규모의 딥러닝 모형을 구축한다면, 다중 GPU 혹은 다중 서버를 지원하는지의 여부도 중요할 것이다. 또한 딥러닝 모형을 처음 학습하는 경우에는 사용설명서가 많은지 예제 프로그램이 많은지 여부도 중요한 기준이 될 것이다.The deep learning framework is software designed to help develop deep learning models. Some of its important functions include “automatic differentiation” and “utilization of GPU”. The list of popular deep learning framework includes Caffe (BVLC) and Theano (University of Montreal). And recently, Microsoft's deep learning framework, Microsoft Cognitive Toolkit, was released as open-source license, following Google’s Tensorflow a year earlier. The early deep learning frameworks have been developed mainly for research at universities. Beginning with the inception of Tensorflow, however, it seems that companies such as Microsoft and Facebook have started to join the competition of framework development. Given the trend, Google and other companies are expected to continue investing in the deep learning framework to bring forward the initiative in the artificial intelligence business. From this point of view, we think it is a good time to compare some of deep learning frameworks. So we compare three deep learning frameworks which can be used as a Python library. Those are Google's Tensorflow, Microsoft’s CNTK, and Theano which is sort of a predecessor of the preceding two.
The most common and important function of deep learning frameworks is the ability to perform automatic differentiation. Basically all the mathematical expressions of deep learning models can be represented as computational graphs, which consist of nodes and edges. Partial derivatives on each edge of a computational graph can then be obtained. With the partial derivatives, we can let software compute differentiation of any node with respect to any variable by utilizing chain rule of Calculus.
First of all, the convenience of coding is in the order of CNTK, Tensorflow, and Theano. The criterion is simply based on the lengths of the codes and the learning curve and the ease of coding are not the main concern. According to the criteria, Theano was the most difficult to implement with, and CNTK and Tensorflow were somewhat easier. With Tensorflow, we need to define weight variables and biases explicitly. The reason that CNTK and Tensorflow are easier to implement with is that those frameworks provide us with more abstraction than Theano. We, however, need to mention that low-level coding is not always bad. It gives us flexibility of coding. With the low-level coding such as in Theano, we can implement and test any new deep learning models or any new search methods that we can think of.
The assessment of the execution speed of each framework is that there is not meaningful difference. According to the experiment, execution speeds of Theano and Tensorflow are very similar, although the experiment was limited to a CNN model. In the case of CNTK, the experimental environment was not maintained as the same. The code written in CNTK has to be run in PC environment without GPU where codes execute as much as 50 times slower than with GPU. But we concluded that the difference of execution speed was within the range of variation caused by the different hardware setup.
In this study, we compared three types of deep learning framework: Theano, Tensorflow, and CNTK. According to Wikipedia, there are 12 available deep learning frameworks. And 15 different attributes differentiate each framework. Some of the important attributes would include interface language (Python, C ++, Java, etc.) and the availability of libraries on various deep learning models such as CNN, RNN, DBN, and etc. And if a user implements a large scale deep learning model, it will also be important to support multiple GPU or multiple servers. Also, if you are learning the deep learning model, it would also be important if there are enough examples and references.deep learning framework, Theano, TensorFlow, CNTK, computational graph, CIFAR-10"
400,딥러닝을 이용한 주변 무선단말 파악방안,이웅섭 /Lee， Woong-sup,2018,한국정보통신학회,"최근 단말-대-단말(Device-to-device, D2D) 통신기술이 차세대 무선통신시스템의 핵심기술로 큰 관심을 받고 있다. 이러한 단말간 통신에서는 자신의 주변에 어떠한 단말이 있는지 파악하는 주변단말 탐색(Neighbor discovery)이매우 중요하다. 본 논문에서는 최근 큰 관심을 받고 있는 딥러닝(Deep learning) 기술을 활용하여 단말간 통신에서 주변단말을 파악하는 방안에 대해서 제안한다. 제안 방안은 기존의 방안과 달리 무선채널의 공간적 연관성을 이용하여단말간의 신호 전송 없이 단말이 기지국으로 전송하는 상향링크 파일럿 신호를 기반으로 주변 단말을 찾고 따라서기존의 방식에 비해 신호전송 복잡도(signaling complexity)를 크게 줄일 수 있다. 또한 제안 방안에서는 떨어져 있는거리에 따라서 주변 단말을 분류 가능하여 기존 방안에 비해서 좀 더 세밀한 단말 탐색이 가능하다. 마지막으로 본 논문에서는 tensorflow를 이용한 컴퓨터 시뮬레이션을 통해 제안 방안의 성능을 검증하였다.Recently, the device-to-device (D2D) communication has been conceived as the key technology for the next-generationmobile communication systems. The neighbor discovery in which the nearby users are found, is essential for the properoperation of the D2D communication. In this paper, we propose new neighbor discovery scheme based on deep learningtechnology which has gained a lot of attention recently. In the proposed scheme, the neighboring users can be foundusing the uplink pilot transmission of users only, unlike conventional neighbor discovery schemes in which direct pilotcommunication among users is required, such that the signaling overhead can be greatly reduced in our proposed scheme.
Moreover, the neighbors with different proximity can also be classified accordingly which enables more accurate neighbordiscovery compared to the conventional schemes. The performance of our proposed scheme is verified through thetensorflow-based computer simulations.
키워드 : 딥러닝, 주변단말 탐색, 단말-대-단말 통신Deep learning, Neighbor discovery, Device-to-device communication, Wireless channel correlation"
401,신뢰성있는 딥러닝 기반 분석 모델을 참조하기 위한 딥러닝 기술 언어,문종혁 /Jong Hyeok Mun,2021,한국정보처리학회,"최근 딥러닝은 하드웨어 성능이 향상됨에 따라 자연어 처리, 영상 인식 등의 다양한 기술에 접목되어 활용되고 있다. 이러한 기술들을 활용해 지능형교통 시스템(ITS), 스마트홈, 헬스케어 등의 산업분야에서 데이터를 분석하여 고속도로 속도위반 차량 검출, 에너지 사용량 제어, 응급상황 등과 같은고품질의 서비스를 제공하며, 고품질의 서비스를 제공하기 위해서는 정확도가 향상된 딥러닝 모델이 적용되어야 한다. 이를 위해 서비스 환경의 데이터를분석하기 위한 딥러닝 모델을 개발할 때, 개발자는 신뢰성이 검증된 최신의 딥러닝 모델을 적용할 수 있어야 한다. 이는 개발자가 참조하는 딥러닝 모델에적용된 학습 데이터셋의 정확도를 측정하여 검증할 수 있다. 이러한 검증을 위해서 개발자는 학습 데이터셋, 딥러닝의 계층구조 및 개발 환경 등과 같은내용을 포함하는 딥러닝 모델을 문서화하여 적용하기 위한 구조적인 정보가 필요하다. 본 논문에서는 신뢰성있는 딥러닝 기반 데이터 분석 모델을 참조하기위한 딥러닝 기술 언어를 제안한다. 제안하는 기술 언어는 신뢰성 있는 딥러닝 모델을 개발하는데 필요한 학습데이터셋, 개발 환경 및 설정 등의 정보와더불어 딥러닝 모델의 계층구조를 표현할 수 있다. 제안하는 딥러닝 기술 언어를 이용하여 개발자는 지능형 교통 시스템에서 참조하는 분석 모델의정확도를 검증할 수 있다. 실험에서는 제안하는 언어의 유효성을 검증하기 위해, 번호판 인식 모델을 중심으로 딥러닝 기술 문서의 적용과정을 보인다With the recent advancements of deep learning, companies such as smart home, healthcare, and intelligent transportation systems areutilizing its functionality to provide high-quality services for vehicle detection, emergency situation detection, and controlling energyconsumption. To provide reliable services in such sensitive systems, deep learning models are required to have high accuracy. In orderto develop a deep learning model for analyzing previously mentioned services, developers should utilize the state of the art deep learningmodels that have already been verified for higher accuracy. The developers can verify the accuracy of the referenced model by validatingthe model on the dataset. For this validation, the developer needs structural information to document and apply deep learning models,including metadata such as learning dataset, network architecture, and development environments. In this paper, we propose a descriptionlanguage that represents the network architecture of the deep learning model along with its metadata that are necessary to develop adeep learning model. Through the proposed description language, developers can easily verify the accuracy of the referenced deep learningmodel. Our experiments demonstrate the application scenario of a deep learning description document that focuses on the license platerecognition for the detection of illegally parked vehicles.Trusted Deep Learning, Model Reference, Deep Learning Description Language, Traffic Situation Analysis Model"
402,딥러닝 분석을 이용한 중국 역내·외 위안화 변동성 예측,이우식 /Lee Woosik,2016,한국데이터정보과학회,"2008년 글로벌 금융위기 이후 중국은 위안화 국제화의 점진적 추진을 진행하면서 중국상하이 외환시장과 중국홍콩 외환시장에서 거래되는 통화인 역내위안화와 역외위안화를 형성시켰다. 본 연구는 위안화 국제화와 점진적인 중국 자본계정 개방에 따라 급변하는 외환시장상황의 변동성을 정확하게 파악하기 위해서 GARCH모형 (일반화된 자기회귀 조건부이분산성모형)에 다단계인공신경망을 결합한 MLP-GARCH 모형과 GARCH모형과 기계학습의 일종인 딥러닝 (deep learning)을 통합한 DL-GARCH을 가지고 위안화 변동성예측을 비교 실험과 분석을 하였다. 비교분석 결과 DLGARCH 모형은 MLP-GARCH보다 모형 위안화 역내·외 환율변동성 예측 면에서 더욱 더 개선된예측값을 제공하였다. 그래서 이분산시계열모형을 딥러닝과 결합한 DL-GARCH 모형은 시계열의환율 변동성 예측 문제에 딥러닝을 응용할 수 있음을 확인하였다. 향후 이분산시계열과 결합된 딥러닝 모형은 다른 금융시계열 데이터에 응용하여 그 일반화 가능성을 높일 수 있을 것이다.The People’s Republic of China has vigorously been pursuing the internationalization of the Chinese Yuan or Renminbi after the financial crisis of 2008. In this view, an abrupt increase of use of the Chinese Yuan in the onshore and offshore markets are important milestones to be one of important currencies. One of the most frequently used methods to forecast volatility is GARCH model. Since a prediction error of the GARCH model has been reported quite high, a lot of efforts have been made to improve forecasting capability of the GARCH model. In this paper, we have proposed MLPGARCH and a DL-GARCH by employing Artificial Neural Network to the GARCH. In an application to forecasting Chinese Yuan volatility, we have successfully shown their overall outperformance in forecasting over the GARCH.Deep learning, DL-GARCH, GARCH, The Chinese Yuan, volatility."
403,딥러닝 기법을 이용한 내일강수 예측,하지훈,2016,한국지능시스템학회,"정확한 강수예측을 위해서는 예측인자 선정과 예측방법에 대한 선택이 매우 중요하다. 최근에는 강수예측 방법으로기계학습 기법이 많이 사용되고 있으며, 그 중에서도 특히 인공신경망을 사용한 강수예측 방법은 좋은 성능을 보였다. 본논문에서는 딥러닝 기법 중 하나인 DBN(deep belief network)를 이용한 새로운 강수예측 방법을 제안한다. DBN는 비지도사전 학습을 통해 초기 가중치를 설정하여 기존 인공신경망의 문제점을 보완한다. 예측인자로는 기온, 전일-전주 강수일, 태양과 달 궤도 관련 자료를 선정하였다. 기온과 전일-전주 강수일은 서울에서의 1974년부터 2013년까지 총 40년간의AWS(automatic weather system) 관측 자료를 사용하였고, 태양과 달의 궤도 관련 자료는 서울을 중심으로 계산한 결과를사용하였다. 전체 기간에서 일부는 학습 자료로 사용하여 예측모델을 생성하였고, 나머지를 생성한 모델의 검증 자료로사용하였다. 모델 검증 결과로 나온 예측값들은 확률값을 가지며 임계치를 이용하여 강수유무를 판별하였다. 강수 정확도의척도로 양분예보기법 중 CSI(critical successive index)와 Bias(frequency bias)를 계산하였다. 이를 통해 DBN와 MLP(multilayer perceptron)의 성능을 비교한 결과 DBN의 강수 예측 정확도가 높았고, 수행속도 또한 2배 이상 빨랐다.For accurate precipitation forecasts the choice of weather faoctrs and prediction method is very important. Recently, machinele arning has been widely used for forecasting precipitation, and artificial neural network, one of machine learning techniques, showed good performance. In this paper, we suggest a new method for forecasting precipitation using DBN, one of deep learning techniques.
DBN has an advantage that initial weights are set by unsupervised learning, so this compensates for the defects of artificial neural networks. We used past precipitation, temperature, and the parameters of the sun and moon's motion as features for forecasting precipitation. The dataset consists of observation data which had been measured for 40 years from AWS in Seoul. Experiments were based on 8-fold cross validation. As a result of estimation, we got probabilities of test dataset, so threshold was used for the decision of precipitation. CSI and Bias were used for indicating the precision of precipitation. Our experimental results showed that DBN performed better than MLP.Deep learning, Deep belief network, Precipitation, Forecast"
404,딥러닝 신경망모형을 이용한 실시간 교통정보수집 연구,김대현 /KIM DAE HYON,2016,사단법인 인문사회과학기술융합학회,"비디오 기반의 실시간 교통정보수집 시스템은 자율주행을 포함한 지능형교통체계의 주요 서브시스템중 하나이며, 우수한 예측(추정)력 때문에 핵심 알고리즘으로 인공신경망 모형이 주로 사용되고 있다. 특히, 최근 과학 학술지 네이처에 구글 딥마인드의 알파고가 공개되면서 딥러닝과 인공지능에 대한 전 세계적인 관심이 집중되고 있다. 최근까지 가장 널리 사용된 인공신경망모형은 역전파 모형이었으나 최근 컴퓨터 하드웨어 및 알고리즘의 새로운 개발과 더불어 딥러닝 알고리즘을 이용한 딥인공신경망 모형에 대한 관심이 높아지고 있다. 그러나 아직 딥러닝 인공신경망에 대한 연구는 초기단계에 있는 실정이며 적용분야도 극히 한정되어 있다. 본 연구에서는 실시간 도로교통정보 수집을 위한 영상처리분야에서 딥러닝 알고리즘을 활용하고 실험결과를 통하여 향후 교통정보수집‧분석 분야에서의 딥러닝 인공신경망 모형의 적용가능성을 살펴보고자 한다. 특히, 본 연구에서 도출한 딥러닝 인공신경망모형의 실험결과를 기존 역전파 모형에 의한 실험결과와 비교하여 모형의 우수성을 비교‧평가하고자 한다.Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Normally, artificial neural networks have been used in real-time applications for collecting traffic information because of their remarkable performance in terms of predictive accuracy and real-time computing. Especially, the recent publication of a paper in the journal Nature and the success of Google DeepMind’s world-beating Go program AlphaGo caused significant increases of the public interest in artificial intelligence. Even though various types of learning models have been proposed, the Backpropagation has been the most popular neural network model. However, the Deep learning neural network model has attracted much interest recently. In this study, the Deep leaning neural network will be applied in image processing techniques for the real-time traffic information collection to investigate the performance of Deep learning algorithm in traffic engineering fields. In addition, the experimental results from Deep learing algorithm will be compared with the results from Backpropagation model to evaluate and propose better performing model.Intelligent Transportation Systems (ITS), artificial neural networks, Deep learning, traffic information, image processing"
405,딥러닝의 모형과 응용사례,안성만 /SungMahn Ahn,2016,한국지능정보시스템학회,"딥러닝은 인공신경망(neural network)이라는 인공지능분야의 모형이 발전된 형태로서, 계층구조로 이루어진 인공신경망의 내부계층(hidden layer)이 여러 단계로 이루어진 구조이다. 딥러닝에서의 주요 모형은 합성곱신경망(convolutional neural network), 순환신경망(recurrent neural network), 그리고 심층신뢰신경망(deep belief network)의 세가지라고 할 수있다. 그 중에서 현재 흥미로운 연구가 많이 발표되어서 관심이 집중되고 있는 모형은 지도학습(supervised learning)모형인 처음 두 개의 모형이다. 따라서 본 논문에서는 지도학습모형의 가중치를 최적화하는 기본적인 방법인 오류역전파 알고리즘을 살펴본 뒤에 합성곱신경망과 순환신경망의 구조와 응용사례 등을 살펴보고자 한다. 본문에서 다루지 않은 모형인 심층신뢰신경망은 아직까지는 합성곱신경망 이나 순환신경망보다는 상대적으로 주목을 덜 받고 있다. 그러나 심층신뢰신경망은 CNN이나 RNN과는 달리 비지도학습(unsupervised learning)모형이며, 사람이나 동물은 관찰을 통해서 스스로 학습한다는 점에서 궁극적으로는 비지도학습모형이 더 많이 연구되어야 할 주제가 될 것이다.Deep learning model is a kind of neural networks that allows multiple hidden layers. There are various deep learning architectures such as convolutional neural networks, deep belief networks and recurrent neural networks. Those have been applied to fields like computer vision, automatic speech recognition, natural language processing, audio recognition and bioinformatics where they have been shown to produce state-of-the-art results on various tasks. Among those architectures, convolutional neural networks and recurrent neural networks are classified as the supervised learning model. And in recent years, those supervised learning models have gained more popularity than unsupervised learning models such as deep belief networks, because supervised learning models have shown fashionable applications in such fields mentioned above.
Deep learning models can be trained with backpropagation algorithm. Backpropagation is an abbreviation for “backward propagation of errors” and a common method of training artificial neural networks used in conjunction with an optimization method such as gradient descent. The method calculates the gradient of an error function with respect to all the weights in the network. The gradient is fed to the optimization method which in turn uses it to update the weights, in an attempt to minimize the error function.
Convolutional neural networks use a special architecture which is particularly well-adapted to classify images. Using this architecture makes convolutional networks fast to train. This, in turn, helps us train deep, muti-layer networks, which are very good at classifying images. These days, deep convolutional networks are used in most neural networks for image recognition. Convolutional neural networks use three basic ideas: local receptive fields, shared weights, and pooling. By local receptive fields, we mean that each neuron in the first(or any) hidden layer will be connected to a small region of the input(or previous layer’s) neurons. Shared weights mean that we’re going to use the same weights and bias for each of the local receptive field. This means that all the neurons in the hidden layer detect exactly the same feature, just at different locations in the input image. In addition to the convolutional layers just described, convolutional neural networks also contain pooling layers. Pooling layers are usually used immediately after convolutional layers. What the pooling layers do is to simplify the information in the output from the convolutional layer.
Recent convolutional network architectures have 10 to 20 hidden layers and billions of connections between units. Training deep learning networks has taken weeks several years ago, but thanks to progress in GPU and algorithm enhancement, training time has reduced to several hours.
Neural networks with time-varying behavior are known as recurrent neural networks or RNNs. A recurrent neural network is a class of artificial neural network where connections between units form a directed cycle. This creates an internal state of the network which allows it to exhibit dynamic temporal behavior. Unlike feedforward neural networks, RNNs can use their internal memory to process arbitrary sequences of inputs. Early RNN models turned out to be very difficult to train, harder even than deep feedforward networks. The reason is the unstable gradient problem such as vanishing gradient and exploding gradient. The gradient can get smaller and smaller as it is propagated back through layers. This makes learning in early layers extremely slow. The problem actually gets worse in RNNs, since gradients aren’t just propagated backward through layers, they’re propagated backward through time. If the network runs for a long time, that can make the gradient extremely unstable and hard to learn from. It has been possible to incorporate an idea known as long short-term memory units (LSTMs) into RNNs. LSTMs make it much easier to get good results when training RNNs, and many recent papers make use of LSTMs or related ideas.Deep learning, Convolutional neural networks, Recurrent neural networks, Error backpropagation algorithm"
406,딥러닝분석과 기술적 분석 지표를 이용한 한국 코스피주가지수 방향성 예측,이우식 /Lee Woosik,2017,한국데이터정보과학회,"2016년 3월 구글 (Google)의 바둑인공지능 알파고 (AlphaGo)가 이세돌 9단과의 바둑대결에서승리한 이후 다양한 분야에서 인공지능 사용에 대한 관심이 높아지고 있는 가운데 금융투자 분야에서도 인공지능과 투자자문 전문가의 합성어인 로보어드바이저 (Robo-Advisor)에 대한 관심이 높아지고 있다. 인공지능 (artificial intelligence)기반의 의사결정은 비용 절감은 물론 효과적인 의사결정을가능하게 한다는 점에서 큰 장점이 있다. 본 연구에서는 기술적 분석 (technical analysis) 지표와 딥러닝 (deep learning) 모형을 결합하여 한국 코스피 지수를 예측하는 모형을 개발하고 제시한 모형들의 예측력을 비교, 분석한다. 분석 결과 기술적 분석 지표에 딥러닝 알고리즘을 결합한 모형이 주가지수 방향성 예측 문제에 응용될 수 있음을 확인하였다. 향후 본 연구에서 제안된 기술적 분석 지표와딥러닝모형을 결합한 기법은 로보어드바이저서비스에 응용할 수 있는 일반화 가능성을 보여준다Since Google’s AlphaGo defeated a world champion of Go players in 2016, there have been many interests in the deep learning. In the financial sector, a Robo-Advisor using deep learning gains a significant attention, which builds and manages portfolios of financial instruments for investors.In this paper, we have proposed the a deep learning algorithm geared toward identification and forecast of the KOSPI index direction,and we also have compared the accuracy of the prediction.In an application of forecasting the financial market index direction, we have shown that the Robo-Advisor using deep learning has a significant effect on finance industry. The Robo-Advisor collects a massive data such as earnings statements, news reports and regulatory filings, analyzes those and recommends investors how to view market trends and identify the best time to purchase financial assets. On the other hand, the Robo-Advisor allows businesses to learn more about their customers, develop better marketing strategies, increase sales and decrease costs.Artificial intelligence, deep learning, FinTech, Robo-Advisor, technical analysis."
407,딥러닝의 효율적인 학습을 위한 학습데이터 선별 기법에 관한 연구,주경돈 /Gyoung-don Joo,2017,한국정보과학회,"빅데이터의 증가와 병렬 하드웨어 처리, 학습 알고리즘의 개선은 딥러닝의 발전을 가져왔으며 인공지능이 적용 가능한 영역을 획기적으로 확장시키고 있다. 하지만 이러한 대규모의 학습 데이터에 기반한 딥러닝은 학습시간이 증가되고, 더 나아가 학습 데이터의 구축을 위한 레이블링(labeling) 비용이 데이터 수에 비례하여 증가하게 된다. 따라서 본 논문에서는, 딥러닝의 정확도를 최대한 유지하면서 필요한 학습 데이터의 수를 줄일수 있도록, 전체 데이터에서 제한된 수의 학습 데이터의 후보를 선별하는 기법에 대하여 연구한다. 특히 기존의 능동 학습에 기반한 기법들이 학습 데이터의 수가 매우 적어질 경우 딥러닝의 정확도가 급격히 떨어지는 문제를 해결할 수 있는 새로운 기법을 제안하였으며, 제안된 기법으로 선별된 학습 데이터는 그 수의 변화에 대해 딥러닝이 일관되게 경쟁력있는 정확도를 가짐을 다양한 실험을 통하여 검증하였다.The increase of big data, parallel hardware processing and enhanced learning algorithms have led to the development of deep learning and are dramatically expanding the domain of applications based on artificial intelligence. However, such large-scale training data extends the learning time, and furthermore, the labeling cost for building the training set increases in proportion to the volume of data. Therefore, in this paper, we study how to select a limited number of candidates for training set from given data so as to reduce the number of training set while maintaining the accuracy of deep learning. Especially, we found that, with the training set selected by the previous work, the accuracy of deep learning drastically decreases when the number of training set is very small and so proposed new methods to resolve this fall-off in accuracy. Through experiments, we showed that deep learning consistently has the competitive accuracy with the training set selected by the proposed method regardless of the size of training set.Deep Learning, Train Set Selection, Active Learning, Gamma distribution"
408,딥러닝을 이용한 범용적 스테그아날리시스,김현재 /Hyunjae Kim,2017,한국정보과학회,"스테그아날리시스(Steganalysis)란 이미지 등 일반적인 자료에 암호화된 정보를 은닉하는 스테가노그래피(Steganography)에 대한 검출 및 분석 방법으로, 기계학습 기반 방법론을 포함한다. 기존 기계학습 기반 스테그아날리시스는 영상(Image)의 특징(Feature) 추출 및 모델링에 기반하며, 최근 딥러닝(Deep Learning)의 적용으로 검출 정확도가 큰 폭으로 향상되었다. 하지만 현존하는 스테그아날리시스 모델은 단일 스테가노그래피 기법에 대해 국한되어 있어 학습에 사용되지 않은 스테고(Stego) 이미지의 경우 검출이 불가능한 결정적 한계를 가진다. 본 연구에서는 다양한 스테가노그래피 기법으로 생성된 스테고이미지에 딥러닝을 적용하여 스테그아날리시스를 학습하는 범용적 모델을 제안한다. 다양한 실험을 통해 제안 기법의 효용성 및 가능성을 확인하고, 범용적 스테그아날리시스 모델이 각각에 특화된 검출 기법과 유사한 정확도로 스테고 이미지를 검출할 수 있음을 보인다.Steganalysis is to detect information hidden by steganography inside general data such as images. There are stegoanalysis techniques that use machine learning (ML). Existing ML approaches to steganalysis are based on extracting features from stego images and modeling them.
Recently deep learning-based methodologies have shown significant improvements in detection accuracy. However, all the existing methods, including deep learning-based ones, have a critical limitation in that they can only detect stego images that are created by a specific steganography method. In this paper, we propose a generalized steganalysis method that can model multiple types of stego images using deep learning. Through various experiments, we confirm the effectiveness of our approach and envision directions for future research. In particular, we show that our method can detect each type of steganography with the same level of accuracy as that of a steganalysis method dedicated to that type of steganography, thereby demonstrating the general applicability of our approach to multiple types of stego images.steganalysis, steganography, deep learning, generalized steganalysis model"
409,"딥러닝 프레임워크의 비교: 티아노, 텐서플로, CNTK를 중심으로",정여진 /Yeojin Chung,2017,한국지능정보시스템학회,"딥러닝 프레임워크의 대표적인 기능으로는 ‘자동미분’과 ‘GPU의 활용’ 등을 들 수 있다. 본 논문은 파이썬의 라이브러리 형태로 사용 가능한 프레임워크 중에서 구글의 텐서플로와 마이크로소프트의 CNTK, 그리고 텐서 플로의 원조라고 할 수 있는 티아노를 비교하였다. 본문에서는 자동미분의 개념과 GPU의 활용형태를 간단히 설명하고, 그 다음에 logistic regression을 실행하는 예를 통하여 각 프레임워크의 문법을 알아본 뒤에, 마지막으로 대표적인 딥러닝 응용인 CNN의 예제를 실행시켜보고 코딩의 편의성과 실행속도 등을 확인해 보았다. 그 결과, 편의성의 관점에서 보면 티아노가 가장 코딩 하기가 어렵고, CNTK와 텐서플로는 많은 부분이 비슷하게 추상화 되어 있어서 코딩이 비슷하지만 가중치와 편향을 직접 정의하느냐의 여부에서 차이를 보였다. 그리고 각프레임워크의 실행속도에 대한 평가는 ‘큰 차이는 없다’는 것이다. 텐서플로는 티아노에 비하여 속도가 느리다는 평가가 있어왔는데, 본 연구의 실험에 의하면, 비록 CNN 모형에 국한되었지만, 텐서플로가 아주 조금이지만빠른 것으로 나타났다. CNTK의 경우에도, 비록 실험환경이 달랐지만, 실험환경의 차이에 의한 속도의 차이의 편차범위 이내에 있는 것으로 판단이 되었다. 본 연구에서는 세 종류의 딥러닝 프레임워크만을 살펴보았는데,위키피디아에 따르면 딥러닝 프레임워크의 종류는 12가지가 있으며, 각 프레임워크의 특징을 15가지 속성으로구분하여 차이를 특정하고 있다. 그 많은 속성 중에서 사용자의 입장에서 볼 때 중요한 속성은 어떤 언어(파이썬, C++, Java, 등)로 사용가능한지, 어떤 딥러닝 모형에 대한 라이브러리가 잘 구현되어 있는지 등일 것이다. 그리고 사용자가 대규모의 딥러닝 모형을 구축한다면, 다중 GPU 혹은 다중 서버를 지원하는지의 여부도 중요할 것이다. 또한 딥러닝 모형을 처음 학습하는 경우에는 사용설명서가 많은지 예제 프로그램이 많은지 여부도 중요한 기준이 될 것이다.The deep learning framework is software designed to help develop deep learning models. Some of its important functions include “automatic differentiation” and “utilization of GPU”. The list of popular deep learning framework includes Caffe (BVLC) and Theano (University of Montreal). And recently, Microsoft's deep learning framework, Microsoft Cognitive Toolkit, was released as open-source license, following Google’s Tensorflow a year earlier. The early deep learning frameworks have been developed mainly for research at universities. Beginning with the inception of Tensorflow, however, it seems that companies such as Microsoft and Facebook have started to join the competition of framework development. Given the trend, Google and other companies are expected to continue investing in the deep learning framework to bring forward the initiative in the artificial intelligence business. From this point of view, we think it is a good time to compare some of deep learning frameworks. So we compare three deep learning frameworks which can be used as a Python library. Those are Google's Tensorflow, Microsoft’s CNTK, and Theano which is sort of a predecessor of the preceding two.
The most common and important function of deep learning frameworks is the ability to perform automatic differentiation. Basically all the mathematical expressions of deep learning models can be represented as computational graphs, which consist of nodes and edges. Partial derivatives on each edge of a computational graph can then be obtained. With the partial derivatives, we can let software compute differentiation of any node with respect to any variable by utilizing chain rule of Calculus.
First of all, the convenience of coding is in the order of CNTK, Tensorflow, and Theano. The criterion is simply based on the lengths of the codes and the learning curve and the ease of coding are not the main concern. According to the criteria, Theano was the most difficult to implement with, and CNTK and Tensorflow were somewhat easier. With Tensorflow, we need to define weight variables and biases explicitly. The reason that CNTK and Tensorflow are easier to implement with is that those frameworks provide us with more abstraction than Theano. We, however, need to mention that low-level coding is not always bad. It gives us flexibility of coding. With the low-level coding such as in Theano, we can implement and test any new deep learning models or any new search methods that we can think of.
The assessment of the execution speed of each framework is that there is not meaningful difference. According to the experiment, execution speeds of Theano and Tensorflow are very similar, although the experiment was limited to a CNN model. In the case of CNTK, the experimental environment was not maintained as the same. The code written in CNTK has to be run in PC environment without GPU where codes execute as much as 50 times slower than with GPU. But we concluded that the difference of execution speed was within the range of variation caused by the different hardware setup.
In this study, we compared three types of deep learning framework: Theano, Tensorflow, and CNTK. According to Wikipedia, there are 12 available deep learning frameworks. And 15 different attributes differentiate each framework. Some of the important attributes would include interface language (Python, C ++, Java, etc.) and the availability of libraries on various deep learning models such as CNN, RNN, DBN, and etc. And if a user implements a large scale deep learning model, it will also be important to support multiple GPU or multiple servers. Also, if you are learning the deep learning model, it would also be important if there are enough examples and references.deep learning framework, Theano, TensorFlow, CNTK, computational graph, CIFAR-10"
410,딥러닝을 이용한 주변 무선단말 파악방안,이웅섭 /Lee， Woong-sup,2018,한국정보통신학회,"최근 단말-대-단말(Device-to-device, D2D) 통신기술이 차세대 무선통신시스템의 핵심기술로 큰 관심을 받고 있다. 이러한 단말간 통신에서는 자신의 주변에 어떠한 단말이 있는지 파악하는 주변단말 탐색(Neighbor discovery)이매우 중요하다. 본 논문에서는 최근 큰 관심을 받고 있는 딥러닝(Deep learning) 기술을 활용하여 단말간 통신에서 주변단말을 파악하는 방안에 대해서 제안한다. 제안 방안은 기존의 방안과 달리 무선채널의 공간적 연관성을 이용하여단말간의 신호 전송 없이 단말이 기지국으로 전송하는 상향링크 파일럿 신호를 기반으로 주변 단말을 찾고 따라서기존의 방식에 비해 신호전송 복잡도(signaling complexity)를 크게 줄일 수 있다. 또한 제안 방안에서는 떨어져 있는거리에 따라서 주변 단말을 분류 가능하여 기존 방안에 비해서 좀 더 세밀한 단말 탐색이 가능하다. 마지막으로 본 논문에서는 tensorflow를 이용한 컴퓨터 시뮬레이션을 통해 제안 방안의 성능을 검증하였다.Recently, the device-to-device (D2D) communication has been conceived as the key technology for the next-generationmobile communication systems. The neighbor discovery in which the nearby users are found, is essential for the properoperation of the D2D communication. In this paper, we propose new neighbor discovery scheme based on deep learningtechnology which has gained a lot of attention recently. In the proposed scheme, the neighboring users can be foundusing the uplink pilot transmission of users only, unlike conventional neighbor discovery schemes in which direct pilotcommunication among users is required, such that the signaling overhead can be greatly reduced in our proposed scheme.
Moreover, the neighbors with different proximity can also be classified accordingly which enables more accurate neighbordiscovery compared to the conventional schemes. The performance of our proposed scheme is verified through thetensorflow-based computer simulations.
키워드 : 딥러닝, 주변단말 탐색, 단말-대-단말 통신Deep learning, Neighbor discovery, Device-to-device communication, Wireless channel correlation"
411,신뢰성있는 딥러닝 기반 분석 모델을 참조하기 위한 딥러닝 기술 언어,문종혁 /Jong Hyeok Mun,2021,한국정보처리학회,"최근 딥러닝은 하드웨어 성능이 향상됨에 따라 자연어 처리, 영상 인식 등의 다양한 기술에 접목되어 활용되고 있다. 이러한 기술들을 활용해 지능형교통 시스템(ITS), 스마트홈, 헬스케어 등의 산업분야에서 데이터를 분석하여 고속도로 속도위반 차량 검출, 에너지 사용량 제어, 응급상황 등과 같은고품질의 서비스를 제공하며, 고품질의 서비스를 제공하기 위해서는 정확도가 향상된 딥러닝 모델이 적용되어야 한다. 이를 위해 서비스 환경의 데이터를분석하기 위한 딥러닝 모델을 개발할 때, 개발자는 신뢰성이 검증된 최신의 딥러닝 모델을 적용할 수 있어야 한다. 이는 개발자가 참조하는 딥러닝 모델에적용된 학습 데이터셋의 정확도를 측정하여 검증할 수 있다. 이러한 검증을 위해서 개발자는 학습 데이터셋, 딥러닝의 계층구조 및 개발 환경 등과 같은내용을 포함하는 딥러닝 모델을 문서화하여 적용하기 위한 구조적인 정보가 필요하다. 본 논문에서는 신뢰성있는 딥러닝 기반 데이터 분석 모델을 참조하기위한 딥러닝 기술 언어를 제안한다. 제안하는 기술 언어는 신뢰성 있는 딥러닝 모델을 개발하는데 필요한 학습데이터셋, 개발 환경 및 설정 등의 정보와더불어 딥러닝 모델의 계층구조를 표현할 수 있다. 제안하는 딥러닝 기술 언어를 이용하여 개발자는 지능형 교통 시스템에서 참조하는 분석 모델의정확도를 검증할 수 있다. 실험에서는 제안하는 언어의 유효성을 검증하기 위해, 번호판 인식 모델을 중심으로 딥러닝 기술 문서의 적용과정을 보인다With the recent advancements of deep learning, companies such as smart home, healthcare, and intelligent transportation systems areutilizing its functionality to provide high-quality services for vehicle detection, emergency situation detection, and controlling energyconsumption. To provide reliable services in such sensitive systems, deep learning models are required to have high accuracy. In orderto develop a deep learning model for analyzing previously mentioned services, developers should utilize the state of the art deep learningmodels that have already been verified for higher accuracy. The developers can verify the accuracy of the referenced model by validatingthe model on the dataset. For this validation, the developer needs structural information to document and apply deep learning models,including metadata such as learning dataset, network architecture, and development environments. In this paper, we propose a descriptionlanguage that represents the network architecture of the deep learning model along with its metadata that are necessary to develop adeep learning model. Through the proposed description language, developers can easily verify the accuracy of the referenced deep learningmodel. Our experiments demonstrate the application scenario of a deep learning description document that focuses on the license platerecognition for the detection of illegally parked vehicles.Trusted Deep Learning, Model Reference, Deep Learning Description Language, Traffic Situation Analysis Model"
412,딥러닝 분석을 이용한 중국 역내·외 위안화 변동성 예측,이우식 /Lee Woosik,2016,한국데이터정보과학회,"2008년 글로벌 금융위기 이후 중국은 위안화 국제화의 점진적 추진을 진행하면서 중국상하이 외환시장과 중국홍콩 외환시장에서 거래되는 통화인 역내위안화와 역외위안화를 형성시켰다. 본 연구는 위안화 국제화와 점진적인 중국 자본계정 개방에 따라 급변하는 외환시장상황의 변동성을 정확하게 파악하기 위해서 GARCH모형 (일반화된 자기회귀 조건부이분산성모형)에 다단계인공신경망을 결합한 MLP-GARCH 모형과 GARCH모형과 기계학습의 일종인 딥러닝 (deep learning)을 통합한 DL-GARCH을 가지고 위안화 변동성예측을 비교 실험과 분석을 하였다. 비교분석 결과 DLGARCH 모형은 MLP-GARCH보다 모형 위안화 역내·외 환율변동성 예측 면에서 더욱 더 개선된예측값을 제공하였다. 그래서 이분산시계열모형을 딥러닝과 결합한 DL-GARCH 모형은 시계열의환율 변동성 예측 문제에 딥러닝을 응용할 수 있음을 확인하였다. 향후 이분산시계열과 결합된 딥러닝 모형은 다른 금융시계열 데이터에 응용하여 그 일반화 가능성을 높일 수 있을 것이다.The People’s Republic of China has vigorously been pursuing the internationalization of the Chinese Yuan or Renminbi after the financial crisis of 2008. In this view, an abrupt increase of use of the Chinese Yuan in the onshore and offshore markets are important milestones to be one of important currencies. One of the most frequently used methods to forecast volatility is GARCH model. Since a prediction error of the GARCH model has been reported quite high, a lot of efforts have been made to improve forecasting capability of the GARCH model. In this paper, we have proposed MLPGARCH and a DL-GARCH by employing Artificial Neural Network to the GARCH. In an application to forecasting Chinese Yuan volatility, we have successfully shown their overall outperformance in forecasting over the GARCH.Deep learning, DL-GARCH, GARCH, The Chinese Yuan, volatility."
413,딥러닝 기법을 이용한 내일강수 예측,하지훈,2016,한국지능시스템학회,"정확한 강수예측을 위해서는 예측인자 선정과 예측방법에 대한 선택이 매우 중요하다. 최근에는 강수예측 방법으로기계학습 기법이 많이 사용되고 있으며, 그 중에서도 특히 인공신경망을 사용한 강수예측 방법은 좋은 성능을 보였다. 본논문에서는 딥러닝 기법 중 하나인 DBN(deep belief network)를 이용한 새로운 강수예측 방법을 제안한다. DBN는 비지도사전 학습을 통해 초기 가중치를 설정하여 기존 인공신경망의 문제점을 보완한다. 예측인자로는 기온, 전일-전주 강수일, 태양과 달 궤도 관련 자료를 선정하였다. 기온과 전일-전주 강수일은 서울에서의 1974년부터 2013년까지 총 40년간의AWS(automatic weather system) 관측 자료를 사용하였고, 태양과 달의 궤도 관련 자료는 서울을 중심으로 계산한 결과를사용하였다. 전체 기간에서 일부는 학습 자료로 사용하여 예측모델을 생성하였고, 나머지를 생성한 모델의 검증 자료로사용하였다. 모델 검증 결과로 나온 예측값들은 확률값을 가지며 임계치를 이용하여 강수유무를 판별하였다. 강수 정확도의척도로 양분예보기법 중 CSI(critical successive index)와 Bias(frequency bias)를 계산하였다. 이를 통해 DBN와 MLP(multilayer perceptron)의 성능을 비교한 결과 DBN의 강수 예측 정확도가 높았고, 수행속도 또한 2배 이상 빨랐다.For accurate precipitation forecasts the choice of weather faoctrs and prediction method is very important. Recently, machinele arning has been widely used for forecasting precipitation, and artificial neural network, one of machine learning techniques, showed good performance. In this paper, we suggest a new method for forecasting precipitation using DBN, one of deep learning techniques.
DBN has an advantage that initial weights are set by unsupervised learning, so this compensates for the defects of artificial neural networks. We used past precipitation, temperature, and the parameters of the sun and moon's motion as features for forecasting precipitation. The dataset consists of observation data which had been measured for 40 years from AWS in Seoul. Experiments were based on 8-fold cross validation. As a result of estimation, we got probabilities of test dataset, so threshold was used for the decision of precipitation. CSI and Bias were used for indicating the precision of precipitation. Our experimental results showed that DBN performed better than MLP.Deep learning, Deep belief network, Precipitation, Forecast"
414,딥러닝 신경망모형을 이용한 실시간 교통정보수집 연구,김대현 /KIM DAE HYON,2016,사단법인 인문사회과학기술융합학회,"비디오 기반의 실시간 교통정보수집 시스템은 자율주행을 포함한 지능형교통체계의 주요 서브시스템중 하나이며, 우수한 예측(추정)력 때문에 핵심 알고리즘으로 인공신경망 모형이 주로 사용되고 있다. 특히, 최근 과학 학술지 네이처에 구글 딥마인드의 알파고가 공개되면서 딥러닝과 인공지능에 대한 전 세계적인 관심이 집중되고 있다. 최근까지 가장 널리 사용된 인공신경망모형은 역전파 모형이었으나 최근 컴퓨터 하드웨어 및 알고리즘의 새로운 개발과 더불어 딥러닝 알고리즘을 이용한 딥인공신경망 모형에 대한 관심이 높아지고 있다. 그러나 아직 딥러닝 인공신경망에 대한 연구는 초기단계에 있는 실정이며 적용분야도 극히 한정되어 있다. 본 연구에서는 실시간 도로교통정보 수집을 위한 영상처리분야에서 딥러닝 알고리즘을 활용하고 실험결과를 통하여 향후 교통정보수집‧분석 분야에서의 딥러닝 인공신경망 모형의 적용가능성을 살펴보고자 한다. 특히, 본 연구에서 도출한 딥러닝 인공신경망모형의 실험결과를 기존 역전파 모형에 의한 실험결과와 비교하여 모형의 우수성을 비교‧평가하고자 한다.Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Normally, artificial neural networks have been used in real-time applications for collecting traffic information because of their remarkable performance in terms of predictive accuracy and real-time computing. Especially, the recent publication of a paper in the journal Nature and the success of Google DeepMind’s world-beating Go program AlphaGo caused significant increases of the public interest in artificial intelligence. Even though various types of learning models have been proposed, the Backpropagation has been the most popular neural network model. However, the Deep learning neural network model has attracted much interest recently. In this study, the Deep leaning neural network will be applied in image processing techniques for the real-time traffic information collection to investigate the performance of Deep learning algorithm in traffic engineering fields. In addition, the experimental results from Deep learing algorithm will be compared with the results from Backpropagation model to evaluate and propose better performing model.Intelligent Transportation Systems (ITS), artificial neural networks, Deep learning, traffic information, image processing"
415,딥러닝의 모형과 응용사례,안성만 /SungMahn Ahn,2016,한국지능정보시스템학회,"딥러닝은 인공신경망(neural network)이라는 인공지능분야의 모형이 발전된 형태로서, 계층구조로 이루어진 인공신경망의 내부계층(hidden layer)이 여러 단계로 이루어진 구조이다. 딥러닝에서의 주요 모형은 합성곱신경망(convolutional neural network), 순환신경망(recurrent neural network), 그리고 심층신뢰신경망(deep belief network)의 세가지라고 할 수있다. 그 중에서 현재 흥미로운 연구가 많이 발표되어서 관심이 집중되고 있는 모형은 지도학습(supervised learning)모형인 처음 두 개의 모형이다. 따라서 본 논문에서는 지도학습모형의 가중치를 최적화하는 기본적인 방법인 오류역전파 알고리즘을 살펴본 뒤에 합성곱신경망과 순환신경망의 구조와 응용사례 등을 살펴보고자 한다. 본문에서 다루지 않은 모형인 심층신뢰신경망은 아직까지는 합성곱신경망 이나 순환신경망보다는 상대적으로 주목을 덜 받고 있다. 그러나 심층신뢰신경망은 CNN이나 RNN과는 달리 비지도학습(unsupervised learning)모형이며, 사람이나 동물은 관찰을 통해서 스스로 학습한다는 점에서 궁극적으로는 비지도학습모형이 더 많이 연구되어야 할 주제가 될 것이다.Deep learning model is a kind of neural networks that allows multiple hidden layers. There are various deep learning architectures such as convolutional neural networks, deep belief networks and recurrent neural networks. Those have been applied to fields like computer vision, automatic speech recognition, natural language processing, audio recognition and bioinformatics where they have been shown to produce state-of-the-art results on various tasks. Among those architectures, convolutional neural networks and recurrent neural networks are classified as the supervised learning model. And in recent years, those supervised learning models have gained more popularity than unsupervised learning models such as deep belief networks, because supervised learning models have shown fashionable applications in such fields mentioned above.
Deep learning models can be trained with backpropagation algorithm. Backpropagation is an abbreviation for “backward propagation of errors” and a common method of training artificial neural networks used in conjunction with an optimization method such as gradient descent. The method calculates the gradient of an error function with respect to all the weights in the network. The gradient is fed to the optimization method which in turn uses it to update the weights, in an attempt to minimize the error function.
Convolutional neural networks use a special architecture which is particularly well-adapted to classify images. Using this architecture makes convolutional networks fast to train. This, in turn, helps us train deep, muti-layer networks, which are very good at classifying images. These days, deep convolutional networks are used in most neural networks for image recognition. Convolutional neural networks use three basic ideas: local receptive fields, shared weights, and pooling. By local receptive fields, we mean that each neuron in the first(or any) hidden layer will be connected to a small region of the input(or previous layer’s) neurons. Shared weights mean that we’re going to use the same weights and bias for each of the local receptive field. This means that all the neurons in the hidden layer detect exactly the same feature, just at different locations in the input image. In addition to the convolutional layers just described, convolutional neural networks also contain pooling layers. Pooling layers are usually used immediately after convolutional layers. What the pooling layers do is to simplify the information in the output from the convolutional layer.
Recent convolutional network architectures have 10 to 20 hidden layers and billions of connections between units. Training deep learning networks has taken weeks several years ago, but thanks to progress in GPU and algorithm enhancement, training time has reduced to several hours.
Neural networks with time-varying behavior are known as recurrent neural networks or RNNs. A recurrent neural network is a class of artificial neural network where connections between units form a directed cycle. This creates an internal state of the network which allows it to exhibit dynamic temporal behavior. Unlike feedforward neural networks, RNNs can use their internal memory to process arbitrary sequences of inputs. Early RNN models turned out to be very difficult to train, harder even than deep feedforward networks. The reason is the unstable gradient problem such as vanishing gradient and exploding gradient. The gradient can get smaller and smaller as it is propagated back through layers. This makes learning in early layers extremely slow. The problem actually gets worse in RNNs, since gradients aren’t just propagated backward through layers, they’re propagated backward through time. If the network runs for a long time, that can make the gradient extremely unstable and hard to learn from. It has been possible to incorporate an idea known as long short-term memory units (LSTMs) into RNNs. LSTMs make it much easier to get good results when training RNNs, and many recent papers make use of LSTMs or related ideas.Deep learning, Convolutional neural networks, Recurrent neural networks, Error backpropagation algorithm"
416,딥러닝분석과 기술적 분석 지표를 이용한 한국 코스피주가지수 방향성 예측,이우식 /Lee Woosik,2017,한국데이터정보과학회,"2016년 3월 구글 (Google)의 바둑인공지능 알파고 (AlphaGo)가 이세돌 9단과의 바둑대결에서승리한 이후 다양한 분야에서 인공지능 사용에 대한 관심이 높아지고 있는 가운데 금융투자 분야에서도 인공지능과 투자자문 전문가의 합성어인 로보어드바이저 (Robo-Advisor)에 대한 관심이 높아지고 있다. 인공지능 (artificial intelligence)기반의 의사결정은 비용 절감은 물론 효과적인 의사결정을가능하게 한다는 점에서 큰 장점이 있다. 본 연구에서는 기술적 분석 (technical analysis) 지표와 딥러닝 (deep learning) 모형을 결합하여 한국 코스피 지수를 예측하는 모형을 개발하고 제시한 모형들의 예측력을 비교, 분석한다. 분석 결과 기술적 분석 지표에 딥러닝 알고리즘을 결합한 모형이 주가지수 방향성 예측 문제에 응용될 수 있음을 확인하였다. 향후 본 연구에서 제안된 기술적 분석 지표와딥러닝모형을 결합한 기법은 로보어드바이저서비스에 응용할 수 있는 일반화 가능성을 보여준다Since Google’s AlphaGo defeated a world champion of Go players in 2016, there have been many interests in the deep learning. In the financial sector, a Robo-Advisor using deep learning gains a significant attention, which builds and manages portfolios of financial instruments for investors.In this paper, we have proposed the a deep learning algorithm geared toward identification and forecast of the KOSPI index direction,and we also have compared the accuracy of the prediction.In an application of forecasting the financial market index direction, we have shown that the Robo-Advisor using deep learning has a significant effect on finance industry. The Robo-Advisor collects a massive data such as earnings statements, news reports and regulatory filings, analyzes those and recommends investors how to view market trends and identify the best time to purchase financial assets. On the other hand, the Robo-Advisor allows businesses to learn more about their customers, develop better marketing strategies, increase sales and decrease costs.Artificial intelligence, deep learning, FinTech, Robo-Advisor, technical analysis."
417,딥러닝의 효율적인 학습을 위한 학습데이터 선별 기법에 관한 연구,주경돈 /Gyoung-don Joo,2017,한국정보과학회,"빅데이터의 증가와 병렬 하드웨어 처리, 학습 알고리즘의 개선은 딥러닝의 발전을 가져왔으며 인공지능이 적용 가능한 영역을 획기적으로 확장시키고 있다. 하지만 이러한 대규모의 학습 데이터에 기반한 딥러닝은 학습시간이 증가되고, 더 나아가 학습 데이터의 구축을 위한 레이블링(labeling) 비용이 데이터 수에 비례하여 증가하게 된다. 따라서 본 논문에서는, 딥러닝의 정확도를 최대한 유지하면서 필요한 학습 데이터의 수를 줄일수 있도록, 전체 데이터에서 제한된 수의 학습 데이터의 후보를 선별하는 기법에 대하여 연구한다. 특히 기존의 능동 학습에 기반한 기법들이 학습 데이터의 수가 매우 적어질 경우 딥러닝의 정확도가 급격히 떨어지는 문제를 해결할 수 있는 새로운 기법을 제안하였으며, 제안된 기법으로 선별된 학습 데이터는 그 수의 변화에 대해 딥러닝이 일관되게 경쟁력있는 정확도를 가짐을 다양한 실험을 통하여 검증하였다.The increase of big data, parallel hardware processing and enhanced learning algorithms have led to the development of deep learning and are dramatically expanding the domain of applications based on artificial intelligence. However, such large-scale training data extends the learning time, and furthermore, the labeling cost for building the training set increases in proportion to the volume of data. Therefore, in this paper, we study how to select a limited number of candidates for training set from given data so as to reduce the number of training set while maintaining the accuracy of deep learning. Especially, we found that, with the training set selected by the previous work, the accuracy of deep learning drastically decreases when the number of training set is very small and so proposed new methods to resolve this fall-off in accuracy. Through experiments, we showed that deep learning consistently has the competitive accuracy with the training set selected by the proposed method regardless of the size of training set.Deep Learning, Train Set Selection, Active Learning, Gamma distribution"
418,딥러닝을 이용한 범용적 스테그아날리시스,김현재 /Hyunjae Kim,2017,한국정보과학회,"스테그아날리시스(Steganalysis)란 이미지 등 일반적인 자료에 암호화된 정보를 은닉하는 스테가노그래피(Steganography)에 대한 검출 및 분석 방법으로, 기계학습 기반 방법론을 포함한다. 기존 기계학습 기반 스테그아날리시스는 영상(Image)의 특징(Feature) 추출 및 모델링에 기반하며, 최근 딥러닝(Deep Learning)의 적용으로 검출 정확도가 큰 폭으로 향상되었다. 하지만 현존하는 스테그아날리시스 모델은 단일 스테가노그래피 기법에 대해 국한되어 있어 학습에 사용되지 않은 스테고(Stego) 이미지의 경우 검출이 불가능한 결정적 한계를 가진다. 본 연구에서는 다양한 스테가노그래피 기법으로 생성된 스테고이미지에 딥러닝을 적용하여 스테그아날리시스를 학습하는 범용적 모델을 제안한다. 다양한 실험을 통해 제안 기법의 효용성 및 가능성을 확인하고, 범용적 스테그아날리시스 모델이 각각에 특화된 검출 기법과 유사한 정확도로 스테고 이미지를 검출할 수 있음을 보인다.Steganalysis is to detect information hidden by steganography inside general data such as images. There are stegoanalysis techniques that use machine learning (ML). Existing ML approaches to steganalysis are based on extracting features from stego images and modeling them.
Recently deep learning-based methodologies have shown significant improvements in detection accuracy. However, all the existing methods, including deep learning-based ones, have a critical limitation in that they can only detect stego images that are created by a specific steganography method. In this paper, we propose a generalized steganalysis method that can model multiple types of stego images using deep learning. Through various experiments, we confirm the effectiveness of our approach and envision directions for future research. In particular, we show that our method can detect each type of steganography with the same level of accuracy as that of a steganalysis method dedicated to that type of steganography, thereby demonstrating the general applicability of our approach to multiple types of stego images.steganalysis, steganography, deep learning, generalized steganalysis model"
419,"딥러닝 프레임워크의 비교: 티아노, 텐서플로, CNTK를 중심으로",정여진 /Yeojin Chung,2017,한국지능정보시스템학회,"딥러닝 프레임워크의 대표적인 기능으로는 ‘자동미분’과 ‘GPU의 활용’ 등을 들 수 있다. 본 논문은 파이썬의 라이브러리 형태로 사용 가능한 프레임워크 중에서 구글의 텐서플로와 마이크로소프트의 CNTK, 그리고 텐서 플로의 원조라고 할 수 있는 티아노를 비교하였다. 본문에서는 자동미분의 개념과 GPU의 활용형태를 간단히 설명하고, 그 다음에 logistic regression을 실행하는 예를 통하여 각 프레임워크의 문법을 알아본 뒤에, 마지막으로 대표적인 딥러닝 응용인 CNN의 예제를 실행시켜보고 코딩의 편의성과 실행속도 등을 확인해 보았다. 그 결과, 편의성의 관점에서 보면 티아노가 가장 코딩 하기가 어렵고, CNTK와 텐서플로는 많은 부분이 비슷하게 추상화 되어 있어서 코딩이 비슷하지만 가중치와 편향을 직접 정의하느냐의 여부에서 차이를 보였다. 그리고 각프레임워크의 실행속도에 대한 평가는 ‘큰 차이는 없다’는 것이다. 텐서플로는 티아노에 비하여 속도가 느리다는 평가가 있어왔는데, 본 연구의 실험에 의하면, 비록 CNN 모형에 국한되었지만, 텐서플로가 아주 조금이지만빠른 것으로 나타났다. CNTK의 경우에도, 비록 실험환경이 달랐지만, 실험환경의 차이에 의한 속도의 차이의 편차범위 이내에 있는 것으로 판단이 되었다. 본 연구에서는 세 종류의 딥러닝 프레임워크만을 살펴보았는데,위키피디아에 따르면 딥러닝 프레임워크의 종류는 12가지가 있으며, 각 프레임워크의 특징을 15가지 속성으로구분하여 차이를 특정하고 있다. 그 많은 속성 중에서 사용자의 입장에서 볼 때 중요한 속성은 어떤 언어(파이썬, C++, Java, 등)로 사용가능한지, 어떤 딥러닝 모형에 대한 라이브러리가 잘 구현되어 있는지 등일 것이다. 그리고 사용자가 대규모의 딥러닝 모형을 구축한다면, 다중 GPU 혹은 다중 서버를 지원하는지의 여부도 중요할 것이다. 또한 딥러닝 모형을 처음 학습하는 경우에는 사용설명서가 많은지 예제 프로그램이 많은지 여부도 중요한 기준이 될 것이다.The deep learning framework is software designed to help develop deep learning models. Some of its important functions include “automatic differentiation” and “utilization of GPU”. The list of popular deep learning framework includes Caffe (BVLC) and Theano (University of Montreal). And recently, Microsoft's deep learning framework, Microsoft Cognitive Toolkit, was released as open-source license, following Google’s Tensorflow a year earlier. The early deep learning frameworks have been developed mainly for research at universities. Beginning with the inception of Tensorflow, however, it seems that companies such as Microsoft and Facebook have started to join the competition of framework development. Given the trend, Google and other companies are expected to continue investing in the deep learning framework to bring forward the initiative in the artificial intelligence business. From this point of view, we think it is a good time to compare some of deep learning frameworks. So we compare three deep learning frameworks which can be used as a Python library. Those are Google's Tensorflow, Microsoft’s CNTK, and Theano which is sort of a predecessor of the preceding two.
The most common and important function of deep learning frameworks is the ability to perform automatic differentiation. Basically all the mathematical expressions of deep learning models can be represented as computational graphs, which consist of nodes and edges. Partial derivatives on each edge of a computational graph can then be obtained. With the partial derivatives, we can let software compute differentiation of any node with respect to any variable by utilizing chain rule of Calculus.
First of all, the convenience of coding is in the order of CNTK, Tensorflow, and Theano. The criterion is simply based on the lengths of the codes and the learning curve and the ease of coding are not the main concern. According to the criteria, Theano was the most difficult to implement with, and CNTK and Tensorflow were somewhat easier. With Tensorflow, we need to define weight variables and biases explicitly. The reason that CNTK and Tensorflow are easier to implement with is that those frameworks provide us with more abstraction than Theano. We, however, need to mention that low-level coding is not always bad. It gives us flexibility of coding. With the low-level coding such as in Theano, we can implement and test any new deep learning models or any new search methods that we can think of.
The assessment of the execution speed of each framework is that there is not meaningful difference. According to the experiment, execution speeds of Theano and Tensorflow are very similar, although the experiment was limited to a CNN model. In the case of CNTK, the experimental environment was not maintained as the same. The code written in CNTK has to be run in PC environment without GPU where codes execute as much as 50 times slower than with GPU. But we concluded that the difference of execution speed was within the range of variation caused by the different hardware setup.
In this study, we compared three types of deep learning framework: Theano, Tensorflow, and CNTK. According to Wikipedia, there are 12 available deep learning frameworks. And 15 different attributes differentiate each framework. Some of the important attributes would include interface language (Python, C ++, Java, etc.) and the availability of libraries on various deep learning models such as CNN, RNN, DBN, and etc. And if a user implements a large scale deep learning model, it will also be important to support multiple GPU or multiple servers. Also, if you are learning the deep learning model, it would also be important if there are enough examples and references.deep learning framework, Theano, TensorFlow, CNTK, computational graph, CIFAR-10"
420,딥러닝을 이용한 주변 무선단말 파악방안,이웅섭 /Lee， Woong-sup,2018,한국정보통신학회,"최근 단말-대-단말(Device-to-device, D2D) 통신기술이 차세대 무선통신시스템의 핵심기술로 큰 관심을 받고 있다. 이러한 단말간 통신에서는 자신의 주변에 어떠한 단말이 있는지 파악하는 주변단말 탐색(Neighbor discovery)이매우 중요하다. 본 논문에서는 최근 큰 관심을 받고 있는 딥러닝(Deep learning) 기술을 활용하여 단말간 통신에서 주변단말을 파악하는 방안에 대해서 제안한다. 제안 방안은 기존의 방안과 달리 무선채널의 공간적 연관성을 이용하여단말간의 신호 전송 없이 단말이 기지국으로 전송하는 상향링크 파일럿 신호를 기반으로 주변 단말을 찾고 따라서기존의 방식에 비해 신호전송 복잡도(signaling complexity)를 크게 줄일 수 있다. 또한 제안 방안에서는 떨어져 있는거리에 따라서 주변 단말을 분류 가능하여 기존 방안에 비해서 좀 더 세밀한 단말 탐색이 가능하다. 마지막으로 본 논문에서는 tensorflow를 이용한 컴퓨터 시뮬레이션을 통해 제안 방안의 성능을 검증하였다.Recently, the device-to-device (D2D) communication has been conceived as the key technology for the next-generationmobile communication systems. The neighbor discovery in which the nearby users are found, is essential for the properoperation of the D2D communication. In this paper, we propose new neighbor discovery scheme based on deep learningtechnology which has gained a lot of attention recently. In the proposed scheme, the neighboring users can be foundusing the uplink pilot transmission of users only, unlike conventional neighbor discovery schemes in which direct pilotcommunication among users is required, such that the signaling overhead can be greatly reduced in our proposed scheme.
Moreover, the neighbors with different proximity can also be classified accordingly which enables more accurate neighbordiscovery compared to the conventional schemes. The performance of our proposed scheme is verified through thetensorflow-based computer simulations.
키워드 : 딥러닝, 주변단말 탐색, 단말-대-단말 통신Deep learning, Neighbor discovery, Device-to-device communication, Wireless channel correlation"
421,신뢰성있는 딥러닝 기반 분석 모델을 참조하기 위한 딥러닝 기술 언어,문종혁 /Jong Hyeok Mun,2021,한국정보처리학회,"최근 딥러닝은 하드웨어 성능이 향상됨에 따라 자연어 처리, 영상 인식 등의 다양한 기술에 접목되어 활용되고 있다. 이러한 기술들을 활용해 지능형교통 시스템(ITS), 스마트홈, 헬스케어 등의 산업분야에서 데이터를 분석하여 고속도로 속도위반 차량 검출, 에너지 사용량 제어, 응급상황 등과 같은고품질의 서비스를 제공하며, 고품질의 서비스를 제공하기 위해서는 정확도가 향상된 딥러닝 모델이 적용되어야 한다. 이를 위해 서비스 환경의 데이터를분석하기 위한 딥러닝 모델을 개발할 때, 개발자는 신뢰성이 검증된 최신의 딥러닝 모델을 적용할 수 있어야 한다. 이는 개발자가 참조하는 딥러닝 모델에적용된 학습 데이터셋의 정확도를 측정하여 검증할 수 있다. 이러한 검증을 위해서 개발자는 학습 데이터셋, 딥러닝의 계층구조 및 개발 환경 등과 같은내용을 포함하는 딥러닝 모델을 문서화하여 적용하기 위한 구조적인 정보가 필요하다. 본 논문에서는 신뢰성있는 딥러닝 기반 데이터 분석 모델을 참조하기위한 딥러닝 기술 언어를 제안한다. 제안하는 기술 언어는 신뢰성 있는 딥러닝 모델을 개발하는데 필요한 학습데이터셋, 개발 환경 및 설정 등의 정보와더불어 딥러닝 모델의 계층구조를 표현할 수 있다. 제안하는 딥러닝 기술 언어를 이용하여 개발자는 지능형 교통 시스템에서 참조하는 분석 모델의정확도를 검증할 수 있다. 실험에서는 제안하는 언어의 유효성을 검증하기 위해, 번호판 인식 모델을 중심으로 딥러닝 기술 문서의 적용과정을 보인다With the recent advancements of deep learning, companies such as smart home, healthcare, and intelligent transportation systems areutilizing its functionality to provide high-quality services for vehicle detection, emergency situation detection, and controlling energyconsumption. To provide reliable services in such sensitive systems, deep learning models are required to have high accuracy. In orderto develop a deep learning model for analyzing previously mentioned services, developers should utilize the state of the art deep learningmodels that have already been verified for higher accuracy. The developers can verify the accuracy of the referenced model by validatingthe model on the dataset. For this validation, the developer needs structural information to document and apply deep learning models,including metadata such as learning dataset, network architecture, and development environments. In this paper, we propose a descriptionlanguage that represents the network architecture of the deep learning model along with its metadata that are necessary to develop adeep learning model. Through the proposed description language, developers can easily verify the accuracy of the referenced deep learningmodel. Our experiments demonstrate the application scenario of a deep learning description document that focuses on the license platerecognition for the detection of illegally parked vehicles.Trusted Deep Learning, Model Reference, Deep Learning Description Language, Traffic Situation Analysis Model"
422,딥러닝 분석을 이용한 중국 역내·외 위안화 변동성 예측,이우식 /Lee Woosik,2016,한국데이터정보과학회,"2008년 글로벌 금융위기 이후 중국은 위안화 국제화의 점진적 추진을 진행하면서 중국상하이 외환시장과 중국홍콩 외환시장에서 거래되는 통화인 역내위안화와 역외위안화를 형성시켰다. 본 연구는 위안화 국제화와 점진적인 중국 자본계정 개방에 따라 급변하는 외환시장상황의 변동성을 정확하게 파악하기 위해서 GARCH모형 (일반화된 자기회귀 조건부이분산성모형)에 다단계인공신경망을 결합한 MLP-GARCH 모형과 GARCH모형과 기계학습의 일종인 딥러닝 (deep learning)을 통합한 DL-GARCH을 가지고 위안화 변동성예측을 비교 실험과 분석을 하였다. 비교분석 결과 DLGARCH 모형은 MLP-GARCH보다 모형 위안화 역내·외 환율변동성 예측 면에서 더욱 더 개선된예측값을 제공하였다. 그래서 이분산시계열모형을 딥러닝과 결합한 DL-GARCH 모형은 시계열의환율 변동성 예측 문제에 딥러닝을 응용할 수 있음을 확인하였다. 향후 이분산시계열과 결합된 딥러닝 모형은 다른 금융시계열 데이터에 응용하여 그 일반화 가능성을 높일 수 있을 것이다.The People’s Republic of China has vigorously been pursuing the internationalization of the Chinese Yuan or Renminbi after the financial crisis of 2008. In this view, an abrupt increase of use of the Chinese Yuan in the onshore and offshore markets are important milestones to be one of important currencies. One of the most frequently used methods to forecast volatility is GARCH model. Since a prediction error of the GARCH model has been reported quite high, a lot of efforts have been made to improve forecasting capability of the GARCH model. In this paper, we have proposed MLPGARCH and a DL-GARCH by employing Artificial Neural Network to the GARCH. In an application to forecasting Chinese Yuan volatility, we have successfully shown their overall outperformance in forecasting over the GARCH.Deep learning, DL-GARCH, GARCH, The Chinese Yuan, volatility."
423,딥러닝 기법을 이용한 내일강수 예측,하지훈,2016,한국지능시스템학회,"정확한 강수예측을 위해서는 예측인자 선정과 예측방법에 대한 선택이 매우 중요하다. 최근에는 강수예측 방법으로기계학습 기법이 많이 사용되고 있으며, 그 중에서도 특히 인공신경망을 사용한 강수예측 방법은 좋은 성능을 보였다. 본논문에서는 딥러닝 기법 중 하나인 DBN(deep belief network)를 이용한 새로운 강수예측 방법을 제안한다. DBN는 비지도사전 학습을 통해 초기 가중치를 설정하여 기존 인공신경망의 문제점을 보완한다. 예측인자로는 기온, 전일-전주 강수일, 태양과 달 궤도 관련 자료를 선정하였다. 기온과 전일-전주 강수일은 서울에서의 1974년부터 2013년까지 총 40년간의AWS(automatic weather system) 관측 자료를 사용하였고, 태양과 달의 궤도 관련 자료는 서울을 중심으로 계산한 결과를사용하였다. 전체 기간에서 일부는 학습 자료로 사용하여 예측모델을 생성하였고, 나머지를 생성한 모델의 검증 자료로사용하였다. 모델 검증 결과로 나온 예측값들은 확률값을 가지며 임계치를 이용하여 강수유무를 판별하였다. 강수 정확도의척도로 양분예보기법 중 CSI(critical successive index)와 Bias(frequency bias)를 계산하였다. 이를 통해 DBN와 MLP(multilayer perceptron)의 성능을 비교한 결과 DBN의 강수 예측 정확도가 높았고, 수행속도 또한 2배 이상 빨랐다.For accurate precipitation forecasts the choice of weather faoctrs and prediction method is very important. Recently, machinele arning has been widely used for forecasting precipitation, and artificial neural network, one of machine learning techniques, showed good performance. In this paper, we suggest a new method for forecasting precipitation using DBN, one of deep learning techniques.
DBN has an advantage that initial weights are set by unsupervised learning, so this compensates for the defects of artificial neural networks. We used past precipitation, temperature, and the parameters of the sun and moon's motion as features for forecasting precipitation. The dataset consists of observation data which had been measured for 40 years from AWS in Seoul. Experiments were based on 8-fold cross validation. As a result of estimation, we got probabilities of test dataset, so threshold was used for the decision of precipitation. CSI and Bias were used for indicating the precision of precipitation. Our experimental results showed that DBN performed better than MLP.Deep learning, Deep belief network, Precipitation, Forecast"
424,딥러닝 신경망모형을 이용한 실시간 교통정보수집 연구,김대현 /KIM DAE HYON,2016,사단법인 인문사회과학기술융합학회,"비디오 기반의 실시간 교통정보수집 시스템은 자율주행을 포함한 지능형교통체계의 주요 서브시스템중 하나이며, 우수한 예측(추정)력 때문에 핵심 알고리즘으로 인공신경망 모형이 주로 사용되고 있다. 특히, 최근 과학 학술지 네이처에 구글 딥마인드의 알파고가 공개되면서 딥러닝과 인공지능에 대한 전 세계적인 관심이 집중되고 있다. 최근까지 가장 널리 사용된 인공신경망모형은 역전파 모형이었으나 최근 컴퓨터 하드웨어 및 알고리즘의 새로운 개발과 더불어 딥러닝 알고리즘을 이용한 딥인공신경망 모형에 대한 관심이 높아지고 있다. 그러나 아직 딥러닝 인공신경망에 대한 연구는 초기단계에 있는 실정이며 적용분야도 극히 한정되어 있다. 본 연구에서는 실시간 도로교통정보 수집을 위한 영상처리분야에서 딥러닝 알고리즘을 활용하고 실험결과를 통하여 향후 교통정보수집‧분석 분야에서의 딥러닝 인공신경망 모형의 적용가능성을 살펴보고자 한다. 특히, 본 연구에서 도출한 딥러닝 인공신경망모형의 실험결과를 기존 역전파 모형에 의한 실험결과와 비교하여 모형의 우수성을 비교‧평가하고자 한다.Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Normally, artificial neural networks have been used in real-time applications for collecting traffic information because of their remarkable performance in terms of predictive accuracy and real-time computing. Especially, the recent publication of a paper in the journal Nature and the success of Google DeepMind’s world-beating Go program AlphaGo caused significant increases of the public interest in artificial intelligence. Even though various types of learning models have been proposed, the Backpropagation has been the most popular neural network model. However, the Deep learning neural network model has attracted much interest recently. In this study, the Deep leaning neural network will be applied in image processing techniques for the real-time traffic information collection to investigate the performance of Deep learning algorithm in traffic engineering fields. In addition, the experimental results from Deep learing algorithm will be compared with the results from Backpropagation model to evaluate and propose better performing model.Intelligent Transportation Systems (ITS), artificial neural networks, Deep learning, traffic information, image processing"
425,딥러닝의 모형과 응용사례,안성만 /SungMahn Ahn,2016,한국지능정보시스템학회,"딥러닝은 인공신경망(neural network)이라는 인공지능분야의 모형이 발전된 형태로서, 계층구조로 이루어진 인공신경망의 내부계층(hidden layer)이 여러 단계로 이루어진 구조이다. 딥러닝에서의 주요 모형은 합성곱신경망(convolutional neural network), 순환신경망(recurrent neural network), 그리고 심층신뢰신경망(deep belief network)의 세가지라고 할 수있다. 그 중에서 현재 흥미로운 연구가 많이 발표되어서 관심이 집중되고 있는 모형은 지도학습(supervised learning)모형인 처음 두 개의 모형이다. 따라서 본 논문에서는 지도학습모형의 가중치를 최적화하는 기본적인 방법인 오류역전파 알고리즘을 살펴본 뒤에 합성곱신경망과 순환신경망의 구조와 응용사례 등을 살펴보고자 한다. 본문에서 다루지 않은 모형인 심층신뢰신경망은 아직까지는 합성곱신경망 이나 순환신경망보다는 상대적으로 주목을 덜 받고 있다. 그러나 심층신뢰신경망은 CNN이나 RNN과는 달리 비지도학습(unsupervised learning)모형이며, 사람이나 동물은 관찰을 통해서 스스로 학습한다는 점에서 궁극적으로는 비지도학습모형이 더 많이 연구되어야 할 주제가 될 것이다.Deep learning model is a kind of neural networks that allows multiple hidden layers. There are various deep learning architectures such as convolutional neural networks, deep belief networks and recurrent neural networks. Those have been applied to fields like computer vision, automatic speech recognition, natural language processing, audio recognition and bioinformatics where they have been shown to produce state-of-the-art results on various tasks. Among those architectures, convolutional neural networks and recurrent neural networks are classified as the supervised learning model. And in recent years, those supervised learning models have gained more popularity than unsupervised learning models such as deep belief networks, because supervised learning models have shown fashionable applications in such fields mentioned above.
Deep learning models can be trained with backpropagation algorithm. Backpropagation is an abbreviation for “backward propagation of errors” and a common method of training artificial neural networks used in conjunction with an optimization method such as gradient descent. The method calculates the gradient of an error function with respect to all the weights in the network. The gradient is fed to the optimization method which in turn uses it to update the weights, in an attempt to minimize the error function.
Convolutional neural networks use a special architecture which is particularly well-adapted to classify images. Using this architecture makes convolutional networks fast to train. This, in turn, helps us train deep, muti-layer networks, which are very good at classifying images. These days, deep convolutional networks are used in most neural networks for image recognition. Convolutional neural networks use three basic ideas: local receptive fields, shared weights, and pooling. By local receptive fields, we mean that each neuron in the first(or any) hidden layer will be connected to a small region of the input(or previous layer’s) neurons. Shared weights mean that we’re going to use the same weights and bias for each of the local receptive field. This means that all the neurons in the hidden layer detect exactly the same feature, just at different locations in the input image. In addition to the convolutional layers just described, convolutional neural networks also contain pooling layers. Pooling layers are usually used immediately after convolutional layers. What the pooling layers do is to simplify the information in the output from the convolutional layer.
Recent convolutional network architectures have 10 to 20 hidden layers and billions of connections between units. Training deep learning networks has taken weeks several years ago, but thanks to progress in GPU and algorithm enhancement, training time has reduced to several hours.
Neural networks with time-varying behavior are known as recurrent neural networks or RNNs. A recurrent neural network is a class of artificial neural network where connections between units form a directed cycle. This creates an internal state of the network which allows it to exhibit dynamic temporal behavior. Unlike feedforward neural networks, RNNs can use their internal memory to process arbitrary sequences of inputs. Early RNN models turned out to be very difficult to train, harder even than deep feedforward networks. The reason is the unstable gradient problem such as vanishing gradient and exploding gradient. The gradient can get smaller and smaller as it is propagated back through layers. This makes learning in early layers extremely slow. The problem actually gets worse in RNNs, since gradients aren’t just propagated backward through layers, they’re propagated backward through time. If the network runs for a long time, that can make the gradient extremely unstable and hard to learn from. It has been possible to incorporate an idea known as long short-term memory units (LSTMs) into RNNs. LSTMs make it much easier to get good results when training RNNs, and many recent papers make use of LSTMs or related ideas.Deep learning, Convolutional neural networks, Recurrent neural networks, Error backpropagation algorithm"
426,딥러닝분석과 기술적 분석 지표를 이용한 한국 코스피주가지수 방향성 예측,이우식 /Lee Woosik,2017,한국데이터정보과학회,"2016년 3월 구글 (Google)의 바둑인공지능 알파고 (AlphaGo)가 이세돌 9단과의 바둑대결에서승리한 이후 다양한 분야에서 인공지능 사용에 대한 관심이 높아지고 있는 가운데 금융투자 분야에서도 인공지능과 투자자문 전문가의 합성어인 로보어드바이저 (Robo-Advisor)에 대한 관심이 높아지고 있다. 인공지능 (artificial intelligence)기반의 의사결정은 비용 절감은 물론 효과적인 의사결정을가능하게 한다는 점에서 큰 장점이 있다. 본 연구에서는 기술적 분석 (technical analysis) 지표와 딥러닝 (deep learning) 모형을 결합하여 한국 코스피 지수를 예측하는 모형을 개발하고 제시한 모형들의 예측력을 비교, 분석한다. 분석 결과 기술적 분석 지표에 딥러닝 알고리즘을 결합한 모형이 주가지수 방향성 예측 문제에 응용될 수 있음을 확인하였다. 향후 본 연구에서 제안된 기술적 분석 지표와딥러닝모형을 결합한 기법은 로보어드바이저서비스에 응용할 수 있는 일반화 가능성을 보여준다Since Google’s AlphaGo defeated a world champion of Go players in 2016, there have been many interests in the deep learning. In the financial sector, a Robo-Advisor using deep learning gains a significant attention, which builds and manages portfolios of financial instruments for investors.In this paper, we have proposed the a deep learning algorithm geared toward identification and forecast of the KOSPI index direction,and we also have compared the accuracy of the prediction.In an application of forecasting the financial market index direction, we have shown that the Robo-Advisor using deep learning has a significant effect on finance industry. The Robo-Advisor collects a massive data such as earnings statements, news reports and regulatory filings, analyzes those and recommends investors how to view market trends and identify the best time to purchase financial assets. On the other hand, the Robo-Advisor allows businesses to learn more about their customers, develop better marketing strategies, increase sales and decrease costs.Artificial intelligence, deep learning, FinTech, Robo-Advisor, technical analysis."
427,딥러닝의 효율적인 학습을 위한 학습데이터 선별 기법에 관한 연구,주경돈 /Gyoung-don Joo,2017,한국정보과학회,"빅데이터의 증가와 병렬 하드웨어 처리, 학습 알고리즘의 개선은 딥러닝의 발전을 가져왔으며 인공지능이 적용 가능한 영역을 획기적으로 확장시키고 있다. 하지만 이러한 대규모의 학습 데이터에 기반한 딥러닝은 학습시간이 증가되고, 더 나아가 학습 데이터의 구축을 위한 레이블링(labeling) 비용이 데이터 수에 비례하여 증가하게 된다. 따라서 본 논문에서는, 딥러닝의 정확도를 최대한 유지하면서 필요한 학습 데이터의 수를 줄일수 있도록, 전체 데이터에서 제한된 수의 학습 데이터의 후보를 선별하는 기법에 대하여 연구한다. 특히 기존의 능동 학습에 기반한 기법들이 학습 데이터의 수가 매우 적어질 경우 딥러닝의 정확도가 급격히 떨어지는 문제를 해결할 수 있는 새로운 기법을 제안하였으며, 제안된 기법으로 선별된 학습 데이터는 그 수의 변화에 대해 딥러닝이 일관되게 경쟁력있는 정확도를 가짐을 다양한 실험을 통하여 검증하였다.The increase of big data, parallel hardware processing and enhanced learning algorithms have led to the development of deep learning and are dramatically expanding the domain of applications based on artificial intelligence. However, such large-scale training data extends the learning time, and furthermore, the labeling cost for building the training set increases in proportion to the volume of data. Therefore, in this paper, we study how to select a limited number of candidates for training set from given data so as to reduce the number of training set while maintaining the accuracy of deep learning. Especially, we found that, with the training set selected by the previous work, the accuracy of deep learning drastically decreases when the number of training set is very small and so proposed new methods to resolve this fall-off in accuracy. Through experiments, we showed that deep learning consistently has the competitive accuracy with the training set selected by the proposed method regardless of the size of training set.Deep Learning, Train Set Selection, Active Learning, Gamma distribution"
428,딥러닝을 이용한 범용적 스테그아날리시스,김현재 /Hyunjae Kim,2017,한국정보과학회,"스테그아날리시스(Steganalysis)란 이미지 등 일반적인 자료에 암호화된 정보를 은닉하는 스테가노그래피(Steganography)에 대한 검출 및 분석 방법으로, 기계학습 기반 방법론을 포함한다. 기존 기계학습 기반 스테그아날리시스는 영상(Image)의 특징(Feature) 추출 및 모델링에 기반하며, 최근 딥러닝(Deep Learning)의 적용으로 검출 정확도가 큰 폭으로 향상되었다. 하지만 현존하는 스테그아날리시스 모델은 단일 스테가노그래피 기법에 대해 국한되어 있어 학습에 사용되지 않은 스테고(Stego) 이미지의 경우 검출이 불가능한 결정적 한계를 가진다. 본 연구에서는 다양한 스테가노그래피 기법으로 생성된 스테고이미지에 딥러닝을 적용하여 스테그아날리시스를 학습하는 범용적 모델을 제안한다. 다양한 실험을 통해 제안 기법의 효용성 및 가능성을 확인하고, 범용적 스테그아날리시스 모델이 각각에 특화된 검출 기법과 유사한 정확도로 스테고 이미지를 검출할 수 있음을 보인다.Steganalysis is to detect information hidden by steganography inside general data such as images. There are stegoanalysis techniques that use machine learning (ML). Existing ML approaches to steganalysis are based on extracting features from stego images and modeling them.
Recently deep learning-based methodologies have shown significant improvements in detection accuracy. However, all the existing methods, including deep learning-based ones, have a critical limitation in that they can only detect stego images that are created by a specific steganography method. In this paper, we propose a generalized steganalysis method that can model multiple types of stego images using deep learning. Through various experiments, we confirm the effectiveness of our approach and envision directions for future research. In particular, we show that our method can detect each type of steganography with the same level of accuracy as that of a steganalysis method dedicated to that type of steganography, thereby demonstrating the general applicability of our approach to multiple types of stego images.steganalysis, steganography, deep learning, generalized steganalysis model"
429,"딥러닝 프레임워크의 비교: 티아노, 텐서플로, CNTK를 중심으로",정여진 /Yeojin Chung,2017,한국지능정보시스템학회,"딥러닝 프레임워크의 대표적인 기능으로는 ‘자동미분’과 ‘GPU의 활용’ 등을 들 수 있다. 본 논문은 파이썬의 라이브러리 형태로 사용 가능한 프레임워크 중에서 구글의 텐서플로와 마이크로소프트의 CNTK, 그리고 텐서 플로의 원조라고 할 수 있는 티아노를 비교하였다. 본문에서는 자동미분의 개념과 GPU의 활용형태를 간단히 설명하고, 그 다음에 logistic regression을 실행하는 예를 통하여 각 프레임워크의 문법을 알아본 뒤에, 마지막으로 대표적인 딥러닝 응용인 CNN의 예제를 실행시켜보고 코딩의 편의성과 실행속도 등을 확인해 보았다. 그 결과, 편의성의 관점에서 보면 티아노가 가장 코딩 하기가 어렵고, CNTK와 텐서플로는 많은 부분이 비슷하게 추상화 되어 있어서 코딩이 비슷하지만 가중치와 편향을 직접 정의하느냐의 여부에서 차이를 보였다. 그리고 각프레임워크의 실행속도에 대한 평가는 ‘큰 차이는 없다’는 것이다. 텐서플로는 티아노에 비하여 속도가 느리다는 평가가 있어왔는데, 본 연구의 실험에 의하면, 비록 CNN 모형에 국한되었지만, 텐서플로가 아주 조금이지만빠른 것으로 나타났다. CNTK의 경우에도, 비록 실험환경이 달랐지만, 실험환경의 차이에 의한 속도의 차이의 편차범위 이내에 있는 것으로 판단이 되었다. 본 연구에서는 세 종류의 딥러닝 프레임워크만을 살펴보았는데,위키피디아에 따르면 딥러닝 프레임워크의 종류는 12가지가 있으며, 각 프레임워크의 특징을 15가지 속성으로구분하여 차이를 특정하고 있다. 그 많은 속성 중에서 사용자의 입장에서 볼 때 중요한 속성은 어떤 언어(파이썬, C++, Java, 등)로 사용가능한지, 어떤 딥러닝 모형에 대한 라이브러리가 잘 구현되어 있는지 등일 것이다. 그리고 사용자가 대규모의 딥러닝 모형을 구축한다면, 다중 GPU 혹은 다중 서버를 지원하는지의 여부도 중요할 것이다. 또한 딥러닝 모형을 처음 학습하는 경우에는 사용설명서가 많은지 예제 프로그램이 많은지 여부도 중요한 기준이 될 것이다.The deep learning framework is software designed to help develop deep learning models. Some of its important functions include “automatic differentiation” and “utilization of GPU”. The list of popular deep learning framework includes Caffe (BVLC) and Theano (University of Montreal). And recently, Microsoft's deep learning framework, Microsoft Cognitive Toolkit, was released as open-source license, following Google’s Tensorflow a year earlier. The early deep learning frameworks have been developed mainly for research at universities. Beginning with the inception of Tensorflow, however, it seems that companies such as Microsoft and Facebook have started to join the competition of framework development. Given the trend, Google and other companies are expected to continue investing in the deep learning framework to bring forward the initiative in the artificial intelligence business. From this point of view, we think it is a good time to compare some of deep learning frameworks. So we compare three deep learning frameworks which can be used as a Python library. Those are Google's Tensorflow, Microsoft’s CNTK, and Theano which is sort of a predecessor of the preceding two.
The most common and important function of deep learning frameworks is the ability to perform automatic differentiation. Basically all the mathematical expressions of deep learning models can be represented as computational graphs, which consist of nodes and edges. Partial derivatives on each edge of a computational graph can then be obtained. With the partial derivatives, we can let software compute differentiation of any node with respect to any variable by utilizing chain rule of Calculus.
First of all, the convenience of coding is in the order of CNTK, Tensorflow, and Theano. The criterion is simply based on the lengths of the codes and the learning curve and the ease of coding are not the main concern. According to the criteria, Theano was the most difficult to implement with, and CNTK and Tensorflow were somewhat easier. With Tensorflow, we need to define weight variables and biases explicitly. The reason that CNTK and Tensorflow are easier to implement with is that those frameworks provide us with more abstraction than Theano. We, however, need to mention that low-level coding is not always bad. It gives us flexibility of coding. With the low-level coding such as in Theano, we can implement and test any new deep learning models or any new search methods that we can think of.
The assessment of the execution speed of each framework is that there is not meaningful difference. According to the experiment, execution speeds of Theano and Tensorflow are very similar, although the experiment was limited to a CNN model. In the case of CNTK, the experimental environment was not maintained as the same. The code written in CNTK has to be run in PC environment without GPU where codes execute as much as 50 times slower than with GPU. But we concluded that the difference of execution speed was within the range of variation caused by the different hardware setup.
In this study, we compared three types of deep learning framework: Theano, Tensorflow, and CNTK. According to Wikipedia, there are 12 available deep learning frameworks. And 15 different attributes differentiate each framework. Some of the important attributes would include interface language (Python, C ++, Java, etc.) and the availability of libraries on various deep learning models such as CNN, RNN, DBN, and etc. And if a user implements a large scale deep learning model, it will also be important to support multiple GPU or multiple servers. Also, if you are learning the deep learning model, it would also be important if there are enough examples and references.deep learning framework, Theano, TensorFlow, CNTK, computational graph, CIFAR-10"
430,딥러닝을 이용한 주변 무선단말 파악방안,이웅섭 /Lee， Woong-sup,2018,한국정보통신학회,"최근 단말-대-단말(Device-to-device, D2D) 통신기술이 차세대 무선통신시스템의 핵심기술로 큰 관심을 받고 있다. 이러한 단말간 통신에서는 자신의 주변에 어떠한 단말이 있는지 파악하는 주변단말 탐색(Neighbor discovery)이매우 중요하다. 본 논문에서는 최근 큰 관심을 받고 있는 딥러닝(Deep learning) 기술을 활용하여 단말간 통신에서 주변단말을 파악하는 방안에 대해서 제안한다. 제안 방안은 기존의 방안과 달리 무선채널의 공간적 연관성을 이용하여단말간의 신호 전송 없이 단말이 기지국으로 전송하는 상향링크 파일럿 신호를 기반으로 주변 단말을 찾고 따라서기존의 방식에 비해 신호전송 복잡도(signaling complexity)를 크게 줄일 수 있다. 또한 제안 방안에서는 떨어져 있는거리에 따라서 주변 단말을 분류 가능하여 기존 방안에 비해서 좀 더 세밀한 단말 탐색이 가능하다. 마지막으로 본 논문에서는 tensorflow를 이용한 컴퓨터 시뮬레이션을 통해 제안 방안의 성능을 검증하였다.Recently, the device-to-device (D2D) communication has been conceived as the key technology for the next-generationmobile communication systems. The neighbor discovery in which the nearby users are found, is essential for the properoperation of the D2D communication. In this paper, we propose new neighbor discovery scheme based on deep learningtechnology which has gained a lot of attention recently. In the proposed scheme, the neighboring users can be foundusing the uplink pilot transmission of users only, unlike conventional neighbor discovery schemes in which direct pilotcommunication among users is required, such that the signaling overhead can be greatly reduced in our proposed scheme.
Moreover, the neighbors with different proximity can also be classified accordingly which enables more accurate neighbordiscovery compared to the conventional schemes. The performance of our proposed scheme is verified through thetensorflow-based computer simulations.
키워드 : 딥러닝, 주변단말 탐색, 단말-대-단말 통신Deep learning, Neighbor discovery, Device-to-device communication, Wireless channel correlation"
431,신뢰성있는 딥러닝 기반 분석 모델을 참조하기 위한 딥러닝 기술 언어,문종혁 /Jong Hyeok Mun,2021,한국정보처리학회,"최근 딥러닝은 하드웨어 성능이 향상됨에 따라 자연어 처리, 영상 인식 등의 다양한 기술에 접목되어 활용되고 있다. 이러한 기술들을 활용해 지능형교통 시스템(ITS), 스마트홈, 헬스케어 등의 산업분야에서 데이터를 분석하여 고속도로 속도위반 차량 검출, 에너지 사용량 제어, 응급상황 등과 같은고품질의 서비스를 제공하며, 고품질의 서비스를 제공하기 위해서는 정확도가 향상된 딥러닝 모델이 적용되어야 한다. 이를 위해 서비스 환경의 데이터를분석하기 위한 딥러닝 모델을 개발할 때, 개발자는 신뢰성이 검증된 최신의 딥러닝 모델을 적용할 수 있어야 한다. 이는 개발자가 참조하는 딥러닝 모델에적용된 학습 데이터셋의 정확도를 측정하여 검증할 수 있다. 이러한 검증을 위해서 개발자는 학습 데이터셋, 딥러닝의 계층구조 및 개발 환경 등과 같은내용을 포함하는 딥러닝 모델을 문서화하여 적용하기 위한 구조적인 정보가 필요하다. 본 논문에서는 신뢰성있는 딥러닝 기반 데이터 분석 모델을 참조하기위한 딥러닝 기술 언어를 제안한다. 제안하는 기술 언어는 신뢰성 있는 딥러닝 모델을 개발하는데 필요한 학습데이터셋, 개발 환경 및 설정 등의 정보와더불어 딥러닝 모델의 계층구조를 표현할 수 있다. 제안하는 딥러닝 기술 언어를 이용하여 개발자는 지능형 교통 시스템에서 참조하는 분석 모델의정확도를 검증할 수 있다. 실험에서는 제안하는 언어의 유효성을 검증하기 위해, 번호판 인식 모델을 중심으로 딥러닝 기술 문서의 적용과정을 보인다With the recent advancements of deep learning, companies such as smart home, healthcare, and intelligent transportation systems areutilizing its functionality to provide high-quality services for vehicle detection, emergency situation detection, and controlling energyconsumption. To provide reliable services in such sensitive systems, deep learning models are required to have high accuracy. In orderto develop a deep learning model for analyzing previously mentioned services, developers should utilize the state of the art deep learningmodels that have already been verified for higher accuracy. The developers can verify the accuracy of the referenced model by validatingthe model on the dataset. For this validation, the developer needs structural information to document and apply deep learning models,including metadata such as learning dataset, network architecture, and development environments. In this paper, we propose a descriptionlanguage that represents the network architecture of the deep learning model along with its metadata that are necessary to develop adeep learning model. Through the proposed description language, developers can easily verify the accuracy of the referenced deep learningmodel. Our experiments demonstrate the application scenario of a deep learning description document that focuses on the license platerecognition for the detection of illegally parked vehicles.Trusted Deep Learning, Model Reference, Deep Learning Description Language, Traffic Situation Analysis Model"
432,딥러닝 분석을 이용한 중국 역내·외 위안화 변동성 예측,이우식 /Lee Woosik,2016,한국데이터정보과학회,"2008년 글로벌 금융위기 이후 중국은 위안화 국제화의 점진적 추진을 진행하면서 중국상하이 외환시장과 중국홍콩 외환시장에서 거래되는 통화인 역내위안화와 역외위안화를 형성시켰다. 본 연구는 위안화 국제화와 점진적인 중국 자본계정 개방에 따라 급변하는 외환시장상황의 변동성을 정확하게 파악하기 위해서 GARCH모형 (일반화된 자기회귀 조건부이분산성모형)에 다단계인공신경망을 결합한 MLP-GARCH 모형과 GARCH모형과 기계학습의 일종인 딥러닝 (deep learning)을 통합한 DL-GARCH을 가지고 위안화 변동성예측을 비교 실험과 분석을 하였다. 비교분석 결과 DLGARCH 모형은 MLP-GARCH보다 모형 위안화 역내·외 환율변동성 예측 면에서 더욱 더 개선된예측값을 제공하였다. 그래서 이분산시계열모형을 딥러닝과 결합한 DL-GARCH 모형은 시계열의환율 변동성 예측 문제에 딥러닝을 응용할 수 있음을 확인하였다. 향후 이분산시계열과 결합된 딥러닝 모형은 다른 금융시계열 데이터에 응용하여 그 일반화 가능성을 높일 수 있을 것이다.The People’s Republic of China has vigorously been pursuing the internationalization of the Chinese Yuan or Renminbi after the financial crisis of 2008. In this view, an abrupt increase of use of the Chinese Yuan in the onshore and offshore markets are important milestones to be one of important currencies. One of the most frequently used methods to forecast volatility is GARCH model. Since a prediction error of the GARCH model has been reported quite high, a lot of efforts have been made to improve forecasting capability of the GARCH model. In this paper, we have proposed MLPGARCH and a DL-GARCH by employing Artificial Neural Network to the GARCH. In an application to forecasting Chinese Yuan volatility, we have successfully shown their overall outperformance in forecasting over the GARCH.Deep learning, DL-GARCH, GARCH, The Chinese Yuan, volatility."
433,딥러닝 기법을 이용한 내일강수 예측,하지훈,2016,한국지능시스템학회,"정확한 강수예측을 위해서는 예측인자 선정과 예측방법에 대한 선택이 매우 중요하다. 최근에는 강수예측 방법으로기계학습 기법이 많이 사용되고 있으며, 그 중에서도 특히 인공신경망을 사용한 강수예측 방법은 좋은 성능을 보였다. 본논문에서는 딥러닝 기법 중 하나인 DBN(deep belief network)를 이용한 새로운 강수예측 방법을 제안한다. DBN는 비지도사전 학습을 통해 초기 가중치를 설정하여 기존 인공신경망의 문제점을 보완한다. 예측인자로는 기온, 전일-전주 강수일, 태양과 달 궤도 관련 자료를 선정하였다. 기온과 전일-전주 강수일은 서울에서의 1974년부터 2013년까지 총 40년간의AWS(automatic weather system) 관측 자료를 사용하였고, 태양과 달의 궤도 관련 자료는 서울을 중심으로 계산한 결과를사용하였다. 전체 기간에서 일부는 학습 자료로 사용하여 예측모델을 생성하였고, 나머지를 생성한 모델의 검증 자료로사용하였다. 모델 검증 결과로 나온 예측값들은 확률값을 가지며 임계치를 이용하여 강수유무를 판별하였다. 강수 정확도의척도로 양분예보기법 중 CSI(critical successive index)와 Bias(frequency bias)를 계산하였다. 이를 통해 DBN와 MLP(multilayer perceptron)의 성능을 비교한 결과 DBN의 강수 예측 정확도가 높았고, 수행속도 또한 2배 이상 빨랐다.For accurate precipitation forecasts the choice of weather faoctrs and prediction method is very important. Recently, machinele arning has been widely used for forecasting precipitation, and artificial neural network, one of machine learning techniques, showed good performance. In this paper, we suggest a new method for forecasting precipitation using DBN, one of deep learning techniques.
DBN has an advantage that initial weights are set by unsupervised learning, so this compensates for the defects of artificial neural networks. We used past precipitation, temperature, and the parameters of the sun and moon's motion as features for forecasting precipitation. The dataset consists of observation data which had been measured for 40 years from AWS in Seoul. Experiments were based on 8-fold cross validation. As a result of estimation, we got probabilities of test dataset, so threshold was used for the decision of precipitation. CSI and Bias were used for indicating the precision of precipitation. Our experimental results showed that DBN performed better than MLP.Deep learning, Deep belief network, Precipitation, Forecast"
434,딥러닝 신경망모형을 이용한 실시간 교통정보수집 연구,김대현 /KIM DAE HYON,2016,사단법인 인문사회과학기술융합학회,"비디오 기반의 실시간 교통정보수집 시스템은 자율주행을 포함한 지능형교통체계의 주요 서브시스템중 하나이며, 우수한 예측(추정)력 때문에 핵심 알고리즘으로 인공신경망 모형이 주로 사용되고 있다. 특히, 최근 과학 학술지 네이처에 구글 딥마인드의 알파고가 공개되면서 딥러닝과 인공지능에 대한 전 세계적인 관심이 집중되고 있다. 최근까지 가장 널리 사용된 인공신경망모형은 역전파 모형이었으나 최근 컴퓨터 하드웨어 및 알고리즘의 새로운 개발과 더불어 딥러닝 알고리즘을 이용한 딥인공신경망 모형에 대한 관심이 높아지고 있다. 그러나 아직 딥러닝 인공신경망에 대한 연구는 초기단계에 있는 실정이며 적용분야도 극히 한정되어 있다. 본 연구에서는 실시간 도로교통정보 수집을 위한 영상처리분야에서 딥러닝 알고리즘을 활용하고 실험결과를 통하여 향후 교통정보수집‧분석 분야에서의 딥러닝 인공신경망 모형의 적용가능성을 살펴보고자 한다. 특히, 본 연구에서 도출한 딥러닝 인공신경망모형의 실험결과를 기존 역전파 모형에 의한 실험결과와 비교하여 모형의 우수성을 비교‧평가하고자 한다.Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Normally, artificial neural networks have been used in real-time applications for collecting traffic information because of their remarkable performance in terms of predictive accuracy and real-time computing. Especially, the recent publication of a paper in the journal Nature and the success of Google DeepMind’s world-beating Go program AlphaGo caused significant increases of the public interest in artificial intelligence. Even though various types of learning models have been proposed, the Backpropagation has been the most popular neural network model. However, the Deep learning neural network model has attracted much interest recently. In this study, the Deep leaning neural network will be applied in image processing techniques for the real-time traffic information collection to investigate the performance of Deep learning algorithm in traffic engineering fields. In addition, the experimental results from Deep learing algorithm will be compared with the results from Backpropagation model to evaluate and propose better performing model.Intelligent Transportation Systems (ITS), artificial neural networks, Deep learning, traffic information, image processing"
435,딥러닝의 모형과 응용사례,안성만 /SungMahn Ahn,2016,한국지능정보시스템학회,"딥러닝은 인공신경망(neural network)이라는 인공지능분야의 모형이 발전된 형태로서, 계층구조로 이루어진 인공신경망의 내부계층(hidden layer)이 여러 단계로 이루어진 구조이다. 딥러닝에서의 주요 모형은 합성곱신경망(convolutional neural network), 순환신경망(recurrent neural network), 그리고 심층신뢰신경망(deep belief network)의 세가지라고 할 수있다. 그 중에서 현재 흥미로운 연구가 많이 발표되어서 관심이 집중되고 있는 모형은 지도학습(supervised learning)모형인 처음 두 개의 모형이다. 따라서 본 논문에서는 지도학습모형의 가중치를 최적화하는 기본적인 방법인 오류역전파 알고리즘을 살펴본 뒤에 합성곱신경망과 순환신경망의 구조와 응용사례 등을 살펴보고자 한다. 본문에서 다루지 않은 모형인 심층신뢰신경망은 아직까지는 합성곱신경망 이나 순환신경망보다는 상대적으로 주목을 덜 받고 있다. 그러나 심층신뢰신경망은 CNN이나 RNN과는 달리 비지도학습(unsupervised learning)모형이며, 사람이나 동물은 관찰을 통해서 스스로 학습한다는 점에서 궁극적으로는 비지도학습모형이 더 많이 연구되어야 할 주제가 될 것이다.Deep learning model is a kind of neural networks that allows multiple hidden layers. There are various deep learning architectures such as convolutional neural networks, deep belief networks and recurrent neural networks. Those have been applied to fields like computer vision, automatic speech recognition, natural language processing, audio recognition and bioinformatics where they have been shown to produce state-of-the-art results on various tasks. Among those architectures, convolutional neural networks and recurrent neural networks are classified as the supervised learning model. And in recent years, those supervised learning models have gained more popularity than unsupervised learning models such as deep belief networks, because supervised learning models have shown fashionable applications in such fields mentioned above.
Deep learning models can be trained with backpropagation algorithm. Backpropagation is an abbreviation for “backward propagation of errors” and a common method of training artificial neural networks used in conjunction with an optimization method such as gradient descent. The method calculates the gradient of an error function with respect to all the weights in the network. The gradient is fed to the optimization method which in turn uses it to update the weights, in an attempt to minimize the error function.
Convolutional neural networks use a special architecture which is particularly well-adapted to classify images. Using this architecture makes convolutional networks fast to train. This, in turn, helps us train deep, muti-layer networks, which are very good at classifying images. These days, deep convolutional networks are used in most neural networks for image recognition. Convolutional neural networks use three basic ideas: local receptive fields, shared weights, and pooling. By local receptive fields, we mean that each neuron in the first(or any) hidden layer will be connected to a small region of the input(or previous layer’s) neurons. Shared weights mean that we’re going to use the same weights and bias for each of the local receptive field. This means that all the neurons in the hidden layer detect exactly the same feature, just at different locations in the input image. In addition to the convolutional layers just described, convolutional neural networks also contain pooling layers. Pooling layers are usually used immediately after convolutional layers. What the pooling layers do is to simplify the information in the output from the convolutional layer.
Recent convolutional network architectures have 10 to 20 hidden layers and billions of connections between units. Training deep learning networks has taken weeks several years ago, but thanks to progress in GPU and algorithm enhancement, training time has reduced to several hours.
Neural networks with time-varying behavior are known as recurrent neural networks or RNNs. A recurrent neural network is a class of artificial neural network where connections between units form a directed cycle. This creates an internal state of the network which allows it to exhibit dynamic temporal behavior. Unlike feedforward neural networks, RNNs can use their internal memory to process arbitrary sequences of inputs. Early RNN models turned out to be very difficult to train, harder even than deep feedforward networks. The reason is the unstable gradient problem such as vanishing gradient and exploding gradient. The gradient can get smaller and smaller as it is propagated back through layers. This makes learning in early layers extremely slow. The problem actually gets worse in RNNs, since gradients aren’t just propagated backward through layers, they’re propagated backward through time. If the network runs for a long time, that can make the gradient extremely unstable and hard to learn from. It has been possible to incorporate an idea known as long short-term memory units (LSTMs) into RNNs. LSTMs make it much easier to get good results when training RNNs, and many recent papers make use of LSTMs or related ideas.Deep learning, Convolutional neural networks, Recurrent neural networks, Error backpropagation algorithm"
436,딥러닝분석과 기술적 분석 지표를 이용한 한국 코스피주가지수 방향성 예측,이우식 /Lee Woosik,2017,한국데이터정보과학회,"2016년 3월 구글 (Google)의 바둑인공지능 알파고 (AlphaGo)가 이세돌 9단과의 바둑대결에서승리한 이후 다양한 분야에서 인공지능 사용에 대한 관심이 높아지고 있는 가운데 금융투자 분야에서도 인공지능과 투자자문 전문가의 합성어인 로보어드바이저 (Robo-Advisor)에 대한 관심이 높아지고 있다. 인공지능 (artificial intelligence)기반의 의사결정은 비용 절감은 물론 효과적인 의사결정을가능하게 한다는 점에서 큰 장점이 있다. 본 연구에서는 기술적 분석 (technical analysis) 지표와 딥러닝 (deep learning) 모형을 결합하여 한국 코스피 지수를 예측하는 모형을 개발하고 제시한 모형들의 예측력을 비교, 분석한다. 분석 결과 기술적 분석 지표에 딥러닝 알고리즘을 결합한 모형이 주가지수 방향성 예측 문제에 응용될 수 있음을 확인하였다. 향후 본 연구에서 제안된 기술적 분석 지표와딥러닝모형을 결합한 기법은 로보어드바이저서비스에 응용할 수 있는 일반화 가능성을 보여준다Since Google’s AlphaGo defeated a world champion of Go players in 2016, there have been many interests in the deep learning. In the financial sector, a Robo-Advisor using deep learning gains a significant attention, which builds and manages portfolios of financial instruments for investors.In this paper, we have proposed the a deep learning algorithm geared toward identification and forecast of the KOSPI index direction,and we also have compared the accuracy of the prediction.In an application of forecasting the financial market index direction, we have shown that the Robo-Advisor using deep learning has a significant effect on finance industry. The Robo-Advisor collects a massive data such as earnings statements, news reports and regulatory filings, analyzes those and recommends investors how to view market trends and identify the best time to purchase financial assets. On the other hand, the Robo-Advisor allows businesses to learn more about their customers, develop better marketing strategies, increase sales and decrease costs.Artificial intelligence, deep learning, FinTech, Robo-Advisor, technical analysis."
437,딥러닝의 효율적인 학습을 위한 학습데이터 선별 기법에 관한 연구,주경돈 /Gyoung-don Joo,2017,한국정보과학회,"빅데이터의 증가와 병렬 하드웨어 처리, 학습 알고리즘의 개선은 딥러닝의 발전을 가져왔으며 인공지능이 적용 가능한 영역을 획기적으로 확장시키고 있다. 하지만 이러한 대규모의 학습 데이터에 기반한 딥러닝은 학습시간이 증가되고, 더 나아가 학습 데이터의 구축을 위한 레이블링(labeling) 비용이 데이터 수에 비례하여 증가하게 된다. 따라서 본 논문에서는, 딥러닝의 정확도를 최대한 유지하면서 필요한 학습 데이터의 수를 줄일수 있도록, 전체 데이터에서 제한된 수의 학습 데이터의 후보를 선별하는 기법에 대하여 연구한다. 특히 기존의 능동 학습에 기반한 기법들이 학습 데이터의 수가 매우 적어질 경우 딥러닝의 정확도가 급격히 떨어지는 문제를 해결할 수 있는 새로운 기법을 제안하였으며, 제안된 기법으로 선별된 학습 데이터는 그 수의 변화에 대해 딥러닝이 일관되게 경쟁력있는 정확도를 가짐을 다양한 실험을 통하여 검증하였다.The increase of big data, parallel hardware processing and enhanced learning algorithms have led to the development of deep learning and are dramatically expanding the domain of applications based on artificial intelligence. However, such large-scale training data extends the learning time, and furthermore, the labeling cost for building the training set increases in proportion to the volume of data. Therefore, in this paper, we study how to select a limited number of candidates for training set from given data so as to reduce the number of training set while maintaining the accuracy of deep learning. Especially, we found that, with the training set selected by the previous work, the accuracy of deep learning drastically decreases when the number of training set is very small and so proposed new methods to resolve this fall-off in accuracy. Through experiments, we showed that deep learning consistently has the competitive accuracy with the training set selected by the proposed method regardless of the size of training set.Deep Learning, Train Set Selection, Active Learning, Gamma distribution"
438,딥러닝을 이용한 범용적 스테그아날리시스,김현재 /Hyunjae Kim,2017,한국정보과학회,"스테그아날리시스(Steganalysis)란 이미지 등 일반적인 자료에 암호화된 정보를 은닉하는 스테가노그래피(Steganography)에 대한 검출 및 분석 방법으로, 기계학습 기반 방법론을 포함한다. 기존 기계학습 기반 스테그아날리시스는 영상(Image)의 특징(Feature) 추출 및 모델링에 기반하며, 최근 딥러닝(Deep Learning)의 적용으로 검출 정확도가 큰 폭으로 향상되었다. 하지만 현존하는 스테그아날리시스 모델은 단일 스테가노그래피 기법에 대해 국한되어 있어 학습에 사용되지 않은 스테고(Stego) 이미지의 경우 검출이 불가능한 결정적 한계를 가진다. 본 연구에서는 다양한 스테가노그래피 기법으로 생성된 스테고이미지에 딥러닝을 적용하여 스테그아날리시스를 학습하는 범용적 모델을 제안한다. 다양한 실험을 통해 제안 기법의 효용성 및 가능성을 확인하고, 범용적 스테그아날리시스 모델이 각각에 특화된 검출 기법과 유사한 정확도로 스테고 이미지를 검출할 수 있음을 보인다.Steganalysis is to detect information hidden by steganography inside general data such as images. There are stegoanalysis techniques that use machine learning (ML). Existing ML approaches to steganalysis are based on extracting features from stego images and modeling them.
Recently deep learning-based methodologies have shown significant improvements in detection accuracy. However, all the existing methods, including deep learning-based ones, have a critical limitation in that they can only detect stego images that are created by a specific steganography method. In this paper, we propose a generalized steganalysis method that can model multiple types of stego images using deep learning. Through various experiments, we confirm the effectiveness of our approach and envision directions for future research. In particular, we show that our method can detect each type of steganography with the same level of accuracy as that of a steganalysis method dedicated to that type of steganography, thereby demonstrating the general applicability of our approach to multiple types of stego images.steganalysis, steganography, deep learning, generalized steganalysis model"
439,"딥러닝 프레임워크의 비교: 티아노, 텐서플로, CNTK를 중심으로",정여진 /Yeojin Chung,2017,한국지능정보시스템학회,"딥러닝 프레임워크의 대표적인 기능으로는 ‘자동미분’과 ‘GPU의 활용’ 등을 들 수 있다. 본 논문은 파이썬의 라이브러리 형태로 사용 가능한 프레임워크 중에서 구글의 텐서플로와 마이크로소프트의 CNTK, 그리고 텐서 플로의 원조라고 할 수 있는 티아노를 비교하였다. 본문에서는 자동미분의 개념과 GPU의 활용형태를 간단히 설명하고, 그 다음에 logistic regression을 실행하는 예를 통하여 각 프레임워크의 문법을 알아본 뒤에, 마지막으로 대표적인 딥러닝 응용인 CNN의 예제를 실행시켜보고 코딩의 편의성과 실행속도 등을 확인해 보았다. 그 결과, 편의성의 관점에서 보면 티아노가 가장 코딩 하기가 어렵고, CNTK와 텐서플로는 많은 부분이 비슷하게 추상화 되어 있어서 코딩이 비슷하지만 가중치와 편향을 직접 정의하느냐의 여부에서 차이를 보였다. 그리고 각프레임워크의 실행속도에 대한 평가는 ‘큰 차이는 없다’는 것이다. 텐서플로는 티아노에 비하여 속도가 느리다는 평가가 있어왔는데, 본 연구의 실험에 의하면, 비록 CNN 모형에 국한되었지만, 텐서플로가 아주 조금이지만빠른 것으로 나타났다. CNTK의 경우에도, 비록 실험환경이 달랐지만, 실험환경의 차이에 의한 속도의 차이의 편차범위 이내에 있는 것으로 판단이 되었다. 본 연구에서는 세 종류의 딥러닝 프레임워크만을 살펴보았는데,위키피디아에 따르면 딥러닝 프레임워크의 종류는 12가지가 있으며, 각 프레임워크의 특징을 15가지 속성으로구분하여 차이를 특정하고 있다. 그 많은 속성 중에서 사용자의 입장에서 볼 때 중요한 속성은 어떤 언어(파이썬, C++, Java, 등)로 사용가능한지, 어떤 딥러닝 모형에 대한 라이브러리가 잘 구현되어 있는지 등일 것이다. 그리고 사용자가 대규모의 딥러닝 모형을 구축한다면, 다중 GPU 혹은 다중 서버를 지원하는지의 여부도 중요할 것이다. 또한 딥러닝 모형을 처음 학습하는 경우에는 사용설명서가 많은지 예제 프로그램이 많은지 여부도 중요한 기준이 될 것이다.The deep learning framework is software designed to help develop deep learning models. Some of its important functions include “automatic differentiation” and “utilization of GPU”. The list of popular deep learning framework includes Caffe (BVLC) and Theano (University of Montreal). And recently, Microsoft's deep learning framework, Microsoft Cognitive Toolkit, was released as open-source license, following Google’s Tensorflow a year earlier. The early deep learning frameworks have been developed mainly for research at universities. Beginning with the inception of Tensorflow, however, it seems that companies such as Microsoft and Facebook have started to join the competition of framework development. Given the trend, Google and other companies are expected to continue investing in the deep learning framework to bring forward the initiative in the artificial intelligence business. From this point of view, we think it is a good time to compare some of deep learning frameworks. So we compare three deep learning frameworks which can be used as a Python library. Those are Google's Tensorflow, Microsoft’s CNTK, and Theano which is sort of a predecessor of the preceding two.
The most common and important function of deep learning frameworks is the ability to perform automatic differentiation. Basically all the mathematical expressions of deep learning models can be represented as computational graphs, which consist of nodes and edges. Partial derivatives on each edge of a computational graph can then be obtained. With the partial derivatives, we can let software compute differentiation of any node with respect to any variable by utilizing chain rule of Calculus.
First of all, the convenience of coding is in the order of CNTK, Tensorflow, and Theano. The criterion is simply based on the lengths of the codes and the learning curve and the ease of coding are not the main concern. According to the criteria, Theano was the most difficult to implement with, and CNTK and Tensorflow were somewhat easier. With Tensorflow, we need to define weight variables and biases explicitly. The reason that CNTK and Tensorflow are easier to implement with is that those frameworks provide us with more abstraction than Theano. We, however, need to mention that low-level coding is not always bad. It gives us flexibility of coding. With the low-level coding such as in Theano, we can implement and test any new deep learning models or any new search methods that we can think of.
The assessment of the execution speed of each framework is that there is not meaningful difference. According to the experiment, execution speeds of Theano and Tensorflow are very similar, although the experiment was limited to a CNN model. In the case of CNTK, the experimental environment was not maintained as the same. The code written in CNTK has to be run in PC environment without GPU where codes execute as much as 50 times slower than with GPU. But we concluded that the difference of execution speed was within the range of variation caused by the different hardware setup.
In this study, we compared three types of deep learning framework: Theano, Tensorflow, and CNTK. According to Wikipedia, there are 12 available deep learning frameworks. And 15 different attributes differentiate each framework. Some of the important attributes would include interface language (Python, C ++, Java, etc.) and the availability of libraries on various deep learning models such as CNN, RNN, DBN, and etc. And if a user implements a large scale deep learning model, it will also be important to support multiple GPU or multiple servers. Also, if you are learning the deep learning model, it would also be important if there are enough examples and references.deep learning framework, Theano, TensorFlow, CNTK, computational graph, CIFAR-10"
440,딥러닝을 이용한 주변 무선단말 파악방안,이웅섭 /Lee， Woong-sup,2018,한국정보통신학회,"최근 단말-대-단말(Device-to-device, D2D) 통신기술이 차세대 무선통신시스템의 핵심기술로 큰 관심을 받고 있다. 이러한 단말간 통신에서는 자신의 주변에 어떠한 단말이 있는지 파악하는 주변단말 탐색(Neighbor discovery)이매우 중요하다. 본 논문에서는 최근 큰 관심을 받고 있는 딥러닝(Deep learning) 기술을 활용하여 단말간 통신에서 주변단말을 파악하는 방안에 대해서 제안한다. 제안 방안은 기존의 방안과 달리 무선채널의 공간적 연관성을 이용하여단말간의 신호 전송 없이 단말이 기지국으로 전송하는 상향링크 파일럿 신호를 기반으로 주변 단말을 찾고 따라서기존의 방식에 비해 신호전송 복잡도(signaling complexity)를 크게 줄일 수 있다. 또한 제안 방안에서는 떨어져 있는거리에 따라서 주변 단말을 분류 가능하여 기존 방안에 비해서 좀 더 세밀한 단말 탐색이 가능하다. 마지막으로 본 논문에서는 tensorflow를 이용한 컴퓨터 시뮬레이션을 통해 제안 방안의 성능을 검증하였다.Recently, the device-to-device (D2D) communication has been conceived as the key technology for the next-generationmobile communication systems. The neighbor discovery in which the nearby users are found, is essential for the properoperation of the D2D communication. In this paper, we propose new neighbor discovery scheme based on deep learningtechnology which has gained a lot of attention recently. In the proposed scheme, the neighboring users can be foundusing the uplink pilot transmission of users only, unlike conventional neighbor discovery schemes in which direct pilotcommunication among users is required, such that the signaling overhead can be greatly reduced in our proposed scheme.
Moreover, the neighbors with different proximity can also be classified accordingly which enables more accurate neighbordiscovery compared to the conventional schemes. The performance of our proposed scheme is verified through thetensorflow-based computer simulations.
키워드 : 딥러닝, 주변단말 탐색, 단말-대-단말 통신Deep learning, Neighbor discovery, Device-to-device communication, Wireless channel correlation"
441,신뢰성있는 딥러닝 기반 분석 모델을 참조하기 위한 딥러닝 기술 언어,문종혁 /Jong Hyeok Mun,2021,한국정보처리학회,"최근 딥러닝은 하드웨어 성능이 향상됨에 따라 자연어 처리, 영상 인식 등의 다양한 기술에 접목되어 활용되고 있다. 이러한 기술들을 활용해 지능형교통 시스템(ITS), 스마트홈, 헬스케어 등의 산업분야에서 데이터를 분석하여 고속도로 속도위반 차량 검출, 에너지 사용량 제어, 응급상황 등과 같은고품질의 서비스를 제공하며, 고품질의 서비스를 제공하기 위해서는 정확도가 향상된 딥러닝 모델이 적용되어야 한다. 이를 위해 서비스 환경의 데이터를분석하기 위한 딥러닝 모델을 개발할 때, 개발자는 신뢰성이 검증된 최신의 딥러닝 모델을 적용할 수 있어야 한다. 이는 개발자가 참조하는 딥러닝 모델에적용된 학습 데이터셋의 정확도를 측정하여 검증할 수 있다. 이러한 검증을 위해서 개발자는 학습 데이터셋, 딥러닝의 계층구조 및 개발 환경 등과 같은내용을 포함하는 딥러닝 모델을 문서화하여 적용하기 위한 구조적인 정보가 필요하다. 본 논문에서는 신뢰성있는 딥러닝 기반 데이터 분석 모델을 참조하기위한 딥러닝 기술 언어를 제안한다. 제안하는 기술 언어는 신뢰성 있는 딥러닝 모델을 개발하는데 필요한 학습데이터셋, 개발 환경 및 설정 등의 정보와더불어 딥러닝 모델의 계층구조를 표현할 수 있다. 제안하는 딥러닝 기술 언어를 이용하여 개발자는 지능형 교통 시스템에서 참조하는 분석 모델의정확도를 검증할 수 있다. 실험에서는 제안하는 언어의 유효성을 검증하기 위해, 번호판 인식 모델을 중심으로 딥러닝 기술 문서의 적용과정을 보인다With the recent advancements of deep learning, companies such as smart home, healthcare, and intelligent transportation systems areutilizing its functionality to provide high-quality services for vehicle detection, emergency situation detection, and controlling energyconsumption. To provide reliable services in such sensitive systems, deep learning models are required to have high accuracy. In orderto develop a deep learning model for analyzing previously mentioned services, developers should utilize the state of the art deep learningmodels that have already been verified for higher accuracy. The developers can verify the accuracy of the referenced model by validatingthe model on the dataset. For this validation, the developer needs structural information to document and apply deep learning models,including metadata such as learning dataset, network architecture, and development environments. In this paper, we propose a descriptionlanguage that represents the network architecture of the deep learning model along with its metadata that are necessary to develop adeep learning model. Through the proposed description language, developers can easily verify the accuracy of the referenced deep learningmodel. Our experiments demonstrate the application scenario of a deep learning description document that focuses on the license platerecognition for the detection of illegally parked vehicles.Trusted Deep Learning, Model Reference, Deep Learning Description Language, Traffic Situation Analysis Model"
442,딥러닝 분석을 이용한 중국 역내·외 위안화 변동성 예측,이우식 /Lee Woosik,2016,한국데이터정보과학회,"2008년 글로벌 금융위기 이후 중국은 위안화 국제화의 점진적 추진을 진행하면서 중국상하이 외환시장과 중국홍콩 외환시장에서 거래되는 통화인 역내위안화와 역외위안화를 형성시켰다. 본 연구는 위안화 국제화와 점진적인 중국 자본계정 개방에 따라 급변하는 외환시장상황의 변동성을 정확하게 파악하기 위해서 GARCH모형 (일반화된 자기회귀 조건부이분산성모형)에 다단계인공신경망을 결합한 MLP-GARCH 모형과 GARCH모형과 기계학습의 일종인 딥러닝 (deep learning)을 통합한 DL-GARCH을 가지고 위안화 변동성예측을 비교 실험과 분석을 하였다. 비교분석 결과 DLGARCH 모형은 MLP-GARCH보다 모형 위안화 역내·외 환율변동성 예측 면에서 더욱 더 개선된예측값을 제공하였다. 그래서 이분산시계열모형을 딥러닝과 결합한 DL-GARCH 모형은 시계열의환율 변동성 예측 문제에 딥러닝을 응용할 수 있음을 확인하였다. 향후 이분산시계열과 결합된 딥러닝 모형은 다른 금융시계열 데이터에 응용하여 그 일반화 가능성을 높일 수 있을 것이다.The People’s Republic of China has vigorously been pursuing the internationalization of the Chinese Yuan or Renminbi after the financial crisis of 2008. In this view, an abrupt increase of use of the Chinese Yuan in the onshore and offshore markets are important milestones to be one of important currencies. One of the most frequently used methods to forecast volatility is GARCH model. Since a prediction error of the GARCH model has been reported quite high, a lot of efforts have been made to improve forecasting capability of the GARCH model. In this paper, we have proposed MLPGARCH and a DL-GARCH by employing Artificial Neural Network to the GARCH. In an application to forecasting Chinese Yuan volatility, we have successfully shown their overall outperformance in forecasting over the GARCH.Deep learning, DL-GARCH, GARCH, The Chinese Yuan, volatility."
443,딥러닝 기법을 이용한 내일강수 예측,하지훈,2016,한국지능시스템학회,"정확한 강수예측을 위해서는 예측인자 선정과 예측방법에 대한 선택이 매우 중요하다. 최근에는 강수예측 방법으로기계학습 기법이 많이 사용되고 있으며, 그 중에서도 특히 인공신경망을 사용한 강수예측 방법은 좋은 성능을 보였다. 본논문에서는 딥러닝 기법 중 하나인 DBN(deep belief network)를 이용한 새로운 강수예측 방법을 제안한다. DBN는 비지도사전 학습을 통해 초기 가중치를 설정하여 기존 인공신경망의 문제점을 보완한다. 예측인자로는 기온, 전일-전주 강수일, 태양과 달 궤도 관련 자료를 선정하였다. 기온과 전일-전주 강수일은 서울에서의 1974년부터 2013년까지 총 40년간의AWS(automatic weather system) 관측 자료를 사용하였고, 태양과 달의 궤도 관련 자료는 서울을 중심으로 계산한 결과를사용하였다. 전체 기간에서 일부는 학습 자료로 사용하여 예측모델을 생성하였고, 나머지를 생성한 모델의 검증 자료로사용하였다. 모델 검증 결과로 나온 예측값들은 확률값을 가지며 임계치를 이용하여 강수유무를 판별하였다. 강수 정확도의척도로 양분예보기법 중 CSI(critical successive index)와 Bias(frequency bias)를 계산하였다. 이를 통해 DBN와 MLP(multilayer perceptron)의 성능을 비교한 결과 DBN의 강수 예측 정확도가 높았고, 수행속도 또한 2배 이상 빨랐다.For accurate precipitation forecasts the choice of weather faoctrs and prediction method is very important. Recently, machinele arning has been widely used for forecasting precipitation, and artificial neural network, one of machine learning techniques, showed good performance. In this paper, we suggest a new method for forecasting precipitation using DBN, one of deep learning techniques.
DBN has an advantage that initial weights are set by unsupervised learning, so this compensates for the defects of artificial neural networks. We used past precipitation, temperature, and the parameters of the sun and moon's motion as features for forecasting precipitation. The dataset consists of observation data which had been measured for 40 years from AWS in Seoul. Experiments were based on 8-fold cross validation. As a result of estimation, we got probabilities of test dataset, so threshold was used for the decision of precipitation. CSI and Bias were used for indicating the precision of precipitation. Our experimental results showed that DBN performed better than MLP.Deep learning, Deep belief network, Precipitation, Forecast"
444,딥러닝 신경망모형을 이용한 실시간 교통정보수집 연구,김대현 /KIM DAE HYON,2016,사단법인 인문사회과학기술융합학회,"비디오 기반의 실시간 교통정보수집 시스템은 자율주행을 포함한 지능형교통체계의 주요 서브시스템중 하나이며, 우수한 예측(추정)력 때문에 핵심 알고리즘으로 인공신경망 모형이 주로 사용되고 있다. 특히, 최근 과학 학술지 네이처에 구글 딥마인드의 알파고가 공개되면서 딥러닝과 인공지능에 대한 전 세계적인 관심이 집중되고 있다. 최근까지 가장 널리 사용된 인공신경망모형은 역전파 모형이었으나 최근 컴퓨터 하드웨어 및 알고리즘의 새로운 개발과 더불어 딥러닝 알고리즘을 이용한 딥인공신경망 모형에 대한 관심이 높아지고 있다. 그러나 아직 딥러닝 인공신경망에 대한 연구는 초기단계에 있는 실정이며 적용분야도 극히 한정되어 있다. 본 연구에서는 실시간 도로교통정보 수집을 위한 영상처리분야에서 딥러닝 알고리즘을 활용하고 실험결과를 통하여 향후 교통정보수집‧분석 분야에서의 딥러닝 인공신경망 모형의 적용가능성을 살펴보고자 한다. 특히, 본 연구에서 도출한 딥러닝 인공신경망모형의 실험결과를 기존 역전파 모형에 의한 실험결과와 비교하여 모형의 우수성을 비교‧평가하고자 한다.Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Normally, artificial neural networks have been used in real-time applications for collecting traffic information because of their remarkable performance in terms of predictive accuracy and real-time computing. Especially, the recent publication of a paper in the journal Nature and the success of Google DeepMind’s world-beating Go program AlphaGo caused significant increases of the public interest in artificial intelligence. Even though various types of learning models have been proposed, the Backpropagation has been the most popular neural network model. However, the Deep learning neural network model has attracted much interest recently. In this study, the Deep leaning neural network will be applied in image processing techniques for the real-time traffic information collection to investigate the performance of Deep learning algorithm in traffic engineering fields. In addition, the experimental results from Deep learing algorithm will be compared with the results from Backpropagation model to evaluate and propose better performing model.Intelligent Transportation Systems (ITS), artificial neural networks, Deep learning, traffic information, image processing"
445,딥러닝의 모형과 응용사례,안성만 /SungMahn Ahn,2016,한국지능정보시스템학회,"딥러닝은 인공신경망(neural network)이라는 인공지능분야의 모형이 발전된 형태로서, 계층구조로 이루어진 인공신경망의 내부계층(hidden layer)이 여러 단계로 이루어진 구조이다. 딥러닝에서의 주요 모형은 합성곱신경망(convolutional neural network), 순환신경망(recurrent neural network), 그리고 심층신뢰신경망(deep belief network)의 세가지라고 할 수있다. 그 중에서 현재 흥미로운 연구가 많이 발표되어서 관심이 집중되고 있는 모형은 지도학습(supervised learning)모형인 처음 두 개의 모형이다. 따라서 본 논문에서는 지도학습모형의 가중치를 최적화하는 기본적인 방법인 오류역전파 알고리즘을 살펴본 뒤에 합성곱신경망과 순환신경망의 구조와 응용사례 등을 살펴보고자 한다. 본문에서 다루지 않은 모형인 심층신뢰신경망은 아직까지는 합성곱신경망 이나 순환신경망보다는 상대적으로 주목을 덜 받고 있다. 그러나 심층신뢰신경망은 CNN이나 RNN과는 달리 비지도학습(unsupervised learning)모형이며, 사람이나 동물은 관찰을 통해서 스스로 학습한다는 점에서 궁극적으로는 비지도학습모형이 더 많이 연구되어야 할 주제가 될 것이다.Deep learning model is a kind of neural networks that allows multiple hidden layers. There are various deep learning architectures such as convolutional neural networks, deep belief networks and recurrent neural networks. Those have been applied to fields like computer vision, automatic speech recognition, natural language processing, audio recognition and bioinformatics where they have been shown to produce state-of-the-art results on various tasks. Among those architectures, convolutional neural networks and recurrent neural networks are classified as the supervised learning model. And in recent years, those supervised learning models have gained more popularity than unsupervised learning models such as deep belief networks, because supervised learning models have shown fashionable applications in such fields mentioned above.
Deep learning models can be trained with backpropagation algorithm. Backpropagation is an abbreviation for “backward propagation of errors” and a common method of training artificial neural networks used in conjunction with an optimization method such as gradient descent. The method calculates the gradient of an error function with respect to all the weights in the network. The gradient is fed to the optimization method which in turn uses it to update the weights, in an attempt to minimize the error function.
Convolutional neural networks use a special architecture which is particularly well-adapted to classify images. Using this architecture makes convolutional networks fast to train. This, in turn, helps us train deep, muti-layer networks, which are very good at classifying images. These days, deep convolutional networks are used in most neural networks for image recognition. Convolutional neural networks use three basic ideas: local receptive fields, shared weights, and pooling. By local receptive fields, we mean that each neuron in the first(or any) hidden layer will be connected to a small region of the input(or previous layer’s) neurons. Shared weights mean that we’re going to use the same weights and bias for each of the local receptive field. This means that all the neurons in the hidden layer detect exactly the same feature, just at different locations in the input image. In addition to the convolutional layers just described, convolutional neural networks also contain pooling layers. Pooling layers are usually used immediately after convolutional layers. What the pooling layers do is to simplify the information in the output from the convolutional layer.
Recent convolutional network architectures have 10 to 20 hidden layers and billions of connections between units. Training deep learning networks has taken weeks several years ago, but thanks to progress in GPU and algorithm enhancement, training time has reduced to several hours.
Neural networks with time-varying behavior are known as recurrent neural networks or RNNs. A recurrent neural network is a class of artificial neural network where connections between units form a directed cycle. This creates an internal state of the network which allows it to exhibit dynamic temporal behavior. Unlike feedforward neural networks, RNNs can use their internal memory to process arbitrary sequences of inputs. Early RNN models turned out to be very difficult to train, harder even than deep feedforward networks. The reason is the unstable gradient problem such as vanishing gradient and exploding gradient. The gradient can get smaller and smaller as it is propagated back through layers. This makes learning in early layers extremely slow. The problem actually gets worse in RNNs, since gradients aren’t just propagated backward through layers, they’re propagated backward through time. If the network runs for a long time, that can make the gradient extremely unstable and hard to learn from. It has been possible to incorporate an idea known as long short-term memory units (LSTMs) into RNNs. LSTMs make it much easier to get good results when training RNNs, and many recent papers make use of LSTMs or related ideas.Deep learning, Convolutional neural networks, Recurrent neural networks, Error backpropagation algorithm"
446,딥러닝분석과 기술적 분석 지표를 이용한 한국 코스피주가지수 방향성 예측,이우식 /Lee Woosik,2017,한국데이터정보과학회,"2016년 3월 구글 (Google)의 바둑인공지능 알파고 (AlphaGo)가 이세돌 9단과의 바둑대결에서승리한 이후 다양한 분야에서 인공지능 사용에 대한 관심이 높아지고 있는 가운데 금융투자 분야에서도 인공지능과 투자자문 전문가의 합성어인 로보어드바이저 (Robo-Advisor)에 대한 관심이 높아지고 있다. 인공지능 (artificial intelligence)기반의 의사결정은 비용 절감은 물론 효과적인 의사결정을가능하게 한다는 점에서 큰 장점이 있다. 본 연구에서는 기술적 분석 (technical analysis) 지표와 딥러닝 (deep learning) 모형을 결합하여 한국 코스피 지수를 예측하는 모형을 개발하고 제시한 모형들의 예측력을 비교, 분석한다. 분석 결과 기술적 분석 지표에 딥러닝 알고리즘을 결합한 모형이 주가지수 방향성 예측 문제에 응용될 수 있음을 확인하였다. 향후 본 연구에서 제안된 기술적 분석 지표와딥러닝모형을 결합한 기법은 로보어드바이저서비스에 응용할 수 있는 일반화 가능성을 보여준다Since Google’s AlphaGo defeated a world champion of Go players in 2016, there have been many interests in the deep learning. In the financial sector, a Robo-Advisor using deep learning gains a significant attention, which builds and manages portfolios of financial instruments for investors.In this paper, we have proposed the a deep learning algorithm geared toward identification and forecast of the KOSPI index direction,and we also have compared the accuracy of the prediction.In an application of forecasting the financial market index direction, we have shown that the Robo-Advisor using deep learning has a significant effect on finance industry. The Robo-Advisor collects a massive data such as earnings statements, news reports and regulatory filings, analyzes those and recommends investors how to view market trends and identify the best time to purchase financial assets. On the other hand, the Robo-Advisor allows businesses to learn more about their customers, develop better marketing strategies, increase sales and decrease costs.Artificial intelligence, deep learning, FinTech, Robo-Advisor, technical analysis."
447,딥러닝의 효율적인 학습을 위한 학습데이터 선별 기법에 관한 연구,주경돈 /Gyoung-don Joo,2017,한국정보과학회,"빅데이터의 증가와 병렬 하드웨어 처리, 학습 알고리즘의 개선은 딥러닝의 발전을 가져왔으며 인공지능이 적용 가능한 영역을 획기적으로 확장시키고 있다. 하지만 이러한 대규모의 학습 데이터에 기반한 딥러닝은 학습시간이 증가되고, 더 나아가 학습 데이터의 구축을 위한 레이블링(labeling) 비용이 데이터 수에 비례하여 증가하게 된다. 따라서 본 논문에서는, 딥러닝의 정확도를 최대한 유지하면서 필요한 학습 데이터의 수를 줄일수 있도록, 전체 데이터에서 제한된 수의 학습 데이터의 후보를 선별하는 기법에 대하여 연구한다. 특히 기존의 능동 학습에 기반한 기법들이 학습 데이터의 수가 매우 적어질 경우 딥러닝의 정확도가 급격히 떨어지는 문제를 해결할 수 있는 새로운 기법을 제안하였으며, 제안된 기법으로 선별된 학습 데이터는 그 수의 변화에 대해 딥러닝이 일관되게 경쟁력있는 정확도를 가짐을 다양한 실험을 통하여 검증하였다.The increase of big data, parallel hardware processing and enhanced learning algorithms have led to the development of deep learning and are dramatically expanding the domain of applications based on artificial intelligence. However, such large-scale training data extends the learning time, and furthermore, the labeling cost for building the training set increases in proportion to the volume of data. Therefore, in this paper, we study how to select a limited number of candidates for training set from given data so as to reduce the number of training set while maintaining the accuracy of deep learning. Especially, we found that, with the training set selected by the previous work, the accuracy of deep learning drastically decreases when the number of training set is very small and so proposed new methods to resolve this fall-off in accuracy. Through experiments, we showed that deep learning consistently has the competitive accuracy with the training set selected by the proposed method regardless of the size of training set.Deep Learning, Train Set Selection, Active Learning, Gamma distribution"
448,딥러닝을 이용한 범용적 스테그아날리시스,김현재 /Hyunjae Kim,2017,한국정보과학회,"스테그아날리시스(Steganalysis)란 이미지 등 일반적인 자료에 암호화된 정보를 은닉하는 스테가노그래피(Steganography)에 대한 검출 및 분석 방법으로, 기계학습 기반 방법론을 포함한다. 기존 기계학습 기반 스테그아날리시스는 영상(Image)의 특징(Feature) 추출 및 모델링에 기반하며, 최근 딥러닝(Deep Learning)의 적용으로 검출 정확도가 큰 폭으로 향상되었다. 하지만 현존하는 스테그아날리시스 모델은 단일 스테가노그래피 기법에 대해 국한되어 있어 학습에 사용되지 않은 스테고(Stego) 이미지의 경우 검출이 불가능한 결정적 한계를 가진다. 본 연구에서는 다양한 스테가노그래피 기법으로 생성된 스테고이미지에 딥러닝을 적용하여 스테그아날리시스를 학습하는 범용적 모델을 제안한다. 다양한 실험을 통해 제안 기법의 효용성 및 가능성을 확인하고, 범용적 스테그아날리시스 모델이 각각에 특화된 검출 기법과 유사한 정확도로 스테고 이미지를 검출할 수 있음을 보인다.Steganalysis is to detect information hidden by steganography inside general data such as images. There are stegoanalysis techniques that use machine learning (ML). Existing ML approaches to steganalysis are based on extracting features from stego images and modeling them.
Recently deep learning-based methodologies have shown significant improvements in detection accuracy. However, all the existing methods, including deep learning-based ones, have a critical limitation in that they can only detect stego images that are created by a specific steganography method. In this paper, we propose a generalized steganalysis method that can model multiple types of stego images using deep learning. Through various experiments, we confirm the effectiveness of our approach and envision directions for future research. In particular, we show that our method can detect each type of steganography with the same level of accuracy as that of a steganalysis method dedicated to that type of steganography, thereby demonstrating the general applicability of our approach to multiple types of stego images.steganalysis, steganography, deep learning, generalized steganalysis model"
449,"딥러닝 프레임워크의 비교: 티아노, 텐서플로, CNTK를 중심으로",정여진 /Yeojin Chung,2017,한국지능정보시스템학회,"딥러닝 프레임워크의 대표적인 기능으로는 ‘자동미분’과 ‘GPU의 활용’ 등을 들 수 있다. 본 논문은 파이썬의 라이브러리 형태로 사용 가능한 프레임워크 중에서 구글의 텐서플로와 마이크로소프트의 CNTK, 그리고 텐서 플로의 원조라고 할 수 있는 티아노를 비교하였다. 본문에서는 자동미분의 개념과 GPU의 활용형태를 간단히 설명하고, 그 다음에 logistic regression을 실행하는 예를 통하여 각 프레임워크의 문법을 알아본 뒤에, 마지막으로 대표적인 딥러닝 응용인 CNN의 예제를 실행시켜보고 코딩의 편의성과 실행속도 등을 확인해 보았다. 그 결과, 편의성의 관점에서 보면 티아노가 가장 코딩 하기가 어렵고, CNTK와 텐서플로는 많은 부분이 비슷하게 추상화 되어 있어서 코딩이 비슷하지만 가중치와 편향을 직접 정의하느냐의 여부에서 차이를 보였다. 그리고 각프레임워크의 실행속도에 대한 평가는 ‘큰 차이는 없다’는 것이다. 텐서플로는 티아노에 비하여 속도가 느리다는 평가가 있어왔는데, 본 연구의 실험에 의하면, 비록 CNN 모형에 국한되었지만, 텐서플로가 아주 조금이지만빠른 것으로 나타났다. CNTK의 경우에도, 비록 실험환경이 달랐지만, 실험환경의 차이에 의한 속도의 차이의 편차범위 이내에 있는 것으로 판단이 되었다. 본 연구에서는 세 종류의 딥러닝 프레임워크만을 살펴보았는데,위키피디아에 따르면 딥러닝 프레임워크의 종류는 12가지가 있으며, 각 프레임워크의 특징을 15가지 속성으로구분하여 차이를 특정하고 있다. 그 많은 속성 중에서 사용자의 입장에서 볼 때 중요한 속성은 어떤 언어(파이썬, C++, Java, 등)로 사용가능한지, 어떤 딥러닝 모형에 대한 라이브러리가 잘 구현되어 있는지 등일 것이다. 그리고 사용자가 대규모의 딥러닝 모형을 구축한다면, 다중 GPU 혹은 다중 서버를 지원하는지의 여부도 중요할 것이다. 또한 딥러닝 모형을 처음 학습하는 경우에는 사용설명서가 많은지 예제 프로그램이 많은지 여부도 중요한 기준이 될 것이다.The deep learning framework is software designed to help develop deep learning models. Some of its important functions include “automatic differentiation” and “utilization of GPU”. The list of popular deep learning framework includes Caffe (BVLC) and Theano (University of Montreal). And recently, Microsoft's deep learning framework, Microsoft Cognitive Toolkit, was released as open-source license, following Google’s Tensorflow a year earlier. The early deep learning frameworks have been developed mainly for research at universities. Beginning with the inception of Tensorflow, however, it seems that companies such as Microsoft and Facebook have started to join the competition of framework development. Given the trend, Google and other companies are expected to continue investing in the deep learning framework to bring forward the initiative in the artificial intelligence business. From this point of view, we think it is a good time to compare some of deep learning frameworks. So we compare three deep learning frameworks which can be used as a Python library. Those are Google's Tensorflow, Microsoft’s CNTK, and Theano which is sort of a predecessor of the preceding two.
The most common and important function of deep learning frameworks is the ability to perform automatic differentiation. Basically all the mathematical expressions of deep learning models can be represented as computational graphs, which consist of nodes and edges. Partial derivatives on each edge of a computational graph can then be obtained. With the partial derivatives, we can let software compute differentiation of any node with respect to any variable by utilizing chain rule of Calculus.
First of all, the convenience of coding is in the order of CNTK, Tensorflow, and Theano. The criterion is simply based on the lengths of the codes and the learning curve and the ease of coding are not the main concern. According to the criteria, Theano was the most difficult to implement with, and CNTK and Tensorflow were somewhat easier. With Tensorflow, we need to define weight variables and biases explicitly. The reason that CNTK and Tensorflow are easier to implement with is that those frameworks provide us with more abstraction than Theano. We, however, need to mention that low-level coding is not always bad. It gives us flexibility of coding. With the low-level coding such as in Theano, we can implement and test any new deep learning models or any new search methods that we can think of.
The assessment of the execution speed of each framework is that there is not meaningful difference. According to the experiment, execution speeds of Theano and Tensorflow are very similar, although the experiment was limited to a CNN model. In the case of CNTK, the experimental environment was not maintained as the same. The code written in CNTK has to be run in PC environment without GPU where codes execute as much as 50 times slower than with GPU. But we concluded that the difference of execution speed was within the range of variation caused by the different hardware setup.
In this study, we compared three types of deep learning framework: Theano, Tensorflow, and CNTK. According to Wikipedia, there are 12 available deep learning frameworks. And 15 different attributes differentiate each framework. Some of the important attributes would include interface language (Python, C ++, Java, etc.) and the availability of libraries on various deep learning models such as CNN, RNN, DBN, and etc. And if a user implements a large scale deep learning model, it will also be important to support multiple GPU or multiple servers. Also, if you are learning the deep learning model, it would also be important if there are enough examples and references.deep learning framework, Theano, TensorFlow, CNTK, computational graph, CIFAR-10"
450,딥러닝을 이용한 주변 무선단말 파악방안,이웅섭 /Lee， Woong-sup,2018,한국정보통신학회,"최근 단말-대-단말(Device-to-device, D2D) 통신기술이 차세대 무선통신시스템의 핵심기술로 큰 관심을 받고 있다. 이러한 단말간 통신에서는 자신의 주변에 어떠한 단말이 있는지 파악하는 주변단말 탐색(Neighbor discovery)이매우 중요하다. 본 논문에서는 최근 큰 관심을 받고 있는 딥러닝(Deep learning) 기술을 활용하여 단말간 통신에서 주변단말을 파악하는 방안에 대해서 제안한다. 제안 방안은 기존의 방안과 달리 무선채널의 공간적 연관성을 이용하여단말간의 신호 전송 없이 단말이 기지국으로 전송하는 상향링크 파일럿 신호를 기반으로 주변 단말을 찾고 따라서기존의 방식에 비해 신호전송 복잡도(signaling complexity)를 크게 줄일 수 있다. 또한 제안 방안에서는 떨어져 있는거리에 따라서 주변 단말을 분류 가능하여 기존 방안에 비해서 좀 더 세밀한 단말 탐색이 가능하다. 마지막으로 본 논문에서는 tensorflow를 이용한 컴퓨터 시뮬레이션을 통해 제안 방안의 성능을 검증하였다.Recently, the device-to-device (D2D) communication has been conceived as the key technology for the next-generationmobile communication systems. The neighbor discovery in which the nearby users are found, is essential for the properoperation of the D2D communication. In this paper, we propose new neighbor discovery scheme based on deep learningtechnology which has gained a lot of attention recently. In the proposed scheme, the neighboring users can be foundusing the uplink pilot transmission of users only, unlike conventional neighbor discovery schemes in which direct pilotcommunication among users is required, such that the signaling overhead can be greatly reduced in our proposed scheme.
Moreover, the neighbors with different proximity can also be classified accordingly which enables more accurate neighbordiscovery compared to the conventional schemes. The performance of our proposed scheme is verified through thetensorflow-based computer simulations.
키워드 : 딥러닝, 주변단말 탐색, 단말-대-단말 통신Deep learning, Neighbor discovery, Device-to-device communication, Wireless channel correlation"
451,신뢰성있는 딥러닝 기반 분석 모델을 참조하기 위한 딥러닝 기술 언어,문종혁 /Jong Hyeok Mun,2021,한국정보처리학회,"최근 딥러닝은 하드웨어 성능이 향상됨에 따라 자연어 처리, 영상 인식 등의 다양한 기술에 접목되어 활용되고 있다. 이러한 기술들을 활용해 지능형교통 시스템(ITS), 스마트홈, 헬스케어 등의 산업분야에서 데이터를 분석하여 고속도로 속도위반 차량 검출, 에너지 사용량 제어, 응급상황 등과 같은고품질의 서비스를 제공하며, 고품질의 서비스를 제공하기 위해서는 정확도가 향상된 딥러닝 모델이 적용되어야 한다. 이를 위해 서비스 환경의 데이터를분석하기 위한 딥러닝 모델을 개발할 때, 개발자는 신뢰성이 검증된 최신의 딥러닝 모델을 적용할 수 있어야 한다. 이는 개발자가 참조하는 딥러닝 모델에적용된 학습 데이터셋의 정확도를 측정하여 검증할 수 있다. 이러한 검증을 위해서 개발자는 학습 데이터셋, 딥러닝의 계층구조 및 개발 환경 등과 같은내용을 포함하는 딥러닝 모델을 문서화하여 적용하기 위한 구조적인 정보가 필요하다. 본 논문에서는 신뢰성있는 딥러닝 기반 데이터 분석 모델을 참조하기위한 딥러닝 기술 언어를 제안한다. 제안하는 기술 언어는 신뢰성 있는 딥러닝 모델을 개발하는데 필요한 학습데이터셋, 개발 환경 및 설정 등의 정보와더불어 딥러닝 모델의 계층구조를 표현할 수 있다. 제안하는 딥러닝 기술 언어를 이용하여 개발자는 지능형 교통 시스템에서 참조하는 분석 모델의정확도를 검증할 수 있다. 실험에서는 제안하는 언어의 유효성을 검증하기 위해, 번호판 인식 모델을 중심으로 딥러닝 기술 문서의 적용과정을 보인다With the recent advancements of deep learning, companies such as smart home, healthcare, and intelligent transportation systems areutilizing its functionality to provide high-quality services for vehicle detection, emergency situation detection, and controlling energyconsumption. To provide reliable services in such sensitive systems, deep learning models are required to have high accuracy. In orderto develop a deep learning model for analyzing previously mentioned services, developers should utilize the state of the art deep learningmodels that have already been verified for higher accuracy. The developers can verify the accuracy of the referenced model by validatingthe model on the dataset. For this validation, the developer needs structural information to document and apply deep learning models,including metadata such as learning dataset, network architecture, and development environments. In this paper, we propose a descriptionlanguage that represents the network architecture of the deep learning model along with its metadata that are necessary to develop adeep learning model. Through the proposed description language, developers can easily verify the accuracy of the referenced deep learningmodel. Our experiments demonstrate the application scenario of a deep learning description document that focuses on the license platerecognition for the detection of illegally parked vehicles.Trusted Deep Learning, Model Reference, Deep Learning Description Language, Traffic Situation Analysis Model"
452,딥러닝 분석을 이용한 중국 역내·외 위안화 변동성 예측,이우식 /Lee Woosik,2016,한국데이터정보과학회,"2008년 글로벌 금융위기 이후 중국은 위안화 국제화의 점진적 추진을 진행하면서 중국상하이 외환시장과 중국홍콩 외환시장에서 거래되는 통화인 역내위안화와 역외위안화를 형성시켰다. 본 연구는 위안화 국제화와 점진적인 중국 자본계정 개방에 따라 급변하는 외환시장상황의 변동성을 정확하게 파악하기 위해서 GARCH모형 (일반화된 자기회귀 조건부이분산성모형)에 다단계인공신경망을 결합한 MLP-GARCH 모형과 GARCH모형과 기계학습의 일종인 딥러닝 (deep learning)을 통합한 DL-GARCH을 가지고 위안화 변동성예측을 비교 실험과 분석을 하였다. 비교분석 결과 DLGARCH 모형은 MLP-GARCH보다 모형 위안화 역내·외 환율변동성 예측 면에서 더욱 더 개선된예측값을 제공하였다. 그래서 이분산시계열모형을 딥러닝과 결합한 DL-GARCH 모형은 시계열의환율 변동성 예측 문제에 딥러닝을 응용할 수 있음을 확인하였다. 향후 이분산시계열과 결합된 딥러닝 모형은 다른 금융시계열 데이터에 응용하여 그 일반화 가능성을 높일 수 있을 것이다.The People’s Republic of China has vigorously been pursuing the internationalization of the Chinese Yuan or Renminbi after the financial crisis of 2008. In this view, an abrupt increase of use of the Chinese Yuan in the onshore and offshore markets are important milestones to be one of important currencies. One of the most frequently used methods to forecast volatility is GARCH model. Since a prediction error of the GARCH model has been reported quite high, a lot of efforts have been made to improve forecasting capability of the GARCH model. In this paper, we have proposed MLPGARCH and a DL-GARCH by employing Artificial Neural Network to the GARCH. In an application to forecasting Chinese Yuan volatility, we have successfully shown their overall outperformance in forecasting over the GARCH.Deep learning, DL-GARCH, GARCH, The Chinese Yuan, volatility."
453,딥러닝 기법을 이용한 내일강수 예측,하지훈,2016,한국지능시스템학회,"정확한 강수예측을 위해서는 예측인자 선정과 예측방법에 대한 선택이 매우 중요하다. 최근에는 강수예측 방법으로기계학습 기법이 많이 사용되고 있으며, 그 중에서도 특히 인공신경망을 사용한 강수예측 방법은 좋은 성능을 보였다. 본논문에서는 딥러닝 기법 중 하나인 DBN(deep belief network)를 이용한 새로운 강수예측 방법을 제안한다. DBN는 비지도사전 학습을 통해 초기 가중치를 설정하여 기존 인공신경망의 문제점을 보완한다. 예측인자로는 기온, 전일-전주 강수일, 태양과 달 궤도 관련 자료를 선정하였다. 기온과 전일-전주 강수일은 서울에서의 1974년부터 2013년까지 총 40년간의AWS(automatic weather system) 관측 자료를 사용하였고, 태양과 달의 궤도 관련 자료는 서울을 중심으로 계산한 결과를사용하였다. 전체 기간에서 일부는 학습 자료로 사용하여 예측모델을 생성하였고, 나머지를 생성한 모델의 검증 자료로사용하였다. 모델 검증 결과로 나온 예측값들은 확률값을 가지며 임계치를 이용하여 강수유무를 판별하였다. 강수 정확도의척도로 양분예보기법 중 CSI(critical successive index)와 Bias(frequency bias)를 계산하였다. 이를 통해 DBN와 MLP(multilayer perceptron)의 성능을 비교한 결과 DBN의 강수 예측 정확도가 높았고, 수행속도 또한 2배 이상 빨랐다.For accurate precipitation forecasts the choice of weather faoctrs and prediction method is very important. Recently, machinele arning has been widely used for forecasting precipitation, and artificial neural network, one of machine learning techniques, showed good performance. In this paper, we suggest a new method for forecasting precipitation using DBN, one of deep learning techniques.
DBN has an advantage that initial weights are set by unsupervised learning, so this compensates for the defects of artificial neural networks. We used past precipitation, temperature, and the parameters of the sun and moon's motion as features for forecasting precipitation. The dataset consists of observation data which had been measured for 40 years from AWS in Seoul. Experiments were based on 8-fold cross validation. As a result of estimation, we got probabilities of test dataset, so threshold was used for the decision of precipitation. CSI and Bias were used for indicating the precision of precipitation. Our experimental results showed that DBN performed better than MLP.Deep learning, Deep belief network, Precipitation, Forecast"
454,딥러닝 신경망모형을 이용한 실시간 교통정보수집 연구,김대현 /KIM DAE HYON,2016,사단법인 인문사회과학기술융합학회,"비디오 기반의 실시간 교통정보수집 시스템은 자율주행을 포함한 지능형교통체계의 주요 서브시스템중 하나이며, 우수한 예측(추정)력 때문에 핵심 알고리즘으로 인공신경망 모형이 주로 사용되고 있다. 특히, 최근 과학 학술지 네이처에 구글 딥마인드의 알파고가 공개되면서 딥러닝과 인공지능에 대한 전 세계적인 관심이 집중되고 있다. 최근까지 가장 널리 사용된 인공신경망모형은 역전파 모형이었으나 최근 컴퓨터 하드웨어 및 알고리즘의 새로운 개발과 더불어 딥러닝 알고리즘을 이용한 딥인공신경망 모형에 대한 관심이 높아지고 있다. 그러나 아직 딥러닝 인공신경망에 대한 연구는 초기단계에 있는 실정이며 적용분야도 극히 한정되어 있다. 본 연구에서는 실시간 도로교통정보 수집을 위한 영상처리분야에서 딥러닝 알고리즘을 활용하고 실험결과를 통하여 향후 교통정보수집‧분석 분야에서의 딥러닝 인공신경망 모형의 적용가능성을 살펴보고자 한다. 특히, 본 연구에서 도출한 딥러닝 인공신경망모형의 실험결과를 기존 역전파 모형에 의한 실험결과와 비교하여 모형의 우수성을 비교‧평가하고자 한다.Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Normally, artificial neural networks have been used in real-time applications for collecting traffic information because of their remarkable performance in terms of predictive accuracy and real-time computing. Especially, the recent publication of a paper in the journal Nature and the success of Google DeepMind’s world-beating Go program AlphaGo caused significant increases of the public interest in artificial intelligence. Even though various types of learning models have been proposed, the Backpropagation has been the most popular neural network model. However, the Deep learning neural network model has attracted much interest recently. In this study, the Deep leaning neural network will be applied in image processing techniques for the real-time traffic information collection to investigate the performance of Deep learning algorithm in traffic engineering fields. In addition, the experimental results from Deep learing algorithm will be compared with the results from Backpropagation model to evaluate and propose better performing model.Intelligent Transportation Systems (ITS), artificial neural networks, Deep learning, traffic information, image processing"
455,딥러닝의 모형과 응용사례,안성만 /SungMahn Ahn,2016,한국지능정보시스템학회,"딥러닝은 인공신경망(neural network)이라는 인공지능분야의 모형이 발전된 형태로서, 계층구조로 이루어진 인공신경망의 내부계층(hidden layer)이 여러 단계로 이루어진 구조이다. 딥러닝에서의 주요 모형은 합성곱신경망(convolutional neural network), 순환신경망(recurrent neural network), 그리고 심층신뢰신경망(deep belief network)의 세가지라고 할 수있다. 그 중에서 현재 흥미로운 연구가 많이 발표되어서 관심이 집중되고 있는 모형은 지도학습(supervised learning)모형인 처음 두 개의 모형이다. 따라서 본 논문에서는 지도학습모형의 가중치를 최적화하는 기본적인 방법인 오류역전파 알고리즘을 살펴본 뒤에 합성곱신경망과 순환신경망의 구조와 응용사례 등을 살펴보고자 한다. 본문에서 다루지 않은 모형인 심층신뢰신경망은 아직까지는 합성곱신경망 이나 순환신경망보다는 상대적으로 주목을 덜 받고 있다. 그러나 심층신뢰신경망은 CNN이나 RNN과는 달리 비지도학습(unsupervised learning)모형이며, 사람이나 동물은 관찰을 통해서 스스로 학습한다는 점에서 궁극적으로는 비지도학습모형이 더 많이 연구되어야 할 주제가 될 것이다.Deep learning model is a kind of neural networks that allows multiple hidden layers. There are various deep learning architectures such as convolutional neural networks, deep belief networks and recurrent neural networks. Those have been applied to fields like computer vision, automatic speech recognition, natural language processing, audio recognition and bioinformatics where they have been shown to produce state-of-the-art results on various tasks. Among those architectures, convolutional neural networks and recurrent neural networks are classified as the supervised learning model. And in recent years, those supervised learning models have gained more popularity than unsupervised learning models such as deep belief networks, because supervised learning models have shown fashionable applications in such fields mentioned above.
Deep learning models can be trained with backpropagation algorithm. Backpropagation is an abbreviation for “backward propagation of errors” and a common method of training artificial neural networks used in conjunction with an optimization method such as gradient descent. The method calculates the gradient of an error function with respect to all the weights in the network. The gradient is fed to the optimization method which in turn uses it to update the weights, in an attempt to minimize the error function.
Convolutional neural networks use a special architecture which is particularly well-adapted to classify images. Using this architecture makes convolutional networks fast to train. This, in turn, helps us train deep, muti-layer networks, which are very good at classifying images. These days, deep convolutional networks are used in most neural networks for image recognition. Convolutional neural networks use three basic ideas: local receptive fields, shared weights, and pooling. By local receptive fields, we mean that each neuron in the first(or any) hidden layer will be connected to a small region of the input(or previous layer’s) neurons. Shared weights mean that we’re going to use the same weights and bias for each of the local receptive field. This means that all the neurons in the hidden layer detect exactly the same feature, just at different locations in the input image. In addition to the convolutional layers just described, convolutional neural networks also contain pooling layers. Pooling layers are usually used immediately after convolutional layers. What the pooling layers do is to simplify the information in the output from the convolutional layer.
Recent convolutional network architectures have 10 to 20 hidden layers and billions of connections between units. Training deep learning networks has taken weeks several years ago, but thanks to progress in GPU and algorithm enhancement, training time has reduced to several hours.
Neural networks with time-varying behavior are known as recurrent neural networks or RNNs. A recurrent neural network is a class of artificial neural network where connections between units form a directed cycle. This creates an internal state of the network which allows it to exhibit dynamic temporal behavior. Unlike feedforward neural networks, RNNs can use their internal memory to process arbitrary sequences of inputs. Early RNN models turned out to be very difficult to train, harder even than deep feedforward networks. The reason is the unstable gradient problem such as vanishing gradient and exploding gradient. The gradient can get smaller and smaller as it is propagated back through layers. This makes learning in early layers extremely slow. The problem actually gets worse in RNNs, since gradients aren’t just propagated backward through layers, they’re propagated backward through time. If the network runs for a long time, that can make the gradient extremely unstable and hard to learn from. It has been possible to incorporate an idea known as long short-term memory units (LSTMs) into RNNs. LSTMs make it much easier to get good results when training RNNs, and many recent papers make use of LSTMs or related ideas.Deep learning, Convolutional neural networks, Recurrent neural networks, Error backpropagation algorithm"
456,딥러닝분석과 기술적 분석 지표를 이용한 한국 코스피주가지수 방향성 예측,이우식 /Lee Woosik,2017,한국데이터정보과학회,"2016년 3월 구글 (Google)의 바둑인공지능 알파고 (AlphaGo)가 이세돌 9단과의 바둑대결에서승리한 이후 다양한 분야에서 인공지능 사용에 대한 관심이 높아지고 있는 가운데 금융투자 분야에서도 인공지능과 투자자문 전문가의 합성어인 로보어드바이저 (Robo-Advisor)에 대한 관심이 높아지고 있다. 인공지능 (artificial intelligence)기반의 의사결정은 비용 절감은 물론 효과적인 의사결정을가능하게 한다는 점에서 큰 장점이 있다. 본 연구에서는 기술적 분석 (technical analysis) 지표와 딥러닝 (deep learning) 모형을 결합하여 한국 코스피 지수를 예측하는 모형을 개발하고 제시한 모형들의 예측력을 비교, 분석한다. 분석 결과 기술적 분석 지표에 딥러닝 알고리즘을 결합한 모형이 주가지수 방향성 예측 문제에 응용될 수 있음을 확인하였다. 향후 본 연구에서 제안된 기술적 분석 지표와딥러닝모형을 결합한 기법은 로보어드바이저서비스에 응용할 수 있는 일반화 가능성을 보여준다Since Google’s AlphaGo defeated a world champion of Go players in 2016, there have been many interests in the deep learning. In the financial sector, a Robo-Advisor using deep learning gains a significant attention, which builds and manages portfolios of financial instruments for investors.In this paper, we have proposed the a deep learning algorithm geared toward identification and forecast of the KOSPI index direction,and we also have compared the accuracy of the prediction.In an application of forecasting the financial market index direction, we have shown that the Robo-Advisor using deep learning has a significant effect on finance industry. The Robo-Advisor collects a massive data such as earnings statements, news reports and regulatory filings, analyzes those and recommends investors how to view market trends and identify the best time to purchase financial assets. On the other hand, the Robo-Advisor allows businesses to learn more about their customers, develop better marketing strategies, increase sales and decrease costs.Artificial intelligence, deep learning, FinTech, Robo-Advisor, technical analysis."
457,딥러닝의 효율적인 학습을 위한 학습데이터 선별 기법에 관한 연구,주경돈 /Gyoung-don Joo,2017,한국정보과학회,"빅데이터의 증가와 병렬 하드웨어 처리, 학습 알고리즘의 개선은 딥러닝의 발전을 가져왔으며 인공지능이 적용 가능한 영역을 획기적으로 확장시키고 있다. 하지만 이러한 대규모의 학습 데이터에 기반한 딥러닝은 학습시간이 증가되고, 더 나아가 학습 데이터의 구축을 위한 레이블링(labeling) 비용이 데이터 수에 비례하여 증가하게 된다. 따라서 본 논문에서는, 딥러닝의 정확도를 최대한 유지하면서 필요한 학습 데이터의 수를 줄일수 있도록, 전체 데이터에서 제한된 수의 학습 데이터의 후보를 선별하는 기법에 대하여 연구한다. 특히 기존의 능동 학습에 기반한 기법들이 학습 데이터의 수가 매우 적어질 경우 딥러닝의 정확도가 급격히 떨어지는 문제를 해결할 수 있는 새로운 기법을 제안하였으며, 제안된 기법으로 선별된 학습 데이터는 그 수의 변화에 대해 딥러닝이 일관되게 경쟁력있는 정확도를 가짐을 다양한 실험을 통하여 검증하였다.The increase of big data, parallel hardware processing and enhanced learning algorithms have led to the development of deep learning and are dramatically expanding the domain of applications based on artificial intelligence. However, such large-scale training data extends the learning time, and furthermore, the labeling cost for building the training set increases in proportion to the volume of data. Therefore, in this paper, we study how to select a limited number of candidates for training set from given data so as to reduce the number of training set while maintaining the accuracy of deep learning. Especially, we found that, with the training set selected by the previous work, the accuracy of deep learning drastically decreases when the number of training set is very small and so proposed new methods to resolve this fall-off in accuracy. Through experiments, we showed that deep learning consistently has the competitive accuracy with the training set selected by the proposed method regardless of the size of training set.Deep Learning, Train Set Selection, Active Learning, Gamma distribution"
458,딥러닝을 이용한 범용적 스테그아날리시스,김현재 /Hyunjae Kim,2017,한국정보과학회,"스테그아날리시스(Steganalysis)란 이미지 등 일반적인 자료에 암호화된 정보를 은닉하는 스테가노그래피(Steganography)에 대한 검출 및 분석 방법으로, 기계학습 기반 방법론을 포함한다. 기존 기계학습 기반 스테그아날리시스는 영상(Image)의 특징(Feature) 추출 및 모델링에 기반하며, 최근 딥러닝(Deep Learning)의 적용으로 검출 정확도가 큰 폭으로 향상되었다. 하지만 현존하는 스테그아날리시스 모델은 단일 스테가노그래피 기법에 대해 국한되어 있어 학습에 사용되지 않은 스테고(Stego) 이미지의 경우 검출이 불가능한 결정적 한계를 가진다. 본 연구에서는 다양한 스테가노그래피 기법으로 생성된 스테고이미지에 딥러닝을 적용하여 스테그아날리시스를 학습하는 범용적 모델을 제안한다. 다양한 실험을 통해 제안 기법의 효용성 및 가능성을 확인하고, 범용적 스테그아날리시스 모델이 각각에 특화된 검출 기법과 유사한 정확도로 스테고 이미지를 검출할 수 있음을 보인다.Steganalysis is to detect information hidden by steganography inside general data such as images. There are stegoanalysis techniques that use machine learning (ML). Existing ML approaches to steganalysis are based on extracting features from stego images and modeling them.
Recently deep learning-based methodologies have shown significant improvements in detection accuracy. However, all the existing methods, including deep learning-based ones, have a critical limitation in that they can only detect stego images that are created by a specific steganography method. In this paper, we propose a generalized steganalysis method that can model multiple types of stego images using deep learning. Through various experiments, we confirm the effectiveness of our approach and envision directions for future research. In particular, we show that our method can detect each type of steganography with the same level of accuracy as that of a steganalysis method dedicated to that type of steganography, thereby demonstrating the general applicability of our approach to multiple types of stego images.steganalysis, steganography, deep learning, generalized steganalysis model"
459,"딥러닝 프레임워크의 비교: 티아노, 텐서플로, CNTK를 중심으로",정여진 /Yeojin Chung,2017,한국지능정보시스템학회,"딥러닝 프레임워크의 대표적인 기능으로는 ‘자동미분’과 ‘GPU의 활용’ 등을 들 수 있다. 본 논문은 파이썬의 라이브러리 형태로 사용 가능한 프레임워크 중에서 구글의 텐서플로와 마이크로소프트의 CNTK, 그리고 텐서 플로의 원조라고 할 수 있는 티아노를 비교하였다. 본문에서는 자동미분의 개념과 GPU의 활용형태를 간단히 설명하고, 그 다음에 logistic regression을 실행하는 예를 통하여 각 프레임워크의 문법을 알아본 뒤에, 마지막으로 대표적인 딥러닝 응용인 CNN의 예제를 실행시켜보고 코딩의 편의성과 실행속도 등을 확인해 보았다. 그 결과, 편의성의 관점에서 보면 티아노가 가장 코딩 하기가 어렵고, CNTK와 텐서플로는 많은 부분이 비슷하게 추상화 되어 있어서 코딩이 비슷하지만 가중치와 편향을 직접 정의하느냐의 여부에서 차이를 보였다. 그리고 각프레임워크의 실행속도에 대한 평가는 ‘큰 차이는 없다’는 것이다. 텐서플로는 티아노에 비하여 속도가 느리다는 평가가 있어왔는데, 본 연구의 실험에 의하면, 비록 CNN 모형에 국한되었지만, 텐서플로가 아주 조금이지만빠른 것으로 나타났다. CNTK의 경우에도, 비록 실험환경이 달랐지만, 실험환경의 차이에 의한 속도의 차이의 편차범위 이내에 있는 것으로 판단이 되었다. 본 연구에서는 세 종류의 딥러닝 프레임워크만을 살펴보았는데,위키피디아에 따르면 딥러닝 프레임워크의 종류는 12가지가 있으며, 각 프레임워크의 특징을 15가지 속성으로구분하여 차이를 특정하고 있다. 그 많은 속성 중에서 사용자의 입장에서 볼 때 중요한 속성은 어떤 언어(파이썬, C++, Java, 등)로 사용가능한지, 어떤 딥러닝 모형에 대한 라이브러리가 잘 구현되어 있는지 등일 것이다. 그리고 사용자가 대규모의 딥러닝 모형을 구축한다면, 다중 GPU 혹은 다중 서버를 지원하는지의 여부도 중요할 것이다. 또한 딥러닝 모형을 처음 학습하는 경우에는 사용설명서가 많은지 예제 프로그램이 많은지 여부도 중요한 기준이 될 것이다.The deep learning framework is software designed to help develop deep learning models. Some of its important functions include “automatic differentiation” and “utilization of GPU”. The list of popular deep learning framework includes Caffe (BVLC) and Theano (University of Montreal). And recently, Microsoft's deep learning framework, Microsoft Cognitive Toolkit, was released as open-source license, following Google’s Tensorflow a year earlier. The early deep learning frameworks have been developed mainly for research at universities. Beginning with the inception of Tensorflow, however, it seems that companies such as Microsoft and Facebook have started to join the competition of framework development. Given the trend, Google and other companies are expected to continue investing in the deep learning framework to bring forward the initiative in the artificial intelligence business. From this point of view, we think it is a good time to compare some of deep learning frameworks. So we compare three deep learning frameworks which can be used as a Python library. Those are Google's Tensorflow, Microsoft’s CNTK, and Theano which is sort of a predecessor of the preceding two.
The most common and important function of deep learning frameworks is the ability to perform automatic differentiation. Basically all the mathematical expressions of deep learning models can be represented as computational graphs, which consist of nodes and edges. Partial derivatives on each edge of a computational graph can then be obtained. With the partial derivatives, we can let software compute differentiation of any node with respect to any variable by utilizing chain rule of Calculus.
First of all, the convenience of coding is in the order of CNTK, Tensorflow, and Theano. The criterion is simply based on the lengths of the codes and the learning curve and the ease of coding are not the main concern. According to the criteria, Theano was the most difficult to implement with, and CNTK and Tensorflow were somewhat easier. With Tensorflow, we need to define weight variables and biases explicitly. The reason that CNTK and Tensorflow are easier to implement with is that those frameworks provide us with more abstraction than Theano. We, however, need to mention that low-level coding is not always bad. It gives us flexibility of coding. With the low-level coding such as in Theano, we can implement and test any new deep learning models or any new search methods that we can think of.
The assessment of the execution speed of each framework is that there is not meaningful difference. According to the experiment, execution speeds of Theano and Tensorflow are very similar, although the experiment was limited to a CNN model. In the case of CNTK, the experimental environment was not maintained as the same. The code written in CNTK has to be run in PC environment without GPU where codes execute as much as 50 times slower than with GPU. But we concluded that the difference of execution speed was within the range of variation caused by the different hardware setup.
In this study, we compared three types of deep learning framework: Theano, Tensorflow, and CNTK. According to Wikipedia, there are 12 available deep learning frameworks. And 15 different attributes differentiate each framework. Some of the important attributes would include interface language (Python, C ++, Java, etc.) and the availability of libraries on various deep learning models such as CNN, RNN, DBN, and etc. And if a user implements a large scale deep learning model, it will also be important to support multiple GPU or multiple servers. Also, if you are learning the deep learning model, it would also be important if there are enough examples and references.deep learning framework, Theano, TensorFlow, CNTK, computational graph, CIFAR-10"
460,딥러닝을 이용한 주변 무선단말 파악방안,이웅섭 /Lee， Woong-sup,2018,한국정보통신학회,"최근 단말-대-단말(Device-to-device, D2D) 통신기술이 차세대 무선통신시스템의 핵심기술로 큰 관심을 받고 있다. 이러한 단말간 통신에서는 자신의 주변에 어떠한 단말이 있는지 파악하는 주변단말 탐색(Neighbor discovery)이매우 중요하다. 본 논문에서는 최근 큰 관심을 받고 있는 딥러닝(Deep learning) 기술을 활용하여 단말간 통신에서 주변단말을 파악하는 방안에 대해서 제안한다. 제안 방안은 기존의 방안과 달리 무선채널의 공간적 연관성을 이용하여단말간의 신호 전송 없이 단말이 기지국으로 전송하는 상향링크 파일럿 신호를 기반으로 주변 단말을 찾고 따라서기존의 방식에 비해 신호전송 복잡도(signaling complexity)를 크게 줄일 수 있다. 또한 제안 방안에서는 떨어져 있는거리에 따라서 주변 단말을 분류 가능하여 기존 방안에 비해서 좀 더 세밀한 단말 탐색이 가능하다. 마지막으로 본 논문에서는 tensorflow를 이용한 컴퓨터 시뮬레이션을 통해 제안 방안의 성능을 검증하였다.Recently, the device-to-device (D2D) communication has been conceived as the key technology for the next-generationmobile communication systems. The neighbor discovery in which the nearby users are found, is essential for the properoperation of the D2D communication. In this paper, we propose new neighbor discovery scheme based on deep learningtechnology which has gained a lot of attention recently. In the proposed scheme, the neighboring users can be foundusing the uplink pilot transmission of users only, unlike conventional neighbor discovery schemes in which direct pilotcommunication among users is required, such that the signaling overhead can be greatly reduced in our proposed scheme.
Moreover, the neighbors with different proximity can also be classified accordingly which enables more accurate neighbordiscovery compared to the conventional schemes. The performance of our proposed scheme is verified through thetensorflow-based computer simulations.
키워드 : 딥러닝, 주변단말 탐색, 단말-대-단말 통신Deep learning, Neighbor discovery, Device-to-device communication, Wireless channel correlation"
461,신뢰성있는 딥러닝 기반 분석 모델을 참조하기 위한 딥러닝 기술 언어,문종혁 /Jong Hyeok Mun,2021,한국정보처리학회,"최근 딥러닝은 하드웨어 성능이 향상됨에 따라 자연어 처리, 영상 인식 등의 다양한 기술에 접목되어 활용되고 있다. 이러한 기술들을 활용해 지능형교통 시스템(ITS), 스마트홈, 헬스케어 등의 산업분야에서 데이터를 분석하여 고속도로 속도위반 차량 검출, 에너지 사용량 제어, 응급상황 등과 같은고품질의 서비스를 제공하며, 고품질의 서비스를 제공하기 위해서는 정확도가 향상된 딥러닝 모델이 적용되어야 한다. 이를 위해 서비스 환경의 데이터를분석하기 위한 딥러닝 모델을 개발할 때, 개발자는 신뢰성이 검증된 최신의 딥러닝 모델을 적용할 수 있어야 한다. 이는 개발자가 참조하는 딥러닝 모델에적용된 학습 데이터셋의 정확도를 측정하여 검증할 수 있다. 이러한 검증을 위해서 개발자는 학습 데이터셋, 딥러닝의 계층구조 및 개발 환경 등과 같은내용을 포함하는 딥러닝 모델을 문서화하여 적용하기 위한 구조적인 정보가 필요하다. 본 논문에서는 신뢰성있는 딥러닝 기반 데이터 분석 모델을 참조하기위한 딥러닝 기술 언어를 제안한다. 제안하는 기술 언어는 신뢰성 있는 딥러닝 모델을 개발하는데 필요한 학습데이터셋, 개발 환경 및 설정 등의 정보와더불어 딥러닝 모델의 계층구조를 표현할 수 있다. 제안하는 딥러닝 기술 언어를 이용하여 개발자는 지능형 교통 시스템에서 참조하는 분석 모델의정확도를 검증할 수 있다. 실험에서는 제안하는 언어의 유효성을 검증하기 위해, 번호판 인식 모델을 중심으로 딥러닝 기술 문서의 적용과정을 보인다With the recent advancements of deep learning, companies such as smart home, healthcare, and intelligent transportation systems areutilizing its functionality to provide high-quality services for vehicle detection, emergency situation detection, and controlling energyconsumption. To provide reliable services in such sensitive systems, deep learning models are required to have high accuracy. In orderto develop a deep learning model for analyzing previously mentioned services, developers should utilize the state of the art deep learningmodels that have already been verified for higher accuracy. The developers can verify the accuracy of the referenced model by validatingthe model on the dataset. For this validation, the developer needs structural information to document and apply deep learning models,including metadata such as learning dataset, network architecture, and development environments. In this paper, we propose a descriptionlanguage that represents the network architecture of the deep learning model along with its metadata that are necessary to develop adeep learning model. Through the proposed description language, developers can easily verify the accuracy of the referenced deep learningmodel. Our experiments demonstrate the application scenario of a deep learning description document that focuses on the license platerecognition for the detection of illegally parked vehicles.Trusted Deep Learning, Model Reference, Deep Learning Description Language, Traffic Situation Analysis Model"
462,딥러닝 분석을 이용한 중국 역내·외 위안화 변동성 예측,이우식 /Lee Woosik,2016,한국데이터정보과학회,"2008년 글로벌 금융위기 이후 중국은 위안화 국제화의 점진적 추진을 진행하면서 중국상하이 외환시장과 중국홍콩 외환시장에서 거래되는 통화인 역내위안화와 역외위안화를 형성시켰다. 본 연구는 위안화 국제화와 점진적인 중국 자본계정 개방에 따라 급변하는 외환시장상황의 변동성을 정확하게 파악하기 위해서 GARCH모형 (일반화된 자기회귀 조건부이분산성모형)에 다단계인공신경망을 결합한 MLP-GARCH 모형과 GARCH모형과 기계학습의 일종인 딥러닝 (deep learning)을 통합한 DL-GARCH을 가지고 위안화 변동성예측을 비교 실험과 분석을 하였다. 비교분석 결과 DLGARCH 모형은 MLP-GARCH보다 모형 위안화 역내·외 환율변동성 예측 면에서 더욱 더 개선된예측값을 제공하였다. 그래서 이분산시계열모형을 딥러닝과 결합한 DL-GARCH 모형은 시계열의환율 변동성 예측 문제에 딥러닝을 응용할 수 있음을 확인하였다. 향후 이분산시계열과 결합된 딥러닝 모형은 다른 금융시계열 데이터에 응용하여 그 일반화 가능성을 높일 수 있을 것이다.The People’s Republic of China has vigorously been pursuing the internationalization of the Chinese Yuan or Renminbi after the financial crisis of 2008. In this view, an abrupt increase of use of the Chinese Yuan in the onshore and offshore markets are important milestones to be one of important currencies. One of the most frequently used methods to forecast volatility is GARCH model. Since a prediction error of the GARCH model has been reported quite high, a lot of efforts have been made to improve forecasting capability of the GARCH model. In this paper, we have proposed MLPGARCH and a DL-GARCH by employing Artificial Neural Network to the GARCH. In an application to forecasting Chinese Yuan volatility, we have successfully shown their overall outperformance in forecasting over the GARCH.Deep learning, DL-GARCH, GARCH, The Chinese Yuan, volatility."
463,딥러닝 기법을 이용한 내일강수 예측,하지훈,2016,한국지능시스템학회,"정확한 강수예측을 위해서는 예측인자 선정과 예측방법에 대한 선택이 매우 중요하다. 최근에는 강수예측 방법으로기계학습 기법이 많이 사용되고 있으며, 그 중에서도 특히 인공신경망을 사용한 강수예측 방법은 좋은 성능을 보였다. 본논문에서는 딥러닝 기법 중 하나인 DBN(deep belief network)를 이용한 새로운 강수예측 방법을 제안한다. DBN는 비지도사전 학습을 통해 초기 가중치를 설정하여 기존 인공신경망의 문제점을 보완한다. 예측인자로는 기온, 전일-전주 강수일, 태양과 달 궤도 관련 자료를 선정하였다. 기온과 전일-전주 강수일은 서울에서의 1974년부터 2013년까지 총 40년간의AWS(automatic weather system) 관측 자료를 사용하였고, 태양과 달의 궤도 관련 자료는 서울을 중심으로 계산한 결과를사용하였다. 전체 기간에서 일부는 학습 자료로 사용하여 예측모델을 생성하였고, 나머지를 생성한 모델의 검증 자료로사용하였다. 모델 검증 결과로 나온 예측값들은 확률값을 가지며 임계치를 이용하여 강수유무를 판별하였다. 강수 정확도의척도로 양분예보기법 중 CSI(critical successive index)와 Bias(frequency bias)를 계산하였다. 이를 통해 DBN와 MLP(multilayer perceptron)의 성능을 비교한 결과 DBN의 강수 예측 정확도가 높았고, 수행속도 또한 2배 이상 빨랐다.For accurate precipitation forecasts the choice of weather faoctrs and prediction method is very important. Recently, machinele arning has been widely used for forecasting precipitation, and artificial neural network, one of machine learning techniques, showed good performance. In this paper, we suggest a new method for forecasting precipitation using DBN, one of deep learning techniques.
DBN has an advantage that initial weights are set by unsupervised learning, so this compensates for the defects of artificial neural networks. We used past precipitation, temperature, and the parameters of the sun and moon's motion as features for forecasting precipitation. The dataset consists of observation data which had been measured for 40 years from AWS in Seoul. Experiments were based on 8-fold cross validation. As a result of estimation, we got probabilities of test dataset, so threshold was used for the decision of precipitation. CSI and Bias were used for indicating the precision of precipitation. Our experimental results showed that DBN performed better than MLP.Deep learning, Deep belief network, Precipitation, Forecast"
464,딥러닝 신경망모형을 이용한 실시간 교통정보수집 연구,김대현 /KIM DAE HYON,2016,사단법인 인문사회과학기술융합학회,"비디오 기반의 실시간 교통정보수집 시스템은 자율주행을 포함한 지능형교통체계의 주요 서브시스템중 하나이며, 우수한 예측(추정)력 때문에 핵심 알고리즘으로 인공신경망 모형이 주로 사용되고 있다. 특히, 최근 과학 학술지 네이처에 구글 딥마인드의 알파고가 공개되면서 딥러닝과 인공지능에 대한 전 세계적인 관심이 집중되고 있다. 최근까지 가장 널리 사용된 인공신경망모형은 역전파 모형이었으나 최근 컴퓨터 하드웨어 및 알고리즘의 새로운 개발과 더불어 딥러닝 알고리즘을 이용한 딥인공신경망 모형에 대한 관심이 높아지고 있다. 그러나 아직 딥러닝 인공신경망에 대한 연구는 초기단계에 있는 실정이며 적용분야도 극히 한정되어 있다. 본 연구에서는 실시간 도로교통정보 수집을 위한 영상처리분야에서 딥러닝 알고리즘을 활용하고 실험결과를 통하여 향후 교통정보수집‧분석 분야에서의 딥러닝 인공신경망 모형의 적용가능성을 살펴보고자 한다. 특히, 본 연구에서 도출한 딥러닝 인공신경망모형의 실험결과를 기존 역전파 모형에 의한 실험결과와 비교하여 모형의 우수성을 비교‧평가하고자 한다.Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Normally, artificial neural networks have been used in real-time applications for collecting traffic information because of their remarkable performance in terms of predictive accuracy and real-time computing. Especially, the recent publication of a paper in the journal Nature and the success of Google DeepMind’s world-beating Go program AlphaGo caused significant increases of the public interest in artificial intelligence. Even though various types of learning models have been proposed, the Backpropagation has been the most popular neural network model. However, the Deep learning neural network model has attracted much interest recently. In this study, the Deep leaning neural network will be applied in image processing techniques for the real-time traffic information collection to investigate the performance of Deep learning algorithm in traffic engineering fields. In addition, the experimental results from Deep learing algorithm will be compared with the results from Backpropagation model to evaluate and propose better performing model.Intelligent Transportation Systems (ITS), artificial neural networks, Deep learning, traffic information, image processing"
465,딥러닝의 모형과 응용사례,안성만 /SungMahn Ahn,2016,한국지능정보시스템학회,"딥러닝은 인공신경망(neural network)이라는 인공지능분야의 모형이 발전된 형태로서, 계층구조로 이루어진 인공신경망의 내부계층(hidden layer)이 여러 단계로 이루어진 구조이다. 딥러닝에서의 주요 모형은 합성곱신경망(convolutional neural network), 순환신경망(recurrent neural network), 그리고 심층신뢰신경망(deep belief network)의 세가지라고 할 수있다. 그 중에서 현재 흥미로운 연구가 많이 발표되어서 관심이 집중되고 있는 모형은 지도학습(supervised learning)모형인 처음 두 개의 모형이다. 따라서 본 논문에서는 지도학습모형의 가중치를 최적화하는 기본적인 방법인 오류역전파 알고리즘을 살펴본 뒤에 합성곱신경망과 순환신경망의 구조와 응용사례 등을 살펴보고자 한다. 본문에서 다루지 않은 모형인 심층신뢰신경망은 아직까지는 합성곱신경망 이나 순환신경망보다는 상대적으로 주목을 덜 받고 있다. 그러나 심층신뢰신경망은 CNN이나 RNN과는 달리 비지도학습(unsupervised learning)모형이며, 사람이나 동물은 관찰을 통해서 스스로 학습한다는 점에서 궁극적으로는 비지도학습모형이 더 많이 연구되어야 할 주제가 될 것이다.Deep learning model is a kind of neural networks that allows multiple hidden layers. There are various deep learning architectures such as convolutional neural networks, deep belief networks and recurrent neural networks. Those have been applied to fields like computer vision, automatic speech recognition, natural language processing, audio recognition and bioinformatics where they have been shown to produce state-of-the-art results on various tasks. Among those architectures, convolutional neural networks and recurrent neural networks are classified as the supervised learning model. And in recent years, those supervised learning models have gained more popularity than unsupervised learning models such as deep belief networks, because supervised learning models have shown fashionable applications in such fields mentioned above.
Deep learning models can be trained with backpropagation algorithm. Backpropagation is an abbreviation for “backward propagation of errors” and a common method of training artificial neural networks used in conjunction with an optimization method such as gradient descent. The method calculates the gradient of an error function with respect to all the weights in the network. The gradient is fed to the optimization method which in turn uses it to update the weights, in an attempt to minimize the error function.
Convolutional neural networks use a special architecture which is particularly well-adapted to classify images. Using this architecture makes convolutional networks fast to train. This, in turn, helps us train deep, muti-layer networks, which are very good at classifying images. These days, deep convolutional networks are used in most neural networks for image recognition. Convolutional neural networks use three basic ideas: local receptive fields, shared weights, and pooling. By local receptive fields, we mean that each neuron in the first(or any) hidden layer will be connected to a small region of the input(or previous layer’s) neurons. Shared weights mean that we’re going to use the same weights and bias for each of the local receptive field. This means that all the neurons in the hidden layer detect exactly the same feature, just at different locations in the input image. In addition to the convolutional layers just described, convolutional neural networks also contain pooling layers. Pooling layers are usually used immediately after convolutional layers. What the pooling layers do is to simplify the information in the output from the convolutional layer.
Recent convolutional network architectures have 10 to 20 hidden layers and billions of connections between units. Training deep learning networks has taken weeks several years ago, but thanks to progress in GPU and algorithm enhancement, training time has reduced to several hours.
Neural networks with time-varying behavior are known as recurrent neural networks or RNNs. A recurrent neural network is a class of artificial neural network where connections between units form a directed cycle. This creates an internal state of the network which allows it to exhibit dynamic temporal behavior. Unlike feedforward neural networks, RNNs can use their internal memory to process arbitrary sequences of inputs. Early RNN models turned out to be very difficult to train, harder even than deep feedforward networks. The reason is the unstable gradient problem such as vanishing gradient and exploding gradient. The gradient can get smaller and smaller as it is propagated back through layers. This makes learning in early layers extremely slow. The problem actually gets worse in RNNs, since gradients aren’t just propagated backward through layers, they’re propagated backward through time. If the network runs for a long time, that can make the gradient extremely unstable and hard to learn from. It has been possible to incorporate an idea known as long short-term memory units (LSTMs) into RNNs. LSTMs make it much easier to get good results when training RNNs, and many recent papers make use of LSTMs or related ideas.Deep learning, Convolutional neural networks, Recurrent neural networks, Error backpropagation algorithm"
466,딥러닝분석과 기술적 분석 지표를 이용한 한국 코스피주가지수 방향성 예측,이우식 /Lee Woosik,2017,한국데이터정보과학회,"2016년 3월 구글 (Google)의 바둑인공지능 알파고 (AlphaGo)가 이세돌 9단과의 바둑대결에서승리한 이후 다양한 분야에서 인공지능 사용에 대한 관심이 높아지고 있는 가운데 금융투자 분야에서도 인공지능과 투자자문 전문가의 합성어인 로보어드바이저 (Robo-Advisor)에 대한 관심이 높아지고 있다. 인공지능 (artificial intelligence)기반의 의사결정은 비용 절감은 물론 효과적인 의사결정을가능하게 한다는 점에서 큰 장점이 있다. 본 연구에서는 기술적 분석 (technical analysis) 지표와 딥러닝 (deep learning) 모형을 결합하여 한국 코스피 지수를 예측하는 모형을 개발하고 제시한 모형들의 예측력을 비교, 분석한다. 분석 결과 기술적 분석 지표에 딥러닝 알고리즘을 결합한 모형이 주가지수 방향성 예측 문제에 응용될 수 있음을 확인하였다. 향후 본 연구에서 제안된 기술적 분석 지표와딥러닝모형을 결합한 기법은 로보어드바이저서비스에 응용할 수 있는 일반화 가능성을 보여준다Since Google’s AlphaGo defeated a world champion of Go players in 2016, there have been many interests in the deep learning. In the financial sector, a Robo-Advisor using deep learning gains a significant attention, which builds and manages portfolios of financial instruments for investors.In this paper, we have proposed the a deep learning algorithm geared toward identification and forecast of the KOSPI index direction,and we also have compared the accuracy of the prediction.In an application of forecasting the financial market index direction, we have shown that the Robo-Advisor using deep learning has a significant effect on finance industry. The Robo-Advisor collects a massive data such as earnings statements, news reports and regulatory filings, analyzes those and recommends investors how to view market trends and identify the best time to purchase financial assets. On the other hand, the Robo-Advisor allows businesses to learn more about their customers, develop better marketing strategies, increase sales and decrease costs.Artificial intelligence, deep learning, FinTech, Robo-Advisor, technical analysis."
467,딥러닝의 효율적인 학습을 위한 학습데이터 선별 기법에 관한 연구,주경돈 /Gyoung-don Joo,2017,한국정보과학회,"빅데이터의 증가와 병렬 하드웨어 처리, 학습 알고리즘의 개선은 딥러닝의 발전을 가져왔으며 인공지능이 적용 가능한 영역을 획기적으로 확장시키고 있다. 하지만 이러한 대규모의 학습 데이터에 기반한 딥러닝은 학습시간이 증가되고, 더 나아가 학습 데이터의 구축을 위한 레이블링(labeling) 비용이 데이터 수에 비례하여 증가하게 된다. 따라서 본 논문에서는, 딥러닝의 정확도를 최대한 유지하면서 필요한 학습 데이터의 수를 줄일수 있도록, 전체 데이터에서 제한된 수의 학습 데이터의 후보를 선별하는 기법에 대하여 연구한다. 특히 기존의 능동 학습에 기반한 기법들이 학습 데이터의 수가 매우 적어질 경우 딥러닝의 정확도가 급격히 떨어지는 문제를 해결할 수 있는 새로운 기법을 제안하였으며, 제안된 기법으로 선별된 학습 데이터는 그 수의 변화에 대해 딥러닝이 일관되게 경쟁력있는 정확도를 가짐을 다양한 실험을 통하여 검증하였다.The increase of big data, parallel hardware processing and enhanced learning algorithms have led to the development of deep learning and are dramatically expanding the domain of applications based on artificial intelligence. However, such large-scale training data extends the learning time, and furthermore, the labeling cost for building the training set increases in proportion to the volume of data. Therefore, in this paper, we study how to select a limited number of candidates for training set from given data so as to reduce the number of training set while maintaining the accuracy of deep learning. Especially, we found that, with the training set selected by the previous work, the accuracy of deep learning drastically decreases when the number of training set is very small and so proposed new methods to resolve this fall-off in accuracy. Through experiments, we showed that deep learning consistently has the competitive accuracy with the training set selected by the proposed method regardless of the size of training set.Deep Learning, Train Set Selection, Active Learning, Gamma distribution"
468,딥러닝을 이용한 범용적 스테그아날리시스,김현재 /Hyunjae Kim,2017,한국정보과학회,"스테그아날리시스(Steganalysis)란 이미지 등 일반적인 자료에 암호화된 정보를 은닉하는 스테가노그래피(Steganography)에 대한 검출 및 분석 방법으로, 기계학습 기반 방법론을 포함한다. 기존 기계학습 기반 스테그아날리시스는 영상(Image)의 특징(Feature) 추출 및 모델링에 기반하며, 최근 딥러닝(Deep Learning)의 적용으로 검출 정확도가 큰 폭으로 향상되었다. 하지만 현존하는 스테그아날리시스 모델은 단일 스테가노그래피 기법에 대해 국한되어 있어 학습에 사용되지 않은 스테고(Stego) 이미지의 경우 검출이 불가능한 결정적 한계를 가진다. 본 연구에서는 다양한 스테가노그래피 기법으로 생성된 스테고이미지에 딥러닝을 적용하여 스테그아날리시스를 학습하는 범용적 모델을 제안한다. 다양한 실험을 통해 제안 기법의 효용성 및 가능성을 확인하고, 범용적 스테그아날리시스 모델이 각각에 특화된 검출 기법과 유사한 정확도로 스테고 이미지를 검출할 수 있음을 보인다.Steganalysis is to detect information hidden by steganography inside general data such as images. There are stegoanalysis techniques that use machine learning (ML). Existing ML approaches to steganalysis are based on extracting features from stego images and modeling them.
Recently deep learning-based methodologies have shown significant improvements in detection accuracy. However, all the existing methods, including deep learning-based ones, have a critical limitation in that they can only detect stego images that are created by a specific steganography method. In this paper, we propose a generalized steganalysis method that can model multiple types of stego images using deep learning. Through various experiments, we confirm the effectiveness of our approach and envision directions for future research. In particular, we show that our method can detect each type of steganography with the same level of accuracy as that of a steganalysis method dedicated to that type of steganography, thereby demonstrating the general applicability of our approach to multiple types of stego images.steganalysis, steganography, deep learning, generalized steganalysis model"
469,"딥러닝 프레임워크의 비교: 티아노, 텐서플로, CNTK를 중심으로",정여진 /Yeojin Chung,2017,한국지능정보시스템학회,"딥러닝 프레임워크의 대표적인 기능으로는 ‘자동미분’과 ‘GPU의 활용’ 등을 들 수 있다. 본 논문은 파이썬의 라이브러리 형태로 사용 가능한 프레임워크 중에서 구글의 텐서플로와 마이크로소프트의 CNTK, 그리고 텐서 플로의 원조라고 할 수 있는 티아노를 비교하였다. 본문에서는 자동미분의 개념과 GPU의 활용형태를 간단히 설명하고, 그 다음에 logistic regression을 실행하는 예를 통하여 각 프레임워크의 문법을 알아본 뒤에, 마지막으로 대표적인 딥러닝 응용인 CNN의 예제를 실행시켜보고 코딩의 편의성과 실행속도 등을 확인해 보았다. 그 결과, 편의성의 관점에서 보면 티아노가 가장 코딩 하기가 어렵고, CNTK와 텐서플로는 많은 부분이 비슷하게 추상화 되어 있어서 코딩이 비슷하지만 가중치와 편향을 직접 정의하느냐의 여부에서 차이를 보였다. 그리고 각프레임워크의 실행속도에 대한 평가는 ‘큰 차이는 없다’는 것이다. 텐서플로는 티아노에 비하여 속도가 느리다는 평가가 있어왔는데, 본 연구의 실험에 의하면, 비록 CNN 모형에 국한되었지만, 텐서플로가 아주 조금이지만빠른 것으로 나타났다. CNTK의 경우에도, 비록 실험환경이 달랐지만, 실험환경의 차이에 의한 속도의 차이의 편차범위 이내에 있는 것으로 판단이 되었다. 본 연구에서는 세 종류의 딥러닝 프레임워크만을 살펴보았는데,위키피디아에 따르면 딥러닝 프레임워크의 종류는 12가지가 있으며, 각 프레임워크의 특징을 15가지 속성으로구분하여 차이를 특정하고 있다. 그 많은 속성 중에서 사용자의 입장에서 볼 때 중요한 속성은 어떤 언어(파이썬, C++, Java, 등)로 사용가능한지, 어떤 딥러닝 모형에 대한 라이브러리가 잘 구현되어 있는지 등일 것이다. 그리고 사용자가 대규모의 딥러닝 모형을 구축한다면, 다중 GPU 혹은 다중 서버를 지원하는지의 여부도 중요할 것이다. 또한 딥러닝 모형을 처음 학습하는 경우에는 사용설명서가 많은지 예제 프로그램이 많은지 여부도 중요한 기준이 될 것이다.The deep learning framework is software designed to help develop deep learning models. Some of its important functions include “automatic differentiation” and “utilization of GPU”. The list of popular deep learning framework includes Caffe (BVLC) and Theano (University of Montreal). And recently, Microsoft's deep learning framework, Microsoft Cognitive Toolkit, was released as open-source license, following Google’s Tensorflow a year earlier. The early deep learning frameworks have been developed mainly for research at universities. Beginning with the inception of Tensorflow, however, it seems that companies such as Microsoft and Facebook have started to join the competition of framework development. Given the trend, Google and other companies are expected to continue investing in the deep learning framework to bring forward the initiative in the artificial intelligence business. From this point of view, we think it is a good time to compare some of deep learning frameworks. So we compare three deep learning frameworks which can be used as a Python library. Those are Google's Tensorflow, Microsoft’s CNTK, and Theano which is sort of a predecessor of the preceding two.
The most common and important function of deep learning frameworks is the ability to perform automatic differentiation. Basically all the mathematical expressions of deep learning models can be represented as computational graphs, which consist of nodes and edges. Partial derivatives on each edge of a computational graph can then be obtained. With the partial derivatives, we can let software compute differentiation of any node with respect to any variable by utilizing chain rule of Calculus.
First of all, the convenience of coding is in the order of CNTK, Tensorflow, and Theano. The criterion is simply based on the lengths of the codes and the learning curve and the ease of coding are not the main concern. According to the criteria, Theano was the most difficult to implement with, and CNTK and Tensorflow were somewhat easier. With Tensorflow, we need to define weight variables and biases explicitly. The reason that CNTK and Tensorflow are easier to implement with is that those frameworks provide us with more abstraction than Theano. We, however, need to mention that low-level coding is not always bad. It gives us flexibility of coding. With the low-level coding such as in Theano, we can implement and test any new deep learning models or any new search methods that we can think of.
The assessment of the execution speed of each framework is that there is not meaningful difference. According to the experiment, execution speeds of Theano and Tensorflow are very similar, although the experiment was limited to a CNN model. In the case of CNTK, the experimental environment was not maintained as the same. The code written in CNTK has to be run in PC environment without GPU where codes execute as much as 50 times slower than with GPU. But we concluded that the difference of execution speed was within the range of variation caused by the different hardware setup.
In this study, we compared three types of deep learning framework: Theano, Tensorflow, and CNTK. According to Wikipedia, there are 12 available deep learning frameworks. And 15 different attributes differentiate each framework. Some of the important attributes would include interface language (Python, C ++, Java, etc.) and the availability of libraries on various deep learning models such as CNN, RNN, DBN, and etc. And if a user implements a large scale deep learning model, it will also be important to support multiple GPU or multiple servers. Also, if you are learning the deep learning model, it would also be important if there are enough examples and references.deep learning framework, Theano, TensorFlow, CNTK, computational graph, CIFAR-10"
470,딥러닝을 이용한 주변 무선단말 파악방안,이웅섭 /Lee， Woong-sup,2018,한국정보통신학회,"최근 단말-대-단말(Device-to-device, D2D) 통신기술이 차세대 무선통신시스템의 핵심기술로 큰 관심을 받고 있다. 이러한 단말간 통신에서는 자신의 주변에 어떠한 단말이 있는지 파악하는 주변단말 탐색(Neighbor discovery)이매우 중요하다. 본 논문에서는 최근 큰 관심을 받고 있는 딥러닝(Deep learning) 기술을 활용하여 단말간 통신에서 주변단말을 파악하는 방안에 대해서 제안한다. 제안 방안은 기존의 방안과 달리 무선채널의 공간적 연관성을 이용하여단말간의 신호 전송 없이 단말이 기지국으로 전송하는 상향링크 파일럿 신호를 기반으로 주변 단말을 찾고 따라서기존의 방식에 비해 신호전송 복잡도(signaling complexity)를 크게 줄일 수 있다. 또한 제안 방안에서는 떨어져 있는거리에 따라서 주변 단말을 분류 가능하여 기존 방안에 비해서 좀 더 세밀한 단말 탐색이 가능하다. 마지막으로 본 논문에서는 tensorflow를 이용한 컴퓨터 시뮬레이션을 통해 제안 방안의 성능을 검증하였다.Recently, the device-to-device (D2D) communication has been conceived as the key technology for the next-generationmobile communication systems. The neighbor discovery in which the nearby users are found, is essential for the properoperation of the D2D communication. In this paper, we propose new neighbor discovery scheme based on deep learningtechnology which has gained a lot of attention recently. In the proposed scheme, the neighboring users can be foundusing the uplink pilot transmission of users only, unlike conventional neighbor discovery schemes in which direct pilotcommunication among users is required, such that the signaling overhead can be greatly reduced in our proposed scheme.
Moreover, the neighbors with different proximity can also be classified accordingly which enables more accurate neighbordiscovery compared to the conventional schemes. The performance of our proposed scheme is verified through thetensorflow-based computer simulations.
키워드 : 딥러닝, 주변단말 탐색, 단말-대-단말 통신Deep learning, Neighbor discovery, Device-to-device communication, Wireless channel correlation"
471,신뢰성있는 딥러닝 기반 분석 모델을 참조하기 위한 딥러닝 기술 언어,문종혁 /Jong Hyeok Mun,2021,한국정보처리학회,"최근 딥러닝은 하드웨어 성능이 향상됨에 따라 자연어 처리, 영상 인식 등의 다양한 기술에 접목되어 활용되고 있다. 이러한 기술들을 활용해 지능형교통 시스템(ITS), 스마트홈, 헬스케어 등의 산업분야에서 데이터를 분석하여 고속도로 속도위반 차량 검출, 에너지 사용량 제어, 응급상황 등과 같은고품질의 서비스를 제공하며, 고품질의 서비스를 제공하기 위해서는 정확도가 향상된 딥러닝 모델이 적용되어야 한다. 이를 위해 서비스 환경의 데이터를분석하기 위한 딥러닝 모델을 개발할 때, 개발자는 신뢰성이 검증된 최신의 딥러닝 모델을 적용할 수 있어야 한다. 이는 개발자가 참조하는 딥러닝 모델에적용된 학습 데이터셋의 정확도를 측정하여 검증할 수 있다. 이러한 검증을 위해서 개발자는 학습 데이터셋, 딥러닝의 계층구조 및 개발 환경 등과 같은내용을 포함하는 딥러닝 모델을 문서화하여 적용하기 위한 구조적인 정보가 필요하다. 본 논문에서는 신뢰성있는 딥러닝 기반 데이터 분석 모델을 참조하기위한 딥러닝 기술 언어를 제안한다. 제안하는 기술 언어는 신뢰성 있는 딥러닝 모델을 개발하는데 필요한 학습데이터셋, 개발 환경 및 설정 등의 정보와더불어 딥러닝 모델의 계층구조를 표현할 수 있다. 제안하는 딥러닝 기술 언어를 이용하여 개발자는 지능형 교통 시스템에서 참조하는 분석 모델의정확도를 검증할 수 있다. 실험에서는 제안하는 언어의 유효성을 검증하기 위해, 번호판 인식 모델을 중심으로 딥러닝 기술 문서의 적용과정을 보인다With the recent advancements of deep learning, companies such as smart home, healthcare, and intelligent transportation systems areutilizing its functionality to provide high-quality services for vehicle detection, emergency situation detection, and controlling energyconsumption. To provide reliable services in such sensitive systems, deep learning models are required to have high accuracy. In orderto develop a deep learning model for analyzing previously mentioned services, developers should utilize the state of the art deep learningmodels that have already been verified for higher accuracy. The developers can verify the accuracy of the referenced model by validatingthe model on the dataset. For this validation, the developer needs structural information to document and apply deep learning models,including metadata such as learning dataset, network architecture, and development environments. In this paper, we propose a descriptionlanguage that represents the network architecture of the deep learning model along with its metadata that are necessary to develop adeep learning model. Through the proposed description language, developers can easily verify the accuracy of the referenced deep learningmodel. Our experiments demonstrate the application scenario of a deep learning description document that focuses on the license platerecognition for the detection of illegally parked vehicles.Trusted Deep Learning, Model Reference, Deep Learning Description Language, Traffic Situation Analysis Model"
472,딥러닝 분석을 이용한 중국 역내·외 위안화 변동성 예측,이우식 /Lee Woosik,2016,한국데이터정보과학회,"2008년 글로벌 금융위기 이후 중국은 위안화 국제화의 점진적 추진을 진행하면서 중국상하이 외환시장과 중국홍콩 외환시장에서 거래되는 통화인 역내위안화와 역외위안화를 형성시켰다. 본 연구는 위안화 국제화와 점진적인 중국 자본계정 개방에 따라 급변하는 외환시장상황의 변동성을 정확하게 파악하기 위해서 GARCH모형 (일반화된 자기회귀 조건부이분산성모형)에 다단계인공신경망을 결합한 MLP-GARCH 모형과 GARCH모형과 기계학습의 일종인 딥러닝 (deep learning)을 통합한 DL-GARCH을 가지고 위안화 변동성예측을 비교 실험과 분석을 하였다. 비교분석 결과 DLGARCH 모형은 MLP-GARCH보다 모형 위안화 역내·외 환율변동성 예측 면에서 더욱 더 개선된예측값을 제공하였다. 그래서 이분산시계열모형을 딥러닝과 결합한 DL-GARCH 모형은 시계열의환율 변동성 예측 문제에 딥러닝을 응용할 수 있음을 확인하였다. 향후 이분산시계열과 결합된 딥러닝 모형은 다른 금융시계열 데이터에 응용하여 그 일반화 가능성을 높일 수 있을 것이다.The People’s Republic of China has vigorously been pursuing the internationalization of the Chinese Yuan or Renminbi after the financial crisis of 2008. In this view, an abrupt increase of use of the Chinese Yuan in the onshore and offshore markets are important milestones to be one of important currencies. One of the most frequently used methods to forecast volatility is GARCH model. Since a prediction error of the GARCH model has been reported quite high, a lot of efforts have been made to improve forecasting capability of the GARCH model. In this paper, we have proposed MLPGARCH and a DL-GARCH by employing Artificial Neural Network to the GARCH. In an application to forecasting Chinese Yuan volatility, we have successfully shown their overall outperformance in forecasting over the GARCH.Deep learning, DL-GARCH, GARCH, The Chinese Yuan, volatility."
473,딥러닝 기법을 이용한 내일강수 예측,하지훈,2016,한국지능시스템학회,"정확한 강수예측을 위해서는 예측인자 선정과 예측방법에 대한 선택이 매우 중요하다. 최근에는 강수예측 방법으로기계학습 기법이 많이 사용되고 있으며, 그 중에서도 특히 인공신경망을 사용한 강수예측 방법은 좋은 성능을 보였다. 본논문에서는 딥러닝 기법 중 하나인 DBN(deep belief network)를 이용한 새로운 강수예측 방법을 제안한다. DBN는 비지도사전 학습을 통해 초기 가중치를 설정하여 기존 인공신경망의 문제점을 보완한다. 예측인자로는 기온, 전일-전주 강수일, 태양과 달 궤도 관련 자료를 선정하였다. 기온과 전일-전주 강수일은 서울에서의 1974년부터 2013년까지 총 40년간의AWS(automatic weather system) 관측 자료를 사용하였고, 태양과 달의 궤도 관련 자료는 서울을 중심으로 계산한 결과를사용하였다. 전체 기간에서 일부는 학습 자료로 사용하여 예측모델을 생성하였고, 나머지를 생성한 모델의 검증 자료로사용하였다. 모델 검증 결과로 나온 예측값들은 확률값을 가지며 임계치를 이용하여 강수유무를 판별하였다. 강수 정확도의척도로 양분예보기법 중 CSI(critical successive index)와 Bias(frequency bias)를 계산하였다. 이를 통해 DBN와 MLP(multilayer perceptron)의 성능을 비교한 결과 DBN의 강수 예측 정확도가 높았고, 수행속도 또한 2배 이상 빨랐다.For accurate precipitation forecasts the choice of weather faoctrs and prediction method is very important. Recently, machinele arning has been widely used for forecasting precipitation, and artificial neural network, one of machine learning techniques, showed good performance. In this paper, we suggest a new method for forecasting precipitation using DBN, one of deep learning techniques.
DBN has an advantage that initial weights are set by unsupervised learning, so this compensates for the defects of artificial neural networks. We used past precipitation, temperature, and the parameters of the sun and moon's motion as features for forecasting precipitation. The dataset consists of observation data which had been measured for 40 years from AWS in Seoul. Experiments were based on 8-fold cross validation. As a result of estimation, we got probabilities of test dataset, so threshold was used for the decision of precipitation. CSI and Bias were used for indicating the precision of precipitation. Our experimental results showed that DBN performed better than MLP.Deep learning, Deep belief network, Precipitation, Forecast"
474,딥러닝 신경망모형을 이용한 실시간 교통정보수집 연구,김대현 /KIM DAE HYON,2016,사단법인 인문사회과학기술융합학회,"비디오 기반의 실시간 교통정보수집 시스템은 자율주행을 포함한 지능형교통체계의 주요 서브시스템중 하나이며, 우수한 예측(추정)력 때문에 핵심 알고리즘으로 인공신경망 모형이 주로 사용되고 있다. 특히, 최근 과학 학술지 네이처에 구글 딥마인드의 알파고가 공개되면서 딥러닝과 인공지능에 대한 전 세계적인 관심이 집중되고 있다. 최근까지 가장 널리 사용된 인공신경망모형은 역전파 모형이었으나 최근 컴퓨터 하드웨어 및 알고리즘의 새로운 개발과 더불어 딥러닝 알고리즘을 이용한 딥인공신경망 모형에 대한 관심이 높아지고 있다. 그러나 아직 딥러닝 인공신경망에 대한 연구는 초기단계에 있는 실정이며 적용분야도 극히 한정되어 있다. 본 연구에서는 실시간 도로교통정보 수집을 위한 영상처리분야에서 딥러닝 알고리즘을 활용하고 실험결과를 통하여 향후 교통정보수집‧분석 분야에서의 딥러닝 인공신경망 모형의 적용가능성을 살펴보고자 한다. 특히, 본 연구에서 도출한 딥러닝 인공신경망모형의 실험결과를 기존 역전파 모형에 의한 실험결과와 비교하여 모형의 우수성을 비교‧평가하고자 한다.Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Normally, artificial neural networks have been used in real-time applications for collecting traffic information because of their remarkable performance in terms of predictive accuracy and real-time computing. Especially, the recent publication of a paper in the journal Nature and the success of Google DeepMind’s world-beating Go program AlphaGo caused significant increases of the public interest in artificial intelligence. Even though various types of learning models have been proposed, the Backpropagation has been the most popular neural network model. However, the Deep learning neural network model has attracted much interest recently. In this study, the Deep leaning neural network will be applied in image processing techniques for the real-time traffic information collection to investigate the performance of Deep learning algorithm in traffic engineering fields. In addition, the experimental results from Deep learing algorithm will be compared with the results from Backpropagation model to evaluate and propose better performing model.Intelligent Transportation Systems (ITS), artificial neural networks, Deep learning, traffic information, image processing"
475,딥러닝의 모형과 응용사례,안성만 /SungMahn Ahn,2016,한국지능정보시스템학회,"딥러닝은 인공신경망(neural network)이라는 인공지능분야의 모형이 발전된 형태로서, 계층구조로 이루어진 인공신경망의 내부계층(hidden layer)이 여러 단계로 이루어진 구조이다. 딥러닝에서의 주요 모형은 합성곱신경망(convolutional neural network), 순환신경망(recurrent neural network), 그리고 심층신뢰신경망(deep belief network)의 세가지라고 할 수있다. 그 중에서 현재 흥미로운 연구가 많이 발표되어서 관심이 집중되고 있는 모형은 지도학습(supervised learning)모형인 처음 두 개의 모형이다. 따라서 본 논문에서는 지도학습모형의 가중치를 최적화하는 기본적인 방법인 오류역전파 알고리즘을 살펴본 뒤에 합성곱신경망과 순환신경망의 구조와 응용사례 등을 살펴보고자 한다. 본문에서 다루지 않은 모형인 심층신뢰신경망은 아직까지는 합성곱신경망 이나 순환신경망보다는 상대적으로 주목을 덜 받고 있다. 그러나 심층신뢰신경망은 CNN이나 RNN과는 달리 비지도학습(unsupervised learning)모형이며, 사람이나 동물은 관찰을 통해서 스스로 학습한다는 점에서 궁극적으로는 비지도학습모형이 더 많이 연구되어야 할 주제가 될 것이다.Deep learning model is a kind of neural networks that allows multiple hidden layers. There are various deep learning architectures such as convolutional neural networks, deep belief networks and recurrent neural networks. Those have been applied to fields like computer vision, automatic speech recognition, natural language processing, audio recognition and bioinformatics where they have been shown to produce state-of-the-art results on various tasks. Among those architectures, convolutional neural networks and recurrent neural networks are classified as the supervised learning model. And in recent years, those supervised learning models have gained more popularity than unsupervised learning models such as deep belief networks, because supervised learning models have shown fashionable applications in such fields mentioned above.
Deep learning models can be trained with backpropagation algorithm. Backpropagation is an abbreviation for “backward propagation of errors” and a common method of training artificial neural networks used in conjunction with an optimization method such as gradient descent. The method calculates the gradient of an error function with respect to all the weights in the network. The gradient is fed to the optimization method which in turn uses it to update the weights, in an attempt to minimize the error function.
Convolutional neural networks use a special architecture which is particularly well-adapted to classify images. Using this architecture makes convolutional networks fast to train. This, in turn, helps us train deep, muti-layer networks, which are very good at classifying images. These days, deep convolutional networks are used in most neural networks for image recognition. Convolutional neural networks use three basic ideas: local receptive fields, shared weights, and pooling. By local receptive fields, we mean that each neuron in the first(or any) hidden layer will be connected to a small region of the input(or previous layer’s) neurons. Shared weights mean that we’re going to use the same weights and bias for each of the local receptive field. This means that all the neurons in the hidden layer detect exactly the same feature, just at different locations in the input image. In addition to the convolutional layers just described, convolutional neural networks also contain pooling layers. Pooling layers are usually used immediately after convolutional layers. What the pooling layers do is to simplify the information in the output from the convolutional layer.
Recent convolutional network architectures have 10 to 20 hidden layers and billions of connections between units. Training deep learning networks has taken weeks several years ago, but thanks to progress in GPU and algorithm enhancement, training time has reduced to several hours.
Neural networks with time-varying behavior are known as recurrent neural networks or RNNs. A recurrent neural network is a class of artificial neural network where connections between units form a directed cycle. This creates an internal state of the network which allows it to exhibit dynamic temporal behavior. Unlike feedforward neural networks, RNNs can use their internal memory to process arbitrary sequences of inputs. Early RNN models turned out to be very difficult to train, harder even than deep feedforward networks. The reason is the unstable gradient problem such as vanishing gradient and exploding gradient. The gradient can get smaller and smaller as it is propagated back through layers. This makes learning in early layers extremely slow. The problem actually gets worse in RNNs, since gradients aren’t just propagated backward through layers, they’re propagated backward through time. If the network runs for a long time, that can make the gradient extremely unstable and hard to learn from. It has been possible to incorporate an idea known as long short-term memory units (LSTMs) into RNNs. LSTMs make it much easier to get good results when training RNNs, and many recent papers make use of LSTMs or related ideas.Deep learning, Convolutional neural networks, Recurrent neural networks, Error backpropagation algorithm"
476,딥러닝분석과 기술적 분석 지표를 이용한 한국 코스피주가지수 방향성 예측,이우식 /Lee Woosik,2017,한국데이터정보과학회,"2016년 3월 구글 (Google)의 바둑인공지능 알파고 (AlphaGo)가 이세돌 9단과의 바둑대결에서승리한 이후 다양한 분야에서 인공지능 사용에 대한 관심이 높아지고 있는 가운데 금융투자 분야에서도 인공지능과 투자자문 전문가의 합성어인 로보어드바이저 (Robo-Advisor)에 대한 관심이 높아지고 있다. 인공지능 (artificial intelligence)기반의 의사결정은 비용 절감은 물론 효과적인 의사결정을가능하게 한다는 점에서 큰 장점이 있다. 본 연구에서는 기술적 분석 (technical analysis) 지표와 딥러닝 (deep learning) 모형을 결합하여 한국 코스피 지수를 예측하는 모형을 개발하고 제시한 모형들의 예측력을 비교, 분석한다. 분석 결과 기술적 분석 지표에 딥러닝 알고리즘을 결합한 모형이 주가지수 방향성 예측 문제에 응용될 수 있음을 확인하였다. 향후 본 연구에서 제안된 기술적 분석 지표와딥러닝모형을 결합한 기법은 로보어드바이저서비스에 응용할 수 있는 일반화 가능성을 보여준다Since Google’s AlphaGo defeated a world champion of Go players in 2016, there have been many interests in the deep learning. In the financial sector, a Robo-Advisor using deep learning gains a significant attention, which builds and manages portfolios of financial instruments for investors.In this paper, we have proposed the a deep learning algorithm geared toward identification and forecast of the KOSPI index direction,and we also have compared the accuracy of the prediction.In an application of forecasting the financial market index direction, we have shown that the Robo-Advisor using deep learning has a significant effect on finance industry. The Robo-Advisor collects a massive data such as earnings statements, news reports and regulatory filings, analyzes those and recommends investors how to view market trends and identify the best time to purchase financial assets. On the other hand, the Robo-Advisor allows businesses to learn more about their customers, develop better marketing strategies, increase sales and decrease costs.Artificial intelligence, deep learning, FinTech, Robo-Advisor, technical analysis."
477,딥러닝의 효율적인 학습을 위한 학습데이터 선별 기법에 관한 연구,주경돈 /Gyoung-don Joo,2017,한국정보과학회,"빅데이터의 증가와 병렬 하드웨어 처리, 학습 알고리즘의 개선은 딥러닝의 발전을 가져왔으며 인공지능이 적용 가능한 영역을 획기적으로 확장시키고 있다. 하지만 이러한 대규모의 학습 데이터에 기반한 딥러닝은 학습시간이 증가되고, 더 나아가 학습 데이터의 구축을 위한 레이블링(labeling) 비용이 데이터 수에 비례하여 증가하게 된다. 따라서 본 논문에서는, 딥러닝의 정확도를 최대한 유지하면서 필요한 학습 데이터의 수를 줄일수 있도록, 전체 데이터에서 제한된 수의 학습 데이터의 후보를 선별하는 기법에 대하여 연구한다. 특히 기존의 능동 학습에 기반한 기법들이 학습 데이터의 수가 매우 적어질 경우 딥러닝의 정확도가 급격히 떨어지는 문제를 해결할 수 있는 새로운 기법을 제안하였으며, 제안된 기법으로 선별된 학습 데이터는 그 수의 변화에 대해 딥러닝이 일관되게 경쟁력있는 정확도를 가짐을 다양한 실험을 통하여 검증하였다.The increase of big data, parallel hardware processing and enhanced learning algorithms have led to the development of deep learning and are dramatically expanding the domain of applications based on artificial intelligence. However, such large-scale training data extends the learning time, and furthermore, the labeling cost for building the training set increases in proportion to the volume of data. Therefore, in this paper, we study how to select a limited number of candidates for training set from given data so as to reduce the number of training set while maintaining the accuracy of deep learning. Especially, we found that, with the training set selected by the previous work, the accuracy of deep learning drastically decreases when the number of training set is very small and so proposed new methods to resolve this fall-off in accuracy. Through experiments, we showed that deep learning consistently has the competitive accuracy with the training set selected by the proposed method regardless of the size of training set.Deep Learning, Train Set Selection, Active Learning, Gamma distribution"
478,딥러닝을 이용한 범용적 스테그아날리시스,김현재 /Hyunjae Kim,2017,한국정보과학회,"스테그아날리시스(Steganalysis)란 이미지 등 일반적인 자료에 암호화된 정보를 은닉하는 스테가노그래피(Steganography)에 대한 검출 및 분석 방법으로, 기계학습 기반 방법론을 포함한다. 기존 기계학습 기반 스테그아날리시스는 영상(Image)의 특징(Feature) 추출 및 모델링에 기반하며, 최근 딥러닝(Deep Learning)의 적용으로 검출 정확도가 큰 폭으로 향상되었다. 하지만 현존하는 스테그아날리시스 모델은 단일 스테가노그래피 기법에 대해 국한되어 있어 학습에 사용되지 않은 스테고(Stego) 이미지의 경우 검출이 불가능한 결정적 한계를 가진다. 본 연구에서는 다양한 스테가노그래피 기법으로 생성된 스테고이미지에 딥러닝을 적용하여 스테그아날리시스를 학습하는 범용적 모델을 제안한다. 다양한 실험을 통해 제안 기법의 효용성 및 가능성을 확인하고, 범용적 스테그아날리시스 모델이 각각에 특화된 검출 기법과 유사한 정확도로 스테고 이미지를 검출할 수 있음을 보인다.Steganalysis is to detect information hidden by steganography inside general data such as images. There are stegoanalysis techniques that use machine learning (ML). Existing ML approaches to steganalysis are based on extracting features from stego images and modeling them.
Recently deep learning-based methodologies have shown significant improvements in detection accuracy. However, all the existing methods, including deep learning-based ones, have a critical limitation in that they can only detect stego images that are created by a specific steganography method. In this paper, we propose a generalized steganalysis method that can model multiple types of stego images using deep learning. Through various experiments, we confirm the effectiveness of our approach and envision directions for future research. In particular, we show that our method can detect each type of steganography with the same level of accuracy as that of a steganalysis method dedicated to that type of steganography, thereby demonstrating the general applicability of our approach to multiple types of stego images.steganalysis, steganography, deep learning, generalized steganalysis model"
479,"딥러닝 프레임워크의 비교: 티아노, 텐서플로, CNTK를 중심으로",정여진 /Yeojin Chung,2017,한국지능정보시스템학회,"딥러닝 프레임워크의 대표적인 기능으로는 ‘자동미분’과 ‘GPU의 활용’ 등을 들 수 있다. 본 논문은 파이썬의 라이브러리 형태로 사용 가능한 프레임워크 중에서 구글의 텐서플로와 마이크로소프트의 CNTK, 그리고 텐서 플로의 원조라고 할 수 있는 티아노를 비교하였다. 본문에서는 자동미분의 개념과 GPU의 활용형태를 간단히 설명하고, 그 다음에 logistic regression을 실행하는 예를 통하여 각 프레임워크의 문법을 알아본 뒤에, 마지막으로 대표적인 딥러닝 응용인 CNN의 예제를 실행시켜보고 코딩의 편의성과 실행속도 등을 확인해 보았다. 그 결과, 편의성의 관점에서 보면 티아노가 가장 코딩 하기가 어렵고, CNTK와 텐서플로는 많은 부분이 비슷하게 추상화 되어 있어서 코딩이 비슷하지만 가중치와 편향을 직접 정의하느냐의 여부에서 차이를 보였다. 그리고 각프레임워크의 실행속도에 대한 평가는 ‘큰 차이는 없다’는 것이다. 텐서플로는 티아노에 비하여 속도가 느리다는 평가가 있어왔는데, 본 연구의 실험에 의하면, 비록 CNN 모형에 국한되었지만, 텐서플로가 아주 조금이지만빠른 것으로 나타났다. CNTK의 경우에도, 비록 실험환경이 달랐지만, 실험환경의 차이에 의한 속도의 차이의 편차범위 이내에 있는 것으로 판단이 되었다. 본 연구에서는 세 종류의 딥러닝 프레임워크만을 살펴보았는데,위키피디아에 따르면 딥러닝 프레임워크의 종류는 12가지가 있으며, 각 프레임워크의 특징을 15가지 속성으로구분하여 차이를 특정하고 있다. 그 많은 속성 중에서 사용자의 입장에서 볼 때 중요한 속성은 어떤 언어(파이썬, C++, Java, 등)로 사용가능한지, 어떤 딥러닝 모형에 대한 라이브러리가 잘 구현되어 있는지 등일 것이다. 그리고 사용자가 대규모의 딥러닝 모형을 구축한다면, 다중 GPU 혹은 다중 서버를 지원하는지의 여부도 중요할 것이다. 또한 딥러닝 모형을 처음 학습하는 경우에는 사용설명서가 많은지 예제 프로그램이 많은지 여부도 중요한 기준이 될 것이다.The deep learning framework is software designed to help develop deep learning models. Some of its important functions include “automatic differentiation” and “utilization of GPU”. The list of popular deep learning framework includes Caffe (BVLC) and Theano (University of Montreal). And recently, Microsoft's deep learning framework, Microsoft Cognitive Toolkit, was released as open-source license, following Google’s Tensorflow a year earlier. The early deep learning frameworks have been developed mainly for research at universities. Beginning with the inception of Tensorflow, however, it seems that companies such as Microsoft and Facebook have started to join the competition of framework development. Given the trend, Google and other companies are expected to continue investing in the deep learning framework to bring forward the initiative in the artificial intelligence business. From this point of view, we think it is a good time to compare some of deep learning frameworks. So we compare three deep learning frameworks which can be used as a Python library. Those are Google's Tensorflow, Microsoft’s CNTK, and Theano which is sort of a predecessor of the preceding two.
The most common and important function of deep learning frameworks is the ability to perform automatic differentiation. Basically all the mathematical expressions of deep learning models can be represented as computational graphs, which consist of nodes and edges. Partial derivatives on each edge of a computational graph can then be obtained. With the partial derivatives, we can let software compute differentiation of any node with respect to any variable by utilizing chain rule of Calculus.
First of all, the convenience of coding is in the order of CNTK, Tensorflow, and Theano. The criterion is simply based on the lengths of the codes and the learning curve and the ease of coding are not the main concern. According to the criteria, Theano was the most difficult to implement with, and CNTK and Tensorflow were somewhat easier. With Tensorflow, we need to define weight variables and biases explicitly. The reason that CNTK and Tensorflow are easier to implement with is that those frameworks provide us with more abstraction than Theano. We, however, need to mention that low-level coding is not always bad. It gives us flexibility of coding. With the low-level coding such as in Theano, we can implement and test any new deep learning models or any new search methods that we can think of.
The assessment of the execution speed of each framework is that there is not meaningful difference. According to the experiment, execution speeds of Theano and Tensorflow are very similar, although the experiment was limited to a CNN model. In the case of CNTK, the experimental environment was not maintained as the same. The code written in CNTK has to be run in PC environment without GPU where codes execute as much as 50 times slower than with GPU. But we concluded that the difference of execution speed was within the range of variation caused by the different hardware setup.
In this study, we compared three types of deep learning framework: Theano, Tensorflow, and CNTK. According to Wikipedia, there are 12 available deep learning frameworks. And 15 different attributes differentiate each framework. Some of the important attributes would include interface language (Python, C ++, Java, etc.) and the availability of libraries on various deep learning models such as CNN, RNN, DBN, and etc. And if a user implements a large scale deep learning model, it will also be important to support multiple GPU or multiple servers. Also, if you are learning the deep learning model, it would also be important if there are enough examples and references.deep learning framework, Theano, TensorFlow, CNTK, computational graph, CIFAR-10"
480,딥러닝을 이용한 주변 무선단말 파악방안,이웅섭 /Lee， Woong-sup,2018,한국정보통신학회,"최근 단말-대-단말(Device-to-device, D2D) 통신기술이 차세대 무선통신시스템의 핵심기술로 큰 관심을 받고 있다. 이러한 단말간 통신에서는 자신의 주변에 어떠한 단말이 있는지 파악하는 주변단말 탐색(Neighbor discovery)이매우 중요하다. 본 논문에서는 최근 큰 관심을 받고 있는 딥러닝(Deep learning) 기술을 활용하여 단말간 통신에서 주변단말을 파악하는 방안에 대해서 제안한다. 제안 방안은 기존의 방안과 달리 무선채널의 공간적 연관성을 이용하여단말간의 신호 전송 없이 단말이 기지국으로 전송하는 상향링크 파일럿 신호를 기반으로 주변 단말을 찾고 따라서기존의 방식에 비해 신호전송 복잡도(signaling complexity)를 크게 줄일 수 있다. 또한 제안 방안에서는 떨어져 있는거리에 따라서 주변 단말을 분류 가능하여 기존 방안에 비해서 좀 더 세밀한 단말 탐색이 가능하다. 마지막으로 본 논문에서는 tensorflow를 이용한 컴퓨터 시뮬레이션을 통해 제안 방안의 성능을 검증하였다.Recently, the device-to-device (D2D) communication has been conceived as the key technology for the next-generationmobile communication systems. The neighbor discovery in which the nearby users are found, is essential for the properoperation of the D2D communication. In this paper, we propose new neighbor discovery scheme based on deep learningtechnology which has gained a lot of attention recently. In the proposed scheme, the neighboring users can be foundusing the uplink pilot transmission of users only, unlike conventional neighbor discovery schemes in which direct pilotcommunication among users is required, such that the signaling overhead can be greatly reduced in our proposed scheme.
Moreover, the neighbors with different proximity can also be classified accordingly which enables more accurate neighbordiscovery compared to the conventional schemes. The performance of our proposed scheme is verified through thetensorflow-based computer simulations.
키워드 : 딥러닝, 주변단말 탐색, 단말-대-단말 통신Deep learning, Neighbor discovery, Device-to-device communication, Wireless channel correlation"
481,신뢰성있는 딥러닝 기반 분석 모델을 참조하기 위한 딥러닝 기술 언어,문종혁 /Jong Hyeok Mun,2021,한국정보처리학회,"최근 딥러닝은 하드웨어 성능이 향상됨에 따라 자연어 처리, 영상 인식 등의 다양한 기술에 접목되어 활용되고 있다. 이러한 기술들을 활용해 지능형교통 시스템(ITS), 스마트홈, 헬스케어 등의 산업분야에서 데이터를 분석하여 고속도로 속도위반 차량 검출, 에너지 사용량 제어, 응급상황 등과 같은고품질의 서비스를 제공하며, 고품질의 서비스를 제공하기 위해서는 정확도가 향상된 딥러닝 모델이 적용되어야 한다. 이를 위해 서비스 환경의 데이터를분석하기 위한 딥러닝 모델을 개발할 때, 개발자는 신뢰성이 검증된 최신의 딥러닝 모델을 적용할 수 있어야 한다. 이는 개발자가 참조하는 딥러닝 모델에적용된 학습 데이터셋의 정확도를 측정하여 검증할 수 있다. 이러한 검증을 위해서 개발자는 학습 데이터셋, 딥러닝의 계층구조 및 개발 환경 등과 같은내용을 포함하는 딥러닝 모델을 문서화하여 적용하기 위한 구조적인 정보가 필요하다. 본 논문에서는 신뢰성있는 딥러닝 기반 데이터 분석 모델을 참조하기위한 딥러닝 기술 언어를 제안한다. 제안하는 기술 언어는 신뢰성 있는 딥러닝 모델을 개발하는데 필요한 학습데이터셋, 개발 환경 및 설정 등의 정보와더불어 딥러닝 모델의 계층구조를 표현할 수 있다. 제안하는 딥러닝 기술 언어를 이용하여 개발자는 지능형 교통 시스템에서 참조하는 분석 모델의정확도를 검증할 수 있다. 실험에서는 제안하는 언어의 유효성을 검증하기 위해, 번호판 인식 모델을 중심으로 딥러닝 기술 문서의 적용과정을 보인다With the recent advancements of deep learning, companies such as smart home, healthcare, and intelligent transportation systems areutilizing its functionality to provide high-quality services for vehicle detection, emergency situation detection, and controlling energyconsumption. To provide reliable services in such sensitive systems, deep learning models are required to have high accuracy. In orderto develop a deep learning model for analyzing previously mentioned services, developers should utilize the state of the art deep learningmodels that have already been verified for higher accuracy. The developers can verify the accuracy of the referenced model by validatingthe model on the dataset. For this validation, the developer needs structural information to document and apply deep learning models,including metadata such as learning dataset, network architecture, and development environments. In this paper, we propose a descriptionlanguage that represents the network architecture of the deep learning model along with its metadata that are necessary to develop adeep learning model. Through the proposed description language, developers can easily verify the accuracy of the referenced deep learningmodel. Our experiments demonstrate the application scenario of a deep learning description document that focuses on the license platerecognition for the detection of illegally parked vehicles.Trusted Deep Learning, Model Reference, Deep Learning Description Language, Traffic Situation Analysis Model"
482,딥러닝 분석을 이용한 중국 역내·외 위안화 변동성 예측,이우식 /Lee Woosik,2016,한국데이터정보과학회,"2008년 글로벌 금융위기 이후 중국은 위안화 국제화의 점진적 추진을 진행하면서 중국상하이 외환시장과 중국홍콩 외환시장에서 거래되는 통화인 역내위안화와 역외위안화를 형성시켰다. 본 연구는 위안화 국제화와 점진적인 중국 자본계정 개방에 따라 급변하는 외환시장상황의 변동성을 정확하게 파악하기 위해서 GARCH모형 (일반화된 자기회귀 조건부이분산성모형)에 다단계인공신경망을 결합한 MLP-GARCH 모형과 GARCH모형과 기계학습의 일종인 딥러닝 (deep learning)을 통합한 DL-GARCH을 가지고 위안화 변동성예측을 비교 실험과 분석을 하였다. 비교분석 결과 DLGARCH 모형은 MLP-GARCH보다 모형 위안화 역내·외 환율변동성 예측 면에서 더욱 더 개선된예측값을 제공하였다. 그래서 이분산시계열모형을 딥러닝과 결합한 DL-GARCH 모형은 시계열의환율 변동성 예측 문제에 딥러닝을 응용할 수 있음을 확인하였다. 향후 이분산시계열과 결합된 딥러닝 모형은 다른 금융시계열 데이터에 응용하여 그 일반화 가능성을 높일 수 있을 것이다.The People’s Republic of China has vigorously been pursuing the internationalization of the Chinese Yuan or Renminbi after the financial crisis of 2008. In this view, an abrupt increase of use of the Chinese Yuan in the onshore and offshore markets are important milestones to be one of important currencies. One of the most frequently used methods to forecast volatility is GARCH model. Since a prediction error of the GARCH model has been reported quite high, a lot of efforts have been made to improve forecasting capability of the GARCH model. In this paper, we have proposed MLPGARCH and a DL-GARCH by employing Artificial Neural Network to the GARCH. In an application to forecasting Chinese Yuan volatility, we have successfully shown their overall outperformance in forecasting over the GARCH.Deep learning, DL-GARCH, GARCH, The Chinese Yuan, volatility."
483,딥러닝 기법을 이용한 내일강수 예측,하지훈,2016,한국지능시스템학회,"정확한 강수예측을 위해서는 예측인자 선정과 예측방법에 대한 선택이 매우 중요하다. 최근에는 강수예측 방법으로기계학습 기법이 많이 사용되고 있으며, 그 중에서도 특히 인공신경망을 사용한 강수예측 방법은 좋은 성능을 보였다. 본논문에서는 딥러닝 기법 중 하나인 DBN(deep belief network)를 이용한 새로운 강수예측 방법을 제안한다. DBN는 비지도사전 학습을 통해 초기 가중치를 설정하여 기존 인공신경망의 문제점을 보완한다. 예측인자로는 기온, 전일-전주 강수일, 태양과 달 궤도 관련 자료를 선정하였다. 기온과 전일-전주 강수일은 서울에서의 1974년부터 2013년까지 총 40년간의AWS(automatic weather system) 관측 자료를 사용하였고, 태양과 달의 궤도 관련 자료는 서울을 중심으로 계산한 결과를사용하였다. 전체 기간에서 일부는 학습 자료로 사용하여 예측모델을 생성하였고, 나머지를 생성한 모델의 검증 자료로사용하였다. 모델 검증 결과로 나온 예측값들은 확률값을 가지며 임계치를 이용하여 강수유무를 판별하였다. 강수 정확도의척도로 양분예보기법 중 CSI(critical successive index)와 Bias(frequency bias)를 계산하였다. 이를 통해 DBN와 MLP(multilayer perceptron)의 성능을 비교한 결과 DBN의 강수 예측 정확도가 높았고, 수행속도 또한 2배 이상 빨랐다.For accurate precipitation forecasts the choice of weather faoctrs and prediction method is very important. Recently, machinele arning has been widely used for forecasting precipitation, and artificial neural network, one of machine learning techniques, showed good performance. In this paper, we suggest a new method for forecasting precipitation using DBN, one of deep learning techniques.
DBN has an advantage that initial weights are set by unsupervised learning, so this compensates for the defects of artificial neural networks. We used past precipitation, temperature, and the parameters of the sun and moon's motion as features for forecasting precipitation. The dataset consists of observation data which had been measured for 40 years from AWS in Seoul. Experiments were based on 8-fold cross validation. As a result of estimation, we got probabilities of test dataset, so threshold was used for the decision of precipitation. CSI and Bias were used for indicating the precision of precipitation. Our experimental results showed that DBN performed better than MLP.Deep learning, Deep belief network, Precipitation, Forecast"
484,딥러닝 신경망모형을 이용한 실시간 교통정보수집 연구,김대현 /KIM DAE HYON,2016,사단법인 인문사회과학기술융합학회,"비디오 기반의 실시간 교통정보수집 시스템은 자율주행을 포함한 지능형교통체계의 주요 서브시스템중 하나이며, 우수한 예측(추정)력 때문에 핵심 알고리즘으로 인공신경망 모형이 주로 사용되고 있다. 특히, 최근 과학 학술지 네이처에 구글 딥마인드의 알파고가 공개되면서 딥러닝과 인공지능에 대한 전 세계적인 관심이 집중되고 있다. 최근까지 가장 널리 사용된 인공신경망모형은 역전파 모형이었으나 최근 컴퓨터 하드웨어 및 알고리즘의 새로운 개발과 더불어 딥러닝 알고리즘을 이용한 딥인공신경망 모형에 대한 관심이 높아지고 있다. 그러나 아직 딥러닝 인공신경망에 대한 연구는 초기단계에 있는 실정이며 적용분야도 극히 한정되어 있다. 본 연구에서는 실시간 도로교통정보 수집을 위한 영상처리분야에서 딥러닝 알고리즘을 활용하고 실험결과를 통하여 향후 교통정보수집‧분석 분야에서의 딥러닝 인공신경망 모형의 적용가능성을 살펴보고자 한다. 특히, 본 연구에서 도출한 딥러닝 인공신경망모형의 실험결과를 기존 역전파 모형에 의한 실험결과와 비교하여 모형의 우수성을 비교‧평가하고자 한다.Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Normally, artificial neural networks have been used in real-time applications for collecting traffic information because of their remarkable performance in terms of predictive accuracy and real-time computing. Especially, the recent publication of a paper in the journal Nature and the success of Google DeepMind’s world-beating Go program AlphaGo caused significant increases of the public interest in artificial intelligence. Even though various types of learning models have been proposed, the Backpropagation has been the most popular neural network model. However, the Deep learning neural network model has attracted much interest recently. In this study, the Deep leaning neural network will be applied in image processing techniques for the real-time traffic information collection to investigate the performance of Deep learning algorithm in traffic engineering fields. In addition, the experimental results from Deep learing algorithm will be compared with the results from Backpropagation model to evaluate and propose better performing model.Intelligent Transportation Systems (ITS), artificial neural networks, Deep learning, traffic information, image processing"
485,딥러닝의 모형과 응용사례,안성만 /SungMahn Ahn,2016,한국지능정보시스템학회,"딥러닝은 인공신경망(neural network)이라는 인공지능분야의 모형이 발전된 형태로서, 계층구조로 이루어진 인공신경망의 내부계층(hidden layer)이 여러 단계로 이루어진 구조이다. 딥러닝에서의 주요 모형은 합성곱신경망(convolutional neural network), 순환신경망(recurrent neural network), 그리고 심층신뢰신경망(deep belief network)의 세가지라고 할 수있다. 그 중에서 현재 흥미로운 연구가 많이 발표되어서 관심이 집중되고 있는 모형은 지도학습(supervised learning)모형인 처음 두 개의 모형이다. 따라서 본 논문에서는 지도학습모형의 가중치를 최적화하는 기본적인 방법인 오류역전파 알고리즘을 살펴본 뒤에 합성곱신경망과 순환신경망의 구조와 응용사례 등을 살펴보고자 한다. 본문에서 다루지 않은 모형인 심층신뢰신경망은 아직까지는 합성곱신경망 이나 순환신경망보다는 상대적으로 주목을 덜 받고 있다. 그러나 심층신뢰신경망은 CNN이나 RNN과는 달리 비지도학습(unsupervised learning)모형이며, 사람이나 동물은 관찰을 통해서 스스로 학습한다는 점에서 궁극적으로는 비지도학습모형이 더 많이 연구되어야 할 주제가 될 것이다.Deep learning model is a kind of neural networks that allows multiple hidden layers. There are various deep learning architectures such as convolutional neural networks, deep belief networks and recurrent neural networks. Those have been applied to fields like computer vision, automatic speech recognition, natural language processing, audio recognition and bioinformatics where they have been shown to produce state-of-the-art results on various tasks. Among those architectures, convolutional neural networks and recurrent neural networks are classified as the supervised learning model. And in recent years, those supervised learning models have gained more popularity than unsupervised learning models such as deep belief networks, because supervised learning models have shown fashionable applications in such fields mentioned above.
Deep learning models can be trained with backpropagation algorithm. Backpropagation is an abbreviation for “backward propagation of errors” and a common method of training artificial neural networks used in conjunction with an optimization method such as gradient descent. The method calculates the gradient of an error function with respect to all the weights in the network. The gradient is fed to the optimization method which in turn uses it to update the weights, in an attempt to minimize the error function.
Convolutional neural networks use a special architecture which is particularly well-adapted to classify images. Using this architecture makes convolutional networks fast to train. This, in turn, helps us train deep, muti-layer networks, which are very good at classifying images. These days, deep convolutional networks are used in most neural networks for image recognition. Convolutional neural networks use three basic ideas: local receptive fields, shared weights, and pooling. By local receptive fields, we mean that each neuron in the first(or any) hidden layer will be connected to a small region of the input(or previous layer’s) neurons. Shared weights mean that we’re going to use the same weights and bias for each of the local receptive field. This means that all the neurons in the hidden layer detect exactly the same feature, just at different locations in the input image. In addition to the convolutional layers just described, convolutional neural networks also contain pooling layers. Pooling layers are usually used immediately after convolutional layers. What the pooling layers do is to simplify the information in the output from the convolutional layer.
Recent convolutional network architectures have 10 to 20 hidden layers and billions of connections between units. Training deep learning networks has taken weeks several years ago, but thanks to progress in GPU and algorithm enhancement, training time has reduced to several hours.
Neural networks with time-varying behavior are known as recurrent neural networks or RNNs. A recurrent neural network is a class of artificial neural network where connections between units form a directed cycle. This creates an internal state of the network which allows it to exhibit dynamic temporal behavior. Unlike feedforward neural networks, RNNs can use their internal memory to process arbitrary sequences of inputs. Early RNN models turned out to be very difficult to train, harder even than deep feedforward networks. The reason is the unstable gradient problem such as vanishing gradient and exploding gradient. The gradient can get smaller and smaller as it is propagated back through layers. This makes learning in early layers extremely slow. The problem actually gets worse in RNNs, since gradients aren’t just propagated backward through layers, they’re propagated backward through time. If the network runs for a long time, that can make the gradient extremely unstable and hard to learn from. It has been possible to incorporate an idea known as long short-term memory units (LSTMs) into RNNs. LSTMs make it much easier to get good results when training RNNs, and many recent papers make use of LSTMs or related ideas.Deep learning, Convolutional neural networks, Recurrent neural networks, Error backpropagation algorithm"
486,딥러닝분석과 기술적 분석 지표를 이용한 한국 코스피주가지수 방향성 예측,이우식 /Lee Woosik,2017,한국데이터정보과학회,"2016년 3월 구글 (Google)의 바둑인공지능 알파고 (AlphaGo)가 이세돌 9단과의 바둑대결에서승리한 이후 다양한 분야에서 인공지능 사용에 대한 관심이 높아지고 있는 가운데 금융투자 분야에서도 인공지능과 투자자문 전문가의 합성어인 로보어드바이저 (Robo-Advisor)에 대한 관심이 높아지고 있다. 인공지능 (artificial intelligence)기반의 의사결정은 비용 절감은 물론 효과적인 의사결정을가능하게 한다는 점에서 큰 장점이 있다. 본 연구에서는 기술적 분석 (technical analysis) 지표와 딥러닝 (deep learning) 모형을 결합하여 한국 코스피 지수를 예측하는 모형을 개발하고 제시한 모형들의 예측력을 비교, 분석한다. 분석 결과 기술적 분석 지표에 딥러닝 알고리즘을 결합한 모형이 주가지수 방향성 예측 문제에 응용될 수 있음을 확인하였다. 향후 본 연구에서 제안된 기술적 분석 지표와딥러닝모형을 결합한 기법은 로보어드바이저서비스에 응용할 수 있는 일반화 가능성을 보여준다Since Google’s AlphaGo defeated a world champion of Go players in 2016, there have been many interests in the deep learning. In the financial sector, a Robo-Advisor using deep learning gains a significant attention, which builds and manages portfolios of financial instruments for investors.In this paper, we have proposed the a deep learning algorithm geared toward identification and forecast of the KOSPI index direction,and we also have compared the accuracy of the prediction.In an application of forecasting the financial market index direction, we have shown that the Robo-Advisor using deep learning has a significant effect on finance industry. The Robo-Advisor collects a massive data such as earnings statements, news reports and regulatory filings, analyzes those and recommends investors how to view market trends and identify the best time to purchase financial assets. On the other hand, the Robo-Advisor allows businesses to learn more about their customers, develop better marketing strategies, increase sales and decrease costs.Artificial intelligence, deep learning, FinTech, Robo-Advisor, technical analysis."
487,딥러닝의 효율적인 학습을 위한 학습데이터 선별 기법에 관한 연구,주경돈 /Gyoung-don Joo,2017,한국정보과학회,"빅데이터의 증가와 병렬 하드웨어 처리, 학습 알고리즘의 개선은 딥러닝의 발전을 가져왔으며 인공지능이 적용 가능한 영역을 획기적으로 확장시키고 있다. 하지만 이러한 대규모의 학습 데이터에 기반한 딥러닝은 학습시간이 증가되고, 더 나아가 학습 데이터의 구축을 위한 레이블링(labeling) 비용이 데이터 수에 비례하여 증가하게 된다. 따라서 본 논문에서는, 딥러닝의 정확도를 최대한 유지하면서 필요한 학습 데이터의 수를 줄일수 있도록, 전체 데이터에서 제한된 수의 학습 데이터의 후보를 선별하는 기법에 대하여 연구한다. 특히 기존의 능동 학습에 기반한 기법들이 학습 데이터의 수가 매우 적어질 경우 딥러닝의 정확도가 급격히 떨어지는 문제를 해결할 수 있는 새로운 기법을 제안하였으며, 제안된 기법으로 선별된 학습 데이터는 그 수의 변화에 대해 딥러닝이 일관되게 경쟁력있는 정확도를 가짐을 다양한 실험을 통하여 검증하였다.The increase of big data, parallel hardware processing and enhanced learning algorithms have led to the development of deep learning and are dramatically expanding the domain of applications based on artificial intelligence. However, such large-scale training data extends the learning time, and furthermore, the labeling cost for building the training set increases in proportion to the volume of data. Therefore, in this paper, we study how to select a limited number of candidates for training set from given data so as to reduce the number of training set while maintaining the accuracy of deep learning. Especially, we found that, with the training set selected by the previous work, the accuracy of deep learning drastically decreases when the number of training set is very small and so proposed new methods to resolve this fall-off in accuracy. Through experiments, we showed that deep learning consistently has the competitive accuracy with the training set selected by the proposed method regardless of the size of training set.Deep Learning, Train Set Selection, Active Learning, Gamma distribution"
488,2015 개정 교육과정 통합사회 과목 운영 실태 조사 연구,이정우 /Lee， Jungwoo,2019,부산대학교 과학교육연구소,"이 연구에서는 ‘통합사회’ 과목이 실제 학교에서는 어떻게 편성･운영되고 있으며, 교수･학습 및 평가의 실태는 어떠한지, ‘통합사회’로 인한 교사들의 어려움과 요구사항은 무엇인지에 대해 전국의 고등학교 사회과 교사 159명이 참여한 설문조사를 통해 살펴보았다. 조사 결과, 첫째, 편성 및 운영의 측면에서 ‘통합사회’는 6단위로 최소 시수로 편성된 경우와 2인 이상의 교사가 전공별로 나누어 가르치는 경우가 가장 많았다. 둘째, 교수･학습 및 평가 측면에서 ‘통합사회’에서도 강의나 지필평가 등의 비중이 학생 참여형 수업이나 수행평가 등 새로운 방법보다 높게 나타났으나 ‘통합사회’ 도입으로 인하여 수업에서 학생 참여가 증가하고 수업이나 평가 방법을 다양화하는 등의 변화가 나타나고 있었다. 셋째, 사회과 교사들은 통합적 수업설계에 대한 부담이 가장 크며, 양질의 교수･학습 자료의 개발 및 제공에 대한 요구가 가장 강하였다. 이러한 조사 결과를 토대로 ‘통합사회’의 성공적 안착을 위한 시사점을 제시하였다.The purposes of this study was to investigate that (1) how the ‘integrated society’ course is organized and operated in schools, (2) what the actual conditions of teaching and learning and evaluation of ‘integrated society’ course is, (3) and what the difficulties and requirements of teachers due to ‘integrated society’ are. To achieve these purposes, the results of survey which 159 teachers had participated were analyzed. The results are summarized as follows: First, ‘integration society’ is organized in minimum unit of 6 units and 2 and more teachers per one class teaches separately ‘integrated society’. Second, in terms of teaching and learning, and evaluation, the proportion of lectures and handwriting evaluation in ‘integrated society’ was higher than that of new methods such as student participation and performance assessment. However, after the introduction of ‘integrated society’, teachers tried to change their methods of teaching and learning and evaluation. Third, social studies teachers had the greatest burden on integrated instruction design, and they demand strongest for the development and provision of high quality teaching and learning materials. Based on the results of this survey, suggestions for successful implementation of ‘integrated society’ were suggested.the 2015 revised curriculum, integrated society, social studies teacher, implementation and organization of curriculum"
489,"딥러닝 프레임워크의 비교: 티아노, 텐서플로, CNTK를 중심으로",정여진 /Yeojin Chung,2017,한국지능정보시스템학회,"딥러닝 프레임워크의 대표적인 기능으로는 ‘자동미분’과 ‘GPU의 활용’ 등을 들 수 있다. 본 논문은 파이썬의 라이브러리 형태로 사용 가능한 프레임워크 중에서 구글의 텐서플로와 마이크로소프트의 CNTK, 그리고 텐서 플로의 원조라고 할 수 있는 티아노를 비교하였다. 본문에서는 자동미분의 개념과 GPU의 활용형태를 간단히 설명하고, 그 다음에 logistic regression을 실행하는 예를 통하여 각 프레임워크의 문법을 알아본 뒤에, 마지막으로 대표적인 딥러닝 응용인 CNN의 예제를 실행시켜보고 코딩의 편의성과 실행속도 등을 확인해 보았다. 그 결과, 편의성의 관점에서 보면 티아노가 가장 코딩 하기가 어렵고, CNTK와 텐서플로는 많은 부분이 비슷하게 추상화 되어 있어서 코딩이 비슷하지만 가중치와 편향을 직접 정의하느냐의 여부에서 차이를 보였다. 그리고 각프레임워크의 실행속도에 대한 평가는 ‘큰 차이는 없다’는 것이다. 텐서플로는 티아노에 비하여 속도가 느리다는 평가가 있어왔는데, 본 연구의 실험에 의하면, 비록 CNN 모형에 국한되었지만, 텐서플로가 아주 조금이지만빠른 것으로 나타났다. CNTK의 경우에도, 비록 실험환경이 달랐지만, 실험환경의 차이에 의한 속도의 차이의 편차범위 이내에 있는 것으로 판단이 되었다. 본 연구에서는 세 종류의 딥러닝 프레임워크만을 살펴보았는데,위키피디아에 따르면 딥러닝 프레임워크의 종류는 12가지가 있으며, 각 프레임워크의 특징을 15가지 속성으로구분하여 차이를 특정하고 있다. 그 많은 속성 중에서 사용자의 입장에서 볼 때 중요한 속성은 어떤 언어(파이썬, C++, Java, 등)로 사용가능한지, 어떤 딥러닝 모형에 대한 라이브러리가 잘 구현되어 있는지 등일 것이다. 그리고 사용자가 대규모의 딥러닝 모형을 구축한다면, 다중 GPU 혹은 다중 서버를 지원하는지의 여부도 중요할 것이다. 또한 딥러닝 모형을 처음 학습하는 경우에는 사용설명서가 많은지 예제 프로그램이 많은지 여부도 중요한 기준이 될 것이다.The deep learning framework is software designed to help develop deep learning models. Some of its important functions include “automatic differentiation” and “utilization of GPU”. The list of popular deep learning framework includes Caffe (BVLC) and Theano (University of Montreal). And recently, Microsoft's deep learning framework, Microsoft Cognitive Toolkit, was released as open-source license, following Google’s Tensorflow a year earlier. The early deep learning frameworks have been developed mainly for research at universities. Beginning with the inception of Tensorflow, however, it seems that companies such as Microsoft and Facebook have started to join the competition of framework development. Given the trend, Google and other companies are expected to continue investing in the deep learning framework to bring forward the initiative in the artificial intelligence business. From this point of view, we think it is a good time to compare some of deep learning frameworks. So we compare three deep learning frameworks which can be used as a Python library. Those are Google's Tensorflow, Microsoft’s CNTK, and Theano which is sort of a predecessor of the preceding two.
The most common and important function of deep learning frameworks is the ability to perform automatic differentiation. Basically all the mathematical expressions of deep learning models can be represented as computational graphs, which consist of nodes and edges. Partial derivatives on each edge of a computational graph can then be obtained. With the partial derivatives, we can let software compute differentiation of any node with respect to any variable by utilizing chain rule of Calculus.
First of all, the convenience of coding is in the order of CNTK, Tensorflow, and Theano. The criterion is simply based on the lengths of the codes and the learning curve and the ease of coding are not the main concern. According to the criteria, Theano was the most difficult to implement with, and CNTK and Tensorflow were somewhat easier. With Tensorflow, we need to define weight variables and biases explicitly. The reason that CNTK and Tensorflow are easier to implement with is that those frameworks provide us with more abstraction than Theano. We, however, need to mention that low-level coding is not always bad. It gives us flexibility of coding. With the low-level coding such as in Theano, we can implement and test any new deep learning models or any new search methods that we can think of.
The assessment of the execution speed of each framework is that there is not meaningful difference. According to the experiment, execution speeds of Theano and Tensorflow are very similar, although the experiment was limited to a CNN model. In the case of CNTK, the experimental environment was not maintained as the same. The code written in CNTK has to be run in PC environment without GPU where codes execute as much as 50 times slower than with GPU. But we concluded that the difference of execution speed was within the range of variation caused by the different hardware setup.
In this study, we compared three types of deep learning framework: Theano, Tensorflow, and CNTK. According to Wikipedia, there are 12 available deep learning frameworks. And 15 different attributes differentiate each framework. Some of the important attributes would include interface language (Python, C ++, Java, etc.) and the availability of libraries on various deep learning models such as CNN, RNN, DBN, and etc. And if a user implements a large scale deep learning model, it will also be important to support multiple GPU or multiple servers. Also, if you are learning the deep learning model, it would also be important if there are enough examples and references.deep learning framework, Theano, TensorFlow, CNTK, computational graph, CIFAR-10"
490,딥러닝을 이용한 주변 무선단말 파악방안,이웅섭 /Lee， Woong-sup,2018,한국정보통신학회,"최근 단말-대-단말(Device-to-device, D2D) 통신기술이 차세대 무선통신시스템의 핵심기술로 큰 관심을 받고 있다. 이러한 단말간 통신에서는 자신의 주변에 어떠한 단말이 있는지 파악하는 주변단말 탐색(Neighbor discovery)이매우 중요하다. 본 논문에서는 최근 큰 관심을 받고 있는 딥러닝(Deep learning) 기술을 활용하여 단말간 통신에서 주변단말을 파악하는 방안에 대해서 제안한다. 제안 방안은 기존의 방안과 달리 무선채널의 공간적 연관성을 이용하여단말간의 신호 전송 없이 단말이 기지국으로 전송하는 상향링크 파일럿 신호를 기반으로 주변 단말을 찾고 따라서기존의 방식에 비해 신호전송 복잡도(signaling complexity)를 크게 줄일 수 있다. 또한 제안 방안에서는 떨어져 있는거리에 따라서 주변 단말을 분류 가능하여 기존 방안에 비해서 좀 더 세밀한 단말 탐색이 가능하다. 마지막으로 본 논문에서는 tensorflow를 이용한 컴퓨터 시뮬레이션을 통해 제안 방안의 성능을 검증하였다.Recently, the device-to-device (D2D) communication has been conceived as the key technology for the next-generationmobile communication systems. The neighbor discovery in which the nearby users are found, is essential for the properoperation of the D2D communication. In this paper, we propose new neighbor discovery scheme based on deep learningtechnology which has gained a lot of attention recently. In the proposed scheme, the neighboring users can be foundusing the uplink pilot transmission of users only, unlike conventional neighbor discovery schemes in which direct pilotcommunication among users is required, such that the signaling overhead can be greatly reduced in our proposed scheme.
Moreover, the neighbors with different proximity can also be classified accordingly which enables more accurate neighbordiscovery compared to the conventional schemes. The performance of our proposed scheme is verified through thetensorflow-based computer simulations.
키워드 : 딥러닝, 주변단말 탐색, 단말-대-단말 통신Deep learning, Neighbor discovery, Device-to-device communication, Wireless channel correlation"
491,신뢰성있는 딥러닝 기반 분석 모델을 참조하기 위한 딥러닝 기술 언어,문종혁 /Jong Hyeok Mun,2021,한국정보처리학회,"최근 딥러닝은 하드웨어 성능이 향상됨에 따라 자연어 처리, 영상 인식 등의 다양한 기술에 접목되어 활용되고 있다. 이러한 기술들을 활용해 지능형교통 시스템(ITS), 스마트홈, 헬스케어 등의 산업분야에서 데이터를 분석하여 고속도로 속도위반 차량 검출, 에너지 사용량 제어, 응급상황 등과 같은고품질의 서비스를 제공하며, 고품질의 서비스를 제공하기 위해서는 정확도가 향상된 딥러닝 모델이 적용되어야 한다. 이를 위해 서비스 환경의 데이터를분석하기 위한 딥러닝 모델을 개발할 때, 개발자는 신뢰성이 검증된 최신의 딥러닝 모델을 적용할 수 있어야 한다. 이는 개발자가 참조하는 딥러닝 모델에적용된 학습 데이터셋의 정확도를 측정하여 검증할 수 있다. 이러한 검증을 위해서 개발자는 학습 데이터셋, 딥러닝의 계층구조 및 개발 환경 등과 같은내용을 포함하는 딥러닝 모델을 문서화하여 적용하기 위한 구조적인 정보가 필요하다. 본 논문에서는 신뢰성있는 딥러닝 기반 데이터 분석 모델을 참조하기위한 딥러닝 기술 언어를 제안한다. 제안하는 기술 언어는 신뢰성 있는 딥러닝 모델을 개발하는데 필요한 학습데이터셋, 개발 환경 및 설정 등의 정보와더불어 딥러닝 모델의 계층구조를 표현할 수 있다. 제안하는 딥러닝 기술 언어를 이용하여 개발자는 지능형 교통 시스템에서 참조하는 분석 모델의정확도를 검증할 수 있다. 실험에서는 제안하는 언어의 유효성을 검증하기 위해, 번호판 인식 모델을 중심으로 딥러닝 기술 문서의 적용과정을 보인다With the recent advancements of deep learning, companies such as smart home, healthcare, and intelligent transportation systems areutilizing its functionality to provide high-quality services for vehicle detection, emergency situation detection, and controlling energyconsumption. To provide reliable services in such sensitive systems, deep learning models are required to have high accuracy. In orderto develop a deep learning model for analyzing previously mentioned services, developers should utilize the state of the art deep learningmodels that have already been verified for higher accuracy. The developers can verify the accuracy of the referenced model by validatingthe model on the dataset. For this validation, the developer needs structural information to document and apply deep learning models,including metadata such as learning dataset, network architecture, and development environments. In this paper, we propose a descriptionlanguage that represents the network architecture of the deep learning model along with its metadata that are necessary to develop adeep learning model. Through the proposed description language, developers can easily verify the accuracy of the referenced deep learningmodel. Our experiments demonstrate the application scenario of a deep learning description document that focuses on the license platerecognition for the detection of illegally parked vehicles.Trusted Deep Learning, Model Reference, Deep Learning Description Language, Traffic Situation Analysis Model"
492,딥러닝 분석을 이용한 중국 역내·외 위안화 변동성 예측,이우식 /Lee Woosik,2016,한국데이터정보과학회,"2008년 글로벌 금융위기 이후 중국은 위안화 국제화의 점진적 추진을 진행하면서 중국상하이 외환시장과 중국홍콩 외환시장에서 거래되는 통화인 역내위안화와 역외위안화를 형성시켰다. 본 연구는 위안화 국제화와 점진적인 중국 자본계정 개방에 따라 급변하는 외환시장상황의 변동성을 정확하게 파악하기 위해서 GARCH모형 (일반화된 자기회귀 조건부이분산성모형)에 다단계인공신경망을 결합한 MLP-GARCH 모형과 GARCH모형과 기계학습의 일종인 딥러닝 (deep learning)을 통합한 DL-GARCH을 가지고 위안화 변동성예측을 비교 실험과 분석을 하였다. 비교분석 결과 DLGARCH 모형은 MLP-GARCH보다 모형 위안화 역내·외 환율변동성 예측 면에서 더욱 더 개선된예측값을 제공하였다. 그래서 이분산시계열모형을 딥러닝과 결합한 DL-GARCH 모형은 시계열의환율 변동성 예측 문제에 딥러닝을 응용할 수 있음을 확인하였다. 향후 이분산시계열과 결합된 딥러닝 모형은 다른 금융시계열 데이터에 응용하여 그 일반화 가능성을 높일 수 있을 것이다.The People’s Republic of China has vigorously been pursuing the internationalization of the Chinese Yuan or Renminbi after the financial crisis of 2008. In this view, an abrupt increase of use of the Chinese Yuan in the onshore and offshore markets are important milestones to be one of important currencies. One of the most frequently used methods to forecast volatility is GARCH model. Since a prediction error of the GARCH model has been reported quite high, a lot of efforts have been made to improve forecasting capability of the GARCH model. In this paper, we have proposed MLPGARCH and a DL-GARCH by employing Artificial Neural Network to the GARCH. In an application to forecasting Chinese Yuan volatility, we have successfully shown their overall outperformance in forecasting over the GARCH.Deep learning, DL-GARCH, GARCH, The Chinese Yuan, volatility."
493,딥러닝 기법을 이용한 내일강수 예측,하지훈,2016,한국지능시스템학회,"정확한 강수예측을 위해서는 예측인자 선정과 예측방법에 대한 선택이 매우 중요하다. 최근에는 강수예측 방법으로기계학습 기법이 많이 사용되고 있으며, 그 중에서도 특히 인공신경망을 사용한 강수예측 방법은 좋은 성능을 보였다. 본논문에서는 딥러닝 기법 중 하나인 DBN(deep belief network)를 이용한 새로운 강수예측 방법을 제안한다. DBN는 비지도사전 학습을 통해 초기 가중치를 설정하여 기존 인공신경망의 문제점을 보완한다. 예측인자로는 기온, 전일-전주 강수일, 태양과 달 궤도 관련 자료를 선정하였다. 기온과 전일-전주 강수일은 서울에서의 1974년부터 2013년까지 총 40년간의AWS(automatic weather system) 관측 자료를 사용하였고, 태양과 달의 궤도 관련 자료는 서울을 중심으로 계산한 결과를사용하였다. 전체 기간에서 일부는 학습 자료로 사용하여 예측모델을 생성하였고, 나머지를 생성한 모델의 검증 자료로사용하였다. 모델 검증 결과로 나온 예측값들은 확률값을 가지며 임계치를 이용하여 강수유무를 판별하였다. 강수 정확도의척도로 양분예보기법 중 CSI(critical successive index)와 Bias(frequency bias)를 계산하였다. 이를 통해 DBN와 MLP(multilayer perceptron)의 성능을 비교한 결과 DBN의 강수 예측 정확도가 높았고, 수행속도 또한 2배 이상 빨랐다.For accurate precipitation forecasts the choice of weather faoctrs and prediction method is very important. Recently, machinele arning has been widely used for forecasting precipitation, and artificial neural network, one of machine learning techniques, showed good performance. In this paper, we suggest a new method for forecasting precipitation using DBN, one of deep learning techniques.
DBN has an advantage that initial weights are set by unsupervised learning, so this compensates for the defects of artificial neural networks. We used past precipitation, temperature, and the parameters of the sun and moon's motion as features for forecasting precipitation. The dataset consists of observation data which had been measured for 40 years from AWS in Seoul. Experiments were based on 8-fold cross validation. As a result of estimation, we got probabilities of test dataset, so threshold was used for the decision of precipitation. CSI and Bias were used for indicating the precision of precipitation. Our experimental results showed that DBN performed better than MLP.Deep learning, Deep belief network, Precipitation, Forecast"
494,딥러닝 신경망모형을 이용한 실시간 교통정보수집 연구,김대현 /KIM DAE HYON,2016,사단법인 인문사회과학기술융합학회,"비디오 기반의 실시간 교통정보수집 시스템은 자율주행을 포함한 지능형교통체계의 주요 서브시스템중 하나이며, 우수한 예측(추정)력 때문에 핵심 알고리즘으로 인공신경망 모형이 주로 사용되고 있다. 특히, 최근 과학 학술지 네이처에 구글 딥마인드의 알파고가 공개되면서 딥러닝과 인공지능에 대한 전 세계적인 관심이 집중되고 있다. 최근까지 가장 널리 사용된 인공신경망모형은 역전파 모형이었으나 최근 컴퓨터 하드웨어 및 알고리즘의 새로운 개발과 더불어 딥러닝 알고리즘을 이용한 딥인공신경망 모형에 대한 관심이 높아지고 있다. 그러나 아직 딥러닝 인공신경망에 대한 연구는 초기단계에 있는 실정이며 적용분야도 극히 한정되어 있다. 본 연구에서는 실시간 도로교통정보 수집을 위한 영상처리분야에서 딥러닝 알고리즘을 활용하고 실험결과를 통하여 향후 교통정보수집‧분석 분야에서의 딥러닝 인공신경망 모형의 적용가능성을 살펴보고자 한다. 특히, 본 연구에서 도출한 딥러닝 인공신경망모형의 실험결과를 기존 역전파 모형에 의한 실험결과와 비교하여 모형의 우수성을 비교‧평가하고자 한다.Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Normally, artificial neural networks have been used in real-time applications for collecting traffic information because of their remarkable performance in terms of predictive accuracy and real-time computing. Especially, the recent publication of a paper in the journal Nature and the success of Google DeepMind’s world-beating Go program AlphaGo caused significant increases of the public interest in artificial intelligence. Even though various types of learning models have been proposed, the Backpropagation has been the most popular neural network model. However, the Deep learning neural network model has attracted much interest recently. In this study, the Deep leaning neural network will be applied in image processing techniques for the real-time traffic information collection to investigate the performance of Deep learning algorithm in traffic engineering fields. In addition, the experimental results from Deep learing algorithm will be compared with the results from Backpropagation model to evaluate and propose better performing model.Intelligent Transportation Systems (ITS), artificial neural networks, Deep learning, traffic information, image processing"
495,딥러닝의 모형과 응용사례,안성만 /SungMahn Ahn,2016,한국지능정보시스템학회,"딥러닝은 인공신경망(neural network)이라는 인공지능분야의 모형이 발전된 형태로서, 계층구조로 이루어진 인공신경망의 내부계층(hidden layer)이 여러 단계로 이루어진 구조이다. 딥러닝에서의 주요 모형은 합성곱신경망(convolutional neural network), 순환신경망(recurrent neural network), 그리고 심층신뢰신경망(deep belief network)의 세가지라고 할 수있다. 그 중에서 현재 흥미로운 연구가 많이 발표되어서 관심이 집중되고 있는 모형은 지도학습(supervised learning)모형인 처음 두 개의 모형이다. 따라서 본 논문에서는 지도학습모형의 가중치를 최적화하는 기본적인 방법인 오류역전파 알고리즘을 살펴본 뒤에 합성곱신경망과 순환신경망의 구조와 응용사례 등을 살펴보고자 한다. 본문에서 다루지 않은 모형인 심층신뢰신경망은 아직까지는 합성곱신경망 이나 순환신경망보다는 상대적으로 주목을 덜 받고 있다. 그러나 심층신뢰신경망은 CNN이나 RNN과는 달리 비지도학습(unsupervised learning)모형이며, 사람이나 동물은 관찰을 통해서 스스로 학습한다는 점에서 궁극적으로는 비지도학습모형이 더 많이 연구되어야 할 주제가 될 것이다.Deep learning model is a kind of neural networks that allows multiple hidden layers. There are various deep learning architectures such as convolutional neural networks, deep belief networks and recurrent neural networks. Those have been applied to fields like computer vision, automatic speech recognition, natural language processing, audio recognition and bioinformatics where they have been shown to produce state-of-the-art results on various tasks. Among those architectures, convolutional neural networks and recurrent neural networks are classified as the supervised learning model. And in recent years, those supervised learning models have gained more popularity than unsupervised learning models such as deep belief networks, because supervised learning models have shown fashionable applications in such fields mentioned above.
Deep learning models can be trained with backpropagation algorithm. Backpropagation is an abbreviation for “backward propagation of errors” and a common method of training artificial neural networks used in conjunction with an optimization method such as gradient descent. The method calculates the gradient of an error function with respect to all the weights in the network. The gradient is fed to the optimization method which in turn uses it to update the weights, in an attempt to minimize the error function.
Convolutional neural networks use a special architecture which is particularly well-adapted to classify images. Using this architecture makes convolutional networks fast to train. This, in turn, helps us train deep, muti-layer networks, which are very good at classifying images. These days, deep convolutional networks are used in most neural networks for image recognition. Convolutional neural networks use three basic ideas: local receptive fields, shared weights, and pooling. By local receptive fields, we mean that each neuron in the first(or any) hidden layer will be connected to a small region of the input(or previous layer’s) neurons. Shared weights mean that we’re going to use the same weights and bias for each of the local receptive field. This means that all the neurons in the hidden layer detect exactly the same feature, just at different locations in the input image. In addition to the convolutional layers just described, convolutional neural networks also contain pooling layers. Pooling layers are usually used immediately after convolutional layers. What the pooling layers do is to simplify the information in the output from the convolutional layer.
Recent convolutional network architectures have 10 to 20 hidden layers and billions of connections between units. Training deep learning networks has taken weeks several years ago, but thanks to progress in GPU and algorithm enhancement, training time has reduced to several hours.
Neural networks with time-varying behavior are known as recurrent neural networks or RNNs. A recurrent neural network is a class of artificial neural network where connections between units form a directed cycle. This creates an internal state of the network which allows it to exhibit dynamic temporal behavior. Unlike feedforward neural networks, RNNs can use their internal memory to process arbitrary sequences of inputs. Early RNN models turned out to be very difficult to train, harder even than deep feedforward networks. The reason is the unstable gradient problem such as vanishing gradient and exploding gradient. The gradient can get smaller and smaller as it is propagated back through layers. This makes learning in early layers extremely slow. The problem actually gets worse in RNNs, since gradients aren’t just propagated backward through layers, they’re propagated backward through time. If the network runs for a long time, that can make the gradient extremely unstable and hard to learn from. It has been possible to incorporate an idea known as long short-term memory units (LSTMs) into RNNs. LSTMs make it much easier to get good results when training RNNs, and many recent papers make use of LSTMs or related ideas.Deep learning, Convolutional neural networks, Recurrent neural networks, Error backpropagation algorithm"
496,딥러닝분석과 기술적 분석 지표를 이용한 한국 코스피주가지수 방향성 예측,이우식 /Lee Woosik,2017,한국데이터정보과학회,"2016년 3월 구글 (Google)의 바둑인공지능 알파고 (AlphaGo)가 이세돌 9단과의 바둑대결에서승리한 이후 다양한 분야에서 인공지능 사용에 대한 관심이 높아지고 있는 가운데 금융투자 분야에서도 인공지능과 투자자문 전문가의 합성어인 로보어드바이저 (Robo-Advisor)에 대한 관심이 높아지고 있다. 인공지능 (artificial intelligence)기반의 의사결정은 비용 절감은 물론 효과적인 의사결정을가능하게 한다는 점에서 큰 장점이 있다. 본 연구에서는 기술적 분석 (technical analysis) 지표와 딥러닝 (deep learning) 모형을 결합하여 한국 코스피 지수를 예측하는 모형을 개발하고 제시한 모형들의 예측력을 비교, 분석한다. 분석 결과 기술적 분석 지표에 딥러닝 알고리즘을 결합한 모형이 주가지수 방향성 예측 문제에 응용될 수 있음을 확인하였다. 향후 본 연구에서 제안된 기술적 분석 지표와딥러닝모형을 결합한 기법은 로보어드바이저서비스에 응용할 수 있는 일반화 가능성을 보여준다Since Google’s AlphaGo defeated a world champion of Go players in 2016, there have been many interests in the deep learning. In the financial sector, a Robo-Advisor using deep learning gains a significant attention, which builds and manages portfolios of financial instruments for investors.In this paper, we have proposed the a deep learning algorithm geared toward identification and forecast of the KOSPI index direction,and we also have compared the accuracy of the prediction.In an application of forecasting the financial market index direction, we have shown that the Robo-Advisor using deep learning has a significant effect on finance industry. The Robo-Advisor collects a massive data such as earnings statements, news reports and regulatory filings, analyzes those and recommends investors how to view market trends and identify the best time to purchase financial assets. On the other hand, the Robo-Advisor allows businesses to learn more about their customers, develop better marketing strategies, increase sales and decrease costs.Artificial intelligence, deep learning, FinTech, Robo-Advisor, technical analysis."
497,딥러닝의 효율적인 학습을 위한 학습데이터 선별 기법에 관한 연구,주경돈 /Gyoung-don Joo,2017,한국정보과학회,"빅데이터의 증가와 병렬 하드웨어 처리, 학습 알고리즘의 개선은 딥러닝의 발전을 가져왔으며 인공지능이 적용 가능한 영역을 획기적으로 확장시키고 있다. 하지만 이러한 대규모의 학습 데이터에 기반한 딥러닝은 학습시간이 증가되고, 더 나아가 학습 데이터의 구축을 위한 레이블링(labeling) 비용이 데이터 수에 비례하여 증가하게 된다. 따라서 본 논문에서는, 딥러닝의 정확도를 최대한 유지하면서 필요한 학습 데이터의 수를 줄일수 있도록, 전체 데이터에서 제한된 수의 학습 데이터의 후보를 선별하는 기법에 대하여 연구한다. 특히 기존의 능동 학습에 기반한 기법들이 학습 데이터의 수가 매우 적어질 경우 딥러닝의 정확도가 급격히 떨어지는 문제를 해결할 수 있는 새로운 기법을 제안하였으며, 제안된 기법으로 선별된 학습 데이터는 그 수의 변화에 대해 딥러닝이 일관되게 경쟁력있는 정확도를 가짐을 다양한 실험을 통하여 검증하였다.The increase of big data, parallel hardware processing and enhanced learning algorithms have led to the development of deep learning and are dramatically expanding the domain of applications based on artificial intelligence. However, such large-scale training data extends the learning time, and furthermore, the labeling cost for building the training set increases in proportion to the volume of data. Therefore, in this paper, we study how to select a limited number of candidates for training set from given data so as to reduce the number of training set while maintaining the accuracy of deep learning. Especially, we found that, with the training set selected by the previous work, the accuracy of deep learning drastically decreases when the number of training set is very small and so proposed new methods to resolve this fall-off in accuracy. Through experiments, we showed that deep learning consistently has the competitive accuracy with the training set selected by the proposed method regardless of the size of training set.Deep Learning, Train Set Selection, Active Learning, Gamma distribution"
498,딥러닝을 이용한 범용적 스테그아날리시스,김현재 /Hyunjae Kim,2017,한국정보과학회,"스테그아날리시스(Steganalysis)란 이미지 등 일반적인 자료에 암호화된 정보를 은닉하는 스테가노그래피(Steganography)에 대한 검출 및 분석 방법으로, 기계학습 기반 방법론을 포함한다. 기존 기계학습 기반 스테그아날리시스는 영상(Image)의 특징(Feature) 추출 및 모델링에 기반하며, 최근 딥러닝(Deep Learning)의 적용으로 검출 정확도가 큰 폭으로 향상되었다. 하지만 현존하는 스테그아날리시스 모델은 단일 스테가노그래피 기법에 대해 국한되어 있어 학습에 사용되지 않은 스테고(Stego) 이미지의 경우 검출이 불가능한 결정적 한계를 가진다. 본 연구에서는 다양한 스테가노그래피 기법으로 생성된 스테고이미지에 딥러닝을 적용하여 스테그아날리시스를 학습하는 범용적 모델을 제안한다. 다양한 실험을 통해 제안 기법의 효용성 및 가능성을 확인하고, 범용적 스테그아날리시스 모델이 각각에 특화된 검출 기법과 유사한 정확도로 스테고 이미지를 검출할 수 있음을 보인다.Steganalysis is to detect information hidden by steganography inside general data such as images. There are stegoanalysis techniques that use machine learning (ML). Existing ML approaches to steganalysis are based on extracting features from stego images and modeling them.
Recently deep learning-based methodologies have shown significant improvements in detection accuracy. However, all the existing methods, including deep learning-based ones, have a critical limitation in that they can only detect stego images that are created by a specific steganography method. In this paper, we propose a generalized steganalysis method that can model multiple types of stego images using deep learning. Through various experiments, we confirm the effectiveness of our approach and envision directions for future research. In particular, we show that our method can detect each type of steganography with the same level of accuracy as that of a steganalysis method dedicated to that type of steganography, thereby demonstrating the general applicability of our approach to multiple types of stego images.steganalysis, steganography, deep learning, generalized steganalysis model"
499,"딥러닝 프레임워크의 비교: 티아노, 텐서플로, CNTK를 중심으로",정여진 /Yeojin Chung,2017,한국지능정보시스템학회,"딥러닝 프레임워크의 대표적인 기능으로는 ‘자동미분’과 ‘GPU의 활용’ 등을 들 수 있다. 본 논문은 파이썬의 라이브러리 형태로 사용 가능한 프레임워크 중에서 구글의 텐서플로와 마이크로소프트의 CNTK, 그리고 텐서 플로의 원조라고 할 수 있는 티아노를 비교하였다. 본문에서는 자동미분의 개념과 GPU의 활용형태를 간단히 설명하고, 그 다음에 logistic regression을 실행하는 예를 통하여 각 프레임워크의 문법을 알아본 뒤에, 마지막으로 대표적인 딥러닝 응용인 CNN의 예제를 실행시켜보고 코딩의 편의성과 실행속도 등을 확인해 보았다. 그 결과, 편의성의 관점에서 보면 티아노가 가장 코딩 하기가 어렵고, CNTK와 텐서플로는 많은 부분이 비슷하게 추상화 되어 있어서 코딩이 비슷하지만 가중치와 편향을 직접 정의하느냐의 여부에서 차이를 보였다. 그리고 각프레임워크의 실행속도에 대한 평가는 ‘큰 차이는 없다’는 것이다. 텐서플로는 티아노에 비하여 속도가 느리다는 평가가 있어왔는데, 본 연구의 실험에 의하면, 비록 CNN 모형에 국한되었지만, 텐서플로가 아주 조금이지만빠른 것으로 나타났다. CNTK의 경우에도, 비록 실험환경이 달랐지만, 실험환경의 차이에 의한 속도의 차이의 편차범위 이내에 있는 것으로 판단이 되었다. 본 연구에서는 세 종류의 딥러닝 프레임워크만을 살펴보았는데,위키피디아에 따르면 딥러닝 프레임워크의 종류는 12가지가 있으며, 각 프레임워크의 특징을 15가지 속성으로구분하여 차이를 특정하고 있다. 그 많은 속성 중에서 사용자의 입장에서 볼 때 중요한 속성은 어떤 언어(파이썬, C++, Java, 등)로 사용가능한지, 어떤 딥러닝 모형에 대한 라이브러리가 잘 구현되어 있는지 등일 것이다. 그리고 사용자가 대규모의 딥러닝 모형을 구축한다면, 다중 GPU 혹은 다중 서버를 지원하는지의 여부도 중요할 것이다. 또한 딥러닝 모형을 처음 학습하는 경우에는 사용설명서가 많은지 예제 프로그램이 많은지 여부도 중요한 기준이 될 것이다.The deep learning framework is software designed to help develop deep learning models. Some of its important functions include “automatic differentiation” and “utilization of GPU”. The list of popular deep learning framework includes Caffe (BVLC) and Theano (University of Montreal). And recently, Microsoft's deep learning framework, Microsoft Cognitive Toolkit, was released as open-source license, following Google’s Tensorflow a year earlier. The early deep learning frameworks have been developed mainly for research at universities. Beginning with the inception of Tensorflow, however, it seems that companies such as Microsoft and Facebook have started to join the competition of framework development. Given the trend, Google and other companies are expected to continue investing in the deep learning framework to bring forward the initiative in the artificial intelligence business. From this point of view, we think it is a good time to compare some of deep learning frameworks. So we compare three deep learning frameworks which can be used as a Python library. Those are Google's Tensorflow, Microsoft’s CNTK, and Theano which is sort of a predecessor of the preceding two.
The most common and important function of deep learning frameworks is the ability to perform automatic differentiation. Basically all the mathematical expressions of deep learning models can be represented as computational graphs, which consist of nodes and edges. Partial derivatives on each edge of a computational graph can then be obtained. With the partial derivatives, we can let software compute differentiation of any node with respect to any variable by utilizing chain rule of Calculus.
First of all, the convenience of coding is in the order of CNTK, Tensorflow, and Theano. The criterion is simply based on the lengths of the codes and the learning curve and the ease of coding are not the main concern. According to the criteria, Theano was the most difficult to implement with, and CNTK and Tensorflow were somewhat easier. With Tensorflow, we need to define weight variables and biases explicitly. The reason that CNTK and Tensorflow are easier to implement with is that those frameworks provide us with more abstraction than Theano. We, however, need to mention that low-level coding is not always bad. It gives us flexibility of coding. With the low-level coding such as in Theano, we can implement and test any new deep learning models or any new search methods that we can think of.
The assessment of the execution speed of each framework is that there is not meaningful difference. According to the experiment, execution speeds of Theano and Tensorflow are very similar, although the experiment was limited to a CNN model. In the case of CNTK, the experimental environment was not maintained as the same. The code written in CNTK has to be run in PC environment without GPU where codes execute as much as 50 times slower than with GPU. But we concluded that the difference of execution speed was within the range of variation caused by the different hardware setup.
In this study, we compared three types of deep learning framework: Theano, Tensorflow, and CNTK. According to Wikipedia, there are 12 available deep learning frameworks. And 15 different attributes differentiate each framework. Some of the important attributes would include interface language (Python, C ++, Java, etc.) and the availability of libraries on various deep learning models such as CNN, RNN, DBN, and etc. And if a user implements a large scale deep learning model, it will also be important to support multiple GPU or multiple servers. Also, if you are learning the deep learning model, it would also be important if there are enough examples and references.deep learning framework, Theano, TensorFlow, CNTK, computational graph, CIFAR-10"
500,딥러닝을 이용한 주변 무선단말 파악방안,이웅섭 /Lee， Woong-sup,2018,한국정보통신학회,"최근 단말-대-단말(Device-to-device, D2D) 통신기술이 차세대 무선통신시스템의 핵심기술로 큰 관심을 받고 있다. 이러한 단말간 통신에서는 자신의 주변에 어떠한 단말이 있는지 파악하는 주변단말 탐색(Neighbor discovery)이매우 중요하다. 본 논문에서는 최근 큰 관심을 받고 있는 딥러닝(Deep learning) 기술을 활용하여 단말간 통신에서 주변단말을 파악하는 방안에 대해서 제안한다. 제안 방안은 기존의 방안과 달리 무선채널의 공간적 연관성을 이용하여단말간의 신호 전송 없이 단말이 기지국으로 전송하는 상향링크 파일럿 신호를 기반으로 주변 단말을 찾고 따라서기존의 방식에 비해 신호전송 복잡도(signaling complexity)를 크게 줄일 수 있다. 또한 제안 방안에서는 떨어져 있는거리에 따라서 주변 단말을 분류 가능하여 기존 방안에 비해서 좀 더 세밀한 단말 탐색이 가능하다. 마지막으로 본 논문에서는 tensorflow를 이용한 컴퓨터 시뮬레이션을 통해 제안 방안의 성능을 검증하였다.Recently, the device-to-device (D2D) communication has been conceived as the key technology for the next-generationmobile communication systems. The neighbor discovery in which the nearby users are found, is essential for the properoperation of the D2D communication. In this paper, we propose new neighbor discovery scheme based on deep learningtechnology which has gained a lot of attention recently. In the proposed scheme, the neighboring users can be foundusing the uplink pilot transmission of users only, unlike conventional neighbor discovery schemes in which direct pilotcommunication among users is required, such that the signaling overhead can be greatly reduced in our proposed scheme.
Moreover, the neighbors with different proximity can also be classified accordingly which enables more accurate neighbordiscovery compared to the conventional schemes. The performance of our proposed scheme is verified through thetensorflow-based computer simulations.
키워드 : 딥러닝, 주변단말 탐색, 단말-대-단말 통신Deep learning, Neighbor discovery, Device-to-device communication, Wireless channel correlation"
501,신뢰성있는 딥러닝 기반 분석 모델을 참조하기 위한 딥러닝 기술 언어,문종혁 /Jong Hyeok Mun,2021,한국정보처리학회,"최근 딥러닝은 하드웨어 성능이 향상됨에 따라 자연어 처리, 영상 인식 등의 다양한 기술에 접목되어 활용되고 있다. 이러한 기술들을 활용해 지능형교통 시스템(ITS), 스마트홈, 헬스케어 등의 산업분야에서 데이터를 분석하여 고속도로 속도위반 차량 검출, 에너지 사용량 제어, 응급상황 등과 같은고품질의 서비스를 제공하며, 고품질의 서비스를 제공하기 위해서는 정확도가 향상된 딥러닝 모델이 적용되어야 한다. 이를 위해 서비스 환경의 데이터를분석하기 위한 딥러닝 모델을 개발할 때, 개발자는 신뢰성이 검증된 최신의 딥러닝 모델을 적용할 수 있어야 한다. 이는 개발자가 참조하는 딥러닝 모델에적용된 학습 데이터셋의 정확도를 측정하여 검증할 수 있다. 이러한 검증을 위해서 개발자는 학습 데이터셋, 딥러닝의 계층구조 및 개발 환경 등과 같은내용을 포함하는 딥러닝 모델을 문서화하여 적용하기 위한 구조적인 정보가 필요하다. 본 논문에서는 신뢰성있는 딥러닝 기반 데이터 분석 모델을 참조하기위한 딥러닝 기술 언어를 제안한다. 제안하는 기술 언어는 신뢰성 있는 딥러닝 모델을 개발하는데 필요한 학습데이터셋, 개발 환경 및 설정 등의 정보와더불어 딥러닝 모델의 계층구조를 표현할 수 있다. 제안하는 딥러닝 기술 언어를 이용하여 개발자는 지능형 교통 시스템에서 참조하는 분석 모델의정확도를 검증할 수 있다. 실험에서는 제안하는 언어의 유효성을 검증하기 위해, 번호판 인식 모델을 중심으로 딥러닝 기술 문서의 적용과정을 보인다With the recent advancements of deep learning, companies such as smart home, healthcare, and intelligent transportation systems areutilizing its functionality to provide high-quality services for vehicle detection, emergency situation detection, and controlling energyconsumption. To provide reliable services in such sensitive systems, deep learning models are required to have high accuracy. In orderto develop a deep learning model for analyzing previously mentioned services, developers should utilize the state of the art deep learningmodels that have already been verified for higher accuracy. The developers can verify the accuracy of the referenced model by validatingthe model on the dataset. For this validation, the developer needs structural information to document and apply deep learning models,including metadata such as learning dataset, network architecture, and development environments. In this paper, we propose a descriptionlanguage that represents the network architecture of the deep learning model along with its metadata that are necessary to develop adeep learning model. Through the proposed description language, developers can easily verify the accuracy of the referenced deep learningmodel. Our experiments demonstrate the application scenario of a deep learning description document that focuses on the license platerecognition for the detection of illegally parked vehicles.Trusted Deep Learning, Model Reference, Deep Learning Description Language, Traffic Situation Analysis Model"
502,딥러닝 분석을 이용한 중국 역내·외 위안화 변동성 예측,이우식 /Lee Woosik,2016,한국데이터정보과학회,"2008년 글로벌 금융위기 이후 중국은 위안화 국제화의 점진적 추진을 진행하면서 중국상하이 외환시장과 중국홍콩 외환시장에서 거래되는 통화인 역내위안화와 역외위안화를 형성시켰다. 본 연구는 위안화 국제화와 점진적인 중국 자본계정 개방에 따라 급변하는 외환시장상황의 변동성을 정확하게 파악하기 위해서 GARCH모형 (일반화된 자기회귀 조건부이분산성모형)에 다단계인공신경망을 결합한 MLP-GARCH 모형과 GARCH모형과 기계학습의 일종인 딥러닝 (deep learning)을 통합한 DL-GARCH을 가지고 위안화 변동성예측을 비교 실험과 분석을 하였다. 비교분석 결과 DLGARCH 모형은 MLP-GARCH보다 모형 위안화 역내·외 환율변동성 예측 면에서 더욱 더 개선된예측값을 제공하였다. 그래서 이분산시계열모형을 딥러닝과 결합한 DL-GARCH 모형은 시계열의환율 변동성 예측 문제에 딥러닝을 응용할 수 있음을 확인하였다. 향후 이분산시계열과 결합된 딥러닝 모형은 다른 금융시계열 데이터에 응용하여 그 일반화 가능성을 높일 수 있을 것이다.The People’s Republic of China has vigorously been pursuing the internationalization of the Chinese Yuan or Renminbi after the financial crisis of 2008. In this view, an abrupt increase of use of the Chinese Yuan in the onshore and offshore markets are important milestones to be one of important currencies. One of the most frequently used methods to forecast volatility is GARCH model. Since a prediction error of the GARCH model has been reported quite high, a lot of efforts have been made to improve forecasting capability of the GARCH model. In this paper, we have proposed MLPGARCH and a DL-GARCH by employing Artificial Neural Network to the GARCH. In an application to forecasting Chinese Yuan volatility, we have successfully shown their overall outperformance in forecasting over the GARCH.Deep learning, DL-GARCH, GARCH, The Chinese Yuan, volatility."
503,딥러닝 기법을 이용한 내일강수 예측,하지훈,2016,한국지능시스템학회,"정확한 강수예측을 위해서는 예측인자 선정과 예측방법에 대한 선택이 매우 중요하다. 최근에는 강수예측 방법으로기계학습 기법이 많이 사용되고 있으며, 그 중에서도 특히 인공신경망을 사용한 강수예측 방법은 좋은 성능을 보였다. 본논문에서는 딥러닝 기법 중 하나인 DBN(deep belief network)를 이용한 새로운 강수예측 방법을 제안한다. DBN는 비지도사전 학습을 통해 초기 가중치를 설정하여 기존 인공신경망의 문제점을 보완한다. 예측인자로는 기온, 전일-전주 강수일, 태양과 달 궤도 관련 자료를 선정하였다. 기온과 전일-전주 강수일은 서울에서의 1974년부터 2013년까지 총 40년간의AWS(automatic weather system) 관측 자료를 사용하였고, 태양과 달의 궤도 관련 자료는 서울을 중심으로 계산한 결과를사용하였다. 전체 기간에서 일부는 학습 자료로 사용하여 예측모델을 생성하였고, 나머지를 생성한 모델의 검증 자료로사용하였다. 모델 검증 결과로 나온 예측값들은 확률값을 가지며 임계치를 이용하여 강수유무를 판별하였다. 강수 정확도의척도로 양분예보기법 중 CSI(critical successive index)와 Bias(frequency bias)를 계산하였다. 이를 통해 DBN와 MLP(multilayer perceptron)의 성능을 비교한 결과 DBN의 강수 예측 정확도가 높았고, 수행속도 또한 2배 이상 빨랐다.For accurate precipitation forecasts the choice of weather faoctrs and prediction method is very important. Recently, machinele arning has been widely used for forecasting precipitation, and artificial neural network, one of machine learning techniques, showed good performance. In this paper, we suggest a new method for forecasting precipitation using DBN, one of deep learning techniques.
DBN has an advantage that initial weights are set by unsupervised learning, so this compensates for the defects of artificial neural networks. We used past precipitation, temperature, and the parameters of the sun and moon's motion as features for forecasting precipitation. The dataset consists of observation data which had been measured for 40 years from AWS in Seoul. Experiments were based on 8-fold cross validation. As a result of estimation, we got probabilities of test dataset, so threshold was used for the decision of precipitation. CSI and Bias were used for indicating the precision of precipitation. Our experimental results showed that DBN performed better than MLP.Deep learning, Deep belief network, Precipitation, Forecast"
504,딥러닝 신경망모형을 이용한 실시간 교통정보수집 연구,김대현 /KIM DAE HYON,2016,사단법인 인문사회과학기술융합학회,"비디오 기반의 실시간 교통정보수집 시스템은 자율주행을 포함한 지능형교통체계의 주요 서브시스템중 하나이며, 우수한 예측(추정)력 때문에 핵심 알고리즘으로 인공신경망 모형이 주로 사용되고 있다. 특히, 최근 과학 학술지 네이처에 구글 딥마인드의 알파고가 공개되면서 딥러닝과 인공지능에 대한 전 세계적인 관심이 집중되고 있다. 최근까지 가장 널리 사용된 인공신경망모형은 역전파 모형이었으나 최근 컴퓨터 하드웨어 및 알고리즘의 새로운 개발과 더불어 딥러닝 알고리즘을 이용한 딥인공신경망 모형에 대한 관심이 높아지고 있다. 그러나 아직 딥러닝 인공신경망에 대한 연구는 초기단계에 있는 실정이며 적용분야도 극히 한정되어 있다. 본 연구에서는 실시간 도로교통정보 수집을 위한 영상처리분야에서 딥러닝 알고리즘을 활용하고 실험결과를 통하여 향후 교통정보수집‧분석 분야에서의 딥러닝 인공신경망 모형의 적용가능성을 살펴보고자 한다. 특히, 본 연구에서 도출한 딥러닝 인공신경망모형의 실험결과를 기존 역전파 모형에 의한 실험결과와 비교하여 모형의 우수성을 비교‧평가하고자 한다.Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Normally, artificial neural networks have been used in real-time applications for collecting traffic information because of their remarkable performance in terms of predictive accuracy and real-time computing. Especially, the recent publication of a paper in the journal Nature and the success of Google DeepMind’s world-beating Go program AlphaGo caused significant increases of the public interest in artificial intelligence. Even though various types of learning models have been proposed, the Backpropagation has been the most popular neural network model. However, the Deep learning neural network model has attracted much interest recently. In this study, the Deep leaning neural network will be applied in image processing techniques for the real-time traffic information collection to investigate the performance of Deep learning algorithm in traffic engineering fields. In addition, the experimental results from Deep learing algorithm will be compared with the results from Backpropagation model to evaluate and propose better performing model.Intelligent Transportation Systems (ITS), artificial neural networks, Deep learning, traffic information, image processing"
505,딥러닝의 모형과 응용사례,안성만 /SungMahn Ahn,2016,한국지능정보시스템학회,"딥러닝은 인공신경망(neural network)이라는 인공지능분야의 모형이 발전된 형태로서, 계층구조로 이루어진 인공신경망의 내부계층(hidden layer)이 여러 단계로 이루어진 구조이다. 딥러닝에서의 주요 모형은 합성곱신경망(convolutional neural network), 순환신경망(recurrent neural network), 그리고 심층신뢰신경망(deep belief network)의 세가지라고 할 수있다. 그 중에서 현재 흥미로운 연구가 많이 발표되어서 관심이 집중되고 있는 모형은 지도학습(supervised learning)모형인 처음 두 개의 모형이다. 따라서 본 논문에서는 지도학습모형의 가중치를 최적화하는 기본적인 방법인 오류역전파 알고리즘을 살펴본 뒤에 합성곱신경망과 순환신경망의 구조와 응용사례 등을 살펴보고자 한다. 본문에서 다루지 않은 모형인 심층신뢰신경망은 아직까지는 합성곱신경망 이나 순환신경망보다는 상대적으로 주목을 덜 받고 있다. 그러나 심층신뢰신경망은 CNN이나 RNN과는 달리 비지도학습(unsupervised learning)모형이며, 사람이나 동물은 관찰을 통해서 스스로 학습한다는 점에서 궁극적으로는 비지도학습모형이 더 많이 연구되어야 할 주제가 될 것이다.Deep learning model is a kind of neural networks that allows multiple hidden layers. There are various deep learning architectures such as convolutional neural networks, deep belief networks and recurrent neural networks. Those have been applied to fields like computer vision, automatic speech recognition, natural language processing, audio recognition and bioinformatics where they have been shown to produce state-of-the-art results on various tasks. Among those architectures, convolutional neural networks and recurrent neural networks are classified as the supervised learning model. And in recent years, those supervised learning models have gained more popularity than unsupervised learning models such as deep belief networks, because supervised learning models have shown fashionable applications in such fields mentioned above.
Deep learning models can be trained with backpropagation algorithm. Backpropagation is an abbreviation for “backward propagation of errors” and a common method of training artificial neural networks used in conjunction with an optimization method such as gradient descent. The method calculates the gradient of an error function with respect to all the weights in the network. The gradient is fed to the optimization method which in turn uses it to update the weights, in an attempt to minimize the error function.
Convolutional neural networks use a special architecture which is particularly well-adapted to classify images. Using this architecture makes convolutional networks fast to train. This, in turn, helps us train deep, muti-layer networks, which are very good at classifying images. These days, deep convolutional networks are used in most neural networks for image recognition. Convolutional neural networks use three basic ideas: local receptive fields, shared weights, and pooling. By local receptive fields, we mean that each neuron in the first(or any) hidden layer will be connected to a small region of the input(or previous layer’s) neurons. Shared weights mean that we’re going to use the same weights and bias for each of the local receptive field. This means that all the neurons in the hidden layer detect exactly the same feature, just at different locations in the input image. In addition to the convolutional layers just described, convolutional neural networks also contain pooling layers. Pooling layers are usually used immediately after convolutional layers. What the pooling layers do is to simplify the information in the output from the convolutional layer.
Recent convolutional network architectures have 10 to 20 hidden layers and billions of connections between units. Training deep learning networks has taken weeks several years ago, but thanks to progress in GPU and algorithm enhancement, training time has reduced to several hours.
Neural networks with time-varying behavior are known as recurrent neural networks or RNNs. A recurrent neural network is a class of artificial neural network where connections between units form a directed cycle. This creates an internal state of the network which allows it to exhibit dynamic temporal behavior. Unlike feedforward neural networks, RNNs can use their internal memory to process arbitrary sequences of inputs. Early RNN models turned out to be very difficult to train, harder even than deep feedforward networks. The reason is the unstable gradient problem such as vanishing gradient and exploding gradient. The gradient can get smaller and smaller as it is propagated back through layers. This makes learning in early layers extremely slow. The problem actually gets worse in RNNs, since gradients aren’t just propagated backward through layers, they’re propagated backward through time. If the network runs for a long time, that can make the gradient extremely unstable and hard to learn from. It has been possible to incorporate an idea known as long short-term memory units (LSTMs) into RNNs. LSTMs make it much easier to get good results when training RNNs, and many recent papers make use of LSTMs or related ideas.Deep learning, Convolutional neural networks, Recurrent neural networks, Error backpropagation algorithm"
506,딥러닝분석과 기술적 분석 지표를 이용한 한국 코스피주가지수 방향성 예측,이우식 /Lee Woosik,2017,한국데이터정보과학회,"2016년 3월 구글 (Google)의 바둑인공지능 알파고 (AlphaGo)가 이세돌 9단과의 바둑대결에서승리한 이후 다양한 분야에서 인공지능 사용에 대한 관심이 높아지고 있는 가운데 금융투자 분야에서도 인공지능과 투자자문 전문가의 합성어인 로보어드바이저 (Robo-Advisor)에 대한 관심이 높아지고 있다. 인공지능 (artificial intelligence)기반의 의사결정은 비용 절감은 물론 효과적인 의사결정을가능하게 한다는 점에서 큰 장점이 있다. 본 연구에서는 기술적 분석 (technical analysis) 지표와 딥러닝 (deep learning) 모형을 결합하여 한국 코스피 지수를 예측하는 모형을 개발하고 제시한 모형들의 예측력을 비교, 분석한다. 분석 결과 기술적 분석 지표에 딥러닝 알고리즘을 결합한 모형이 주가지수 방향성 예측 문제에 응용될 수 있음을 확인하였다. 향후 본 연구에서 제안된 기술적 분석 지표와딥러닝모형을 결합한 기법은 로보어드바이저서비스에 응용할 수 있는 일반화 가능성을 보여준다Since Google’s AlphaGo defeated a world champion of Go players in 2016, there have been many interests in the deep learning. In the financial sector, a Robo-Advisor using deep learning gains a significant attention, which builds and manages portfolios of financial instruments for investors.In this paper, we have proposed the a deep learning algorithm geared toward identification and forecast of the KOSPI index direction,and we also have compared the accuracy of the prediction.In an application of forecasting the financial market index direction, we have shown that the Robo-Advisor using deep learning has a significant effect on finance industry. The Robo-Advisor collects a massive data such as earnings statements, news reports and regulatory filings, analyzes those and recommends investors how to view market trends and identify the best time to purchase financial assets. On the other hand, the Robo-Advisor allows businesses to learn more about their customers, develop better marketing strategies, increase sales and decrease costs.Artificial intelligence, deep learning, FinTech, Robo-Advisor, technical analysis."
507,딥러닝의 효율적인 학습을 위한 학습데이터 선별 기법에 관한 연구,주경돈 /Gyoung-don Joo,2017,한국정보과학회,"빅데이터의 증가와 병렬 하드웨어 처리, 학습 알고리즘의 개선은 딥러닝의 발전을 가져왔으며 인공지능이 적용 가능한 영역을 획기적으로 확장시키고 있다. 하지만 이러한 대규모의 학습 데이터에 기반한 딥러닝은 학습시간이 증가되고, 더 나아가 학습 데이터의 구축을 위한 레이블링(labeling) 비용이 데이터 수에 비례하여 증가하게 된다. 따라서 본 논문에서는, 딥러닝의 정확도를 최대한 유지하면서 필요한 학습 데이터의 수를 줄일수 있도록, 전체 데이터에서 제한된 수의 학습 데이터의 후보를 선별하는 기법에 대하여 연구한다. 특히 기존의 능동 학습에 기반한 기법들이 학습 데이터의 수가 매우 적어질 경우 딥러닝의 정확도가 급격히 떨어지는 문제를 해결할 수 있는 새로운 기법을 제안하였으며, 제안된 기법으로 선별된 학습 데이터는 그 수의 변화에 대해 딥러닝이 일관되게 경쟁력있는 정확도를 가짐을 다양한 실험을 통하여 검증하였다.The increase of big data, parallel hardware processing and enhanced learning algorithms have led to the development of deep learning and are dramatically expanding the domain of applications based on artificial intelligence. However, such large-scale training data extends the learning time, and furthermore, the labeling cost for building the training set increases in proportion to the volume of data. Therefore, in this paper, we study how to select a limited number of candidates for training set from given data so as to reduce the number of training set while maintaining the accuracy of deep learning. Especially, we found that, with the training set selected by the previous work, the accuracy of deep learning drastically decreases when the number of training set is very small and so proposed new methods to resolve this fall-off in accuracy. Through experiments, we showed that deep learning consistently has the competitive accuracy with the training set selected by the proposed method regardless of the size of training set.Deep Learning, Train Set Selection, Active Learning, Gamma distribution"
508,딥러닝을 이용한 범용적 스테그아날리시스,김현재 /Hyunjae Kim,2017,한국정보과학회,"스테그아날리시스(Steganalysis)란 이미지 등 일반적인 자료에 암호화된 정보를 은닉하는 스테가노그래피(Steganography)에 대한 검출 및 분석 방법으로, 기계학습 기반 방법론을 포함한다. 기존 기계학습 기반 스테그아날리시스는 영상(Image)의 특징(Feature) 추출 및 모델링에 기반하며, 최근 딥러닝(Deep Learning)의 적용으로 검출 정확도가 큰 폭으로 향상되었다. 하지만 현존하는 스테그아날리시스 모델은 단일 스테가노그래피 기법에 대해 국한되어 있어 학습에 사용되지 않은 스테고(Stego) 이미지의 경우 검출이 불가능한 결정적 한계를 가진다. 본 연구에서는 다양한 스테가노그래피 기법으로 생성된 스테고이미지에 딥러닝을 적용하여 스테그아날리시스를 학습하는 범용적 모델을 제안한다. 다양한 실험을 통해 제안 기법의 효용성 및 가능성을 확인하고, 범용적 스테그아날리시스 모델이 각각에 특화된 검출 기법과 유사한 정확도로 스테고 이미지를 검출할 수 있음을 보인다.Steganalysis is to detect information hidden by steganography inside general data such as images. There are stegoanalysis techniques that use machine learning (ML). Existing ML approaches to steganalysis are based on extracting features from stego images and modeling them.
Recently deep learning-based methodologies have shown significant improvements in detection accuracy. However, all the existing methods, including deep learning-based ones, have a critical limitation in that they can only detect stego images that are created by a specific steganography method. In this paper, we propose a generalized steganalysis method that can model multiple types of stego images using deep learning. Through various experiments, we confirm the effectiveness of our approach and envision directions for future research. In particular, we show that our method can detect each type of steganography with the same level of accuracy as that of a steganalysis method dedicated to that type of steganography, thereby demonstrating the general applicability of our approach to multiple types of stego images.steganalysis, steganography, deep learning, generalized steganalysis model"
509,"딥러닝 프레임워크의 비교: 티아노, 텐서플로, CNTK를 중심으로",정여진 /Yeojin Chung,2017,한국지능정보시스템학회,"딥러닝 프레임워크의 대표적인 기능으로는 ‘자동미분’과 ‘GPU의 활용’ 등을 들 수 있다. 본 논문은 파이썬의 라이브러리 형태로 사용 가능한 프레임워크 중에서 구글의 텐서플로와 마이크로소프트의 CNTK, 그리고 텐서 플로의 원조라고 할 수 있는 티아노를 비교하였다. 본문에서는 자동미분의 개념과 GPU의 활용형태를 간단히 설명하고, 그 다음에 logistic regression을 실행하는 예를 통하여 각 프레임워크의 문법을 알아본 뒤에, 마지막으로 대표적인 딥러닝 응용인 CNN의 예제를 실행시켜보고 코딩의 편의성과 실행속도 등을 확인해 보았다. 그 결과, 편의성의 관점에서 보면 티아노가 가장 코딩 하기가 어렵고, CNTK와 텐서플로는 많은 부분이 비슷하게 추상화 되어 있어서 코딩이 비슷하지만 가중치와 편향을 직접 정의하느냐의 여부에서 차이를 보였다. 그리고 각프레임워크의 실행속도에 대한 평가는 ‘큰 차이는 없다’는 것이다. 텐서플로는 티아노에 비하여 속도가 느리다는 평가가 있어왔는데, 본 연구의 실험에 의하면, 비록 CNN 모형에 국한되었지만, 텐서플로가 아주 조금이지만빠른 것으로 나타났다. CNTK의 경우에도, 비록 실험환경이 달랐지만, 실험환경의 차이에 의한 속도의 차이의 편차범위 이내에 있는 것으로 판단이 되었다. 본 연구에서는 세 종류의 딥러닝 프레임워크만을 살펴보았는데,위키피디아에 따르면 딥러닝 프레임워크의 종류는 12가지가 있으며, 각 프레임워크의 특징을 15가지 속성으로구분하여 차이를 특정하고 있다. 그 많은 속성 중에서 사용자의 입장에서 볼 때 중요한 속성은 어떤 언어(파이썬, C++, Java, 등)로 사용가능한지, 어떤 딥러닝 모형에 대한 라이브러리가 잘 구현되어 있는지 등일 것이다. 그리고 사용자가 대규모의 딥러닝 모형을 구축한다면, 다중 GPU 혹은 다중 서버를 지원하는지의 여부도 중요할 것이다. 또한 딥러닝 모형을 처음 학습하는 경우에는 사용설명서가 많은지 예제 프로그램이 많은지 여부도 중요한 기준이 될 것이다.The deep learning framework is software designed to help develop deep learning models. Some of its important functions include “automatic differentiation” and “utilization of GPU”. The list of popular deep learning framework includes Caffe (BVLC) and Theano (University of Montreal). And recently, Microsoft's deep learning framework, Microsoft Cognitive Toolkit, was released as open-source license, following Google’s Tensorflow a year earlier. The early deep learning frameworks have been developed mainly for research at universities. Beginning with the inception of Tensorflow, however, it seems that companies such as Microsoft and Facebook have started to join the competition of framework development. Given the trend, Google and other companies are expected to continue investing in the deep learning framework to bring forward the initiative in the artificial intelligence business. From this point of view, we think it is a good time to compare some of deep learning frameworks. So we compare three deep learning frameworks which can be used as a Python library. Those are Google's Tensorflow, Microsoft’s CNTK, and Theano which is sort of a predecessor of the preceding two.
The most common and important function of deep learning frameworks is the ability to perform automatic differentiation. Basically all the mathematical expressions of deep learning models can be represented as computational graphs, which consist of nodes and edges. Partial derivatives on each edge of a computational graph can then be obtained. With the partial derivatives, we can let software compute differentiation of any node with respect to any variable by utilizing chain rule of Calculus.
First of all, the convenience of coding is in the order of CNTK, Tensorflow, and Theano. The criterion is simply based on the lengths of the codes and the learning curve and the ease of coding are not the main concern. According to the criteria, Theano was the most difficult to implement with, and CNTK and Tensorflow were somewhat easier. With Tensorflow, we need to define weight variables and biases explicitly. The reason that CNTK and Tensorflow are easier to implement with is that those frameworks provide us with more abstraction than Theano. We, however, need to mention that low-level coding is not always bad. It gives us flexibility of coding. With the low-level coding such as in Theano, we can implement and test any new deep learning models or any new search methods that we can think of.
The assessment of the execution speed of each framework is that there is not meaningful difference. According to the experiment, execution speeds of Theano and Tensorflow are very similar, although the experiment was limited to a CNN model. In the case of CNTK, the experimental environment was not maintained as the same. The code written in CNTK has to be run in PC environment without GPU where codes execute as much as 50 times slower than with GPU. But we concluded that the difference of execution speed was within the range of variation caused by the different hardware setup.
In this study, we compared three types of deep learning framework: Theano, Tensorflow, and CNTK. According to Wikipedia, there are 12 available deep learning frameworks. And 15 different attributes differentiate each framework. Some of the important attributes would include interface language (Python, C ++, Java, etc.) and the availability of libraries on various deep learning models such as CNN, RNN, DBN, and etc. And if a user implements a large scale deep learning model, it will also be important to support multiple GPU or multiple servers. Also, if you are learning the deep learning model, it would also be important if there are enough examples and references.deep learning framework, Theano, TensorFlow, CNTK, computational graph, CIFAR-10"
510,딥러닝을 이용한 주변 무선단말 파악방안,이웅섭 /Lee， Woong-sup,2018,한국정보통신학회,"최근 단말-대-단말(Device-to-device, D2D) 통신기술이 차세대 무선통신시스템의 핵심기술로 큰 관심을 받고 있다. 이러한 단말간 통신에서는 자신의 주변에 어떠한 단말이 있는지 파악하는 주변단말 탐색(Neighbor discovery)이매우 중요하다. 본 논문에서는 최근 큰 관심을 받고 있는 딥러닝(Deep learning) 기술을 활용하여 단말간 통신에서 주변단말을 파악하는 방안에 대해서 제안한다. 제안 방안은 기존의 방안과 달리 무선채널의 공간적 연관성을 이용하여단말간의 신호 전송 없이 단말이 기지국으로 전송하는 상향링크 파일럿 신호를 기반으로 주변 단말을 찾고 따라서기존의 방식에 비해 신호전송 복잡도(signaling complexity)를 크게 줄일 수 있다. 또한 제안 방안에서는 떨어져 있는거리에 따라서 주변 단말을 분류 가능하여 기존 방안에 비해서 좀 더 세밀한 단말 탐색이 가능하다. 마지막으로 본 논문에서는 tensorflow를 이용한 컴퓨터 시뮬레이션을 통해 제안 방안의 성능을 검증하였다.Recently, the device-to-device (D2D) communication has been conceived as the key technology for the next-generationmobile communication systems. The neighbor discovery in which the nearby users are found, is essential for the properoperation of the D2D communication. In this paper, we propose new neighbor discovery scheme based on deep learningtechnology which has gained a lot of attention recently. In the proposed scheme, the neighboring users can be foundusing the uplink pilot transmission of users only, unlike conventional neighbor discovery schemes in which direct pilotcommunication among users is required, such that the signaling overhead can be greatly reduced in our proposed scheme.
Moreover, the neighbors with different proximity can also be classified accordingly which enables more accurate neighbordiscovery compared to the conventional schemes. The performance of our proposed scheme is verified through thetensorflow-based computer simulations.
키워드 : 딥러닝, 주변단말 탐색, 단말-대-단말 통신Deep learning, Neighbor discovery, Device-to-device communication, Wireless channel correlation"
511,신뢰성있는 딥러닝 기반 분석 모델을 참조하기 위한 딥러닝 기술 언어,문종혁 /Jong Hyeok Mun,2021,한국정보처리학회,"최근 딥러닝은 하드웨어 성능이 향상됨에 따라 자연어 처리, 영상 인식 등의 다양한 기술에 접목되어 활용되고 있다. 이러한 기술들을 활용해 지능형교통 시스템(ITS), 스마트홈, 헬스케어 등의 산업분야에서 데이터를 분석하여 고속도로 속도위반 차량 검출, 에너지 사용량 제어, 응급상황 등과 같은고품질의 서비스를 제공하며, 고품질의 서비스를 제공하기 위해서는 정확도가 향상된 딥러닝 모델이 적용되어야 한다. 이를 위해 서비스 환경의 데이터를분석하기 위한 딥러닝 모델을 개발할 때, 개발자는 신뢰성이 검증된 최신의 딥러닝 모델을 적용할 수 있어야 한다. 이는 개발자가 참조하는 딥러닝 모델에적용된 학습 데이터셋의 정확도를 측정하여 검증할 수 있다. 이러한 검증을 위해서 개발자는 학습 데이터셋, 딥러닝의 계층구조 및 개발 환경 등과 같은내용을 포함하는 딥러닝 모델을 문서화하여 적용하기 위한 구조적인 정보가 필요하다. 본 논문에서는 신뢰성있는 딥러닝 기반 데이터 분석 모델을 참조하기위한 딥러닝 기술 언어를 제안한다. 제안하는 기술 언어는 신뢰성 있는 딥러닝 모델을 개발하는데 필요한 학습데이터셋, 개발 환경 및 설정 등의 정보와더불어 딥러닝 모델의 계층구조를 표현할 수 있다. 제안하는 딥러닝 기술 언어를 이용하여 개발자는 지능형 교통 시스템에서 참조하는 분석 모델의정확도를 검증할 수 있다. 실험에서는 제안하는 언어의 유효성을 검증하기 위해, 번호판 인식 모델을 중심으로 딥러닝 기술 문서의 적용과정을 보인다With the recent advancements of deep learning, companies such as smart home, healthcare, and intelligent transportation systems areutilizing its functionality to provide high-quality services for vehicle detection, emergency situation detection, and controlling energyconsumption. To provide reliable services in such sensitive systems, deep learning models are required to have high accuracy. In orderto develop a deep learning model for analyzing previously mentioned services, developers should utilize the state of the art deep learningmodels that have already been verified for higher accuracy. The developers can verify the accuracy of the referenced model by validatingthe model on the dataset. For this validation, the developer needs structural information to document and apply deep learning models,including metadata such as learning dataset, network architecture, and development environments. In this paper, we propose a descriptionlanguage that represents the network architecture of the deep learning model along with its metadata that are necessary to develop adeep learning model. Through the proposed description language, developers can easily verify the accuracy of the referenced deep learningmodel. Our experiments demonstrate the application scenario of a deep learning description document that focuses on the license platerecognition for the detection of illegally parked vehicles.Trusted Deep Learning, Model Reference, Deep Learning Description Language, Traffic Situation Analysis Model"
512,딥러닝 분석을 이용한 중국 역내·외 위안화 변동성 예측,이우식 /Lee Woosik,2016,한국데이터정보과학회,"2008년 글로벌 금융위기 이후 중국은 위안화 국제화의 점진적 추진을 진행하면서 중국상하이 외환시장과 중국홍콩 외환시장에서 거래되는 통화인 역내위안화와 역외위안화를 형성시켰다. 본 연구는 위안화 국제화와 점진적인 중국 자본계정 개방에 따라 급변하는 외환시장상황의 변동성을 정확하게 파악하기 위해서 GARCH모형 (일반화된 자기회귀 조건부이분산성모형)에 다단계인공신경망을 결합한 MLP-GARCH 모형과 GARCH모형과 기계학습의 일종인 딥러닝 (deep learning)을 통합한 DL-GARCH을 가지고 위안화 변동성예측을 비교 실험과 분석을 하였다. 비교분석 결과 DLGARCH 모형은 MLP-GARCH보다 모형 위안화 역내·외 환율변동성 예측 면에서 더욱 더 개선된예측값을 제공하였다. 그래서 이분산시계열모형을 딥러닝과 결합한 DL-GARCH 모형은 시계열의환율 변동성 예측 문제에 딥러닝을 응용할 수 있음을 확인하였다. 향후 이분산시계열과 결합된 딥러닝 모형은 다른 금융시계열 데이터에 응용하여 그 일반화 가능성을 높일 수 있을 것이다.The People’s Republic of China has vigorously been pursuing the internationalization of the Chinese Yuan or Renminbi after the financial crisis of 2008. In this view, an abrupt increase of use of the Chinese Yuan in the onshore and offshore markets are important milestones to be one of important currencies. One of the most frequently used methods to forecast volatility is GARCH model. Since a prediction error of the GARCH model has been reported quite high, a lot of efforts have been made to improve forecasting capability of the GARCH model. In this paper, we have proposed MLPGARCH and a DL-GARCH by employing Artificial Neural Network to the GARCH. In an application to forecasting Chinese Yuan volatility, we have successfully shown their overall outperformance in forecasting over the GARCH.Deep learning, DL-GARCH, GARCH, The Chinese Yuan, volatility."
513,딥러닝 기법을 이용한 내일강수 예측,하지훈,2016,한국지능시스템학회,"정확한 강수예측을 위해서는 예측인자 선정과 예측방법에 대한 선택이 매우 중요하다. 최근에는 강수예측 방법으로기계학습 기법이 많이 사용되고 있으며, 그 중에서도 특히 인공신경망을 사용한 강수예측 방법은 좋은 성능을 보였다. 본논문에서는 딥러닝 기법 중 하나인 DBN(deep belief network)를 이용한 새로운 강수예측 방법을 제안한다. DBN는 비지도사전 학습을 통해 초기 가중치를 설정하여 기존 인공신경망의 문제점을 보완한다. 예측인자로는 기온, 전일-전주 강수일, 태양과 달 궤도 관련 자료를 선정하였다. 기온과 전일-전주 강수일은 서울에서의 1974년부터 2013년까지 총 40년간의AWS(automatic weather system) 관측 자료를 사용하였고, 태양과 달의 궤도 관련 자료는 서울을 중심으로 계산한 결과를사용하였다. 전체 기간에서 일부는 학습 자료로 사용하여 예측모델을 생성하였고, 나머지를 생성한 모델의 검증 자료로사용하였다. 모델 검증 결과로 나온 예측값들은 확률값을 가지며 임계치를 이용하여 강수유무를 판별하였다. 강수 정확도의척도로 양분예보기법 중 CSI(critical successive index)와 Bias(frequency bias)를 계산하였다. 이를 통해 DBN와 MLP(multilayer perceptron)의 성능을 비교한 결과 DBN의 강수 예측 정확도가 높았고, 수행속도 또한 2배 이상 빨랐다.For accurate precipitation forecasts the choice of weather faoctrs and prediction method is very important. Recently, machinele arning has been widely used for forecasting precipitation, and artificial neural network, one of machine learning techniques, showed good performance. In this paper, we suggest a new method for forecasting precipitation using DBN, one of deep learning techniques.
DBN has an advantage that initial weights are set by unsupervised learning, so this compensates for the defects of artificial neural networks. We used past precipitation, temperature, and the parameters of the sun and moon's motion as features for forecasting precipitation. The dataset consists of observation data which had been measured for 40 years from AWS in Seoul. Experiments were based on 8-fold cross validation. As a result of estimation, we got probabilities of test dataset, so threshold was used for the decision of precipitation. CSI and Bias were used for indicating the precision of precipitation. Our experimental results showed that DBN performed better than MLP.Deep learning, Deep belief network, Precipitation, Forecast"
514,딥러닝 신경망모형을 이용한 실시간 교통정보수집 연구,김대현 /KIM DAE HYON,2016,사단법인 인문사회과학기술융합학회,"비디오 기반의 실시간 교통정보수집 시스템은 자율주행을 포함한 지능형교통체계의 주요 서브시스템중 하나이며, 우수한 예측(추정)력 때문에 핵심 알고리즘으로 인공신경망 모형이 주로 사용되고 있다. 특히, 최근 과학 학술지 네이처에 구글 딥마인드의 알파고가 공개되면서 딥러닝과 인공지능에 대한 전 세계적인 관심이 집중되고 있다. 최근까지 가장 널리 사용된 인공신경망모형은 역전파 모형이었으나 최근 컴퓨터 하드웨어 및 알고리즘의 새로운 개발과 더불어 딥러닝 알고리즘을 이용한 딥인공신경망 모형에 대한 관심이 높아지고 있다. 그러나 아직 딥러닝 인공신경망에 대한 연구는 초기단계에 있는 실정이며 적용분야도 극히 한정되어 있다. 본 연구에서는 실시간 도로교통정보 수집을 위한 영상처리분야에서 딥러닝 알고리즘을 활용하고 실험결과를 통하여 향후 교통정보수집‧분석 분야에서의 딥러닝 인공신경망 모형의 적용가능성을 살펴보고자 한다. 특히, 본 연구에서 도출한 딥러닝 인공신경망모형의 실험결과를 기존 역전파 모형에 의한 실험결과와 비교하여 모형의 우수성을 비교‧평가하고자 한다.Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Normally, artificial neural networks have been used in real-time applications for collecting traffic information because of their remarkable performance in terms of predictive accuracy and real-time computing. Especially, the recent publication of a paper in the journal Nature and the success of Google DeepMind’s world-beating Go program AlphaGo caused significant increases of the public interest in artificial intelligence. Even though various types of learning models have been proposed, the Backpropagation has been the most popular neural network model. However, the Deep learning neural network model has attracted much interest recently. In this study, the Deep leaning neural network will be applied in image processing techniques for the real-time traffic information collection to investigate the performance of Deep learning algorithm in traffic engineering fields. In addition, the experimental results from Deep learing algorithm will be compared with the results from Backpropagation model to evaluate and propose better performing model.Intelligent Transportation Systems (ITS), artificial neural networks, Deep learning, traffic information, image processing"
515,딥러닝의 모형과 응용사례,안성만 /SungMahn Ahn,2016,한국지능정보시스템학회,"딥러닝은 인공신경망(neural network)이라는 인공지능분야의 모형이 발전된 형태로서, 계층구조로 이루어진 인공신경망의 내부계층(hidden layer)이 여러 단계로 이루어진 구조이다. 딥러닝에서의 주요 모형은 합성곱신경망(convolutional neural network), 순환신경망(recurrent neural network), 그리고 심층신뢰신경망(deep belief network)의 세가지라고 할 수있다. 그 중에서 현재 흥미로운 연구가 많이 발표되어서 관심이 집중되고 있는 모형은 지도학습(supervised learning)모형인 처음 두 개의 모형이다. 따라서 본 논문에서는 지도학습모형의 가중치를 최적화하는 기본적인 방법인 오류역전파 알고리즘을 살펴본 뒤에 합성곱신경망과 순환신경망의 구조와 응용사례 등을 살펴보고자 한다. 본문에서 다루지 않은 모형인 심층신뢰신경망은 아직까지는 합성곱신경망 이나 순환신경망보다는 상대적으로 주목을 덜 받고 있다. 그러나 심층신뢰신경망은 CNN이나 RNN과는 달리 비지도학습(unsupervised learning)모형이며, 사람이나 동물은 관찰을 통해서 스스로 학습한다는 점에서 궁극적으로는 비지도학습모형이 더 많이 연구되어야 할 주제가 될 것이다.Deep learning model is a kind of neural networks that allows multiple hidden layers. There are various deep learning architectures such as convolutional neural networks, deep belief networks and recurrent neural networks. Those have been applied to fields like computer vision, automatic speech recognition, natural language processing, audio recognition and bioinformatics where they have been shown to produce state-of-the-art results on various tasks. Among those architectures, convolutional neural networks and recurrent neural networks are classified as the supervised learning model. And in recent years, those supervised learning models have gained more popularity than unsupervised learning models such as deep belief networks, because supervised learning models have shown fashionable applications in such fields mentioned above.
Deep learning models can be trained with backpropagation algorithm. Backpropagation is an abbreviation for “backward propagation of errors” and a common method of training artificial neural networks used in conjunction with an optimization method such as gradient descent. The method calculates the gradient of an error function with respect to all the weights in the network. The gradient is fed to the optimization method which in turn uses it to update the weights, in an attempt to minimize the error function.
Convolutional neural networks use a special architecture which is particularly well-adapted to classify images. Using this architecture makes convolutional networks fast to train. This, in turn, helps us train deep, muti-layer networks, which are very good at classifying images. These days, deep convolutional networks are used in most neural networks for image recognition. Convolutional neural networks use three basic ideas: local receptive fields, shared weights, and pooling. By local receptive fields, we mean that each neuron in the first(or any) hidden layer will be connected to a small region of the input(or previous layer’s) neurons. Shared weights mean that we’re going to use the same weights and bias for each of the local receptive field. This means that all the neurons in the hidden layer detect exactly the same feature, just at different locations in the input image. In addition to the convolutional layers just described, convolutional neural networks also contain pooling layers. Pooling layers are usually used immediately after convolutional layers. What the pooling layers do is to simplify the information in the output from the convolutional layer.
Recent convolutional network architectures have 10 to 20 hidden layers and billions of connections between units. Training deep learning networks has taken weeks several years ago, but thanks to progress in GPU and algorithm enhancement, training time has reduced to several hours.
Neural networks with time-varying behavior are known as recurrent neural networks or RNNs. A recurrent neural network is a class of artificial neural network where connections between units form a directed cycle. This creates an internal state of the network which allows it to exhibit dynamic temporal behavior. Unlike feedforward neural networks, RNNs can use their internal memory to process arbitrary sequences of inputs. Early RNN models turned out to be very difficult to train, harder even than deep feedforward networks. The reason is the unstable gradient problem such as vanishing gradient and exploding gradient. The gradient can get smaller and smaller as it is propagated back through layers. This makes learning in early layers extremely slow. The problem actually gets worse in RNNs, since gradients aren’t just propagated backward through layers, they’re propagated backward through time. If the network runs for a long time, that can make the gradient extremely unstable and hard to learn from. It has been possible to incorporate an idea known as long short-term memory units (LSTMs) into RNNs. LSTMs make it much easier to get good results when training RNNs, and many recent papers make use of LSTMs or related ideas.Deep learning, Convolutional neural networks, Recurrent neural networks, Error backpropagation algorithm"
516,딥러닝분석과 기술적 분석 지표를 이용한 한국 코스피주가지수 방향성 예측,이우식 /Lee Woosik,2017,한국데이터정보과학회,"2016년 3월 구글 (Google)의 바둑인공지능 알파고 (AlphaGo)가 이세돌 9단과의 바둑대결에서승리한 이후 다양한 분야에서 인공지능 사용에 대한 관심이 높아지고 있는 가운데 금융투자 분야에서도 인공지능과 투자자문 전문가의 합성어인 로보어드바이저 (Robo-Advisor)에 대한 관심이 높아지고 있다. 인공지능 (artificial intelligence)기반의 의사결정은 비용 절감은 물론 효과적인 의사결정을가능하게 한다는 점에서 큰 장점이 있다. 본 연구에서는 기술적 분석 (technical analysis) 지표와 딥러닝 (deep learning) 모형을 결합하여 한국 코스피 지수를 예측하는 모형을 개발하고 제시한 모형들의 예측력을 비교, 분석한다. 분석 결과 기술적 분석 지표에 딥러닝 알고리즘을 결합한 모형이 주가지수 방향성 예측 문제에 응용될 수 있음을 확인하였다. 향후 본 연구에서 제안된 기술적 분석 지표와딥러닝모형을 결합한 기법은 로보어드바이저서비스에 응용할 수 있는 일반화 가능성을 보여준다Since Google’s AlphaGo defeated a world champion of Go players in 2016, there have been many interests in the deep learning. In the financial sector, a Robo-Advisor using deep learning gains a significant attention, which builds and manages portfolios of financial instruments for investors.In this paper, we have proposed the a deep learning algorithm geared toward identification and forecast of the KOSPI index direction,and we also have compared the accuracy of the prediction.In an application of forecasting the financial market index direction, we have shown that the Robo-Advisor using deep learning has a significant effect on finance industry. The Robo-Advisor collects a massive data such as earnings statements, news reports and regulatory filings, analyzes those and recommends investors how to view market trends and identify the best time to purchase financial assets. On the other hand, the Robo-Advisor allows businesses to learn more about their customers, develop better marketing strategies, increase sales and decrease costs.Artificial intelligence, deep learning, FinTech, Robo-Advisor, technical analysis."
517,딥러닝의 효율적인 학습을 위한 학습데이터 선별 기법에 관한 연구,주경돈 /Gyoung-don Joo,2017,한국정보과학회,"빅데이터의 증가와 병렬 하드웨어 처리, 학습 알고리즘의 개선은 딥러닝의 발전을 가져왔으며 인공지능이 적용 가능한 영역을 획기적으로 확장시키고 있다. 하지만 이러한 대규모의 학습 데이터에 기반한 딥러닝은 학습시간이 증가되고, 더 나아가 학습 데이터의 구축을 위한 레이블링(labeling) 비용이 데이터 수에 비례하여 증가하게 된다. 따라서 본 논문에서는, 딥러닝의 정확도를 최대한 유지하면서 필요한 학습 데이터의 수를 줄일수 있도록, 전체 데이터에서 제한된 수의 학습 데이터의 후보를 선별하는 기법에 대하여 연구한다. 특히 기존의 능동 학습에 기반한 기법들이 학습 데이터의 수가 매우 적어질 경우 딥러닝의 정확도가 급격히 떨어지는 문제를 해결할 수 있는 새로운 기법을 제안하였으며, 제안된 기법으로 선별된 학습 데이터는 그 수의 변화에 대해 딥러닝이 일관되게 경쟁력있는 정확도를 가짐을 다양한 실험을 통하여 검증하였다.The increase of big data, parallel hardware processing and enhanced learning algorithms have led to the development of deep learning and are dramatically expanding the domain of applications based on artificial intelligence. However, such large-scale training data extends the learning time, and furthermore, the labeling cost for building the training set increases in proportion to the volume of data. Therefore, in this paper, we study how to select a limited number of candidates for training set from given data so as to reduce the number of training set while maintaining the accuracy of deep learning. Especially, we found that, with the training set selected by the previous work, the accuracy of deep learning drastically decreases when the number of training set is very small and so proposed new methods to resolve this fall-off in accuracy. Through experiments, we showed that deep learning consistently has the competitive accuracy with the training set selected by the proposed method regardless of the size of training set.Deep Learning, Train Set Selection, Active Learning, Gamma distribution"
518,딥러닝을 이용한 범용적 스테그아날리시스,김현재 /Hyunjae Kim,2017,한국정보과학회,"스테그아날리시스(Steganalysis)란 이미지 등 일반적인 자료에 암호화된 정보를 은닉하는 스테가노그래피(Steganography)에 대한 검출 및 분석 방법으로, 기계학습 기반 방법론을 포함한다. 기존 기계학습 기반 스테그아날리시스는 영상(Image)의 특징(Feature) 추출 및 모델링에 기반하며, 최근 딥러닝(Deep Learning)의 적용으로 검출 정확도가 큰 폭으로 향상되었다. 하지만 현존하는 스테그아날리시스 모델은 단일 스테가노그래피 기법에 대해 국한되어 있어 학습에 사용되지 않은 스테고(Stego) 이미지의 경우 검출이 불가능한 결정적 한계를 가진다. 본 연구에서는 다양한 스테가노그래피 기법으로 생성된 스테고이미지에 딥러닝을 적용하여 스테그아날리시스를 학습하는 범용적 모델을 제안한다. 다양한 실험을 통해 제안 기법의 효용성 및 가능성을 확인하고, 범용적 스테그아날리시스 모델이 각각에 특화된 검출 기법과 유사한 정확도로 스테고 이미지를 검출할 수 있음을 보인다.Steganalysis is to detect information hidden by steganography inside general data such as images. There are stegoanalysis techniques that use machine learning (ML). Existing ML approaches to steganalysis are based on extracting features from stego images and modeling them.
Recently deep learning-based methodologies have shown significant improvements in detection accuracy. However, all the existing methods, including deep learning-based ones, have a critical limitation in that they can only detect stego images that are created by a specific steganography method. In this paper, we propose a generalized steganalysis method that can model multiple types of stego images using deep learning. Through various experiments, we confirm the effectiveness of our approach and envision directions for future research. In particular, we show that our method can detect each type of steganography with the same level of accuracy as that of a steganalysis method dedicated to that type of steganography, thereby demonstrating the general applicability of our approach to multiple types of stego images.steganalysis, steganography, deep learning, generalized steganalysis model"
519,"딥러닝 프레임워크의 비교: 티아노, 텐서플로, CNTK를 중심으로",정여진 /Yeojin Chung,2017,한국지능정보시스템학회,"딥러닝 프레임워크의 대표적인 기능으로는 ‘자동미분’과 ‘GPU의 활용’ 등을 들 수 있다. 본 논문은 파이썬의 라이브러리 형태로 사용 가능한 프레임워크 중에서 구글의 텐서플로와 마이크로소프트의 CNTK, 그리고 텐서 플로의 원조라고 할 수 있는 티아노를 비교하였다. 본문에서는 자동미분의 개념과 GPU의 활용형태를 간단히 설명하고, 그 다음에 logistic regression을 실행하는 예를 통하여 각 프레임워크의 문법을 알아본 뒤에, 마지막으로 대표적인 딥러닝 응용인 CNN의 예제를 실행시켜보고 코딩의 편의성과 실행속도 등을 확인해 보았다. 그 결과, 편의성의 관점에서 보면 티아노가 가장 코딩 하기가 어렵고, CNTK와 텐서플로는 많은 부분이 비슷하게 추상화 되어 있어서 코딩이 비슷하지만 가중치와 편향을 직접 정의하느냐의 여부에서 차이를 보였다. 그리고 각프레임워크의 실행속도에 대한 평가는 ‘큰 차이는 없다’는 것이다. 텐서플로는 티아노에 비하여 속도가 느리다는 평가가 있어왔는데, 본 연구의 실험에 의하면, 비록 CNN 모형에 국한되었지만, 텐서플로가 아주 조금이지만빠른 것으로 나타났다. CNTK의 경우에도, 비록 실험환경이 달랐지만, 실험환경의 차이에 의한 속도의 차이의 편차범위 이내에 있는 것으로 판단이 되었다. 본 연구에서는 세 종류의 딥러닝 프레임워크만을 살펴보았는데,위키피디아에 따르면 딥러닝 프레임워크의 종류는 12가지가 있으며, 각 프레임워크의 특징을 15가지 속성으로구분하여 차이를 특정하고 있다. 그 많은 속성 중에서 사용자의 입장에서 볼 때 중요한 속성은 어떤 언어(파이썬, C++, Java, 등)로 사용가능한지, 어떤 딥러닝 모형에 대한 라이브러리가 잘 구현되어 있는지 등일 것이다. 그리고 사용자가 대규모의 딥러닝 모형을 구축한다면, 다중 GPU 혹은 다중 서버를 지원하는지의 여부도 중요할 것이다. 또한 딥러닝 모형을 처음 학습하는 경우에는 사용설명서가 많은지 예제 프로그램이 많은지 여부도 중요한 기준이 될 것이다.The deep learning framework is software designed to help develop deep learning models. Some of its important functions include “automatic differentiation” and “utilization of GPU”. The list of popular deep learning framework includes Caffe (BVLC) and Theano (University of Montreal). And recently, Microsoft's deep learning framework, Microsoft Cognitive Toolkit, was released as open-source license, following Google’s Tensorflow a year earlier. The early deep learning frameworks have been developed mainly for research at universities. Beginning with the inception of Tensorflow, however, it seems that companies such as Microsoft and Facebook have started to join the competition of framework development. Given the trend, Google and other companies are expected to continue investing in the deep learning framework to bring forward the initiative in the artificial intelligence business. From this point of view, we think it is a good time to compare some of deep learning frameworks. So we compare three deep learning frameworks which can be used as a Python library. Those are Google's Tensorflow, Microsoft’s CNTK, and Theano which is sort of a predecessor of the preceding two.
The most common and important function of deep learning frameworks is the ability to perform automatic differentiation. Basically all the mathematical expressions of deep learning models can be represented as computational graphs, which consist of nodes and edges. Partial derivatives on each edge of a computational graph can then be obtained. With the partial derivatives, we can let software compute differentiation of any node with respect to any variable by utilizing chain rule of Calculus.
First of all, the convenience of coding is in the order of CNTK, Tensorflow, and Theano. The criterion is simply based on the lengths of the codes and the learning curve and the ease of coding are not the main concern. According to the criteria, Theano was the most difficult to implement with, and CNTK and Tensorflow were somewhat easier. With Tensorflow, we need to define weight variables and biases explicitly. The reason that CNTK and Tensorflow are easier to implement with is that those frameworks provide us with more abstraction than Theano. We, however, need to mention that low-level coding is not always bad. It gives us flexibility of coding. With the low-level coding such as in Theano, we can implement and test any new deep learning models or any new search methods that we can think of.
The assessment of the execution speed of each framework is that there is not meaningful difference. According to the experiment, execution speeds of Theano and Tensorflow are very similar, although the experiment was limited to a CNN model. In the case of CNTK, the experimental environment was not maintained as the same. The code written in CNTK has to be run in PC environment without GPU where codes execute as much as 50 times slower than with GPU. But we concluded that the difference of execution speed was within the range of variation caused by the different hardware setup.
In this study, we compared three types of deep learning framework: Theano, Tensorflow, and CNTK. According to Wikipedia, there are 12 available deep learning frameworks. And 15 different attributes differentiate each framework. Some of the important attributes would include interface language (Python, C ++, Java, etc.) and the availability of libraries on various deep learning models such as CNN, RNN, DBN, and etc. And if a user implements a large scale deep learning model, it will also be important to support multiple GPU or multiple servers. Also, if you are learning the deep learning model, it would also be important if there are enough examples and references.deep learning framework, Theano, TensorFlow, CNTK, computational graph, CIFAR-10"
520,딥러닝을 이용한 주변 무선단말 파악방안,이웅섭 /Lee， Woong-sup,2018,한국정보통신학회,"최근 단말-대-단말(Device-to-device, D2D) 통신기술이 차세대 무선통신시스템의 핵심기술로 큰 관심을 받고 있다. 이러한 단말간 통신에서는 자신의 주변에 어떠한 단말이 있는지 파악하는 주변단말 탐색(Neighbor discovery)이매우 중요하다. 본 논문에서는 최근 큰 관심을 받고 있는 딥러닝(Deep learning) 기술을 활용하여 단말간 통신에서 주변단말을 파악하는 방안에 대해서 제안한다. 제안 방안은 기존의 방안과 달리 무선채널의 공간적 연관성을 이용하여단말간의 신호 전송 없이 단말이 기지국으로 전송하는 상향링크 파일럿 신호를 기반으로 주변 단말을 찾고 따라서기존의 방식에 비해 신호전송 복잡도(signaling complexity)를 크게 줄일 수 있다. 또한 제안 방안에서는 떨어져 있는거리에 따라서 주변 단말을 분류 가능하여 기존 방안에 비해서 좀 더 세밀한 단말 탐색이 가능하다. 마지막으로 본 논문에서는 tensorflow를 이용한 컴퓨터 시뮬레이션을 통해 제안 방안의 성능을 검증하였다.Recently, the device-to-device (D2D) communication has been conceived as the key technology for the next-generationmobile communication systems. The neighbor discovery in which the nearby users are found, is essential for the properoperation of the D2D communication. In this paper, we propose new neighbor discovery scheme based on deep learningtechnology which has gained a lot of attention recently. In the proposed scheme, the neighboring users can be foundusing the uplink pilot transmission of users only, unlike conventional neighbor discovery schemes in which direct pilotcommunication among users is required, such that the signaling overhead can be greatly reduced in our proposed scheme.
Moreover, the neighbors with different proximity can also be classified accordingly which enables more accurate neighbordiscovery compared to the conventional schemes. The performance of our proposed scheme is verified through thetensorflow-based computer simulations.
키워드 : 딥러닝, 주변단말 탐색, 단말-대-단말 통신Deep learning, Neighbor discovery, Device-to-device communication, Wireless channel correlation"
521,신뢰성있는 딥러닝 기반 분석 모델을 참조하기 위한 딥러닝 기술 언어,문종혁 /Jong Hyeok Mun,2021,한국정보처리학회,"최근 딥러닝은 하드웨어 성능이 향상됨에 따라 자연어 처리, 영상 인식 등의 다양한 기술에 접목되어 활용되고 있다. 이러한 기술들을 활용해 지능형교통 시스템(ITS), 스마트홈, 헬스케어 등의 산업분야에서 데이터를 분석하여 고속도로 속도위반 차량 검출, 에너지 사용량 제어, 응급상황 등과 같은고품질의 서비스를 제공하며, 고품질의 서비스를 제공하기 위해서는 정확도가 향상된 딥러닝 모델이 적용되어야 한다. 이를 위해 서비스 환경의 데이터를분석하기 위한 딥러닝 모델을 개발할 때, 개발자는 신뢰성이 검증된 최신의 딥러닝 모델을 적용할 수 있어야 한다. 이는 개발자가 참조하는 딥러닝 모델에적용된 학습 데이터셋의 정확도를 측정하여 검증할 수 있다. 이러한 검증을 위해서 개발자는 학습 데이터셋, 딥러닝의 계층구조 및 개발 환경 등과 같은내용을 포함하는 딥러닝 모델을 문서화하여 적용하기 위한 구조적인 정보가 필요하다. 본 논문에서는 신뢰성있는 딥러닝 기반 데이터 분석 모델을 참조하기위한 딥러닝 기술 언어를 제안한다. 제안하는 기술 언어는 신뢰성 있는 딥러닝 모델을 개발하는데 필요한 학습데이터셋, 개발 환경 및 설정 등의 정보와더불어 딥러닝 모델의 계층구조를 표현할 수 있다. 제안하는 딥러닝 기술 언어를 이용하여 개발자는 지능형 교통 시스템에서 참조하는 분석 모델의정확도를 검증할 수 있다. 실험에서는 제안하는 언어의 유효성을 검증하기 위해, 번호판 인식 모델을 중심으로 딥러닝 기술 문서의 적용과정을 보인다With the recent advancements of deep learning, companies such as smart home, healthcare, and intelligent transportation systems areutilizing its functionality to provide high-quality services for vehicle detection, emergency situation detection, and controlling energyconsumption. To provide reliable services in such sensitive systems, deep learning models are required to have high accuracy. In orderto develop a deep learning model for analyzing previously mentioned services, developers should utilize the state of the art deep learningmodels that have already been verified for higher accuracy. The developers can verify the accuracy of the referenced model by validatingthe model on the dataset. For this validation, the developer needs structural information to document and apply deep learning models,including metadata such as learning dataset, network architecture, and development environments. In this paper, we propose a descriptionlanguage that represents the network architecture of the deep learning model along with its metadata that are necessary to develop adeep learning model. Through the proposed description language, developers can easily verify the accuracy of the referenced deep learningmodel. Our experiments demonstrate the application scenario of a deep learning description document that focuses on the license platerecognition for the detection of illegally parked vehicles.Trusted Deep Learning, Model Reference, Deep Learning Description Language, Traffic Situation Analysis Model"
522,딥러닝 분석을 이용한 중국 역내·외 위안화 변동성 예측,이우식 /Lee Woosik,2016,한국데이터정보과학회,"2008년 글로벌 금융위기 이후 중국은 위안화 국제화의 점진적 추진을 진행하면서 중국상하이 외환시장과 중국홍콩 외환시장에서 거래되는 통화인 역내위안화와 역외위안화를 형성시켰다. 본 연구는 위안화 국제화와 점진적인 중국 자본계정 개방에 따라 급변하는 외환시장상황의 변동성을 정확하게 파악하기 위해서 GARCH모형 (일반화된 자기회귀 조건부이분산성모형)에 다단계인공신경망을 결합한 MLP-GARCH 모형과 GARCH모형과 기계학습의 일종인 딥러닝 (deep learning)을 통합한 DL-GARCH을 가지고 위안화 변동성예측을 비교 실험과 분석을 하였다. 비교분석 결과 DLGARCH 모형은 MLP-GARCH보다 모형 위안화 역내·외 환율변동성 예측 면에서 더욱 더 개선된예측값을 제공하였다. 그래서 이분산시계열모형을 딥러닝과 결합한 DL-GARCH 모형은 시계열의환율 변동성 예측 문제에 딥러닝을 응용할 수 있음을 확인하였다. 향후 이분산시계열과 결합된 딥러닝 모형은 다른 금융시계열 데이터에 응용하여 그 일반화 가능성을 높일 수 있을 것이다.The People’s Republic of China has vigorously been pursuing the internationalization of the Chinese Yuan or Renminbi after the financial crisis of 2008. In this view, an abrupt increase of use of the Chinese Yuan in the onshore and offshore markets are important milestones to be one of important currencies. One of the most frequently used methods to forecast volatility is GARCH model. Since a prediction error of the GARCH model has been reported quite high, a lot of efforts have been made to improve forecasting capability of the GARCH model. In this paper, we have proposed MLPGARCH and a DL-GARCH by employing Artificial Neural Network to the GARCH. In an application to forecasting Chinese Yuan volatility, we have successfully shown their overall outperformance in forecasting over the GARCH.Deep learning, DL-GARCH, GARCH, The Chinese Yuan, volatility."
523,딥러닝 기법을 이용한 내일강수 예측,하지훈,2016,한국지능시스템학회,"정확한 강수예측을 위해서는 예측인자 선정과 예측방법에 대한 선택이 매우 중요하다. 최근에는 강수예측 방법으로기계학습 기법이 많이 사용되고 있으며, 그 중에서도 특히 인공신경망을 사용한 강수예측 방법은 좋은 성능을 보였다. 본논문에서는 딥러닝 기법 중 하나인 DBN(deep belief network)를 이용한 새로운 강수예측 방법을 제안한다. DBN는 비지도사전 학습을 통해 초기 가중치를 설정하여 기존 인공신경망의 문제점을 보완한다. 예측인자로는 기온, 전일-전주 강수일, 태양과 달 궤도 관련 자료를 선정하였다. 기온과 전일-전주 강수일은 서울에서의 1974년부터 2013년까지 총 40년간의AWS(automatic weather system) 관측 자료를 사용하였고, 태양과 달의 궤도 관련 자료는 서울을 중심으로 계산한 결과를사용하였다. 전체 기간에서 일부는 학습 자료로 사용하여 예측모델을 생성하였고, 나머지를 생성한 모델의 검증 자료로사용하였다. 모델 검증 결과로 나온 예측값들은 확률값을 가지며 임계치를 이용하여 강수유무를 판별하였다. 강수 정확도의척도로 양분예보기법 중 CSI(critical successive index)와 Bias(frequency bias)를 계산하였다. 이를 통해 DBN와 MLP(multilayer perceptron)의 성능을 비교한 결과 DBN의 강수 예측 정확도가 높았고, 수행속도 또한 2배 이상 빨랐다.For accurate precipitation forecasts the choice of weather faoctrs and prediction method is very important. Recently, machinele arning has been widely used for forecasting precipitation, and artificial neural network, one of machine learning techniques, showed good performance. In this paper, we suggest a new method for forecasting precipitation using DBN, one of deep learning techniques.
DBN has an advantage that initial weights are set by unsupervised learning, so this compensates for the defects of artificial neural networks. We used past precipitation, temperature, and the parameters of the sun and moon's motion as features for forecasting precipitation. The dataset consists of observation data which had been measured for 40 years from AWS in Seoul. Experiments were based on 8-fold cross validation. As a result of estimation, we got probabilities of test dataset, so threshold was used for the decision of precipitation. CSI and Bias were used for indicating the precision of precipitation. Our experimental results showed that DBN performed better than MLP.Deep learning, Deep belief network, Precipitation, Forecast"
524,딥러닝 신경망모형을 이용한 실시간 교통정보수집 연구,김대현 /KIM DAE HYON,2016,사단법인 인문사회과학기술융합학회,"비디오 기반의 실시간 교통정보수집 시스템은 자율주행을 포함한 지능형교통체계의 주요 서브시스템중 하나이며, 우수한 예측(추정)력 때문에 핵심 알고리즘으로 인공신경망 모형이 주로 사용되고 있다. 특히, 최근 과학 학술지 네이처에 구글 딥마인드의 알파고가 공개되면서 딥러닝과 인공지능에 대한 전 세계적인 관심이 집중되고 있다. 최근까지 가장 널리 사용된 인공신경망모형은 역전파 모형이었으나 최근 컴퓨터 하드웨어 및 알고리즘의 새로운 개발과 더불어 딥러닝 알고리즘을 이용한 딥인공신경망 모형에 대한 관심이 높아지고 있다. 그러나 아직 딥러닝 인공신경망에 대한 연구는 초기단계에 있는 실정이며 적용분야도 극히 한정되어 있다. 본 연구에서는 실시간 도로교통정보 수집을 위한 영상처리분야에서 딥러닝 알고리즘을 활용하고 실험결과를 통하여 향후 교통정보수집‧분석 분야에서의 딥러닝 인공신경망 모형의 적용가능성을 살펴보고자 한다. 특히, 본 연구에서 도출한 딥러닝 인공신경망모형의 실험결과를 기존 역전파 모형에 의한 실험결과와 비교하여 모형의 우수성을 비교‧평가하고자 한다.Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Normally, artificial neural networks have been used in real-time applications for collecting traffic information because of their remarkable performance in terms of predictive accuracy and real-time computing. Especially, the recent publication of a paper in the journal Nature and the success of Google DeepMind’s world-beating Go program AlphaGo caused significant increases of the public interest in artificial intelligence. Even though various types of learning models have been proposed, the Backpropagation has been the most popular neural network model. However, the Deep learning neural network model has attracted much interest recently. In this study, the Deep leaning neural network will be applied in image processing techniques for the real-time traffic information collection to investigate the performance of Deep learning algorithm in traffic engineering fields. In addition, the experimental results from Deep learing algorithm will be compared with the results from Backpropagation model to evaluate and propose better performing model.Intelligent Transportation Systems (ITS), artificial neural networks, Deep learning, traffic information, image processing"
525,딥러닝의 모형과 응용사례,안성만 /SungMahn Ahn,2016,한국지능정보시스템학회,"딥러닝은 인공신경망(neural network)이라는 인공지능분야의 모형이 발전된 형태로서, 계층구조로 이루어진 인공신경망의 내부계층(hidden layer)이 여러 단계로 이루어진 구조이다. 딥러닝에서의 주요 모형은 합성곱신경망(convolutional neural network), 순환신경망(recurrent neural network), 그리고 심층신뢰신경망(deep belief network)의 세가지라고 할 수있다. 그 중에서 현재 흥미로운 연구가 많이 발표되어서 관심이 집중되고 있는 모형은 지도학습(supervised learning)모형인 처음 두 개의 모형이다. 따라서 본 논문에서는 지도학습모형의 가중치를 최적화하는 기본적인 방법인 오류역전파 알고리즘을 살펴본 뒤에 합성곱신경망과 순환신경망의 구조와 응용사례 등을 살펴보고자 한다. 본문에서 다루지 않은 모형인 심층신뢰신경망은 아직까지는 합성곱신경망 이나 순환신경망보다는 상대적으로 주목을 덜 받고 있다. 그러나 심층신뢰신경망은 CNN이나 RNN과는 달리 비지도학습(unsupervised learning)모형이며, 사람이나 동물은 관찰을 통해서 스스로 학습한다는 점에서 궁극적으로는 비지도학습모형이 더 많이 연구되어야 할 주제가 될 것이다.Deep learning model is a kind of neural networks that allows multiple hidden layers. There are various deep learning architectures such as convolutional neural networks, deep belief networks and recurrent neural networks. Those have been applied to fields like computer vision, automatic speech recognition, natural language processing, audio recognition and bioinformatics where they have been shown to produce state-of-the-art results on various tasks. Among those architectures, convolutional neural networks and recurrent neural networks are classified as the supervised learning model. And in recent years, those supervised learning models have gained more popularity than unsupervised learning models such as deep belief networks, because supervised learning models have shown fashionable applications in such fields mentioned above.
Deep learning models can be trained with backpropagation algorithm. Backpropagation is an abbreviation for “backward propagation of errors” and a common method of training artificial neural networks used in conjunction with an optimization method such as gradient descent. The method calculates the gradient of an error function with respect to all the weights in the network. The gradient is fed to the optimization method which in turn uses it to update the weights, in an attempt to minimize the error function.
Convolutional neural networks use a special architecture which is particularly well-adapted to classify images. Using this architecture makes convolutional networks fast to train. This, in turn, helps us train deep, muti-layer networks, which are very good at classifying images. These days, deep convolutional networks are used in most neural networks for image recognition. Convolutional neural networks use three basic ideas: local receptive fields, shared weights, and pooling. By local receptive fields, we mean that each neuron in the first(or any) hidden layer will be connected to a small region of the input(or previous layer’s) neurons. Shared weights mean that we’re going to use the same weights and bias for each of the local receptive field. This means that all the neurons in the hidden layer detect exactly the same feature, just at different locations in the input image. In addition to the convolutional layers just described, convolutional neural networks also contain pooling layers. Pooling layers are usually used immediately after convolutional layers. What the pooling layers do is to simplify the information in the output from the convolutional layer.
Recent convolutional network architectures have 10 to 20 hidden layers and billions of connections between units. Training deep learning networks has taken weeks several years ago, but thanks to progress in GPU and algorithm enhancement, training time has reduced to several hours.
Neural networks with time-varying behavior are known as recurrent neural networks or RNNs. A recurrent neural network is a class of artificial neural network where connections between units form a directed cycle. This creates an internal state of the network which allows it to exhibit dynamic temporal behavior. Unlike feedforward neural networks, RNNs can use their internal memory to process arbitrary sequences of inputs. Early RNN models turned out to be very difficult to train, harder even than deep feedforward networks. The reason is the unstable gradient problem such as vanishing gradient and exploding gradient. The gradient can get smaller and smaller as it is propagated back through layers. This makes learning in early layers extremely slow. The problem actually gets worse in RNNs, since gradients aren’t just propagated backward through layers, they’re propagated backward through time. If the network runs for a long time, that can make the gradient extremely unstable and hard to learn from. It has been possible to incorporate an idea known as long short-term memory units (LSTMs) into RNNs. LSTMs make it much easier to get good results when training RNNs, and many recent papers make use of LSTMs or related ideas.Deep learning, Convolutional neural networks, Recurrent neural networks, Error backpropagation algorithm"
526,딥러닝분석과 기술적 분석 지표를 이용한 한국 코스피주가지수 방향성 예측,이우식 /Lee Woosik,2017,한국데이터정보과학회,"2016년 3월 구글 (Google)의 바둑인공지능 알파고 (AlphaGo)가 이세돌 9단과의 바둑대결에서승리한 이후 다양한 분야에서 인공지능 사용에 대한 관심이 높아지고 있는 가운데 금융투자 분야에서도 인공지능과 투자자문 전문가의 합성어인 로보어드바이저 (Robo-Advisor)에 대한 관심이 높아지고 있다. 인공지능 (artificial intelligence)기반의 의사결정은 비용 절감은 물론 효과적인 의사결정을가능하게 한다는 점에서 큰 장점이 있다. 본 연구에서는 기술적 분석 (technical analysis) 지표와 딥러닝 (deep learning) 모형을 결합하여 한국 코스피 지수를 예측하는 모형을 개발하고 제시한 모형들의 예측력을 비교, 분석한다. 분석 결과 기술적 분석 지표에 딥러닝 알고리즘을 결합한 모형이 주가지수 방향성 예측 문제에 응용될 수 있음을 확인하였다. 향후 본 연구에서 제안된 기술적 분석 지표와딥러닝모형을 결합한 기법은 로보어드바이저서비스에 응용할 수 있는 일반화 가능성을 보여준다Since Google’s AlphaGo defeated a world champion of Go players in 2016, there have been many interests in the deep learning. In the financial sector, a Robo-Advisor using deep learning gains a significant attention, which builds and manages portfolios of financial instruments for investors.In this paper, we have proposed the a deep learning algorithm geared toward identification and forecast of the KOSPI index direction,and we also have compared the accuracy of the prediction.In an application of forecasting the financial market index direction, we have shown that the Robo-Advisor using deep learning has a significant effect on finance industry. The Robo-Advisor collects a massive data such as earnings statements, news reports and regulatory filings, analyzes those and recommends investors how to view market trends and identify the best time to purchase financial assets. On the other hand, the Robo-Advisor allows businesses to learn more about their customers, develop better marketing strategies, increase sales and decrease costs.Artificial intelligence, deep learning, FinTech, Robo-Advisor, technical analysis."
527,딥러닝의 효율적인 학습을 위한 학습데이터 선별 기법에 관한 연구,주경돈 /Gyoung-don Joo,2017,한국정보과학회,"빅데이터의 증가와 병렬 하드웨어 처리, 학습 알고리즘의 개선은 딥러닝의 발전을 가져왔으며 인공지능이 적용 가능한 영역을 획기적으로 확장시키고 있다. 하지만 이러한 대규모의 학습 데이터에 기반한 딥러닝은 학습시간이 증가되고, 더 나아가 학습 데이터의 구축을 위한 레이블링(labeling) 비용이 데이터 수에 비례하여 증가하게 된다. 따라서 본 논문에서는, 딥러닝의 정확도를 최대한 유지하면서 필요한 학습 데이터의 수를 줄일수 있도록, 전체 데이터에서 제한된 수의 학습 데이터의 후보를 선별하는 기법에 대하여 연구한다. 특히 기존의 능동 학습에 기반한 기법들이 학습 데이터의 수가 매우 적어질 경우 딥러닝의 정확도가 급격히 떨어지는 문제를 해결할 수 있는 새로운 기법을 제안하였으며, 제안된 기법으로 선별된 학습 데이터는 그 수의 변화에 대해 딥러닝이 일관되게 경쟁력있는 정확도를 가짐을 다양한 실험을 통하여 검증하였다.The increase of big data, parallel hardware processing and enhanced learning algorithms have led to the development of deep learning and are dramatically expanding the domain of applications based on artificial intelligence. However, such large-scale training data extends the learning time, and furthermore, the labeling cost for building the training set increases in proportion to the volume of data. Therefore, in this paper, we study how to select a limited number of candidates for training set from given data so as to reduce the number of training set while maintaining the accuracy of deep learning. Especially, we found that, with the training set selected by the previous work, the accuracy of deep learning drastically decreases when the number of training set is very small and so proposed new methods to resolve this fall-off in accuracy. Through experiments, we showed that deep learning consistently has the competitive accuracy with the training set selected by the proposed method regardless of the size of training set.Deep Learning, Train Set Selection, Active Learning, Gamma distribution"
528,딥러닝을 이용한 범용적 스테그아날리시스,김현재 /Hyunjae Kim,2017,한국정보과학회,"스테그아날리시스(Steganalysis)란 이미지 등 일반적인 자료에 암호화된 정보를 은닉하는 스테가노그래피(Steganography)에 대한 검출 및 분석 방법으로, 기계학습 기반 방법론을 포함한다. 기존 기계학습 기반 스테그아날리시스는 영상(Image)의 특징(Feature) 추출 및 모델링에 기반하며, 최근 딥러닝(Deep Learning)의 적용으로 검출 정확도가 큰 폭으로 향상되었다. 하지만 현존하는 스테그아날리시스 모델은 단일 스테가노그래피 기법에 대해 국한되어 있어 학습에 사용되지 않은 스테고(Stego) 이미지의 경우 검출이 불가능한 결정적 한계를 가진다. 본 연구에서는 다양한 스테가노그래피 기법으로 생성된 스테고이미지에 딥러닝을 적용하여 스테그아날리시스를 학습하는 범용적 모델을 제안한다. 다양한 실험을 통해 제안 기법의 효용성 및 가능성을 확인하고, 범용적 스테그아날리시스 모델이 각각에 특화된 검출 기법과 유사한 정확도로 스테고 이미지를 검출할 수 있음을 보인다.Steganalysis is to detect information hidden by steganography inside general data such as images. There are stegoanalysis techniques that use machine learning (ML). Existing ML approaches to steganalysis are based on extracting features from stego images and modeling them.
Recently deep learning-based methodologies have shown significant improvements in detection accuracy. However, all the existing methods, including deep learning-based ones, have a critical limitation in that they can only detect stego images that are created by a specific steganography method. In this paper, we propose a generalized steganalysis method that can model multiple types of stego images using deep learning. Through various experiments, we confirm the effectiveness of our approach and envision directions for future research. In particular, we show that our method can detect each type of steganography with the same level of accuracy as that of a steganalysis method dedicated to that type of steganography, thereby demonstrating the general applicability of our approach to multiple types of stego images.steganalysis, steganography, deep learning, generalized steganalysis model"
529,"딥러닝 프레임워크의 비교: 티아노, 텐서플로, CNTK를 중심으로",정여진 /Yeojin Chung,2017,한국지능정보시스템학회,"딥러닝 프레임워크의 대표적인 기능으로는 ‘자동미분’과 ‘GPU의 활용’ 등을 들 수 있다. 본 논문은 파이썬의 라이브러리 형태로 사용 가능한 프레임워크 중에서 구글의 텐서플로와 마이크로소프트의 CNTK, 그리고 텐서 플로의 원조라고 할 수 있는 티아노를 비교하였다. 본문에서는 자동미분의 개념과 GPU의 활용형태를 간단히 설명하고, 그 다음에 logistic regression을 실행하는 예를 통하여 각 프레임워크의 문법을 알아본 뒤에, 마지막으로 대표적인 딥러닝 응용인 CNN의 예제를 실행시켜보고 코딩의 편의성과 실행속도 등을 확인해 보았다. 그 결과, 편의성의 관점에서 보면 티아노가 가장 코딩 하기가 어렵고, CNTK와 텐서플로는 많은 부분이 비슷하게 추상화 되어 있어서 코딩이 비슷하지만 가중치와 편향을 직접 정의하느냐의 여부에서 차이를 보였다. 그리고 각프레임워크의 실행속도에 대한 평가는 ‘큰 차이는 없다’는 것이다. 텐서플로는 티아노에 비하여 속도가 느리다는 평가가 있어왔는데, 본 연구의 실험에 의하면, 비록 CNN 모형에 국한되었지만, 텐서플로가 아주 조금이지만빠른 것으로 나타났다. CNTK의 경우에도, 비록 실험환경이 달랐지만, 실험환경의 차이에 의한 속도의 차이의 편차범위 이내에 있는 것으로 판단이 되었다. 본 연구에서는 세 종류의 딥러닝 프레임워크만을 살펴보았는데,위키피디아에 따르면 딥러닝 프레임워크의 종류는 12가지가 있으며, 각 프레임워크의 특징을 15가지 속성으로구분하여 차이를 특정하고 있다. 그 많은 속성 중에서 사용자의 입장에서 볼 때 중요한 속성은 어떤 언어(파이썬, C++, Java, 등)로 사용가능한지, 어떤 딥러닝 모형에 대한 라이브러리가 잘 구현되어 있는지 등일 것이다. 그리고 사용자가 대규모의 딥러닝 모형을 구축한다면, 다중 GPU 혹은 다중 서버를 지원하는지의 여부도 중요할 것이다. 또한 딥러닝 모형을 처음 학습하는 경우에는 사용설명서가 많은지 예제 프로그램이 많은지 여부도 중요한 기준이 될 것이다.The deep learning framework is software designed to help develop deep learning models. Some of its important functions include “automatic differentiation” and “utilization of GPU”. The list of popular deep learning framework includes Caffe (BVLC) and Theano (University of Montreal). And recently, Microsoft's deep learning framework, Microsoft Cognitive Toolkit, was released as open-source license, following Google’s Tensorflow a year earlier. The early deep learning frameworks have been developed mainly for research at universities. Beginning with the inception of Tensorflow, however, it seems that companies such as Microsoft and Facebook have started to join the competition of framework development. Given the trend, Google and other companies are expected to continue investing in the deep learning framework to bring forward the initiative in the artificial intelligence business. From this point of view, we think it is a good time to compare some of deep learning frameworks. So we compare three deep learning frameworks which can be used as a Python library. Those are Google's Tensorflow, Microsoft’s CNTK, and Theano which is sort of a predecessor of the preceding two.
The most common and important function of deep learning frameworks is the ability to perform automatic differentiation. Basically all the mathematical expressions of deep learning models can be represented as computational graphs, which consist of nodes and edges. Partial derivatives on each edge of a computational graph can then be obtained. With the partial derivatives, we can let software compute differentiation of any node with respect to any variable by utilizing chain rule of Calculus.
First of all, the convenience of coding is in the order of CNTK, Tensorflow, and Theano. The criterion is simply based on the lengths of the codes and the learning curve and the ease of coding are not the main concern. According to the criteria, Theano was the most difficult to implement with, and CNTK and Tensorflow were somewhat easier. With Tensorflow, we need to define weight variables and biases explicitly. The reason that CNTK and Tensorflow are easier to implement with is that those frameworks provide us with more abstraction than Theano. We, however, need to mention that low-level coding is not always bad. It gives us flexibility of coding. With the low-level coding such as in Theano, we can implement and test any new deep learning models or any new search methods that we can think of.
The assessment of the execution speed of each framework is that there is not meaningful difference. According to the experiment, execution speeds of Theano and Tensorflow are very similar, although the experiment was limited to a CNN model. In the case of CNTK, the experimental environment was not maintained as the same. The code written in CNTK has to be run in PC environment without GPU where codes execute as much as 50 times slower than with GPU. But we concluded that the difference of execution speed was within the range of variation caused by the different hardware setup.
In this study, we compared three types of deep learning framework: Theano, Tensorflow, and CNTK. According to Wikipedia, there are 12 available deep learning frameworks. And 15 different attributes differentiate each framework. Some of the important attributes would include interface language (Python, C ++, Java, etc.) and the availability of libraries on various deep learning models such as CNN, RNN, DBN, and etc. And if a user implements a large scale deep learning model, it will also be important to support multiple GPU or multiple servers. Also, if you are learning the deep learning model, it would also be important if there are enough examples and references.deep learning framework, Theano, TensorFlow, CNTK, computational graph, CIFAR-10"
530,딥러닝을 이용한 주변 무선단말 파악방안,이웅섭 /Lee， Woong-sup,2018,한국정보통신학회,"최근 단말-대-단말(Device-to-device, D2D) 통신기술이 차세대 무선통신시스템의 핵심기술로 큰 관심을 받고 있다. 이러한 단말간 통신에서는 자신의 주변에 어떠한 단말이 있는지 파악하는 주변단말 탐색(Neighbor discovery)이매우 중요하다. 본 논문에서는 최근 큰 관심을 받고 있는 딥러닝(Deep learning) 기술을 활용하여 단말간 통신에서 주변단말을 파악하는 방안에 대해서 제안한다. 제안 방안은 기존의 방안과 달리 무선채널의 공간적 연관성을 이용하여단말간의 신호 전송 없이 단말이 기지국으로 전송하는 상향링크 파일럿 신호를 기반으로 주변 단말을 찾고 따라서기존의 방식에 비해 신호전송 복잡도(signaling complexity)를 크게 줄일 수 있다. 또한 제안 방안에서는 떨어져 있는거리에 따라서 주변 단말을 분류 가능하여 기존 방안에 비해서 좀 더 세밀한 단말 탐색이 가능하다. 마지막으로 본 논문에서는 tensorflow를 이용한 컴퓨터 시뮬레이션을 통해 제안 방안의 성능을 검증하였다.Recently, the device-to-device (D2D) communication has been conceived as the key technology for the next-generationmobile communication systems. The neighbor discovery in which the nearby users are found, is essential for the properoperation of the D2D communication. In this paper, we propose new neighbor discovery scheme based on deep learningtechnology which has gained a lot of attention recently. In the proposed scheme, the neighboring users can be foundusing the uplink pilot transmission of users only, unlike conventional neighbor discovery schemes in which direct pilotcommunication among users is required, such that the signaling overhead can be greatly reduced in our proposed scheme.
Moreover, the neighbors with different proximity can also be classified accordingly which enables more accurate neighbordiscovery compared to the conventional schemes. The performance of our proposed scheme is verified through thetensorflow-based computer simulations.
키워드 : 딥러닝, 주변단말 탐색, 단말-대-단말 통신Deep learning, Neighbor discovery, Device-to-device communication, Wireless channel correlation"
531,신뢰성있는 딥러닝 기반 분석 모델을 참조하기 위한 딥러닝 기술 언어,문종혁 /Jong Hyeok Mun,2021,한국정보처리학회,"최근 딥러닝은 하드웨어 성능이 향상됨에 따라 자연어 처리, 영상 인식 등의 다양한 기술에 접목되어 활용되고 있다. 이러한 기술들을 활용해 지능형교통 시스템(ITS), 스마트홈, 헬스케어 등의 산업분야에서 데이터를 분석하여 고속도로 속도위반 차량 검출, 에너지 사용량 제어, 응급상황 등과 같은고품질의 서비스를 제공하며, 고품질의 서비스를 제공하기 위해서는 정확도가 향상된 딥러닝 모델이 적용되어야 한다. 이를 위해 서비스 환경의 데이터를분석하기 위한 딥러닝 모델을 개발할 때, 개발자는 신뢰성이 검증된 최신의 딥러닝 모델을 적용할 수 있어야 한다. 이는 개발자가 참조하는 딥러닝 모델에적용된 학습 데이터셋의 정확도를 측정하여 검증할 수 있다. 이러한 검증을 위해서 개발자는 학습 데이터셋, 딥러닝의 계층구조 및 개발 환경 등과 같은내용을 포함하는 딥러닝 모델을 문서화하여 적용하기 위한 구조적인 정보가 필요하다. 본 논문에서는 신뢰성있는 딥러닝 기반 데이터 분석 모델을 참조하기위한 딥러닝 기술 언어를 제안한다. 제안하는 기술 언어는 신뢰성 있는 딥러닝 모델을 개발하는데 필요한 학습데이터셋, 개발 환경 및 설정 등의 정보와더불어 딥러닝 모델의 계층구조를 표현할 수 있다. 제안하는 딥러닝 기술 언어를 이용하여 개발자는 지능형 교통 시스템에서 참조하는 분석 모델의정확도를 검증할 수 있다. 실험에서는 제안하는 언어의 유효성을 검증하기 위해, 번호판 인식 모델을 중심으로 딥러닝 기술 문서의 적용과정을 보인다With the recent advancements of deep learning, companies such as smart home, healthcare, and intelligent transportation systems areutilizing its functionality to provide high-quality services for vehicle detection, emergency situation detection, and controlling energyconsumption. To provide reliable services in such sensitive systems, deep learning models are required to have high accuracy. In orderto develop a deep learning model for analyzing previously mentioned services, developers should utilize the state of the art deep learningmodels that have already been verified for higher accuracy. The developers can verify the accuracy of the referenced model by validatingthe model on the dataset. For this validation, the developer needs structural information to document and apply deep learning models,including metadata such as learning dataset, network architecture, and development environments. In this paper, we propose a descriptionlanguage that represents the network architecture of the deep learning model along with its metadata that are necessary to develop adeep learning model. Through the proposed description language, developers can easily verify the accuracy of the referenced deep learningmodel. Our experiments demonstrate the application scenario of a deep learning description document that focuses on the license platerecognition for the detection of illegally parked vehicles.Trusted Deep Learning, Model Reference, Deep Learning Description Language, Traffic Situation Analysis Model"
532,딥러닝 분석을 이용한 중국 역내·외 위안화 변동성 예측,이우식 /Lee Woosik,2016,한국데이터정보과학회,"2008년 글로벌 금융위기 이후 중국은 위안화 국제화의 점진적 추진을 진행하면서 중국상하이 외환시장과 중국홍콩 외환시장에서 거래되는 통화인 역내위안화와 역외위안화를 형성시켰다. 본 연구는 위안화 국제화와 점진적인 중국 자본계정 개방에 따라 급변하는 외환시장상황의 변동성을 정확하게 파악하기 위해서 GARCH모형 (일반화된 자기회귀 조건부이분산성모형)에 다단계인공신경망을 결합한 MLP-GARCH 모형과 GARCH모형과 기계학습의 일종인 딥러닝 (deep learning)을 통합한 DL-GARCH을 가지고 위안화 변동성예측을 비교 실험과 분석을 하였다. 비교분석 결과 DLGARCH 모형은 MLP-GARCH보다 모형 위안화 역내·외 환율변동성 예측 면에서 더욱 더 개선된예측값을 제공하였다. 그래서 이분산시계열모형을 딥러닝과 결합한 DL-GARCH 모형은 시계열의환율 변동성 예측 문제에 딥러닝을 응용할 수 있음을 확인하였다. 향후 이분산시계열과 결합된 딥러닝 모형은 다른 금융시계열 데이터에 응용하여 그 일반화 가능성을 높일 수 있을 것이다.The People’s Republic of China has vigorously been pursuing the internationalization of the Chinese Yuan or Renminbi after the financial crisis of 2008. In this view, an abrupt increase of use of the Chinese Yuan in the onshore and offshore markets are important milestones to be one of important currencies. One of the most frequently used methods to forecast volatility is GARCH model. Since a prediction error of the GARCH model has been reported quite high, a lot of efforts have been made to improve forecasting capability of the GARCH model. In this paper, we have proposed MLPGARCH and a DL-GARCH by employing Artificial Neural Network to the GARCH. In an application to forecasting Chinese Yuan volatility, we have successfully shown their overall outperformance in forecasting over the GARCH.Deep learning, DL-GARCH, GARCH, The Chinese Yuan, volatility."
533,딥러닝 기법을 이용한 내일강수 예측,하지훈,2016,한국지능시스템학회,"정확한 강수예측을 위해서는 예측인자 선정과 예측방법에 대한 선택이 매우 중요하다. 최근에는 강수예측 방법으로기계학습 기법이 많이 사용되고 있으며, 그 중에서도 특히 인공신경망을 사용한 강수예측 방법은 좋은 성능을 보였다. 본논문에서는 딥러닝 기법 중 하나인 DBN(deep belief network)를 이용한 새로운 강수예측 방법을 제안한다. DBN는 비지도사전 학습을 통해 초기 가중치를 설정하여 기존 인공신경망의 문제점을 보완한다. 예측인자로는 기온, 전일-전주 강수일, 태양과 달 궤도 관련 자료를 선정하였다. 기온과 전일-전주 강수일은 서울에서의 1974년부터 2013년까지 총 40년간의AWS(automatic weather system) 관측 자료를 사용하였고, 태양과 달의 궤도 관련 자료는 서울을 중심으로 계산한 결과를사용하였다. 전체 기간에서 일부는 학습 자료로 사용하여 예측모델을 생성하였고, 나머지를 생성한 모델의 검증 자료로사용하였다. 모델 검증 결과로 나온 예측값들은 확률값을 가지며 임계치를 이용하여 강수유무를 판별하였다. 강수 정확도의척도로 양분예보기법 중 CSI(critical successive index)와 Bias(frequency bias)를 계산하였다. 이를 통해 DBN와 MLP(multilayer perceptron)의 성능을 비교한 결과 DBN의 강수 예측 정확도가 높았고, 수행속도 또한 2배 이상 빨랐다.For accurate precipitation forecasts the choice of weather faoctrs and prediction method is very important. Recently, machinele arning has been widely used for forecasting precipitation, and artificial neural network, one of machine learning techniques, showed good performance. In this paper, we suggest a new method for forecasting precipitation using DBN, one of deep learning techniques.
DBN has an advantage that initial weights are set by unsupervised learning, so this compensates for the defects of artificial neural networks. We used past precipitation, temperature, and the parameters of the sun and moon's motion as features for forecasting precipitation. The dataset consists of observation data which had been measured for 40 years from AWS in Seoul. Experiments were based on 8-fold cross validation. As a result of estimation, we got probabilities of test dataset, so threshold was used for the decision of precipitation. CSI and Bias were used for indicating the precision of precipitation. Our experimental results showed that DBN performed better than MLP.Deep learning, Deep belief network, Precipitation, Forecast"
534,딥러닝 신경망모형을 이용한 실시간 교통정보수집 연구,김대현 /KIM DAE HYON,2016,사단법인 인문사회과학기술융합학회,"비디오 기반의 실시간 교통정보수집 시스템은 자율주행을 포함한 지능형교통체계의 주요 서브시스템중 하나이며, 우수한 예측(추정)력 때문에 핵심 알고리즘으로 인공신경망 모형이 주로 사용되고 있다. 특히, 최근 과학 학술지 네이처에 구글 딥마인드의 알파고가 공개되면서 딥러닝과 인공지능에 대한 전 세계적인 관심이 집중되고 있다. 최근까지 가장 널리 사용된 인공신경망모형은 역전파 모형이었으나 최근 컴퓨터 하드웨어 및 알고리즘의 새로운 개발과 더불어 딥러닝 알고리즘을 이용한 딥인공신경망 모형에 대한 관심이 높아지고 있다. 그러나 아직 딥러닝 인공신경망에 대한 연구는 초기단계에 있는 실정이며 적용분야도 극히 한정되어 있다. 본 연구에서는 실시간 도로교통정보 수집을 위한 영상처리분야에서 딥러닝 알고리즘을 활용하고 실험결과를 통하여 향후 교통정보수집‧분석 분야에서의 딥러닝 인공신경망 모형의 적용가능성을 살펴보고자 한다. 특히, 본 연구에서 도출한 딥러닝 인공신경망모형의 실험결과를 기존 역전파 모형에 의한 실험결과와 비교하여 모형의 우수성을 비교‧평가하고자 한다.Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Normally, artificial neural networks have been used in real-time applications for collecting traffic information because of their remarkable performance in terms of predictive accuracy and real-time computing. Especially, the recent publication of a paper in the journal Nature and the success of Google DeepMind’s world-beating Go program AlphaGo caused significant increases of the public interest in artificial intelligence. Even though various types of learning models have been proposed, the Backpropagation has been the most popular neural network model. However, the Deep learning neural network model has attracted much interest recently. In this study, the Deep leaning neural network will be applied in image processing techniques for the real-time traffic information collection to investigate the performance of Deep learning algorithm in traffic engineering fields. In addition, the experimental results from Deep learing algorithm will be compared with the results from Backpropagation model to evaluate and propose better performing model.Intelligent Transportation Systems (ITS), artificial neural networks, Deep learning, traffic information, image processing"
535,딥러닝의 모형과 응용사례,안성만 /SungMahn Ahn,2016,한국지능정보시스템학회,"딥러닝은 인공신경망(neural network)이라는 인공지능분야의 모형이 발전된 형태로서, 계층구조로 이루어진 인공신경망의 내부계층(hidden layer)이 여러 단계로 이루어진 구조이다. 딥러닝에서의 주요 모형은 합성곱신경망(convolutional neural network), 순환신경망(recurrent neural network), 그리고 심층신뢰신경망(deep belief network)의 세가지라고 할 수있다. 그 중에서 현재 흥미로운 연구가 많이 발표되어서 관심이 집중되고 있는 모형은 지도학습(supervised learning)모형인 처음 두 개의 모형이다. 따라서 본 논문에서는 지도학습모형의 가중치를 최적화하는 기본적인 방법인 오류역전파 알고리즘을 살펴본 뒤에 합성곱신경망과 순환신경망의 구조와 응용사례 등을 살펴보고자 한다. 본문에서 다루지 않은 모형인 심층신뢰신경망은 아직까지는 합성곱신경망 이나 순환신경망보다는 상대적으로 주목을 덜 받고 있다. 그러나 심층신뢰신경망은 CNN이나 RNN과는 달리 비지도학습(unsupervised learning)모형이며, 사람이나 동물은 관찰을 통해서 스스로 학습한다는 점에서 궁극적으로는 비지도학습모형이 더 많이 연구되어야 할 주제가 될 것이다.Deep learning model is a kind of neural networks that allows multiple hidden layers. There are various deep learning architectures such as convolutional neural networks, deep belief networks and recurrent neural networks. Those have been applied to fields like computer vision, automatic speech recognition, natural language processing, audio recognition and bioinformatics where they have been shown to produce state-of-the-art results on various tasks. Among those architectures, convolutional neural networks and recurrent neural networks are classified as the supervised learning model. And in recent years, those supervised learning models have gained more popularity than unsupervised learning models such as deep belief networks, because supervised learning models have shown fashionable applications in such fields mentioned above.
Deep learning models can be trained with backpropagation algorithm. Backpropagation is an abbreviation for “backward propagation of errors” and a common method of training artificial neural networks used in conjunction with an optimization method such as gradient descent. The method calculates the gradient of an error function with respect to all the weights in the network. The gradient is fed to the optimization method which in turn uses it to update the weights, in an attempt to minimize the error function.
Convolutional neural networks use a special architecture which is particularly well-adapted to classify images. Using this architecture makes convolutional networks fast to train. This, in turn, helps us train deep, muti-layer networks, which are very good at classifying images. These days, deep convolutional networks are used in most neural networks for image recognition. Convolutional neural networks use three basic ideas: local receptive fields, shared weights, and pooling. By local receptive fields, we mean that each neuron in the first(or any) hidden layer will be connected to a small region of the input(or previous layer’s) neurons. Shared weights mean that we’re going to use the same weights and bias for each of the local receptive field. This means that all the neurons in the hidden layer detect exactly the same feature, just at different locations in the input image. In addition to the convolutional layers just described, convolutional neural networks also contain pooling layers. Pooling layers are usually used immediately after convolutional layers. What the pooling layers do is to simplify the information in the output from the convolutional layer.
Recent convolutional network architectures have 10 to 20 hidden layers and billions of connections between units. Training deep learning networks has taken weeks several years ago, but thanks to progress in GPU and algorithm enhancement, training time has reduced to several hours.
Neural networks with time-varying behavior are known as recurrent neural networks or RNNs. A recurrent neural network is a class of artificial neural network where connections between units form a directed cycle. This creates an internal state of the network which allows it to exhibit dynamic temporal behavior. Unlike feedforward neural networks, RNNs can use their internal memory to process arbitrary sequences of inputs. Early RNN models turned out to be very difficult to train, harder even than deep feedforward networks. The reason is the unstable gradient problem such as vanishing gradient and exploding gradient. The gradient can get smaller and smaller as it is propagated back through layers. This makes learning in early layers extremely slow. The problem actually gets worse in RNNs, since gradients aren’t just propagated backward through layers, they’re propagated backward through time. If the network runs for a long time, that can make the gradient extremely unstable and hard to learn from. It has been possible to incorporate an idea known as long short-term memory units (LSTMs) into RNNs. LSTMs make it much easier to get good results when training RNNs, and many recent papers make use of LSTMs or related ideas.Deep learning, Convolutional neural networks, Recurrent neural networks, Error backpropagation algorithm"
536,딥러닝분석과 기술적 분석 지표를 이용한 한국 코스피주가지수 방향성 예측,이우식 /Lee Woosik,2017,한국데이터정보과학회,"2016년 3월 구글 (Google)의 바둑인공지능 알파고 (AlphaGo)가 이세돌 9단과의 바둑대결에서승리한 이후 다양한 분야에서 인공지능 사용에 대한 관심이 높아지고 있는 가운데 금융투자 분야에서도 인공지능과 투자자문 전문가의 합성어인 로보어드바이저 (Robo-Advisor)에 대한 관심이 높아지고 있다. 인공지능 (artificial intelligence)기반의 의사결정은 비용 절감은 물론 효과적인 의사결정을가능하게 한다는 점에서 큰 장점이 있다. 본 연구에서는 기술적 분석 (technical analysis) 지표와 딥러닝 (deep learning) 모형을 결합하여 한국 코스피 지수를 예측하는 모형을 개발하고 제시한 모형들의 예측력을 비교, 분석한다. 분석 결과 기술적 분석 지표에 딥러닝 알고리즘을 결합한 모형이 주가지수 방향성 예측 문제에 응용될 수 있음을 확인하였다. 향후 본 연구에서 제안된 기술적 분석 지표와딥러닝모형을 결합한 기법은 로보어드바이저서비스에 응용할 수 있는 일반화 가능성을 보여준다Since Google’s AlphaGo defeated a world champion of Go players in 2016, there have been many interests in the deep learning. In the financial sector, a Robo-Advisor using deep learning gains a significant attention, which builds and manages portfolios of financial instruments for investors.In this paper, we have proposed the a deep learning algorithm geared toward identification and forecast of the KOSPI index direction,and we also have compared the accuracy of the prediction.In an application of forecasting the financial market index direction, we have shown that the Robo-Advisor using deep learning has a significant effect on finance industry. The Robo-Advisor collects a massive data such as earnings statements, news reports and regulatory filings, analyzes those and recommends investors how to view market trends and identify the best time to purchase financial assets. On the other hand, the Robo-Advisor allows businesses to learn more about their customers, develop better marketing strategies, increase sales and decrease costs.Artificial intelligence, deep learning, FinTech, Robo-Advisor, technical analysis."
537,딥러닝의 효율적인 학습을 위한 학습데이터 선별 기법에 관한 연구,주경돈 /Gyoung-don Joo,2017,한국정보과학회,"빅데이터의 증가와 병렬 하드웨어 처리, 학습 알고리즘의 개선은 딥러닝의 발전을 가져왔으며 인공지능이 적용 가능한 영역을 획기적으로 확장시키고 있다. 하지만 이러한 대규모의 학습 데이터에 기반한 딥러닝은 학습시간이 증가되고, 더 나아가 학습 데이터의 구축을 위한 레이블링(labeling) 비용이 데이터 수에 비례하여 증가하게 된다. 따라서 본 논문에서는, 딥러닝의 정확도를 최대한 유지하면서 필요한 학습 데이터의 수를 줄일수 있도록, 전체 데이터에서 제한된 수의 학습 데이터의 후보를 선별하는 기법에 대하여 연구한다. 특히 기존의 능동 학습에 기반한 기법들이 학습 데이터의 수가 매우 적어질 경우 딥러닝의 정확도가 급격히 떨어지는 문제를 해결할 수 있는 새로운 기법을 제안하였으며, 제안된 기법으로 선별된 학습 데이터는 그 수의 변화에 대해 딥러닝이 일관되게 경쟁력있는 정확도를 가짐을 다양한 실험을 통하여 검증하였다.The increase of big data, parallel hardware processing and enhanced learning algorithms have led to the development of deep learning and are dramatically expanding the domain of applications based on artificial intelligence. However, such large-scale training data extends the learning time, and furthermore, the labeling cost for building the training set increases in proportion to the volume of data. Therefore, in this paper, we study how to select a limited number of candidates for training set from given data so as to reduce the number of training set while maintaining the accuracy of deep learning. Especially, we found that, with the training set selected by the previous work, the accuracy of deep learning drastically decreases when the number of training set is very small and so proposed new methods to resolve this fall-off in accuracy. Through experiments, we showed that deep learning consistently has the competitive accuracy with the training set selected by the proposed method regardless of the size of training set.Deep Learning, Train Set Selection, Active Learning, Gamma distribution"
538,딥러닝을 이용한 범용적 스테그아날리시스,김현재 /Hyunjae Kim,2017,한국정보과학회,"스테그아날리시스(Steganalysis)란 이미지 등 일반적인 자료에 암호화된 정보를 은닉하는 스테가노그래피(Steganography)에 대한 검출 및 분석 방법으로, 기계학습 기반 방법론을 포함한다. 기존 기계학습 기반 스테그아날리시스는 영상(Image)의 특징(Feature) 추출 및 모델링에 기반하며, 최근 딥러닝(Deep Learning)의 적용으로 검출 정확도가 큰 폭으로 향상되었다. 하지만 현존하는 스테그아날리시스 모델은 단일 스테가노그래피 기법에 대해 국한되어 있어 학습에 사용되지 않은 스테고(Stego) 이미지의 경우 검출이 불가능한 결정적 한계를 가진다. 본 연구에서는 다양한 스테가노그래피 기법으로 생성된 스테고이미지에 딥러닝을 적용하여 스테그아날리시스를 학습하는 범용적 모델을 제안한다. 다양한 실험을 통해 제안 기법의 효용성 및 가능성을 확인하고, 범용적 스테그아날리시스 모델이 각각에 특화된 검출 기법과 유사한 정확도로 스테고 이미지를 검출할 수 있음을 보인다.Steganalysis is to detect information hidden by steganography inside general data such as images. There are stegoanalysis techniques that use machine learning (ML). Existing ML approaches to steganalysis are based on extracting features from stego images and modeling them.
Recently deep learning-based methodologies have shown significant improvements in detection accuracy. However, all the existing methods, including deep learning-based ones, have a critical limitation in that they can only detect stego images that are created by a specific steganography method. In this paper, we propose a generalized steganalysis method that can model multiple types of stego images using deep learning. Through various experiments, we confirm the effectiveness of our approach and envision directions for future research. In particular, we show that our method can detect each type of steganography with the same level of accuracy as that of a steganalysis method dedicated to that type of steganography, thereby demonstrating the general applicability of our approach to multiple types of stego images.steganalysis, steganography, deep learning, generalized steganalysis model"
539,"딥러닝 프레임워크의 비교: 티아노, 텐서플로, CNTK를 중심으로",정여진 /Yeojin Chung,2017,한국지능정보시스템학회,"딥러닝 프레임워크의 대표적인 기능으로는 ‘자동미분’과 ‘GPU의 활용’ 등을 들 수 있다. 본 논문은 파이썬의 라이브러리 형태로 사용 가능한 프레임워크 중에서 구글의 텐서플로와 마이크로소프트의 CNTK, 그리고 텐서 플로의 원조라고 할 수 있는 티아노를 비교하였다. 본문에서는 자동미분의 개념과 GPU의 활용형태를 간단히 설명하고, 그 다음에 logistic regression을 실행하는 예를 통하여 각 프레임워크의 문법을 알아본 뒤에, 마지막으로 대표적인 딥러닝 응용인 CNN의 예제를 실행시켜보고 코딩의 편의성과 실행속도 등을 확인해 보았다. 그 결과, 편의성의 관점에서 보면 티아노가 가장 코딩 하기가 어렵고, CNTK와 텐서플로는 많은 부분이 비슷하게 추상화 되어 있어서 코딩이 비슷하지만 가중치와 편향을 직접 정의하느냐의 여부에서 차이를 보였다. 그리고 각프레임워크의 실행속도에 대한 평가는 ‘큰 차이는 없다’는 것이다. 텐서플로는 티아노에 비하여 속도가 느리다는 평가가 있어왔는데, 본 연구의 실험에 의하면, 비록 CNN 모형에 국한되었지만, 텐서플로가 아주 조금이지만빠른 것으로 나타났다. CNTK의 경우에도, 비록 실험환경이 달랐지만, 실험환경의 차이에 의한 속도의 차이의 편차범위 이내에 있는 것으로 판단이 되었다. 본 연구에서는 세 종류의 딥러닝 프레임워크만을 살펴보았는데,위키피디아에 따르면 딥러닝 프레임워크의 종류는 12가지가 있으며, 각 프레임워크의 특징을 15가지 속성으로구분하여 차이를 특정하고 있다. 그 많은 속성 중에서 사용자의 입장에서 볼 때 중요한 속성은 어떤 언어(파이썬, C++, Java, 등)로 사용가능한지, 어떤 딥러닝 모형에 대한 라이브러리가 잘 구현되어 있는지 등일 것이다. 그리고 사용자가 대규모의 딥러닝 모형을 구축한다면, 다중 GPU 혹은 다중 서버를 지원하는지의 여부도 중요할 것이다. 또한 딥러닝 모형을 처음 학습하는 경우에는 사용설명서가 많은지 예제 프로그램이 많은지 여부도 중요한 기준이 될 것이다.The deep learning framework is software designed to help develop deep learning models. Some of its important functions include “automatic differentiation” and “utilization of GPU”. The list of popular deep learning framework includes Caffe (BVLC) and Theano (University of Montreal). And recently, Microsoft's deep learning framework, Microsoft Cognitive Toolkit, was released as open-source license, following Google’s Tensorflow a year earlier. The early deep learning frameworks have been developed mainly for research at universities. Beginning with the inception of Tensorflow, however, it seems that companies such as Microsoft and Facebook have started to join the competition of framework development. Given the trend, Google and other companies are expected to continue investing in the deep learning framework to bring forward the initiative in the artificial intelligence business. From this point of view, we think it is a good time to compare some of deep learning frameworks. So we compare three deep learning frameworks which can be used as a Python library. Those are Google's Tensorflow, Microsoft’s CNTK, and Theano which is sort of a predecessor of the preceding two.
The most common and important function of deep learning frameworks is the ability to perform automatic differentiation. Basically all the mathematical expressions of deep learning models can be represented as computational graphs, which consist of nodes and edges. Partial derivatives on each edge of a computational graph can then be obtained. With the partial derivatives, we can let software compute differentiation of any node with respect to any variable by utilizing chain rule of Calculus.
First of all, the convenience of coding is in the order of CNTK, Tensorflow, and Theano. The criterion is simply based on the lengths of the codes and the learning curve and the ease of coding are not the main concern. According to the criteria, Theano was the most difficult to implement with, and CNTK and Tensorflow were somewhat easier. With Tensorflow, we need to define weight variables and biases explicitly. The reason that CNTK and Tensorflow are easier to implement with is that those frameworks provide us with more abstraction than Theano. We, however, need to mention that low-level coding is not always bad. It gives us flexibility of coding. With the low-level coding such as in Theano, we can implement and test any new deep learning models or any new search methods that we can think of.
The assessment of the execution speed of each framework is that there is not meaningful difference. According to the experiment, execution speeds of Theano and Tensorflow are very similar, although the experiment was limited to a CNN model. In the case of CNTK, the experimental environment was not maintained as the same. The code written in CNTK has to be run in PC environment without GPU where codes execute as much as 50 times slower than with GPU. But we concluded that the difference of execution speed was within the range of variation caused by the different hardware setup.
In this study, we compared three types of deep learning framework: Theano, Tensorflow, and CNTK. According to Wikipedia, there are 12 available deep learning frameworks. And 15 different attributes differentiate each framework. Some of the important attributes would include interface language (Python, C ++, Java, etc.) and the availability of libraries on various deep learning models such as CNN, RNN, DBN, and etc. And if a user implements a large scale deep learning model, it will also be important to support multiple GPU or multiple servers. Also, if you are learning the deep learning model, it would also be important if there are enough examples and references.deep learning framework, Theano, TensorFlow, CNTK, computational graph, CIFAR-10"
540,딥러닝을 이용한 주변 무선단말 파악방안,이웅섭 /Lee， Woong-sup,2018,한국정보통신학회,"최근 단말-대-단말(Device-to-device, D2D) 통신기술이 차세대 무선통신시스템의 핵심기술로 큰 관심을 받고 있다. 이러한 단말간 통신에서는 자신의 주변에 어떠한 단말이 있는지 파악하는 주변단말 탐색(Neighbor discovery)이매우 중요하다. 본 논문에서는 최근 큰 관심을 받고 있는 딥러닝(Deep learning) 기술을 활용하여 단말간 통신에서 주변단말을 파악하는 방안에 대해서 제안한다. 제안 방안은 기존의 방안과 달리 무선채널의 공간적 연관성을 이용하여단말간의 신호 전송 없이 단말이 기지국으로 전송하는 상향링크 파일럿 신호를 기반으로 주변 단말을 찾고 따라서기존의 방식에 비해 신호전송 복잡도(signaling complexity)를 크게 줄일 수 있다. 또한 제안 방안에서는 떨어져 있는거리에 따라서 주변 단말을 분류 가능하여 기존 방안에 비해서 좀 더 세밀한 단말 탐색이 가능하다. 마지막으로 본 논문에서는 tensorflow를 이용한 컴퓨터 시뮬레이션을 통해 제안 방안의 성능을 검증하였다.Recently, the device-to-device (D2D) communication has been conceived as the key technology for the next-generationmobile communication systems. The neighbor discovery in which the nearby users are found, is essential for the properoperation of the D2D communication. In this paper, we propose new neighbor discovery scheme based on deep learningtechnology which has gained a lot of attention recently. In the proposed scheme, the neighboring users can be foundusing the uplink pilot transmission of users only, unlike conventional neighbor discovery schemes in which direct pilotcommunication among users is required, such that the signaling overhead can be greatly reduced in our proposed scheme.
Moreover, the neighbors with different proximity can also be classified accordingly which enables more accurate neighbordiscovery compared to the conventional schemes. The performance of our proposed scheme is verified through thetensorflow-based computer simulations.
키워드 : 딥러닝, 주변단말 탐색, 단말-대-단말 통신Deep learning, Neighbor discovery, Device-to-device communication, Wireless channel correlation"
541,신뢰성있는 딥러닝 기반 분석 모델을 참조하기 위한 딥러닝 기술 언어,문종혁 /Jong Hyeok Mun,2021,한국정보처리학회,"최근 딥러닝은 하드웨어 성능이 향상됨에 따라 자연어 처리, 영상 인식 등의 다양한 기술에 접목되어 활용되고 있다. 이러한 기술들을 활용해 지능형교통 시스템(ITS), 스마트홈, 헬스케어 등의 산업분야에서 데이터를 분석하여 고속도로 속도위반 차량 검출, 에너지 사용량 제어, 응급상황 등과 같은고품질의 서비스를 제공하며, 고품질의 서비스를 제공하기 위해서는 정확도가 향상된 딥러닝 모델이 적용되어야 한다. 이를 위해 서비스 환경의 데이터를분석하기 위한 딥러닝 모델을 개발할 때, 개발자는 신뢰성이 검증된 최신의 딥러닝 모델을 적용할 수 있어야 한다. 이는 개발자가 참조하는 딥러닝 모델에적용된 학습 데이터셋의 정확도를 측정하여 검증할 수 있다. 이러한 검증을 위해서 개발자는 학습 데이터셋, 딥러닝의 계층구조 및 개발 환경 등과 같은내용을 포함하는 딥러닝 모델을 문서화하여 적용하기 위한 구조적인 정보가 필요하다. 본 논문에서는 신뢰성있는 딥러닝 기반 데이터 분석 모델을 참조하기위한 딥러닝 기술 언어를 제안한다. 제안하는 기술 언어는 신뢰성 있는 딥러닝 모델을 개발하는데 필요한 학습데이터셋, 개발 환경 및 설정 등의 정보와더불어 딥러닝 모델의 계층구조를 표현할 수 있다. 제안하는 딥러닝 기술 언어를 이용하여 개발자는 지능형 교통 시스템에서 참조하는 분석 모델의정확도를 검증할 수 있다. 실험에서는 제안하는 언어의 유효성을 검증하기 위해, 번호판 인식 모델을 중심으로 딥러닝 기술 문서의 적용과정을 보인다With the recent advancements of deep learning, companies such as smart home, healthcare, and intelligent transportation systems areutilizing its functionality to provide high-quality services for vehicle detection, emergency situation detection, and controlling energyconsumption. To provide reliable services in such sensitive systems, deep learning models are required to have high accuracy. In orderto develop a deep learning model for analyzing previously mentioned services, developers should utilize the state of the art deep learningmodels that have already been verified for higher accuracy. The developers can verify the accuracy of the referenced model by validatingthe model on the dataset. For this validation, the developer needs structural information to document and apply deep learning models,including metadata such as learning dataset, network architecture, and development environments. In this paper, we propose a descriptionlanguage that represents the network architecture of the deep learning model along with its metadata that are necessary to develop adeep learning model. Through the proposed description language, developers can easily verify the accuracy of the referenced deep learningmodel. Our experiments demonstrate the application scenario of a deep learning description document that focuses on the license platerecognition for the detection of illegally parked vehicles.Trusted Deep Learning, Model Reference, Deep Learning Description Language, Traffic Situation Analysis Model"
542,딥러닝 분석을 이용한 중국 역내·외 위안화 변동성 예측,이우식 /Lee Woosik,2016,한국데이터정보과학회,"2008년 글로벌 금융위기 이후 중국은 위안화 국제화의 점진적 추진을 진행하면서 중국상하이 외환시장과 중국홍콩 외환시장에서 거래되는 통화인 역내위안화와 역외위안화를 형성시켰다. 본 연구는 위안화 국제화와 점진적인 중국 자본계정 개방에 따라 급변하는 외환시장상황의 변동성을 정확하게 파악하기 위해서 GARCH모형 (일반화된 자기회귀 조건부이분산성모형)에 다단계인공신경망을 결합한 MLP-GARCH 모형과 GARCH모형과 기계학습의 일종인 딥러닝 (deep learning)을 통합한 DL-GARCH을 가지고 위안화 변동성예측을 비교 실험과 분석을 하였다. 비교분석 결과 DLGARCH 모형은 MLP-GARCH보다 모형 위안화 역내·외 환율변동성 예측 면에서 더욱 더 개선된예측값을 제공하였다. 그래서 이분산시계열모형을 딥러닝과 결합한 DL-GARCH 모형은 시계열의환율 변동성 예측 문제에 딥러닝을 응용할 수 있음을 확인하였다. 향후 이분산시계열과 결합된 딥러닝 모형은 다른 금융시계열 데이터에 응용하여 그 일반화 가능성을 높일 수 있을 것이다.The People’s Republic of China has vigorously been pursuing the internationalization of the Chinese Yuan or Renminbi after the financial crisis of 2008. In this view, an abrupt increase of use of the Chinese Yuan in the onshore and offshore markets are important milestones to be one of important currencies. One of the most frequently used methods to forecast volatility is GARCH model. Since a prediction error of the GARCH model has been reported quite high, a lot of efforts have been made to improve forecasting capability of the GARCH model. In this paper, we have proposed MLPGARCH and a DL-GARCH by employing Artificial Neural Network to the GARCH. In an application to forecasting Chinese Yuan volatility, we have successfully shown their overall outperformance in forecasting over the GARCH.Deep learning, DL-GARCH, GARCH, The Chinese Yuan, volatility."
543,딥러닝 기법을 이용한 내일강수 예측,하지훈,2016,한국지능시스템학회,"정확한 강수예측을 위해서는 예측인자 선정과 예측방법에 대한 선택이 매우 중요하다. 최근에는 강수예측 방법으로기계학습 기법이 많이 사용되고 있으며, 그 중에서도 특히 인공신경망을 사용한 강수예측 방법은 좋은 성능을 보였다. 본논문에서는 딥러닝 기법 중 하나인 DBN(deep belief network)를 이용한 새로운 강수예측 방법을 제안한다. DBN는 비지도사전 학습을 통해 초기 가중치를 설정하여 기존 인공신경망의 문제점을 보완한다. 예측인자로는 기온, 전일-전주 강수일, 태양과 달 궤도 관련 자료를 선정하였다. 기온과 전일-전주 강수일은 서울에서의 1974년부터 2013년까지 총 40년간의AWS(automatic weather system) 관측 자료를 사용하였고, 태양과 달의 궤도 관련 자료는 서울을 중심으로 계산한 결과를사용하였다. 전체 기간에서 일부는 학습 자료로 사용하여 예측모델을 생성하였고, 나머지를 생성한 모델의 검증 자료로사용하였다. 모델 검증 결과로 나온 예측값들은 확률값을 가지며 임계치를 이용하여 강수유무를 판별하였다. 강수 정확도의척도로 양분예보기법 중 CSI(critical successive index)와 Bias(frequency bias)를 계산하였다. 이를 통해 DBN와 MLP(multilayer perceptron)의 성능을 비교한 결과 DBN의 강수 예측 정확도가 높았고, 수행속도 또한 2배 이상 빨랐다.For accurate precipitation forecasts the choice of weather faoctrs and prediction method is very important. Recently, machinele arning has been widely used for forecasting precipitation, and artificial neural network, one of machine learning techniques, showed good performance. In this paper, we suggest a new method for forecasting precipitation using DBN, one of deep learning techniques.
DBN has an advantage that initial weights are set by unsupervised learning, so this compensates for the defects of artificial neural networks. We used past precipitation, temperature, and the parameters of the sun and moon's motion as features for forecasting precipitation. The dataset consists of observation data which had been measured for 40 years from AWS in Seoul. Experiments were based on 8-fold cross validation. As a result of estimation, we got probabilities of test dataset, so threshold was used for the decision of precipitation. CSI and Bias were used for indicating the precision of precipitation. Our experimental results showed that DBN performed better than MLP.Deep learning, Deep belief network, Precipitation, Forecast"
544,딥러닝 신경망모형을 이용한 실시간 교통정보수집 연구,김대현 /KIM DAE HYON,2016,사단법인 인문사회과학기술융합학회,"비디오 기반의 실시간 교통정보수집 시스템은 자율주행을 포함한 지능형교통체계의 주요 서브시스템중 하나이며, 우수한 예측(추정)력 때문에 핵심 알고리즘으로 인공신경망 모형이 주로 사용되고 있다. 특히, 최근 과학 학술지 네이처에 구글 딥마인드의 알파고가 공개되면서 딥러닝과 인공지능에 대한 전 세계적인 관심이 집중되고 있다. 최근까지 가장 널리 사용된 인공신경망모형은 역전파 모형이었으나 최근 컴퓨터 하드웨어 및 알고리즘의 새로운 개발과 더불어 딥러닝 알고리즘을 이용한 딥인공신경망 모형에 대한 관심이 높아지고 있다. 그러나 아직 딥러닝 인공신경망에 대한 연구는 초기단계에 있는 실정이며 적용분야도 극히 한정되어 있다. 본 연구에서는 실시간 도로교통정보 수집을 위한 영상처리분야에서 딥러닝 알고리즘을 활용하고 실험결과를 통하여 향후 교통정보수집‧분석 분야에서의 딥러닝 인공신경망 모형의 적용가능성을 살펴보고자 한다. 특히, 본 연구에서 도출한 딥러닝 인공신경망모형의 실험결과를 기존 역전파 모형에 의한 실험결과와 비교하여 모형의 우수성을 비교‧평가하고자 한다.Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Normally, artificial neural networks have been used in real-time applications for collecting traffic information because of their remarkable performance in terms of predictive accuracy and real-time computing. Especially, the recent publication of a paper in the journal Nature and the success of Google DeepMind’s world-beating Go program AlphaGo caused significant increases of the public interest in artificial intelligence. Even though various types of learning models have been proposed, the Backpropagation has been the most popular neural network model. However, the Deep learning neural network model has attracted much interest recently. In this study, the Deep leaning neural network will be applied in image processing techniques for the real-time traffic information collection to investigate the performance of Deep learning algorithm in traffic engineering fields. In addition, the experimental results from Deep learing algorithm will be compared with the results from Backpropagation model to evaluate and propose better performing model.Intelligent Transportation Systems (ITS), artificial neural networks, Deep learning, traffic information, image processing"
545,딥러닝의 모형과 응용사례,안성만 /SungMahn Ahn,2016,한국지능정보시스템학회,"딥러닝은 인공신경망(neural network)이라는 인공지능분야의 모형이 발전된 형태로서, 계층구조로 이루어진 인공신경망의 내부계층(hidden layer)이 여러 단계로 이루어진 구조이다. 딥러닝에서의 주요 모형은 합성곱신경망(convolutional neural network), 순환신경망(recurrent neural network), 그리고 심층신뢰신경망(deep belief network)의 세가지라고 할 수있다. 그 중에서 현재 흥미로운 연구가 많이 발표되어서 관심이 집중되고 있는 모형은 지도학습(supervised learning)모형인 처음 두 개의 모형이다. 따라서 본 논문에서는 지도학습모형의 가중치를 최적화하는 기본적인 방법인 오류역전파 알고리즘을 살펴본 뒤에 합성곱신경망과 순환신경망의 구조와 응용사례 등을 살펴보고자 한다. 본문에서 다루지 않은 모형인 심층신뢰신경망은 아직까지는 합성곱신경망 이나 순환신경망보다는 상대적으로 주목을 덜 받고 있다. 그러나 심층신뢰신경망은 CNN이나 RNN과는 달리 비지도학습(unsupervised learning)모형이며, 사람이나 동물은 관찰을 통해서 스스로 학습한다는 점에서 궁극적으로는 비지도학습모형이 더 많이 연구되어야 할 주제가 될 것이다.Deep learning model is a kind of neural networks that allows multiple hidden layers. There are various deep learning architectures such as convolutional neural networks, deep belief networks and recurrent neural networks. Those have been applied to fields like computer vision, automatic speech recognition, natural language processing, audio recognition and bioinformatics where they have been shown to produce state-of-the-art results on various tasks. Among those architectures, convolutional neural networks and recurrent neural networks are classified as the supervised learning model. And in recent years, those supervised learning models have gained more popularity than unsupervised learning models such as deep belief networks, because supervised learning models have shown fashionable applications in such fields mentioned above.
Deep learning models can be trained with backpropagation algorithm. Backpropagation is an abbreviation for “backward propagation of errors” and a common method of training artificial neural networks used in conjunction with an optimization method such as gradient descent. The method calculates the gradient of an error function with respect to all the weights in the network. The gradient is fed to the optimization method which in turn uses it to update the weights, in an attempt to minimize the error function.
Convolutional neural networks use a special architecture which is particularly well-adapted to classify images. Using this architecture makes convolutional networks fast to train. This, in turn, helps us train deep, muti-layer networks, which are very good at classifying images. These days, deep convolutional networks are used in most neural networks for image recognition. Convolutional neural networks use three basic ideas: local receptive fields, shared weights, and pooling. By local receptive fields, we mean that each neuron in the first(or any) hidden layer will be connected to a small region of the input(or previous layer’s) neurons. Shared weights mean that we’re going to use the same weights and bias for each of the local receptive field. This means that all the neurons in the hidden layer detect exactly the same feature, just at different locations in the input image. In addition to the convolutional layers just described, convolutional neural networks also contain pooling layers. Pooling layers are usually used immediately after convolutional layers. What the pooling layers do is to simplify the information in the output from the convolutional layer.
Recent convolutional network architectures have 10 to 20 hidden layers and billions of connections between units. Training deep learning networks has taken weeks several years ago, but thanks to progress in GPU and algorithm enhancement, training time has reduced to several hours.
Neural networks with time-varying behavior are known as recurrent neural networks or RNNs. A recurrent neural network is a class of artificial neural network where connections between units form a directed cycle. This creates an internal state of the network which allows it to exhibit dynamic temporal behavior. Unlike feedforward neural networks, RNNs can use their internal memory to process arbitrary sequences of inputs. Early RNN models turned out to be very difficult to train, harder even than deep feedforward networks. The reason is the unstable gradient problem such as vanishing gradient and exploding gradient. The gradient can get smaller and smaller as it is propagated back through layers. This makes learning in early layers extremely slow. The problem actually gets worse in RNNs, since gradients aren’t just propagated backward through layers, they’re propagated backward through time. If the network runs for a long time, that can make the gradient extremely unstable and hard to learn from. It has been possible to incorporate an idea known as long short-term memory units (LSTMs) into RNNs. LSTMs make it much easier to get good results when training RNNs, and many recent papers make use of LSTMs or related ideas.Deep learning, Convolutional neural networks, Recurrent neural networks, Error backpropagation algorithm"
546,딥러닝분석과 기술적 분석 지표를 이용한 한국 코스피주가지수 방향성 예측,이우식 /Lee Woosik,2017,한국데이터정보과학회,"2016년 3월 구글 (Google)의 바둑인공지능 알파고 (AlphaGo)가 이세돌 9단과의 바둑대결에서승리한 이후 다양한 분야에서 인공지능 사용에 대한 관심이 높아지고 있는 가운데 금융투자 분야에서도 인공지능과 투자자문 전문가의 합성어인 로보어드바이저 (Robo-Advisor)에 대한 관심이 높아지고 있다. 인공지능 (artificial intelligence)기반의 의사결정은 비용 절감은 물론 효과적인 의사결정을가능하게 한다는 점에서 큰 장점이 있다. 본 연구에서는 기술적 분석 (technical analysis) 지표와 딥러닝 (deep learning) 모형을 결합하여 한국 코스피 지수를 예측하는 모형을 개발하고 제시한 모형들의 예측력을 비교, 분석한다. 분석 결과 기술적 분석 지표에 딥러닝 알고리즘을 결합한 모형이 주가지수 방향성 예측 문제에 응용될 수 있음을 확인하였다. 향후 본 연구에서 제안된 기술적 분석 지표와딥러닝모형을 결합한 기법은 로보어드바이저서비스에 응용할 수 있는 일반화 가능성을 보여준다Since Google’s AlphaGo defeated a world champion of Go players in 2016, there have been many interests in the deep learning. In the financial sector, a Robo-Advisor using deep learning gains a significant attention, which builds and manages portfolios of financial instruments for investors.In this paper, we have proposed the a deep learning algorithm geared toward identification and forecast of the KOSPI index direction,and we also have compared the accuracy of the prediction.In an application of forecasting the financial market index direction, we have shown that the Robo-Advisor using deep learning has a significant effect on finance industry. The Robo-Advisor collects a massive data such as earnings statements, news reports and regulatory filings, analyzes those and recommends investors how to view market trends and identify the best time to purchase financial assets. On the other hand, the Robo-Advisor allows businesses to learn more about their customers, develop better marketing strategies, increase sales and decrease costs.Artificial intelligence, deep learning, FinTech, Robo-Advisor, technical analysis."
547,딥러닝의 효율적인 학습을 위한 학습데이터 선별 기법에 관한 연구,주경돈 /Gyoung-don Joo,2017,한국정보과학회,"빅데이터의 증가와 병렬 하드웨어 처리, 학습 알고리즘의 개선은 딥러닝의 발전을 가져왔으며 인공지능이 적용 가능한 영역을 획기적으로 확장시키고 있다. 하지만 이러한 대규모의 학습 데이터에 기반한 딥러닝은 학습시간이 증가되고, 더 나아가 학습 데이터의 구축을 위한 레이블링(labeling) 비용이 데이터 수에 비례하여 증가하게 된다. 따라서 본 논문에서는, 딥러닝의 정확도를 최대한 유지하면서 필요한 학습 데이터의 수를 줄일수 있도록, 전체 데이터에서 제한된 수의 학습 데이터의 후보를 선별하는 기법에 대하여 연구한다. 특히 기존의 능동 학습에 기반한 기법들이 학습 데이터의 수가 매우 적어질 경우 딥러닝의 정확도가 급격히 떨어지는 문제를 해결할 수 있는 새로운 기법을 제안하였으며, 제안된 기법으로 선별된 학습 데이터는 그 수의 변화에 대해 딥러닝이 일관되게 경쟁력있는 정확도를 가짐을 다양한 실험을 통하여 검증하였다.The increase of big data, parallel hardware processing and enhanced learning algorithms have led to the development of deep learning and are dramatically expanding the domain of applications based on artificial intelligence. However, such large-scale training data extends the learning time, and furthermore, the labeling cost for building the training set increases in proportion to the volume of data. Therefore, in this paper, we study how to select a limited number of candidates for training set from given data so as to reduce the number of training set while maintaining the accuracy of deep learning. Especially, we found that, with the training set selected by the previous work, the accuracy of deep learning drastically decreases when the number of training set is very small and so proposed new methods to resolve this fall-off in accuracy. Through experiments, we showed that deep learning consistently has the competitive accuracy with the training set selected by the proposed method regardless of the size of training set.Deep Learning, Train Set Selection, Active Learning, Gamma distribution"
548,딥러닝을 이용한 범용적 스테그아날리시스,김현재 /Hyunjae Kim,2017,한국정보과학회,"스테그아날리시스(Steganalysis)란 이미지 등 일반적인 자료에 암호화된 정보를 은닉하는 스테가노그래피(Steganography)에 대한 검출 및 분석 방법으로, 기계학습 기반 방법론을 포함한다. 기존 기계학습 기반 스테그아날리시스는 영상(Image)의 특징(Feature) 추출 및 모델링에 기반하며, 최근 딥러닝(Deep Learning)의 적용으로 검출 정확도가 큰 폭으로 향상되었다. 하지만 현존하는 스테그아날리시스 모델은 단일 스테가노그래피 기법에 대해 국한되어 있어 학습에 사용되지 않은 스테고(Stego) 이미지의 경우 검출이 불가능한 결정적 한계를 가진다. 본 연구에서는 다양한 스테가노그래피 기법으로 생성된 스테고이미지에 딥러닝을 적용하여 스테그아날리시스를 학습하는 범용적 모델을 제안한다. 다양한 실험을 통해 제안 기법의 효용성 및 가능성을 확인하고, 범용적 스테그아날리시스 모델이 각각에 특화된 검출 기법과 유사한 정확도로 스테고 이미지를 검출할 수 있음을 보인다.Steganalysis is to detect information hidden by steganography inside general data such as images. There are stegoanalysis techniques that use machine learning (ML). Existing ML approaches to steganalysis are based on extracting features from stego images and modeling them.
Recently deep learning-based methodologies have shown significant improvements in detection accuracy. However, all the existing methods, including deep learning-based ones, have a critical limitation in that they can only detect stego images that are created by a specific steganography method. In this paper, we propose a generalized steganalysis method that can model multiple types of stego images using deep learning. Through various experiments, we confirm the effectiveness of our approach and envision directions for future research. In particular, we show that our method can detect each type of steganography with the same level of accuracy as that of a steganalysis method dedicated to that type of steganography, thereby demonstrating the general applicability of our approach to multiple types of stego images.steganalysis, steganography, deep learning, generalized steganalysis model"
549,"딥러닝 프레임워크의 비교: 티아노, 텐서플로, CNTK를 중심으로",정여진 /Yeojin Chung,2017,한국지능정보시스템학회,"딥러닝 프레임워크의 대표적인 기능으로는 ‘자동미분’과 ‘GPU의 활용’ 등을 들 수 있다. 본 논문은 파이썬의 라이브러리 형태로 사용 가능한 프레임워크 중에서 구글의 텐서플로와 마이크로소프트의 CNTK, 그리고 텐서 플로의 원조라고 할 수 있는 티아노를 비교하였다. 본문에서는 자동미분의 개념과 GPU의 활용형태를 간단히 설명하고, 그 다음에 logistic regression을 실행하는 예를 통하여 각 프레임워크의 문법을 알아본 뒤에, 마지막으로 대표적인 딥러닝 응용인 CNN의 예제를 실행시켜보고 코딩의 편의성과 실행속도 등을 확인해 보았다. 그 결과, 편의성의 관점에서 보면 티아노가 가장 코딩 하기가 어렵고, CNTK와 텐서플로는 많은 부분이 비슷하게 추상화 되어 있어서 코딩이 비슷하지만 가중치와 편향을 직접 정의하느냐의 여부에서 차이를 보였다. 그리고 각프레임워크의 실행속도에 대한 평가는 ‘큰 차이는 없다’는 것이다. 텐서플로는 티아노에 비하여 속도가 느리다는 평가가 있어왔는데, 본 연구의 실험에 의하면, 비록 CNN 모형에 국한되었지만, 텐서플로가 아주 조금이지만빠른 것으로 나타났다. CNTK의 경우에도, 비록 실험환경이 달랐지만, 실험환경의 차이에 의한 속도의 차이의 편차범위 이내에 있는 것으로 판단이 되었다. 본 연구에서는 세 종류의 딥러닝 프레임워크만을 살펴보았는데,위키피디아에 따르면 딥러닝 프레임워크의 종류는 12가지가 있으며, 각 프레임워크의 특징을 15가지 속성으로구분하여 차이를 특정하고 있다. 그 많은 속성 중에서 사용자의 입장에서 볼 때 중요한 속성은 어떤 언어(파이썬, C++, Java, 등)로 사용가능한지, 어떤 딥러닝 모형에 대한 라이브러리가 잘 구현되어 있는지 등일 것이다. 그리고 사용자가 대규모의 딥러닝 모형을 구축한다면, 다중 GPU 혹은 다중 서버를 지원하는지의 여부도 중요할 것이다. 또한 딥러닝 모형을 처음 학습하는 경우에는 사용설명서가 많은지 예제 프로그램이 많은지 여부도 중요한 기준이 될 것이다.The deep learning framework is software designed to help develop deep learning models. Some of its important functions include “automatic differentiation” and “utilization of GPU”. The list of popular deep learning framework includes Caffe (BVLC) and Theano (University of Montreal). And recently, Microsoft's deep learning framework, Microsoft Cognitive Toolkit, was released as open-source license, following Google’s Tensorflow a year earlier. The early deep learning frameworks have been developed mainly for research at universities. Beginning with the inception of Tensorflow, however, it seems that companies such as Microsoft and Facebook have started to join the competition of framework development. Given the trend, Google and other companies are expected to continue investing in the deep learning framework to bring forward the initiative in the artificial intelligence business. From this point of view, we think it is a good time to compare some of deep learning frameworks. So we compare three deep learning frameworks which can be used as a Python library. Those are Google's Tensorflow, Microsoft’s CNTK, and Theano which is sort of a predecessor of the preceding two.
The most common and important function of deep learning frameworks is the ability to perform automatic differentiation. Basically all the mathematical expressions of deep learning models can be represented as computational graphs, which consist of nodes and edges. Partial derivatives on each edge of a computational graph can then be obtained. With the partial derivatives, we can let software compute differentiation of any node with respect to any variable by utilizing chain rule of Calculus.
First of all, the convenience of coding is in the order of CNTK, Tensorflow, and Theano. The criterion is simply based on the lengths of the codes and the learning curve and the ease of coding are not the main concern. According to the criteria, Theano was the most difficult to implement with, and CNTK and Tensorflow were somewhat easier. With Tensorflow, we need to define weight variables and biases explicitly. The reason that CNTK and Tensorflow are easier to implement with is that those frameworks provide us with more abstraction than Theano. We, however, need to mention that low-level coding is not always bad. It gives us flexibility of coding. With the low-level coding such as in Theano, we can implement and test any new deep learning models or any new search methods that we can think of.
The assessment of the execution speed of each framework is that there is not meaningful difference. According to the experiment, execution speeds of Theano and Tensorflow are very similar, although the experiment was limited to a CNN model. In the case of CNTK, the experimental environment was not maintained as the same. The code written in CNTK has to be run in PC environment without GPU where codes execute as much as 50 times slower than with GPU. But we concluded that the difference of execution speed was within the range of variation caused by the different hardware setup.
In this study, we compared three types of deep learning framework: Theano, Tensorflow, and CNTK. According to Wikipedia, there are 12 available deep learning frameworks. And 15 different attributes differentiate each framework. Some of the important attributes would include interface language (Python, C ++, Java, etc.) and the availability of libraries on various deep learning models such as CNN, RNN, DBN, and etc. And if a user implements a large scale deep learning model, it will also be important to support multiple GPU or multiple servers. Also, if you are learning the deep learning model, it would also be important if there are enough examples and references.deep learning framework, Theano, TensorFlow, CNTK, computational graph, CIFAR-10"
550,딥러닝을 이용한 주변 무선단말 파악방안,이웅섭 /Lee， Woong-sup,2018,한국정보통신학회,"최근 단말-대-단말(Device-to-device, D2D) 통신기술이 차세대 무선통신시스템의 핵심기술로 큰 관심을 받고 있다. 이러한 단말간 통신에서는 자신의 주변에 어떠한 단말이 있는지 파악하는 주변단말 탐색(Neighbor discovery)이매우 중요하다. 본 논문에서는 최근 큰 관심을 받고 있는 딥러닝(Deep learning) 기술을 활용하여 단말간 통신에서 주변단말을 파악하는 방안에 대해서 제안한다. 제안 방안은 기존의 방안과 달리 무선채널의 공간적 연관성을 이용하여단말간의 신호 전송 없이 단말이 기지국으로 전송하는 상향링크 파일럿 신호를 기반으로 주변 단말을 찾고 따라서기존의 방식에 비해 신호전송 복잡도(signaling complexity)를 크게 줄일 수 있다. 또한 제안 방안에서는 떨어져 있는거리에 따라서 주변 단말을 분류 가능하여 기존 방안에 비해서 좀 더 세밀한 단말 탐색이 가능하다. 마지막으로 본 논문에서는 tensorflow를 이용한 컴퓨터 시뮬레이션을 통해 제안 방안의 성능을 검증하였다.Recently, the device-to-device (D2D) communication has been conceived as the key technology for the next-generationmobile communication systems. The neighbor discovery in which the nearby users are found, is essential for the properoperation of the D2D communication. In this paper, we propose new neighbor discovery scheme based on deep learningtechnology which has gained a lot of attention recently. In the proposed scheme, the neighboring users can be foundusing the uplink pilot transmission of users only, unlike conventional neighbor discovery schemes in which direct pilotcommunication among users is required, such that the signaling overhead can be greatly reduced in our proposed scheme.
Moreover, the neighbors with different proximity can also be classified accordingly which enables more accurate neighbordiscovery compared to the conventional schemes. The performance of our proposed scheme is verified through thetensorflow-based computer simulations.
키워드 : 딥러닝, 주변단말 탐색, 단말-대-단말 통신Deep learning, Neighbor discovery, Device-to-device communication, Wireless channel correlation"
551,신뢰성있는 딥러닝 기반 분석 모델을 참조하기 위한 딥러닝 기술 언어,문종혁 /Jong Hyeok Mun,2021,한국정보처리학회,"최근 딥러닝은 하드웨어 성능이 향상됨에 따라 자연어 처리, 영상 인식 등의 다양한 기술에 접목되어 활용되고 있다. 이러한 기술들을 활용해 지능형교통 시스템(ITS), 스마트홈, 헬스케어 등의 산업분야에서 데이터를 분석하여 고속도로 속도위반 차량 검출, 에너지 사용량 제어, 응급상황 등과 같은고품질의 서비스를 제공하며, 고품질의 서비스를 제공하기 위해서는 정확도가 향상된 딥러닝 모델이 적용되어야 한다. 이를 위해 서비스 환경의 데이터를분석하기 위한 딥러닝 모델을 개발할 때, 개발자는 신뢰성이 검증된 최신의 딥러닝 모델을 적용할 수 있어야 한다. 이는 개발자가 참조하는 딥러닝 모델에적용된 학습 데이터셋의 정확도를 측정하여 검증할 수 있다. 이러한 검증을 위해서 개발자는 학습 데이터셋, 딥러닝의 계층구조 및 개발 환경 등과 같은내용을 포함하는 딥러닝 모델을 문서화하여 적용하기 위한 구조적인 정보가 필요하다. 본 논문에서는 신뢰성있는 딥러닝 기반 데이터 분석 모델을 참조하기위한 딥러닝 기술 언어를 제안한다. 제안하는 기술 언어는 신뢰성 있는 딥러닝 모델을 개발하는데 필요한 학습데이터셋, 개발 환경 및 설정 등의 정보와더불어 딥러닝 모델의 계층구조를 표현할 수 있다. 제안하는 딥러닝 기술 언어를 이용하여 개발자는 지능형 교통 시스템에서 참조하는 분석 모델의정확도를 검증할 수 있다. 실험에서는 제안하는 언어의 유효성을 검증하기 위해, 번호판 인식 모델을 중심으로 딥러닝 기술 문서의 적용과정을 보인다With the recent advancements of deep learning, companies such as smart home, healthcare, and intelligent transportation systems areutilizing its functionality to provide high-quality services for vehicle detection, emergency situation detection, and controlling energyconsumption. To provide reliable services in such sensitive systems, deep learning models are required to have high accuracy. In orderto develop a deep learning model for analyzing previously mentioned services, developers should utilize the state of the art deep learningmodels that have already been verified for higher accuracy. The developers can verify the accuracy of the referenced model by validatingthe model on the dataset. For this validation, the developer needs structural information to document and apply deep learning models,including metadata such as learning dataset, network architecture, and development environments. In this paper, we propose a descriptionlanguage that represents the network architecture of the deep learning model along with its metadata that are necessary to develop adeep learning model. Through the proposed description language, developers can easily verify the accuracy of the referenced deep learningmodel. Our experiments demonstrate the application scenario of a deep learning description document that focuses on the license platerecognition for the detection of illegally parked vehicles.Trusted Deep Learning, Model Reference, Deep Learning Description Language, Traffic Situation Analysis Model"
552,딥러닝 분석을 이용한 중국 역내·외 위안화 변동성 예측,이우식 /Lee Woosik,2016,한국데이터정보과학회,"2008년 글로벌 금융위기 이후 중국은 위안화 국제화의 점진적 추진을 진행하면서 중국상하이 외환시장과 중국홍콩 외환시장에서 거래되는 통화인 역내위안화와 역외위안화를 형성시켰다. 본 연구는 위안화 국제화와 점진적인 중국 자본계정 개방에 따라 급변하는 외환시장상황의 변동성을 정확하게 파악하기 위해서 GARCH모형 (일반화된 자기회귀 조건부이분산성모형)에 다단계인공신경망을 결합한 MLP-GARCH 모형과 GARCH모형과 기계학습의 일종인 딥러닝 (deep learning)을 통합한 DL-GARCH을 가지고 위안화 변동성예측을 비교 실험과 분석을 하였다. 비교분석 결과 DLGARCH 모형은 MLP-GARCH보다 모형 위안화 역내·외 환율변동성 예측 면에서 더욱 더 개선된예측값을 제공하였다. 그래서 이분산시계열모형을 딥러닝과 결합한 DL-GARCH 모형은 시계열의환율 변동성 예측 문제에 딥러닝을 응용할 수 있음을 확인하였다. 향후 이분산시계열과 결합된 딥러닝 모형은 다른 금융시계열 데이터에 응용하여 그 일반화 가능성을 높일 수 있을 것이다.The People’s Republic of China has vigorously been pursuing the internationalization of the Chinese Yuan or Renminbi after the financial crisis of 2008. In this view, an abrupt increase of use of the Chinese Yuan in the onshore and offshore markets are important milestones to be one of important currencies. One of the most frequently used methods to forecast volatility is GARCH model. Since a prediction error of the GARCH model has been reported quite high, a lot of efforts have been made to improve forecasting capability of the GARCH model. In this paper, we have proposed MLPGARCH and a DL-GARCH by employing Artificial Neural Network to the GARCH. In an application to forecasting Chinese Yuan volatility, we have successfully shown their overall outperformance in forecasting over the GARCH.Deep learning, DL-GARCH, GARCH, The Chinese Yuan, volatility."
553,딥러닝 기법을 이용한 내일강수 예측,하지훈,2016,한국지능시스템학회,"정확한 강수예측을 위해서는 예측인자 선정과 예측방법에 대한 선택이 매우 중요하다. 최근에는 강수예측 방법으로기계학습 기법이 많이 사용되고 있으며, 그 중에서도 특히 인공신경망을 사용한 강수예측 방법은 좋은 성능을 보였다. 본논문에서는 딥러닝 기법 중 하나인 DBN(deep belief network)를 이용한 새로운 강수예측 방법을 제안한다. DBN는 비지도사전 학습을 통해 초기 가중치를 설정하여 기존 인공신경망의 문제점을 보완한다. 예측인자로는 기온, 전일-전주 강수일, 태양과 달 궤도 관련 자료를 선정하였다. 기온과 전일-전주 강수일은 서울에서의 1974년부터 2013년까지 총 40년간의AWS(automatic weather system) 관측 자료를 사용하였고, 태양과 달의 궤도 관련 자료는 서울을 중심으로 계산한 결과를사용하였다. 전체 기간에서 일부는 학습 자료로 사용하여 예측모델을 생성하였고, 나머지를 생성한 모델의 검증 자료로사용하였다. 모델 검증 결과로 나온 예측값들은 확률값을 가지며 임계치를 이용하여 강수유무를 판별하였다. 강수 정확도의척도로 양분예보기법 중 CSI(critical successive index)와 Bias(frequency bias)를 계산하였다. 이를 통해 DBN와 MLP(multilayer perceptron)의 성능을 비교한 결과 DBN의 강수 예측 정확도가 높았고, 수행속도 또한 2배 이상 빨랐다.For accurate precipitation forecasts the choice of weather faoctrs and prediction method is very important. Recently, machinele arning has been widely used for forecasting precipitation, and artificial neural network, one of machine learning techniques, showed good performance. In this paper, we suggest a new method for forecasting precipitation using DBN, one of deep learning techniques.
DBN has an advantage that initial weights are set by unsupervised learning, so this compensates for the defects of artificial neural networks. We used past precipitation, temperature, and the parameters of the sun and moon's motion as features for forecasting precipitation. The dataset consists of observation data which had been measured for 40 years from AWS in Seoul. Experiments were based on 8-fold cross validation. As a result of estimation, we got probabilities of test dataset, so threshold was used for the decision of precipitation. CSI and Bias were used for indicating the precision of precipitation. Our experimental results showed that DBN performed better than MLP.Deep learning, Deep belief network, Precipitation, Forecast"
554,딥러닝 신경망모형을 이용한 실시간 교통정보수집 연구,김대현 /KIM DAE HYON,2016,사단법인 인문사회과학기술융합학회,"비디오 기반의 실시간 교통정보수집 시스템은 자율주행을 포함한 지능형교통체계의 주요 서브시스템중 하나이며, 우수한 예측(추정)력 때문에 핵심 알고리즘으로 인공신경망 모형이 주로 사용되고 있다. 특히, 최근 과학 학술지 네이처에 구글 딥마인드의 알파고가 공개되면서 딥러닝과 인공지능에 대한 전 세계적인 관심이 집중되고 있다. 최근까지 가장 널리 사용된 인공신경망모형은 역전파 모형이었으나 최근 컴퓨터 하드웨어 및 알고리즘의 새로운 개발과 더불어 딥러닝 알고리즘을 이용한 딥인공신경망 모형에 대한 관심이 높아지고 있다. 그러나 아직 딥러닝 인공신경망에 대한 연구는 초기단계에 있는 실정이며 적용분야도 극히 한정되어 있다. 본 연구에서는 실시간 도로교통정보 수집을 위한 영상처리분야에서 딥러닝 알고리즘을 활용하고 실험결과를 통하여 향후 교통정보수집‧분석 분야에서의 딥러닝 인공신경망 모형의 적용가능성을 살펴보고자 한다. 특히, 본 연구에서 도출한 딥러닝 인공신경망모형의 실험결과를 기존 역전파 모형에 의한 실험결과와 비교하여 모형의 우수성을 비교‧평가하고자 한다.Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Normally, artificial neural networks have been used in real-time applications for collecting traffic information because of their remarkable performance in terms of predictive accuracy and real-time computing. Especially, the recent publication of a paper in the journal Nature and the success of Google DeepMind’s world-beating Go program AlphaGo caused significant increases of the public interest in artificial intelligence. Even though various types of learning models have been proposed, the Backpropagation has been the most popular neural network model. However, the Deep learning neural network model has attracted much interest recently. In this study, the Deep leaning neural network will be applied in image processing techniques for the real-time traffic information collection to investigate the performance of Deep learning algorithm in traffic engineering fields. In addition, the experimental results from Deep learing algorithm will be compared with the results from Backpropagation model to evaluate and propose better performing model.Intelligent Transportation Systems (ITS), artificial neural networks, Deep learning, traffic information, image processing"
555,딥러닝의 모형과 응용사례,안성만 /SungMahn Ahn,2016,한국지능정보시스템학회,"딥러닝은 인공신경망(neural network)이라는 인공지능분야의 모형이 발전된 형태로서, 계층구조로 이루어진 인공신경망의 내부계층(hidden layer)이 여러 단계로 이루어진 구조이다. 딥러닝에서의 주요 모형은 합성곱신경망(convolutional neural network), 순환신경망(recurrent neural network), 그리고 심층신뢰신경망(deep belief network)의 세가지라고 할 수있다. 그 중에서 현재 흥미로운 연구가 많이 발표되어서 관심이 집중되고 있는 모형은 지도학습(supervised learning)모형인 처음 두 개의 모형이다. 따라서 본 논문에서는 지도학습모형의 가중치를 최적화하는 기본적인 방법인 오류역전파 알고리즘을 살펴본 뒤에 합성곱신경망과 순환신경망의 구조와 응용사례 등을 살펴보고자 한다. 본문에서 다루지 않은 모형인 심층신뢰신경망은 아직까지는 합성곱신경망 이나 순환신경망보다는 상대적으로 주목을 덜 받고 있다. 그러나 심층신뢰신경망은 CNN이나 RNN과는 달리 비지도학습(unsupervised learning)모형이며, 사람이나 동물은 관찰을 통해서 스스로 학습한다는 점에서 궁극적으로는 비지도학습모형이 더 많이 연구되어야 할 주제가 될 것이다.Deep learning model is a kind of neural networks that allows multiple hidden layers. There are various deep learning architectures such as convolutional neural networks, deep belief networks and recurrent neural networks. Those have been applied to fields like computer vision, automatic speech recognition, natural language processing, audio recognition and bioinformatics where they have been shown to produce state-of-the-art results on various tasks. Among those architectures, convolutional neural networks and recurrent neural networks are classified as the supervised learning model. And in recent years, those supervised learning models have gained more popularity than unsupervised learning models such as deep belief networks, because supervised learning models have shown fashionable applications in such fields mentioned above.
Deep learning models can be trained with backpropagation algorithm. Backpropagation is an abbreviation for “backward propagation of errors” and a common method of training artificial neural networks used in conjunction with an optimization method such as gradient descent. The method calculates the gradient of an error function with respect to all the weights in the network. The gradient is fed to the optimization method which in turn uses it to update the weights, in an attempt to minimize the error function.
Convolutional neural networks use a special architecture which is particularly well-adapted to classify images. Using this architecture makes convolutional networks fast to train. This, in turn, helps us train deep, muti-layer networks, which are very good at classifying images. These days, deep convolutional networks are used in most neural networks for image recognition. Convolutional neural networks use three basic ideas: local receptive fields, shared weights, and pooling. By local receptive fields, we mean that each neuron in the first(or any) hidden layer will be connected to a small region of the input(or previous layer’s) neurons. Shared weights mean that we’re going to use the same weights and bias for each of the local receptive field. This means that all the neurons in the hidden layer detect exactly the same feature, just at different locations in the input image. In addition to the convolutional layers just described, convolutional neural networks also contain pooling layers. Pooling layers are usually used immediately after convolutional layers. What the pooling layers do is to simplify the information in the output from the convolutional layer.
Recent convolutional network architectures have 10 to 20 hidden layers and billions of connections between units. Training deep learning networks has taken weeks several years ago, but thanks to progress in GPU and algorithm enhancement, training time has reduced to several hours.
Neural networks with time-varying behavior are known as recurrent neural networks or RNNs. A recurrent neural network is a class of artificial neural network where connections between units form a directed cycle. This creates an internal state of the network which allows it to exhibit dynamic temporal behavior. Unlike feedforward neural networks, RNNs can use their internal memory to process arbitrary sequences of inputs. Early RNN models turned out to be very difficult to train, harder even than deep feedforward networks. The reason is the unstable gradient problem such as vanishing gradient and exploding gradient. The gradient can get smaller and smaller as it is propagated back through layers. This makes learning in early layers extremely slow. The problem actually gets worse in RNNs, since gradients aren’t just propagated backward through layers, they’re propagated backward through time. If the network runs for a long time, that can make the gradient extremely unstable and hard to learn from. It has been possible to incorporate an idea known as long short-term memory units (LSTMs) into RNNs. LSTMs make it much easier to get good results when training RNNs, and many recent papers make use of LSTMs or related ideas.Deep learning, Convolutional neural networks, Recurrent neural networks, Error backpropagation algorithm"
556,딥러닝분석과 기술적 분석 지표를 이용한 한국 코스피주가지수 방향성 예측,이우식 /Lee Woosik,2017,한국데이터정보과학회,"2016년 3월 구글 (Google)의 바둑인공지능 알파고 (AlphaGo)가 이세돌 9단과의 바둑대결에서승리한 이후 다양한 분야에서 인공지능 사용에 대한 관심이 높아지고 있는 가운데 금융투자 분야에서도 인공지능과 투자자문 전문가의 합성어인 로보어드바이저 (Robo-Advisor)에 대한 관심이 높아지고 있다. 인공지능 (artificial intelligence)기반의 의사결정은 비용 절감은 물론 효과적인 의사결정을가능하게 한다는 점에서 큰 장점이 있다. 본 연구에서는 기술적 분석 (technical analysis) 지표와 딥러닝 (deep learning) 모형을 결합하여 한국 코스피 지수를 예측하는 모형을 개발하고 제시한 모형들의 예측력을 비교, 분석한다. 분석 결과 기술적 분석 지표에 딥러닝 알고리즘을 결합한 모형이 주가지수 방향성 예측 문제에 응용될 수 있음을 확인하였다. 향후 본 연구에서 제안된 기술적 분석 지표와딥러닝모형을 결합한 기법은 로보어드바이저서비스에 응용할 수 있는 일반화 가능성을 보여준다Since Google’s AlphaGo defeated a world champion of Go players in 2016, there have been many interests in the deep learning. In the financial sector, a Robo-Advisor using deep learning gains a significant attention, which builds and manages portfolios of financial instruments for investors.In this paper, we have proposed the a deep learning algorithm geared toward identification and forecast of the KOSPI index direction,and we also have compared the accuracy of the prediction.In an application of forecasting the financial market index direction, we have shown that the Robo-Advisor using deep learning has a significant effect on finance industry. The Robo-Advisor collects a massive data such as earnings statements, news reports and regulatory filings, analyzes those and recommends investors how to view market trends and identify the best time to purchase financial assets. On the other hand, the Robo-Advisor allows businesses to learn more about their customers, develop better marketing strategies, increase sales and decrease costs.Artificial intelligence, deep learning, FinTech, Robo-Advisor, technical analysis."
557,딥러닝의 효율적인 학습을 위한 학습데이터 선별 기법에 관한 연구,주경돈 /Gyoung-don Joo,2017,한국정보과학회,"빅데이터의 증가와 병렬 하드웨어 처리, 학습 알고리즘의 개선은 딥러닝의 발전을 가져왔으며 인공지능이 적용 가능한 영역을 획기적으로 확장시키고 있다. 하지만 이러한 대규모의 학습 데이터에 기반한 딥러닝은 학습시간이 증가되고, 더 나아가 학습 데이터의 구축을 위한 레이블링(labeling) 비용이 데이터 수에 비례하여 증가하게 된다. 따라서 본 논문에서는, 딥러닝의 정확도를 최대한 유지하면서 필요한 학습 데이터의 수를 줄일수 있도록, 전체 데이터에서 제한된 수의 학습 데이터의 후보를 선별하는 기법에 대하여 연구한다. 특히 기존의 능동 학습에 기반한 기법들이 학습 데이터의 수가 매우 적어질 경우 딥러닝의 정확도가 급격히 떨어지는 문제를 해결할 수 있는 새로운 기법을 제안하였으며, 제안된 기법으로 선별된 학습 데이터는 그 수의 변화에 대해 딥러닝이 일관되게 경쟁력있는 정확도를 가짐을 다양한 실험을 통하여 검증하였다.The increase of big data, parallel hardware processing and enhanced learning algorithms have led to the development of deep learning and are dramatically expanding the domain of applications based on artificial intelligence. However, such large-scale training data extends the learning time, and furthermore, the labeling cost for building the training set increases in proportion to the volume of data. Therefore, in this paper, we study how to select a limited number of candidates for training set from given data so as to reduce the number of training set while maintaining the accuracy of deep learning. Especially, we found that, with the training set selected by the previous work, the accuracy of deep learning drastically decreases when the number of training set is very small and so proposed new methods to resolve this fall-off in accuracy. Through experiments, we showed that deep learning consistently has the competitive accuracy with the training set selected by the proposed method regardless of the size of training set.Deep Learning, Train Set Selection, Active Learning, Gamma distribution"
558,딥러닝을 이용한 범용적 스테그아날리시스,김현재 /Hyunjae Kim,2017,한국정보과학회,"스테그아날리시스(Steganalysis)란 이미지 등 일반적인 자료에 암호화된 정보를 은닉하는 스테가노그래피(Steganography)에 대한 검출 및 분석 방법으로, 기계학습 기반 방법론을 포함한다. 기존 기계학습 기반 스테그아날리시스는 영상(Image)의 특징(Feature) 추출 및 모델링에 기반하며, 최근 딥러닝(Deep Learning)의 적용으로 검출 정확도가 큰 폭으로 향상되었다. 하지만 현존하는 스테그아날리시스 모델은 단일 스테가노그래피 기법에 대해 국한되어 있어 학습에 사용되지 않은 스테고(Stego) 이미지의 경우 검출이 불가능한 결정적 한계를 가진다. 본 연구에서는 다양한 스테가노그래피 기법으로 생성된 스테고이미지에 딥러닝을 적용하여 스테그아날리시스를 학습하는 범용적 모델을 제안한다. 다양한 실험을 통해 제안 기법의 효용성 및 가능성을 확인하고, 범용적 스테그아날리시스 모델이 각각에 특화된 검출 기법과 유사한 정확도로 스테고 이미지를 검출할 수 있음을 보인다.Steganalysis is to detect information hidden by steganography inside general data such as images. There are stegoanalysis techniques that use machine learning (ML). Existing ML approaches to steganalysis are based on extracting features from stego images and modeling them.
Recently deep learning-based methodologies have shown significant improvements in detection accuracy. However, all the existing methods, including deep learning-based ones, have a critical limitation in that they can only detect stego images that are created by a specific steganography method. In this paper, we propose a generalized steganalysis method that can model multiple types of stego images using deep learning. Through various experiments, we confirm the effectiveness of our approach and envision directions for future research. In particular, we show that our method can detect each type of steganography with the same level of accuracy as that of a steganalysis method dedicated to that type of steganography, thereby demonstrating the general applicability of our approach to multiple types of stego images.steganalysis, steganography, deep learning, generalized steganalysis model"
559,"딥러닝 프레임워크의 비교: 티아노, 텐서플로, CNTK를 중심으로",정여진 /Yeojin Chung,2017,한국지능정보시스템학회,"딥러닝 프레임워크의 대표적인 기능으로는 ‘자동미분’과 ‘GPU의 활용’ 등을 들 수 있다. 본 논문은 파이썬의 라이브러리 형태로 사용 가능한 프레임워크 중에서 구글의 텐서플로와 마이크로소프트의 CNTK, 그리고 텐서 플로의 원조라고 할 수 있는 티아노를 비교하였다. 본문에서는 자동미분의 개념과 GPU의 활용형태를 간단히 설명하고, 그 다음에 logistic regression을 실행하는 예를 통하여 각 프레임워크의 문법을 알아본 뒤에, 마지막으로 대표적인 딥러닝 응용인 CNN의 예제를 실행시켜보고 코딩의 편의성과 실행속도 등을 확인해 보았다. 그 결과, 편의성의 관점에서 보면 티아노가 가장 코딩 하기가 어렵고, CNTK와 텐서플로는 많은 부분이 비슷하게 추상화 되어 있어서 코딩이 비슷하지만 가중치와 편향을 직접 정의하느냐의 여부에서 차이를 보였다. 그리고 각프레임워크의 실행속도에 대한 평가는 ‘큰 차이는 없다’는 것이다. 텐서플로는 티아노에 비하여 속도가 느리다는 평가가 있어왔는데, 본 연구의 실험에 의하면, 비록 CNN 모형에 국한되었지만, 텐서플로가 아주 조금이지만빠른 것으로 나타났다. CNTK의 경우에도, 비록 실험환경이 달랐지만, 실험환경의 차이에 의한 속도의 차이의 편차범위 이내에 있는 것으로 판단이 되었다. 본 연구에서는 세 종류의 딥러닝 프레임워크만을 살펴보았는데,위키피디아에 따르면 딥러닝 프레임워크의 종류는 12가지가 있으며, 각 프레임워크의 특징을 15가지 속성으로구분하여 차이를 특정하고 있다. 그 많은 속성 중에서 사용자의 입장에서 볼 때 중요한 속성은 어떤 언어(파이썬, C++, Java, 등)로 사용가능한지, 어떤 딥러닝 모형에 대한 라이브러리가 잘 구현되어 있는지 등일 것이다. 그리고 사용자가 대규모의 딥러닝 모형을 구축한다면, 다중 GPU 혹은 다중 서버를 지원하는지의 여부도 중요할 것이다. 또한 딥러닝 모형을 처음 학습하는 경우에는 사용설명서가 많은지 예제 프로그램이 많은지 여부도 중요한 기준이 될 것이다.The deep learning framework is software designed to help develop deep learning models. Some of its important functions include “automatic differentiation” and “utilization of GPU”. The list of popular deep learning framework includes Caffe (BVLC) and Theano (University of Montreal). And recently, Microsoft's deep learning framework, Microsoft Cognitive Toolkit, was released as open-source license, following Google’s Tensorflow a year earlier. The early deep learning frameworks have been developed mainly for research at universities. Beginning with the inception of Tensorflow, however, it seems that companies such as Microsoft and Facebook have started to join the competition of framework development. Given the trend, Google and other companies are expected to continue investing in the deep learning framework to bring forward the initiative in the artificial intelligence business. From this point of view, we think it is a good time to compare some of deep learning frameworks. So we compare three deep learning frameworks which can be used as a Python library. Those are Google's Tensorflow, Microsoft’s CNTK, and Theano which is sort of a predecessor of the preceding two.
The most common and important function of deep learning frameworks is the ability to perform automatic differentiation. Basically all the mathematical expressions of deep learning models can be represented as computational graphs, which consist of nodes and edges. Partial derivatives on each edge of a computational graph can then be obtained. With the partial derivatives, we can let software compute differentiation of any node with respect to any variable by utilizing chain rule of Calculus.
First of all, the convenience of coding is in the order of CNTK, Tensorflow, and Theano. The criterion is simply based on the lengths of the codes and the learning curve and the ease of coding are not the main concern. According to the criteria, Theano was the most difficult to implement with, and CNTK and Tensorflow were somewhat easier. With Tensorflow, we need to define weight variables and biases explicitly. The reason that CNTK and Tensorflow are easier to implement with is that those frameworks provide us with more abstraction than Theano. We, however, need to mention that low-level coding is not always bad. It gives us flexibility of coding. With the low-level coding such as in Theano, we can implement and test any new deep learning models or any new search methods that we can think of.
The assessment of the execution speed of each framework is that there is not meaningful difference. According to the experiment, execution speeds of Theano and Tensorflow are very similar, although the experiment was limited to a CNN model. In the case of CNTK, the experimental environment was not maintained as the same. The code written in CNTK has to be run in PC environment without GPU where codes execute as much as 50 times slower than with GPU. But we concluded that the difference of execution speed was within the range of variation caused by the different hardware setup.
In this study, we compared three types of deep learning framework: Theano, Tensorflow, and CNTK. According to Wikipedia, there are 12 available deep learning frameworks. And 15 different attributes differentiate each framework. Some of the important attributes would include interface language (Python, C ++, Java, etc.) and the availability of libraries on various deep learning models such as CNN, RNN, DBN, and etc. And if a user implements a large scale deep learning model, it will also be important to support multiple GPU or multiple servers. Also, if you are learning the deep learning model, it would also be important if there are enough examples and references.deep learning framework, Theano, TensorFlow, CNTK, computational graph, CIFAR-10"
560,딥러닝을 이용한 주변 무선단말 파악방안,이웅섭 /Lee， Woong-sup,2018,한국정보통신학회,"최근 단말-대-단말(Device-to-device, D2D) 통신기술이 차세대 무선통신시스템의 핵심기술로 큰 관심을 받고 있다. 이러한 단말간 통신에서는 자신의 주변에 어떠한 단말이 있는지 파악하는 주변단말 탐색(Neighbor discovery)이매우 중요하다. 본 논문에서는 최근 큰 관심을 받고 있는 딥러닝(Deep learning) 기술을 활용하여 단말간 통신에서 주변단말을 파악하는 방안에 대해서 제안한다. 제안 방안은 기존의 방안과 달리 무선채널의 공간적 연관성을 이용하여단말간의 신호 전송 없이 단말이 기지국으로 전송하는 상향링크 파일럿 신호를 기반으로 주변 단말을 찾고 따라서기존의 방식에 비해 신호전송 복잡도(signaling complexity)를 크게 줄일 수 있다. 또한 제안 방안에서는 떨어져 있는거리에 따라서 주변 단말을 분류 가능하여 기존 방안에 비해서 좀 더 세밀한 단말 탐색이 가능하다. 마지막으로 본 논문에서는 tensorflow를 이용한 컴퓨터 시뮬레이션을 통해 제안 방안의 성능을 검증하였다.Recently, the device-to-device (D2D) communication has been conceived as the key technology for the next-generationmobile communication systems. The neighbor discovery in which the nearby users are found, is essential for the properoperation of the D2D communication. In this paper, we propose new neighbor discovery scheme based on deep learningtechnology which has gained a lot of attention recently. In the proposed scheme, the neighboring users can be foundusing the uplink pilot transmission of users only, unlike conventional neighbor discovery schemes in which direct pilotcommunication among users is required, such that the signaling overhead can be greatly reduced in our proposed scheme.
Moreover, the neighbors with different proximity can also be classified accordingly which enables more accurate neighbordiscovery compared to the conventional schemes. The performance of our proposed scheme is verified through thetensorflow-based computer simulations.
키워드 : 딥러닝, 주변단말 탐색, 단말-대-단말 통신Deep learning, Neighbor discovery, Device-to-device communication, Wireless channel correlation"
561,신뢰성있는 딥러닝 기반 분석 모델을 참조하기 위한 딥러닝 기술 언어,문종혁 /Jong Hyeok Mun,2021,한국정보처리학회,"최근 딥러닝은 하드웨어 성능이 향상됨에 따라 자연어 처리, 영상 인식 등의 다양한 기술에 접목되어 활용되고 있다. 이러한 기술들을 활용해 지능형교통 시스템(ITS), 스마트홈, 헬스케어 등의 산업분야에서 데이터를 분석하여 고속도로 속도위반 차량 검출, 에너지 사용량 제어, 응급상황 등과 같은고품질의 서비스를 제공하며, 고품질의 서비스를 제공하기 위해서는 정확도가 향상된 딥러닝 모델이 적용되어야 한다. 이를 위해 서비스 환경의 데이터를분석하기 위한 딥러닝 모델을 개발할 때, 개발자는 신뢰성이 검증된 최신의 딥러닝 모델을 적용할 수 있어야 한다. 이는 개발자가 참조하는 딥러닝 모델에적용된 학습 데이터셋의 정확도를 측정하여 검증할 수 있다. 이러한 검증을 위해서 개발자는 학습 데이터셋, 딥러닝의 계층구조 및 개발 환경 등과 같은내용을 포함하는 딥러닝 모델을 문서화하여 적용하기 위한 구조적인 정보가 필요하다. 본 논문에서는 신뢰성있는 딥러닝 기반 데이터 분석 모델을 참조하기위한 딥러닝 기술 언어를 제안한다. 제안하는 기술 언어는 신뢰성 있는 딥러닝 모델을 개발하는데 필요한 학습데이터셋, 개발 환경 및 설정 등의 정보와더불어 딥러닝 모델의 계층구조를 표현할 수 있다. 제안하는 딥러닝 기술 언어를 이용하여 개발자는 지능형 교통 시스템에서 참조하는 분석 모델의정확도를 검증할 수 있다. 실험에서는 제안하는 언어의 유효성을 검증하기 위해, 번호판 인식 모델을 중심으로 딥러닝 기술 문서의 적용과정을 보인다With the recent advancements of deep learning, companies such as smart home, healthcare, and intelligent transportation systems areutilizing its functionality to provide high-quality services for vehicle detection, emergency situation detection, and controlling energyconsumption. To provide reliable services in such sensitive systems, deep learning models are required to have high accuracy. In orderto develop a deep learning model for analyzing previously mentioned services, developers should utilize the state of the art deep learningmodels that have already been verified for higher accuracy. The developers can verify the accuracy of the referenced model by validatingthe model on the dataset. For this validation, the developer needs structural information to document and apply deep learning models,including metadata such as learning dataset, network architecture, and development environments. In this paper, we propose a descriptionlanguage that represents the network architecture of the deep learning model along with its metadata that are necessary to develop adeep learning model. Through the proposed description language, developers can easily verify the accuracy of the referenced deep learningmodel. Our experiments demonstrate the application scenario of a deep learning description document that focuses on the license platerecognition for the detection of illegally parked vehicles.Trusted Deep Learning, Model Reference, Deep Learning Description Language, Traffic Situation Analysis Model"
562,딥러닝 분석을 이용한 중국 역내·외 위안화 변동성 예측,이우식 /Lee Woosik,2016,한국데이터정보과학회,"2008년 글로벌 금융위기 이후 중국은 위안화 국제화의 점진적 추진을 진행하면서 중국상하이 외환시장과 중국홍콩 외환시장에서 거래되는 통화인 역내위안화와 역외위안화를 형성시켰다. 본 연구는 위안화 국제화와 점진적인 중국 자본계정 개방에 따라 급변하는 외환시장상황의 변동성을 정확하게 파악하기 위해서 GARCH모형 (일반화된 자기회귀 조건부이분산성모형)에 다단계인공신경망을 결합한 MLP-GARCH 모형과 GARCH모형과 기계학습의 일종인 딥러닝 (deep learning)을 통합한 DL-GARCH을 가지고 위안화 변동성예측을 비교 실험과 분석을 하였다. 비교분석 결과 DLGARCH 모형은 MLP-GARCH보다 모형 위안화 역내·외 환율변동성 예측 면에서 더욱 더 개선된예측값을 제공하였다. 그래서 이분산시계열모형을 딥러닝과 결합한 DL-GARCH 모형은 시계열의환율 변동성 예측 문제에 딥러닝을 응용할 수 있음을 확인하였다. 향후 이분산시계열과 결합된 딥러닝 모형은 다른 금융시계열 데이터에 응용하여 그 일반화 가능성을 높일 수 있을 것이다.The People’s Republic of China has vigorously been pursuing the internationalization of the Chinese Yuan or Renminbi after the financial crisis of 2008. In this view, an abrupt increase of use of the Chinese Yuan in the onshore and offshore markets are important milestones to be one of important currencies. One of the most frequently used methods to forecast volatility is GARCH model. Since a prediction error of the GARCH model has been reported quite high, a lot of efforts have been made to improve forecasting capability of the GARCH model. In this paper, we have proposed MLPGARCH and a DL-GARCH by employing Artificial Neural Network to the GARCH. In an application to forecasting Chinese Yuan volatility, we have successfully shown their overall outperformance in forecasting over the GARCH.Deep learning, DL-GARCH, GARCH, The Chinese Yuan, volatility."
563,딥러닝 기법을 이용한 내일강수 예측,하지훈,2016,한국지능시스템학회,"정확한 강수예측을 위해서는 예측인자 선정과 예측방법에 대한 선택이 매우 중요하다. 최근에는 강수예측 방법으로기계학습 기법이 많이 사용되고 있으며, 그 중에서도 특히 인공신경망을 사용한 강수예측 방법은 좋은 성능을 보였다. 본논문에서는 딥러닝 기법 중 하나인 DBN(deep belief network)를 이용한 새로운 강수예측 방법을 제안한다. DBN는 비지도사전 학습을 통해 초기 가중치를 설정하여 기존 인공신경망의 문제점을 보완한다. 예측인자로는 기온, 전일-전주 강수일, 태양과 달 궤도 관련 자료를 선정하였다. 기온과 전일-전주 강수일은 서울에서의 1974년부터 2013년까지 총 40년간의AWS(automatic weather system) 관측 자료를 사용하였고, 태양과 달의 궤도 관련 자료는 서울을 중심으로 계산한 결과를사용하였다. 전체 기간에서 일부는 학습 자료로 사용하여 예측모델을 생성하였고, 나머지를 생성한 모델의 검증 자료로사용하였다. 모델 검증 결과로 나온 예측값들은 확률값을 가지며 임계치를 이용하여 강수유무를 판별하였다. 강수 정확도의척도로 양분예보기법 중 CSI(critical successive index)와 Bias(frequency bias)를 계산하였다. 이를 통해 DBN와 MLP(multilayer perceptron)의 성능을 비교한 결과 DBN의 강수 예측 정확도가 높았고, 수행속도 또한 2배 이상 빨랐다.For accurate precipitation forecasts the choice of weather faoctrs and prediction method is very important. Recently, machinele arning has been widely used for forecasting precipitation, and artificial neural network, one of machine learning techniques, showed good performance. In this paper, we suggest a new method for forecasting precipitation using DBN, one of deep learning techniques.
DBN has an advantage that initial weights are set by unsupervised learning, so this compensates for the defects of artificial neural networks. We used past precipitation, temperature, and the parameters of the sun and moon's motion as features for forecasting precipitation. The dataset consists of observation data which had been measured for 40 years from AWS in Seoul. Experiments were based on 8-fold cross validation. As a result of estimation, we got probabilities of test dataset, so threshold was used for the decision of precipitation. CSI and Bias were used for indicating the precision of precipitation. Our experimental results showed that DBN performed better than MLP.Deep learning, Deep belief network, Precipitation, Forecast"
564,딥러닝 신경망모형을 이용한 실시간 교통정보수집 연구,김대현 /KIM DAE HYON,2016,사단법인 인문사회과학기술융합학회,"비디오 기반의 실시간 교통정보수집 시스템은 자율주행을 포함한 지능형교통체계의 주요 서브시스템중 하나이며, 우수한 예측(추정)력 때문에 핵심 알고리즘으로 인공신경망 모형이 주로 사용되고 있다. 특히, 최근 과학 학술지 네이처에 구글 딥마인드의 알파고가 공개되면서 딥러닝과 인공지능에 대한 전 세계적인 관심이 집중되고 있다. 최근까지 가장 널리 사용된 인공신경망모형은 역전파 모형이었으나 최근 컴퓨터 하드웨어 및 알고리즘의 새로운 개발과 더불어 딥러닝 알고리즘을 이용한 딥인공신경망 모형에 대한 관심이 높아지고 있다. 그러나 아직 딥러닝 인공신경망에 대한 연구는 초기단계에 있는 실정이며 적용분야도 극히 한정되어 있다. 본 연구에서는 실시간 도로교통정보 수집을 위한 영상처리분야에서 딥러닝 알고리즘을 활용하고 실험결과를 통하여 향후 교통정보수집‧분석 분야에서의 딥러닝 인공신경망 모형의 적용가능성을 살펴보고자 한다. 특히, 본 연구에서 도출한 딥러닝 인공신경망모형의 실험결과를 기존 역전파 모형에 의한 실험결과와 비교하여 모형의 우수성을 비교‧평가하고자 한다.Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Normally, artificial neural networks have been used in real-time applications for collecting traffic information because of their remarkable performance in terms of predictive accuracy and real-time computing. Especially, the recent publication of a paper in the journal Nature and the success of Google DeepMind’s world-beating Go program AlphaGo caused significant increases of the public interest in artificial intelligence. Even though various types of learning models have been proposed, the Backpropagation has been the most popular neural network model. However, the Deep learning neural network model has attracted much interest recently. In this study, the Deep leaning neural network will be applied in image processing techniques for the real-time traffic information collection to investigate the performance of Deep learning algorithm in traffic engineering fields. In addition, the experimental results from Deep learing algorithm will be compared with the results from Backpropagation model to evaluate and propose better performing model.Intelligent Transportation Systems (ITS), artificial neural networks, Deep learning, traffic information, image processing"
565,딥러닝의 모형과 응용사례,안성만 /SungMahn Ahn,2016,한국지능정보시스템학회,"딥러닝은 인공신경망(neural network)이라는 인공지능분야의 모형이 발전된 형태로서, 계층구조로 이루어진 인공신경망의 내부계층(hidden layer)이 여러 단계로 이루어진 구조이다. 딥러닝에서의 주요 모형은 합성곱신경망(convolutional neural network), 순환신경망(recurrent neural network), 그리고 심층신뢰신경망(deep belief network)의 세가지라고 할 수있다. 그 중에서 현재 흥미로운 연구가 많이 발표되어서 관심이 집중되고 있는 모형은 지도학습(supervised learning)모형인 처음 두 개의 모형이다. 따라서 본 논문에서는 지도학습모형의 가중치를 최적화하는 기본적인 방법인 오류역전파 알고리즘을 살펴본 뒤에 합성곱신경망과 순환신경망의 구조와 응용사례 등을 살펴보고자 한다. 본문에서 다루지 않은 모형인 심층신뢰신경망은 아직까지는 합성곱신경망 이나 순환신경망보다는 상대적으로 주목을 덜 받고 있다. 그러나 심층신뢰신경망은 CNN이나 RNN과는 달리 비지도학습(unsupervised learning)모형이며, 사람이나 동물은 관찰을 통해서 스스로 학습한다는 점에서 궁극적으로는 비지도학습모형이 더 많이 연구되어야 할 주제가 될 것이다.Deep learning model is a kind of neural networks that allows multiple hidden layers. There are various deep learning architectures such as convolutional neural networks, deep belief networks and recurrent neural networks. Those have been applied to fields like computer vision, automatic speech recognition, natural language processing, audio recognition and bioinformatics where they have been shown to produce state-of-the-art results on various tasks. Among those architectures, convolutional neural networks and recurrent neural networks are classified as the supervised learning model. And in recent years, those supervised learning models have gained more popularity than unsupervised learning models such as deep belief networks, because supervised learning models have shown fashionable applications in such fields mentioned above.
Deep learning models can be trained with backpropagation algorithm. Backpropagation is an abbreviation for “backward propagation of errors” and a common method of training artificial neural networks used in conjunction with an optimization method such as gradient descent. The method calculates the gradient of an error function with respect to all the weights in the network. The gradient is fed to the optimization method which in turn uses it to update the weights, in an attempt to minimize the error function.
Convolutional neural networks use a special architecture which is particularly well-adapted to classify images. Using this architecture makes convolutional networks fast to train. This, in turn, helps us train deep, muti-layer networks, which are very good at classifying images. These days, deep convolutional networks are used in most neural networks for image recognition. Convolutional neural networks use three basic ideas: local receptive fields, shared weights, and pooling. By local receptive fields, we mean that each neuron in the first(or any) hidden layer will be connected to a small region of the input(or previous layer’s) neurons. Shared weights mean that we’re going to use the same weights and bias for each of the local receptive field. This means that all the neurons in the hidden layer detect exactly the same feature, just at different locations in the input image. In addition to the convolutional layers just described, convolutional neural networks also contain pooling layers. Pooling layers are usually used immediately after convolutional layers. What the pooling layers do is to simplify the information in the output from the convolutional layer.
Recent convolutional network architectures have 10 to 20 hidden layers and billions of connections between units. Training deep learning networks has taken weeks several years ago, but thanks to progress in GPU and algorithm enhancement, training time has reduced to several hours.
Neural networks with time-varying behavior are known as recurrent neural networks or RNNs. A recurrent neural network is a class of artificial neural network where connections between units form a directed cycle. This creates an internal state of the network which allows it to exhibit dynamic temporal behavior. Unlike feedforward neural networks, RNNs can use their internal memory to process arbitrary sequences of inputs. Early RNN models turned out to be very difficult to train, harder even than deep feedforward networks. The reason is the unstable gradient problem such as vanishing gradient and exploding gradient. The gradient can get smaller and smaller as it is propagated back through layers. This makes learning in early layers extremely slow. The problem actually gets worse in RNNs, since gradients aren’t just propagated backward through layers, they’re propagated backward through time. If the network runs for a long time, that can make the gradient extremely unstable and hard to learn from. It has been possible to incorporate an idea known as long short-term memory units (LSTMs) into RNNs. LSTMs make it much easier to get good results when training RNNs, and many recent papers make use of LSTMs or related ideas.Deep learning, Convolutional neural networks, Recurrent neural networks, Error backpropagation algorithm"
566,딥러닝분석과 기술적 분석 지표를 이용한 한국 코스피주가지수 방향성 예측,이우식 /Lee Woosik,2017,한국데이터정보과학회,"2016년 3월 구글 (Google)의 바둑인공지능 알파고 (AlphaGo)가 이세돌 9단과의 바둑대결에서승리한 이후 다양한 분야에서 인공지능 사용에 대한 관심이 높아지고 있는 가운데 금융투자 분야에서도 인공지능과 투자자문 전문가의 합성어인 로보어드바이저 (Robo-Advisor)에 대한 관심이 높아지고 있다. 인공지능 (artificial intelligence)기반의 의사결정은 비용 절감은 물론 효과적인 의사결정을가능하게 한다는 점에서 큰 장점이 있다. 본 연구에서는 기술적 분석 (technical analysis) 지표와 딥러닝 (deep learning) 모형을 결합하여 한국 코스피 지수를 예측하는 모형을 개발하고 제시한 모형들의 예측력을 비교, 분석한다. 분석 결과 기술적 분석 지표에 딥러닝 알고리즘을 결합한 모형이 주가지수 방향성 예측 문제에 응용될 수 있음을 확인하였다. 향후 본 연구에서 제안된 기술적 분석 지표와딥러닝모형을 결합한 기법은 로보어드바이저서비스에 응용할 수 있는 일반화 가능성을 보여준다Since Google’s AlphaGo defeated a world champion of Go players in 2016, there have been many interests in the deep learning. In the financial sector, a Robo-Advisor using deep learning gains a significant attention, which builds and manages portfolios of financial instruments for investors.In this paper, we have proposed the a deep learning algorithm geared toward identification and forecast of the KOSPI index direction,and we also have compared the accuracy of the prediction.In an application of forecasting the financial market index direction, we have shown that the Robo-Advisor using deep learning has a significant effect on finance industry. The Robo-Advisor collects a massive data such as earnings statements, news reports and regulatory filings, analyzes those and recommends investors how to view market trends and identify the best time to purchase financial assets. On the other hand, the Robo-Advisor allows businesses to learn more about their customers, develop better marketing strategies, increase sales and decrease costs.Artificial intelligence, deep learning, FinTech, Robo-Advisor, technical analysis."
567,딥러닝의 효율적인 학습을 위한 학습데이터 선별 기법에 관한 연구,주경돈 /Gyoung-don Joo,2017,한국정보과학회,"빅데이터의 증가와 병렬 하드웨어 처리, 학습 알고리즘의 개선은 딥러닝의 발전을 가져왔으며 인공지능이 적용 가능한 영역을 획기적으로 확장시키고 있다. 하지만 이러한 대규모의 학습 데이터에 기반한 딥러닝은 학습시간이 증가되고, 더 나아가 학습 데이터의 구축을 위한 레이블링(labeling) 비용이 데이터 수에 비례하여 증가하게 된다. 따라서 본 논문에서는, 딥러닝의 정확도를 최대한 유지하면서 필요한 학습 데이터의 수를 줄일수 있도록, 전체 데이터에서 제한된 수의 학습 데이터의 후보를 선별하는 기법에 대하여 연구한다. 특히 기존의 능동 학습에 기반한 기법들이 학습 데이터의 수가 매우 적어질 경우 딥러닝의 정확도가 급격히 떨어지는 문제를 해결할 수 있는 새로운 기법을 제안하였으며, 제안된 기법으로 선별된 학습 데이터는 그 수의 변화에 대해 딥러닝이 일관되게 경쟁력있는 정확도를 가짐을 다양한 실험을 통하여 검증하였다.The increase of big data, parallel hardware processing and enhanced learning algorithms have led to the development of deep learning and are dramatically expanding the domain of applications based on artificial intelligence. However, such large-scale training data extends the learning time, and furthermore, the labeling cost for building the training set increases in proportion to the volume of data. Therefore, in this paper, we study how to select a limited number of candidates for training set from given data so as to reduce the number of training set while maintaining the accuracy of deep learning. Especially, we found that, with the training set selected by the previous work, the accuracy of deep learning drastically decreases when the number of training set is very small and so proposed new methods to resolve this fall-off in accuracy. Through experiments, we showed that deep learning consistently has the competitive accuracy with the training set selected by the proposed method regardless of the size of training set.Deep Learning, Train Set Selection, Active Learning, Gamma distribution"
568,딥러닝을 이용한 범용적 스테그아날리시스,김현재 /Hyunjae Kim,2017,한국정보과학회,"스테그아날리시스(Steganalysis)란 이미지 등 일반적인 자료에 암호화된 정보를 은닉하는 스테가노그래피(Steganography)에 대한 검출 및 분석 방법으로, 기계학습 기반 방법론을 포함한다. 기존 기계학습 기반 스테그아날리시스는 영상(Image)의 특징(Feature) 추출 및 모델링에 기반하며, 최근 딥러닝(Deep Learning)의 적용으로 검출 정확도가 큰 폭으로 향상되었다. 하지만 현존하는 스테그아날리시스 모델은 단일 스테가노그래피 기법에 대해 국한되어 있어 학습에 사용되지 않은 스테고(Stego) 이미지의 경우 검출이 불가능한 결정적 한계를 가진다. 본 연구에서는 다양한 스테가노그래피 기법으로 생성된 스테고이미지에 딥러닝을 적용하여 스테그아날리시스를 학습하는 범용적 모델을 제안한다. 다양한 실험을 통해 제안 기법의 효용성 및 가능성을 확인하고, 범용적 스테그아날리시스 모델이 각각에 특화된 검출 기법과 유사한 정확도로 스테고 이미지를 검출할 수 있음을 보인다.Steganalysis is to detect information hidden by steganography inside general data such as images. There are stegoanalysis techniques that use machine learning (ML). Existing ML approaches to steganalysis are based on extracting features from stego images and modeling them.
Recently deep learning-based methodologies have shown significant improvements in detection accuracy. However, all the existing methods, including deep learning-based ones, have a critical limitation in that they can only detect stego images that are created by a specific steganography method. In this paper, we propose a generalized steganalysis method that can model multiple types of stego images using deep learning. Through various experiments, we confirm the effectiveness of our approach and envision directions for future research. In particular, we show that our method can detect each type of steganography with the same level of accuracy as that of a steganalysis method dedicated to that type of steganography, thereby demonstrating the general applicability of our approach to multiple types of stego images.steganalysis, steganography, deep learning, generalized steganalysis model"
569,"딥러닝 프레임워크의 비교: 티아노, 텐서플로, CNTK를 중심으로",정여진 /Yeojin Chung,2017,한국지능정보시스템학회,"딥러닝 프레임워크의 대표적인 기능으로는 ‘자동미분’과 ‘GPU의 활용’ 등을 들 수 있다. 본 논문은 파이썬의 라이브러리 형태로 사용 가능한 프레임워크 중에서 구글의 텐서플로와 마이크로소프트의 CNTK, 그리고 텐서 플로의 원조라고 할 수 있는 티아노를 비교하였다. 본문에서는 자동미분의 개념과 GPU의 활용형태를 간단히 설명하고, 그 다음에 logistic regression을 실행하는 예를 통하여 각 프레임워크의 문법을 알아본 뒤에, 마지막으로 대표적인 딥러닝 응용인 CNN의 예제를 실행시켜보고 코딩의 편의성과 실행속도 등을 확인해 보았다. 그 결과, 편의성의 관점에서 보면 티아노가 가장 코딩 하기가 어렵고, CNTK와 텐서플로는 많은 부분이 비슷하게 추상화 되어 있어서 코딩이 비슷하지만 가중치와 편향을 직접 정의하느냐의 여부에서 차이를 보였다. 그리고 각프레임워크의 실행속도에 대한 평가는 ‘큰 차이는 없다’는 것이다. 텐서플로는 티아노에 비하여 속도가 느리다는 평가가 있어왔는데, 본 연구의 실험에 의하면, 비록 CNN 모형에 국한되었지만, 텐서플로가 아주 조금이지만빠른 것으로 나타났다. CNTK의 경우에도, 비록 실험환경이 달랐지만, 실험환경의 차이에 의한 속도의 차이의 편차범위 이내에 있는 것으로 판단이 되었다. 본 연구에서는 세 종류의 딥러닝 프레임워크만을 살펴보았는데,위키피디아에 따르면 딥러닝 프레임워크의 종류는 12가지가 있으며, 각 프레임워크의 특징을 15가지 속성으로구분하여 차이를 특정하고 있다. 그 많은 속성 중에서 사용자의 입장에서 볼 때 중요한 속성은 어떤 언어(파이썬, C++, Java, 등)로 사용가능한지, 어떤 딥러닝 모형에 대한 라이브러리가 잘 구현되어 있는지 등일 것이다. 그리고 사용자가 대규모의 딥러닝 모형을 구축한다면, 다중 GPU 혹은 다중 서버를 지원하는지의 여부도 중요할 것이다. 또한 딥러닝 모형을 처음 학습하는 경우에는 사용설명서가 많은지 예제 프로그램이 많은지 여부도 중요한 기준이 될 것이다.The deep learning framework is software designed to help develop deep learning models. Some of its important functions include “automatic differentiation” and “utilization of GPU”. The list of popular deep learning framework includes Caffe (BVLC) and Theano (University of Montreal). And recently, Microsoft's deep learning framework, Microsoft Cognitive Toolkit, was released as open-source license, following Google’s Tensorflow a year earlier. The early deep learning frameworks have been developed mainly for research at universities. Beginning with the inception of Tensorflow, however, it seems that companies such as Microsoft and Facebook have started to join the competition of framework development. Given the trend, Google and other companies are expected to continue investing in the deep learning framework to bring forward the initiative in the artificial intelligence business. From this point of view, we think it is a good time to compare some of deep learning frameworks. So we compare three deep learning frameworks which can be used as a Python library. Those are Google's Tensorflow, Microsoft’s CNTK, and Theano which is sort of a predecessor of the preceding two.
The most common and important function of deep learning frameworks is the ability to perform automatic differentiation. Basically all the mathematical expressions of deep learning models can be represented as computational graphs, which consist of nodes and edges. Partial derivatives on each edge of a computational graph can then be obtained. With the partial derivatives, we can let software compute differentiation of any node with respect to any variable by utilizing chain rule of Calculus.
First of all, the convenience of coding is in the order of CNTK, Tensorflow, and Theano. The criterion is simply based on the lengths of the codes and the learning curve and the ease of coding are not the main concern. According to the criteria, Theano was the most difficult to implement with, and CNTK and Tensorflow were somewhat easier. With Tensorflow, we need to define weight variables and biases explicitly. The reason that CNTK and Tensorflow are easier to implement with is that those frameworks provide us with more abstraction than Theano. We, however, need to mention that low-level coding is not always bad. It gives us flexibility of coding. With the low-level coding such as in Theano, we can implement and test any new deep learning models or any new search methods that we can think of.
The assessment of the execution speed of each framework is that there is not meaningful difference. According to the experiment, execution speeds of Theano and Tensorflow are very similar, although the experiment was limited to a CNN model. In the case of CNTK, the experimental environment was not maintained as the same. The code written in CNTK has to be run in PC environment without GPU where codes execute as much as 50 times slower than with GPU. But we concluded that the difference of execution speed was within the range of variation caused by the different hardware setup.
In this study, we compared three types of deep learning framework: Theano, Tensorflow, and CNTK. According to Wikipedia, there are 12 available deep learning frameworks. And 15 different attributes differentiate each framework. Some of the important attributes would include interface language (Python, C ++, Java, etc.) and the availability of libraries on various deep learning models such as CNN, RNN, DBN, and etc. And if a user implements a large scale deep learning model, it will also be important to support multiple GPU or multiple servers. Also, if you are learning the deep learning model, it would also be important if there are enough examples and references.deep learning framework, Theano, TensorFlow, CNTK, computational graph, CIFAR-10"
570,딥러닝을 이용한 주변 무선단말 파악방안,이웅섭 /Lee， Woong-sup,2018,한국정보통신학회,"최근 단말-대-단말(Device-to-device, D2D) 통신기술이 차세대 무선통신시스템의 핵심기술로 큰 관심을 받고 있다. 이러한 단말간 통신에서는 자신의 주변에 어떠한 단말이 있는지 파악하는 주변단말 탐색(Neighbor discovery)이매우 중요하다. 본 논문에서는 최근 큰 관심을 받고 있는 딥러닝(Deep learning) 기술을 활용하여 단말간 통신에서 주변단말을 파악하는 방안에 대해서 제안한다. 제안 방안은 기존의 방안과 달리 무선채널의 공간적 연관성을 이용하여단말간의 신호 전송 없이 단말이 기지국으로 전송하는 상향링크 파일럿 신호를 기반으로 주변 단말을 찾고 따라서기존의 방식에 비해 신호전송 복잡도(signaling complexity)를 크게 줄일 수 있다. 또한 제안 방안에서는 떨어져 있는거리에 따라서 주변 단말을 분류 가능하여 기존 방안에 비해서 좀 더 세밀한 단말 탐색이 가능하다. 마지막으로 본 논문에서는 tensorflow를 이용한 컴퓨터 시뮬레이션을 통해 제안 방안의 성능을 검증하였다.Recently, the device-to-device (D2D) communication has been conceived as the key technology for the next-generationmobile communication systems. The neighbor discovery in which the nearby users are found, is essential for the properoperation of the D2D communication. In this paper, we propose new neighbor discovery scheme based on deep learningtechnology which has gained a lot of attention recently. In the proposed scheme, the neighboring users can be foundusing the uplink pilot transmission of users only, unlike conventional neighbor discovery schemes in which direct pilotcommunication among users is required, such that the signaling overhead can be greatly reduced in our proposed scheme.
Moreover, the neighbors with different proximity can also be classified accordingly which enables more accurate neighbordiscovery compared to the conventional schemes. The performance of our proposed scheme is verified through thetensorflow-based computer simulations.
키워드 : 딥러닝, 주변단말 탐색, 단말-대-단말 통신Deep learning, Neighbor discovery, Device-to-device communication, Wireless channel correlation"
571,신뢰성있는 딥러닝 기반 분석 모델을 참조하기 위한 딥러닝 기술 언어,문종혁 /Jong Hyeok Mun,2021,한국정보처리학회,"최근 딥러닝은 하드웨어 성능이 향상됨에 따라 자연어 처리, 영상 인식 등의 다양한 기술에 접목되어 활용되고 있다. 이러한 기술들을 활용해 지능형교통 시스템(ITS), 스마트홈, 헬스케어 등의 산업분야에서 데이터를 분석하여 고속도로 속도위반 차량 검출, 에너지 사용량 제어, 응급상황 등과 같은고품질의 서비스를 제공하며, 고품질의 서비스를 제공하기 위해서는 정확도가 향상된 딥러닝 모델이 적용되어야 한다. 이를 위해 서비스 환경의 데이터를분석하기 위한 딥러닝 모델을 개발할 때, 개발자는 신뢰성이 검증된 최신의 딥러닝 모델을 적용할 수 있어야 한다. 이는 개발자가 참조하는 딥러닝 모델에적용된 학습 데이터셋의 정확도를 측정하여 검증할 수 있다. 이러한 검증을 위해서 개발자는 학습 데이터셋, 딥러닝의 계층구조 및 개발 환경 등과 같은내용을 포함하는 딥러닝 모델을 문서화하여 적용하기 위한 구조적인 정보가 필요하다. 본 논문에서는 신뢰성있는 딥러닝 기반 데이터 분석 모델을 참조하기위한 딥러닝 기술 언어를 제안한다. 제안하는 기술 언어는 신뢰성 있는 딥러닝 모델을 개발하는데 필요한 학습데이터셋, 개발 환경 및 설정 등의 정보와더불어 딥러닝 모델의 계층구조를 표현할 수 있다. 제안하는 딥러닝 기술 언어를 이용하여 개발자는 지능형 교통 시스템에서 참조하는 분석 모델의정확도를 검증할 수 있다. 실험에서는 제안하는 언어의 유효성을 검증하기 위해, 번호판 인식 모델을 중심으로 딥러닝 기술 문서의 적용과정을 보인다With the recent advancements of deep learning, companies such as smart home, healthcare, and intelligent transportation systems areutilizing its functionality to provide high-quality services for vehicle detection, emergency situation detection, and controlling energyconsumption. To provide reliable services in such sensitive systems, deep learning models are required to have high accuracy. In orderto develop a deep learning model for analyzing previously mentioned services, developers should utilize the state of the art deep learningmodels that have already been verified for higher accuracy. The developers can verify the accuracy of the referenced model by validatingthe model on the dataset. For this validation, the developer needs structural information to document and apply deep learning models,including metadata such as learning dataset, network architecture, and development environments. In this paper, we propose a descriptionlanguage that represents the network architecture of the deep learning model along with its metadata that are necessary to develop adeep learning model. Through the proposed description language, developers can easily verify the accuracy of the referenced deep learningmodel. Our experiments demonstrate the application scenario of a deep learning description document that focuses on the license platerecognition for the detection of illegally parked vehicles.Trusted Deep Learning, Model Reference, Deep Learning Description Language, Traffic Situation Analysis Model"
572,딥러닝 분석을 이용한 중국 역내·외 위안화 변동성 예측,이우식 /Lee Woosik,2016,한국데이터정보과학회,"2008년 글로벌 금융위기 이후 중국은 위안화 국제화의 점진적 추진을 진행하면서 중국상하이 외환시장과 중국홍콩 외환시장에서 거래되는 통화인 역내위안화와 역외위안화를 형성시켰다. 본 연구는 위안화 국제화와 점진적인 중국 자본계정 개방에 따라 급변하는 외환시장상황의 변동성을 정확하게 파악하기 위해서 GARCH모형 (일반화된 자기회귀 조건부이분산성모형)에 다단계인공신경망을 결합한 MLP-GARCH 모형과 GARCH모형과 기계학습의 일종인 딥러닝 (deep learning)을 통합한 DL-GARCH을 가지고 위안화 변동성예측을 비교 실험과 분석을 하였다. 비교분석 결과 DLGARCH 모형은 MLP-GARCH보다 모형 위안화 역내·외 환율변동성 예측 면에서 더욱 더 개선된예측값을 제공하였다. 그래서 이분산시계열모형을 딥러닝과 결합한 DL-GARCH 모형은 시계열의환율 변동성 예측 문제에 딥러닝을 응용할 수 있음을 확인하였다. 향후 이분산시계열과 결합된 딥러닝 모형은 다른 금융시계열 데이터에 응용하여 그 일반화 가능성을 높일 수 있을 것이다.The People’s Republic of China has vigorously been pursuing the internationalization of the Chinese Yuan or Renminbi after the financial crisis of 2008. In this view, an abrupt increase of use of the Chinese Yuan in the onshore and offshore markets are important milestones to be one of important currencies. One of the most frequently used methods to forecast volatility is GARCH model. Since a prediction error of the GARCH model has been reported quite high, a lot of efforts have been made to improve forecasting capability of the GARCH model. In this paper, we have proposed MLPGARCH and a DL-GARCH by employing Artificial Neural Network to the GARCH. In an application to forecasting Chinese Yuan volatility, we have successfully shown their overall outperformance in forecasting over the GARCH.Deep learning, DL-GARCH, GARCH, The Chinese Yuan, volatility."
573,딥러닝 기법을 이용한 내일강수 예측,하지훈,2016,한국지능시스템학회,"정확한 강수예측을 위해서는 예측인자 선정과 예측방법에 대한 선택이 매우 중요하다. 최근에는 강수예측 방법으로기계학습 기법이 많이 사용되고 있으며, 그 중에서도 특히 인공신경망을 사용한 강수예측 방법은 좋은 성능을 보였다. 본논문에서는 딥러닝 기법 중 하나인 DBN(deep belief network)를 이용한 새로운 강수예측 방법을 제안한다. DBN는 비지도사전 학습을 통해 초기 가중치를 설정하여 기존 인공신경망의 문제점을 보완한다. 예측인자로는 기온, 전일-전주 강수일, 태양과 달 궤도 관련 자료를 선정하였다. 기온과 전일-전주 강수일은 서울에서의 1974년부터 2013년까지 총 40년간의AWS(automatic weather system) 관측 자료를 사용하였고, 태양과 달의 궤도 관련 자료는 서울을 중심으로 계산한 결과를사용하였다. 전체 기간에서 일부는 학습 자료로 사용하여 예측모델을 생성하였고, 나머지를 생성한 모델의 검증 자료로사용하였다. 모델 검증 결과로 나온 예측값들은 확률값을 가지며 임계치를 이용하여 강수유무를 판별하였다. 강수 정확도의척도로 양분예보기법 중 CSI(critical successive index)와 Bias(frequency bias)를 계산하였다. 이를 통해 DBN와 MLP(multilayer perceptron)의 성능을 비교한 결과 DBN의 강수 예측 정확도가 높았고, 수행속도 또한 2배 이상 빨랐다.For accurate precipitation forecasts the choice of weather faoctrs and prediction method is very important. Recently, machinele arning has been widely used for forecasting precipitation, and artificial neural network, one of machine learning techniques, showed good performance. In this paper, we suggest a new method for forecasting precipitation using DBN, one of deep learning techniques.
DBN has an advantage that initial weights are set by unsupervised learning, so this compensates for the defects of artificial neural networks. We used past precipitation, temperature, and the parameters of the sun and moon's motion as features for forecasting precipitation. The dataset consists of observation data which had been measured for 40 years from AWS in Seoul. Experiments were based on 8-fold cross validation. As a result of estimation, we got probabilities of test dataset, so threshold was used for the decision of precipitation. CSI and Bias were used for indicating the precision of precipitation. Our experimental results showed that DBN performed better than MLP.Deep learning, Deep belief network, Precipitation, Forecast"
574,딥러닝 신경망모형을 이용한 실시간 교통정보수집 연구,김대현 /KIM DAE HYON,2016,사단법인 인문사회과학기술융합학회,"비디오 기반의 실시간 교통정보수집 시스템은 자율주행을 포함한 지능형교통체계의 주요 서브시스템중 하나이며, 우수한 예측(추정)력 때문에 핵심 알고리즘으로 인공신경망 모형이 주로 사용되고 있다. 특히, 최근 과학 학술지 네이처에 구글 딥마인드의 알파고가 공개되면서 딥러닝과 인공지능에 대한 전 세계적인 관심이 집중되고 있다. 최근까지 가장 널리 사용된 인공신경망모형은 역전파 모형이었으나 최근 컴퓨터 하드웨어 및 알고리즘의 새로운 개발과 더불어 딥러닝 알고리즘을 이용한 딥인공신경망 모형에 대한 관심이 높아지고 있다. 그러나 아직 딥러닝 인공신경망에 대한 연구는 초기단계에 있는 실정이며 적용분야도 극히 한정되어 있다. 본 연구에서는 실시간 도로교통정보 수집을 위한 영상처리분야에서 딥러닝 알고리즘을 활용하고 실험결과를 통하여 향후 교통정보수집‧분석 분야에서의 딥러닝 인공신경망 모형의 적용가능성을 살펴보고자 한다. 특히, 본 연구에서 도출한 딥러닝 인공신경망모형의 실험결과를 기존 역전파 모형에 의한 실험결과와 비교하여 모형의 우수성을 비교‧평가하고자 한다.Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Normally, artificial neural networks have been used in real-time applications for collecting traffic information because of their remarkable performance in terms of predictive accuracy and real-time computing. Especially, the recent publication of a paper in the journal Nature and the success of Google DeepMind’s world-beating Go program AlphaGo caused significant increases of the public interest in artificial intelligence. Even though various types of learning models have been proposed, the Backpropagation has been the most popular neural network model. However, the Deep learning neural network model has attracted much interest recently. In this study, the Deep leaning neural network will be applied in image processing techniques for the real-time traffic information collection to investigate the performance of Deep learning algorithm in traffic engineering fields. In addition, the experimental results from Deep learing algorithm will be compared with the results from Backpropagation model to evaluate and propose better performing model.Intelligent Transportation Systems (ITS), artificial neural networks, Deep learning, traffic information, image processing"
575,딥러닝의 모형과 응용사례,안성만 /SungMahn Ahn,2016,한국지능정보시스템학회,"딥러닝은 인공신경망(neural network)이라는 인공지능분야의 모형이 발전된 형태로서, 계층구조로 이루어진 인공신경망의 내부계층(hidden layer)이 여러 단계로 이루어진 구조이다. 딥러닝에서의 주요 모형은 합성곱신경망(convolutional neural network), 순환신경망(recurrent neural network), 그리고 심층신뢰신경망(deep belief network)의 세가지라고 할 수있다. 그 중에서 현재 흥미로운 연구가 많이 발표되어서 관심이 집중되고 있는 모형은 지도학습(supervised learning)모형인 처음 두 개의 모형이다. 따라서 본 논문에서는 지도학습모형의 가중치를 최적화하는 기본적인 방법인 오류역전파 알고리즘을 살펴본 뒤에 합성곱신경망과 순환신경망의 구조와 응용사례 등을 살펴보고자 한다. 본문에서 다루지 않은 모형인 심층신뢰신경망은 아직까지는 합성곱신경망 이나 순환신경망보다는 상대적으로 주목을 덜 받고 있다. 그러나 심층신뢰신경망은 CNN이나 RNN과는 달리 비지도학습(unsupervised learning)모형이며, 사람이나 동물은 관찰을 통해서 스스로 학습한다는 점에서 궁극적으로는 비지도학습모형이 더 많이 연구되어야 할 주제가 될 것이다.Deep learning model is a kind of neural networks that allows multiple hidden layers. There are various deep learning architectures such as convolutional neural networks, deep belief networks and recurrent neural networks. Those have been applied to fields like computer vision, automatic speech recognition, natural language processing, audio recognition and bioinformatics where they have been shown to produce state-of-the-art results on various tasks. Among those architectures, convolutional neural networks and recurrent neural networks are classified as the supervised learning model. And in recent years, those supervised learning models have gained more popularity than unsupervised learning models such as deep belief networks, because supervised learning models have shown fashionable applications in such fields mentioned above.
Deep learning models can be trained with backpropagation algorithm. Backpropagation is an abbreviation for “backward propagation of errors” and a common method of training artificial neural networks used in conjunction with an optimization method such as gradient descent. The method calculates the gradient of an error function with respect to all the weights in the network. The gradient is fed to the optimization method which in turn uses it to update the weights, in an attempt to minimize the error function.
Convolutional neural networks use a special architecture which is particularly well-adapted to classify images. Using this architecture makes convolutional networks fast to train. This, in turn, helps us train deep, muti-layer networks, which are very good at classifying images. These days, deep convolutional networks are used in most neural networks for image recognition. Convolutional neural networks use three basic ideas: local receptive fields, shared weights, and pooling. By local receptive fields, we mean that each neuron in the first(or any) hidden layer will be connected to a small region of the input(or previous layer’s) neurons. Shared weights mean that we’re going to use the same weights and bias for each of the local receptive field. This means that all the neurons in the hidden layer detect exactly the same feature, just at different locations in the input image. In addition to the convolutional layers just described, convolutional neural networks also contain pooling layers. Pooling layers are usually used immediately after convolutional layers. What the pooling layers do is to simplify the information in the output from the convolutional layer.
Recent convolutional network architectures have 10 to 20 hidden layers and billions of connections between units. Training deep learning networks has taken weeks several years ago, but thanks to progress in GPU and algorithm enhancement, training time has reduced to several hours.
Neural networks with time-varying behavior are known as recurrent neural networks or RNNs. A recurrent neural network is a class of artificial neural network where connections between units form a directed cycle. This creates an internal state of the network which allows it to exhibit dynamic temporal behavior. Unlike feedforward neural networks, RNNs can use their internal memory to process arbitrary sequences of inputs. Early RNN models turned out to be very difficult to train, harder even than deep feedforward networks. The reason is the unstable gradient problem such as vanishing gradient and exploding gradient. The gradient can get smaller and smaller as it is propagated back through layers. This makes learning in early layers extremely slow. The problem actually gets worse in RNNs, since gradients aren’t just propagated backward through layers, they’re propagated backward through time. If the network runs for a long time, that can make the gradient extremely unstable and hard to learn from. It has been possible to incorporate an idea known as long short-term memory units (LSTMs) into RNNs. LSTMs make it much easier to get good results when training RNNs, and many recent papers make use of LSTMs or related ideas.Deep learning, Convolutional neural networks, Recurrent neural networks, Error backpropagation algorithm"
576,딥러닝분석과 기술적 분석 지표를 이용한 한국 코스피주가지수 방향성 예측,이우식 /Lee Woosik,2017,한국데이터정보과학회,"2016년 3월 구글 (Google)의 바둑인공지능 알파고 (AlphaGo)가 이세돌 9단과의 바둑대결에서승리한 이후 다양한 분야에서 인공지능 사용에 대한 관심이 높아지고 있는 가운데 금융투자 분야에서도 인공지능과 투자자문 전문가의 합성어인 로보어드바이저 (Robo-Advisor)에 대한 관심이 높아지고 있다. 인공지능 (artificial intelligence)기반의 의사결정은 비용 절감은 물론 효과적인 의사결정을가능하게 한다는 점에서 큰 장점이 있다. 본 연구에서는 기술적 분석 (technical analysis) 지표와 딥러닝 (deep learning) 모형을 결합하여 한국 코스피 지수를 예측하는 모형을 개발하고 제시한 모형들의 예측력을 비교, 분석한다. 분석 결과 기술적 분석 지표에 딥러닝 알고리즘을 결합한 모형이 주가지수 방향성 예측 문제에 응용될 수 있음을 확인하였다. 향후 본 연구에서 제안된 기술적 분석 지표와딥러닝모형을 결합한 기법은 로보어드바이저서비스에 응용할 수 있는 일반화 가능성을 보여준다Since Google’s AlphaGo defeated a world champion of Go players in 2016, there have been many interests in the deep learning. In the financial sector, a Robo-Advisor using deep learning gains a significant attention, which builds and manages portfolios of financial instruments for investors.In this paper, we have proposed the a deep learning algorithm geared toward identification and forecast of the KOSPI index direction,and we also have compared the accuracy of the prediction.In an application of forecasting the financial market index direction, we have shown that the Robo-Advisor using deep learning has a significant effect on finance industry. The Robo-Advisor collects a massive data such as earnings statements, news reports and regulatory filings, analyzes those and recommends investors how to view market trends and identify the best time to purchase financial assets. On the other hand, the Robo-Advisor allows businesses to learn more about their customers, develop better marketing strategies, increase sales and decrease costs.Artificial intelligence, deep learning, FinTech, Robo-Advisor, technical analysis."
577,딥러닝의 효율적인 학습을 위한 학습데이터 선별 기법에 관한 연구,주경돈 /Gyoung-don Joo,2017,한국정보과학회,"빅데이터의 증가와 병렬 하드웨어 처리, 학습 알고리즘의 개선은 딥러닝의 발전을 가져왔으며 인공지능이 적용 가능한 영역을 획기적으로 확장시키고 있다. 하지만 이러한 대규모의 학습 데이터에 기반한 딥러닝은 학습시간이 증가되고, 더 나아가 학습 데이터의 구축을 위한 레이블링(labeling) 비용이 데이터 수에 비례하여 증가하게 된다. 따라서 본 논문에서는, 딥러닝의 정확도를 최대한 유지하면서 필요한 학습 데이터의 수를 줄일수 있도록, 전체 데이터에서 제한된 수의 학습 데이터의 후보를 선별하는 기법에 대하여 연구한다. 특히 기존의 능동 학습에 기반한 기법들이 학습 데이터의 수가 매우 적어질 경우 딥러닝의 정확도가 급격히 떨어지는 문제를 해결할 수 있는 새로운 기법을 제안하였으며, 제안된 기법으로 선별된 학습 데이터는 그 수의 변화에 대해 딥러닝이 일관되게 경쟁력있는 정확도를 가짐을 다양한 실험을 통하여 검증하였다.The increase of big data, parallel hardware processing and enhanced learning algorithms have led to the development of deep learning and are dramatically expanding the domain of applications based on artificial intelligence. However, such large-scale training data extends the learning time, and furthermore, the labeling cost for building the training set increases in proportion to the volume of data. Therefore, in this paper, we study how to select a limited number of candidates for training set from given data so as to reduce the number of training set while maintaining the accuracy of deep learning. Especially, we found that, with the training set selected by the previous work, the accuracy of deep learning drastically decreases when the number of training set is very small and so proposed new methods to resolve this fall-off in accuracy. Through experiments, we showed that deep learning consistently has the competitive accuracy with the training set selected by the proposed method regardless of the size of training set.Deep Learning, Train Set Selection, Active Learning, Gamma distribution"
578,딥러닝을 이용한 범용적 스테그아날리시스,김현재 /Hyunjae Kim,2017,한국정보과학회,"스테그아날리시스(Steganalysis)란 이미지 등 일반적인 자료에 암호화된 정보를 은닉하는 스테가노그래피(Steganography)에 대한 검출 및 분석 방법으로, 기계학습 기반 방법론을 포함한다. 기존 기계학습 기반 스테그아날리시스는 영상(Image)의 특징(Feature) 추출 및 모델링에 기반하며, 최근 딥러닝(Deep Learning)의 적용으로 검출 정확도가 큰 폭으로 향상되었다. 하지만 현존하는 스테그아날리시스 모델은 단일 스테가노그래피 기법에 대해 국한되어 있어 학습에 사용되지 않은 스테고(Stego) 이미지의 경우 검출이 불가능한 결정적 한계를 가진다. 본 연구에서는 다양한 스테가노그래피 기법으로 생성된 스테고이미지에 딥러닝을 적용하여 스테그아날리시스를 학습하는 범용적 모델을 제안한다. 다양한 실험을 통해 제안 기법의 효용성 및 가능성을 확인하고, 범용적 스테그아날리시스 모델이 각각에 특화된 검출 기법과 유사한 정확도로 스테고 이미지를 검출할 수 있음을 보인다.Steganalysis is to detect information hidden by steganography inside general data such as images. There are stegoanalysis techniques that use machine learning (ML). Existing ML approaches to steganalysis are based on extracting features from stego images and modeling them.
Recently deep learning-based methodologies have shown significant improvements in detection accuracy. However, all the existing methods, including deep learning-based ones, have a critical limitation in that they can only detect stego images that are created by a specific steganography method. In this paper, we propose a generalized steganalysis method that can model multiple types of stego images using deep learning. Through various experiments, we confirm the effectiveness of our approach and envision directions for future research. In particular, we show that our method can detect each type of steganography with the same level of accuracy as that of a steganalysis method dedicated to that type of steganography, thereby demonstrating the general applicability of our approach to multiple types of stego images.steganalysis, steganography, deep learning, generalized steganalysis model"
579,"딥러닝 프레임워크의 비교: 티아노, 텐서플로, CNTK를 중심으로",정여진 /Yeojin Chung,2017,한국지능정보시스템학회,"딥러닝 프레임워크의 대표적인 기능으로는 ‘자동미분’과 ‘GPU의 활용’ 등을 들 수 있다. 본 논문은 파이썬의 라이브러리 형태로 사용 가능한 프레임워크 중에서 구글의 텐서플로와 마이크로소프트의 CNTK, 그리고 텐서 플로의 원조라고 할 수 있는 티아노를 비교하였다. 본문에서는 자동미분의 개념과 GPU의 활용형태를 간단히 설명하고, 그 다음에 logistic regression을 실행하는 예를 통하여 각 프레임워크의 문법을 알아본 뒤에, 마지막으로 대표적인 딥러닝 응용인 CNN의 예제를 실행시켜보고 코딩의 편의성과 실행속도 등을 확인해 보았다. 그 결과, 편의성의 관점에서 보면 티아노가 가장 코딩 하기가 어렵고, CNTK와 텐서플로는 많은 부분이 비슷하게 추상화 되어 있어서 코딩이 비슷하지만 가중치와 편향을 직접 정의하느냐의 여부에서 차이를 보였다. 그리고 각프레임워크의 실행속도에 대한 평가는 ‘큰 차이는 없다’는 것이다. 텐서플로는 티아노에 비하여 속도가 느리다는 평가가 있어왔는데, 본 연구의 실험에 의하면, 비록 CNN 모형에 국한되었지만, 텐서플로가 아주 조금이지만빠른 것으로 나타났다. CNTK의 경우에도, 비록 실험환경이 달랐지만, 실험환경의 차이에 의한 속도의 차이의 편차범위 이내에 있는 것으로 판단이 되었다. 본 연구에서는 세 종류의 딥러닝 프레임워크만을 살펴보았는데,위키피디아에 따르면 딥러닝 프레임워크의 종류는 12가지가 있으며, 각 프레임워크의 특징을 15가지 속성으로구분하여 차이를 특정하고 있다. 그 많은 속성 중에서 사용자의 입장에서 볼 때 중요한 속성은 어떤 언어(파이썬, C++, Java, 등)로 사용가능한지, 어떤 딥러닝 모형에 대한 라이브러리가 잘 구현되어 있는지 등일 것이다. 그리고 사용자가 대규모의 딥러닝 모형을 구축한다면, 다중 GPU 혹은 다중 서버를 지원하는지의 여부도 중요할 것이다. 또한 딥러닝 모형을 처음 학습하는 경우에는 사용설명서가 많은지 예제 프로그램이 많은지 여부도 중요한 기준이 될 것이다.The deep learning framework is software designed to help develop deep learning models. Some of its important functions include “automatic differentiation” and “utilization of GPU”. The list of popular deep learning framework includes Caffe (BVLC) and Theano (University of Montreal). And recently, Microsoft's deep learning framework, Microsoft Cognitive Toolkit, was released as open-source license, following Google’s Tensorflow a year earlier. The early deep learning frameworks have been developed mainly for research at universities. Beginning with the inception of Tensorflow, however, it seems that companies such as Microsoft and Facebook have started to join the competition of framework development. Given the trend, Google and other companies are expected to continue investing in the deep learning framework to bring forward the initiative in the artificial intelligence business. From this point of view, we think it is a good time to compare some of deep learning frameworks. So we compare three deep learning frameworks which can be used as a Python library. Those are Google's Tensorflow, Microsoft’s CNTK, and Theano which is sort of a predecessor of the preceding two.
The most common and important function of deep learning frameworks is the ability to perform automatic differentiation. Basically all the mathematical expressions of deep learning models can be represented as computational graphs, which consist of nodes and edges. Partial derivatives on each edge of a computational graph can then be obtained. With the partial derivatives, we can let software compute differentiation of any node with respect to any variable by utilizing chain rule of Calculus.
First of all, the convenience of coding is in the order of CNTK, Tensorflow, and Theano. The criterion is simply based on the lengths of the codes and the learning curve and the ease of coding are not the main concern. According to the criteria, Theano was the most difficult to implement with, and CNTK and Tensorflow were somewhat easier. With Tensorflow, we need to define weight variables and biases explicitly. The reason that CNTK and Tensorflow are easier to implement with is that those frameworks provide us with more abstraction than Theano. We, however, need to mention that low-level coding is not always bad. It gives us flexibility of coding. With the low-level coding such as in Theano, we can implement and test any new deep learning models or any new search methods that we can think of.
The assessment of the execution speed of each framework is that there is not meaningful difference. According to the experiment, execution speeds of Theano and Tensorflow are very similar, although the experiment was limited to a CNN model. In the case of CNTK, the experimental environment was not maintained as the same. The code written in CNTK has to be run in PC environment without GPU where codes execute as much as 50 times slower than with GPU. But we concluded that the difference of execution speed was within the range of variation caused by the different hardware setup.
In this study, we compared three types of deep learning framework: Theano, Tensorflow, and CNTK. According to Wikipedia, there are 12 available deep learning frameworks. And 15 different attributes differentiate each framework. Some of the important attributes would include interface language (Python, C ++, Java, etc.) and the availability of libraries on various deep learning models such as CNN, RNN, DBN, and etc. And if a user implements a large scale deep learning model, it will also be important to support multiple GPU or multiple servers. Also, if you are learning the deep learning model, it would also be important if there are enough examples and references.deep learning framework, Theano, TensorFlow, CNTK, computational graph, CIFAR-10"
580,딥러닝을 이용한 주변 무선단말 파악방안,이웅섭 /Lee， Woong-sup,2018,한국정보통신학회,"최근 단말-대-단말(Device-to-device, D2D) 통신기술이 차세대 무선통신시스템의 핵심기술로 큰 관심을 받고 있다. 이러한 단말간 통신에서는 자신의 주변에 어떠한 단말이 있는지 파악하는 주변단말 탐색(Neighbor discovery)이매우 중요하다. 본 논문에서는 최근 큰 관심을 받고 있는 딥러닝(Deep learning) 기술을 활용하여 단말간 통신에서 주변단말을 파악하는 방안에 대해서 제안한다. 제안 방안은 기존의 방안과 달리 무선채널의 공간적 연관성을 이용하여단말간의 신호 전송 없이 단말이 기지국으로 전송하는 상향링크 파일럿 신호를 기반으로 주변 단말을 찾고 따라서기존의 방식에 비해 신호전송 복잡도(signaling complexity)를 크게 줄일 수 있다. 또한 제안 방안에서는 떨어져 있는거리에 따라서 주변 단말을 분류 가능하여 기존 방안에 비해서 좀 더 세밀한 단말 탐색이 가능하다. 마지막으로 본 논문에서는 tensorflow를 이용한 컴퓨터 시뮬레이션을 통해 제안 방안의 성능을 검증하였다.Recently, the device-to-device (D2D) communication has been conceived as the key technology for the next-generationmobile communication systems. The neighbor discovery in which the nearby users are found, is essential for the properoperation of the D2D communication. In this paper, we propose new neighbor discovery scheme based on deep learningtechnology which has gained a lot of attention recently. In the proposed scheme, the neighboring users can be foundusing the uplink pilot transmission of users only, unlike conventional neighbor discovery schemes in which direct pilotcommunication among users is required, such that the signaling overhead can be greatly reduced in our proposed scheme.
Moreover, the neighbors with different proximity can also be classified accordingly which enables more accurate neighbordiscovery compared to the conventional schemes. The performance of our proposed scheme is verified through thetensorflow-based computer simulations.
키워드 : 딥러닝, 주변단말 탐색, 단말-대-단말 통신Deep learning, Neighbor discovery, Device-to-device communication, Wireless channel correlation"
581,신뢰성있는 딥러닝 기반 분석 모델을 참조하기 위한 딥러닝 기술 언어,문종혁 /Jong Hyeok Mun,2021,한국정보처리학회,"최근 딥러닝은 하드웨어 성능이 향상됨에 따라 자연어 처리, 영상 인식 등의 다양한 기술에 접목되어 활용되고 있다. 이러한 기술들을 활용해 지능형교통 시스템(ITS), 스마트홈, 헬스케어 등의 산업분야에서 데이터를 분석하여 고속도로 속도위반 차량 검출, 에너지 사용량 제어, 응급상황 등과 같은고품질의 서비스를 제공하며, 고품질의 서비스를 제공하기 위해서는 정확도가 향상된 딥러닝 모델이 적용되어야 한다. 이를 위해 서비스 환경의 데이터를분석하기 위한 딥러닝 모델을 개발할 때, 개발자는 신뢰성이 검증된 최신의 딥러닝 모델을 적용할 수 있어야 한다. 이는 개발자가 참조하는 딥러닝 모델에적용된 학습 데이터셋의 정확도를 측정하여 검증할 수 있다. 이러한 검증을 위해서 개발자는 학습 데이터셋, 딥러닝의 계층구조 및 개발 환경 등과 같은내용을 포함하는 딥러닝 모델을 문서화하여 적용하기 위한 구조적인 정보가 필요하다. 본 논문에서는 신뢰성있는 딥러닝 기반 데이터 분석 모델을 참조하기위한 딥러닝 기술 언어를 제안한다. 제안하는 기술 언어는 신뢰성 있는 딥러닝 모델을 개발하는데 필요한 학습데이터셋, 개발 환경 및 설정 등의 정보와더불어 딥러닝 모델의 계층구조를 표현할 수 있다. 제안하는 딥러닝 기술 언어를 이용하여 개발자는 지능형 교통 시스템에서 참조하는 분석 모델의정확도를 검증할 수 있다. 실험에서는 제안하는 언어의 유효성을 검증하기 위해, 번호판 인식 모델을 중심으로 딥러닝 기술 문서의 적용과정을 보인다With the recent advancements of deep learning, companies such as smart home, healthcare, and intelligent transportation systems areutilizing its functionality to provide high-quality services for vehicle detection, emergency situation detection, and controlling energyconsumption. To provide reliable services in such sensitive systems, deep learning models are required to have high accuracy. In orderto develop a deep learning model for analyzing previously mentioned services, developers should utilize the state of the art deep learningmodels that have already been verified for higher accuracy. The developers can verify the accuracy of the referenced model by validatingthe model on the dataset. For this validation, the developer needs structural information to document and apply deep learning models,including metadata such as learning dataset, network architecture, and development environments. In this paper, we propose a descriptionlanguage that represents the network architecture of the deep learning model along with its metadata that are necessary to develop adeep learning model. Through the proposed description language, developers can easily verify the accuracy of the referenced deep learningmodel. Our experiments demonstrate the application scenario of a deep learning description document that focuses on the license platerecognition for the detection of illegally parked vehicles.Trusted Deep Learning, Model Reference, Deep Learning Description Language, Traffic Situation Analysis Model"
582,딥러닝 분석을 이용한 중국 역내·외 위안화 변동성 예측,이우식 /Lee Woosik,2016,한국데이터정보과학회,"2008년 글로벌 금융위기 이후 중국은 위안화 국제화의 점진적 추진을 진행하면서 중국상하이 외환시장과 중국홍콩 외환시장에서 거래되는 통화인 역내위안화와 역외위안화를 형성시켰다. 본 연구는 위안화 국제화와 점진적인 중국 자본계정 개방에 따라 급변하는 외환시장상황의 변동성을 정확하게 파악하기 위해서 GARCH모형 (일반화된 자기회귀 조건부이분산성모형)에 다단계인공신경망을 결합한 MLP-GARCH 모형과 GARCH모형과 기계학습의 일종인 딥러닝 (deep learning)을 통합한 DL-GARCH을 가지고 위안화 변동성예측을 비교 실험과 분석을 하였다. 비교분석 결과 DLGARCH 모형은 MLP-GARCH보다 모형 위안화 역내·외 환율변동성 예측 면에서 더욱 더 개선된예측값을 제공하였다. 그래서 이분산시계열모형을 딥러닝과 결합한 DL-GARCH 모형은 시계열의환율 변동성 예측 문제에 딥러닝을 응용할 수 있음을 확인하였다. 향후 이분산시계열과 결합된 딥러닝 모형은 다른 금융시계열 데이터에 응용하여 그 일반화 가능성을 높일 수 있을 것이다.The People’s Republic of China has vigorously been pursuing the internationalization of the Chinese Yuan or Renminbi after the financial crisis of 2008. In this view, an abrupt increase of use of the Chinese Yuan in the onshore and offshore markets are important milestones to be one of important currencies. One of the most frequently used methods to forecast volatility is GARCH model. Since a prediction error of the GARCH model has been reported quite high, a lot of efforts have been made to improve forecasting capability of the GARCH model. In this paper, we have proposed MLPGARCH and a DL-GARCH by employing Artificial Neural Network to the GARCH. In an application to forecasting Chinese Yuan volatility, we have successfully shown their overall outperformance in forecasting over the GARCH.Deep learning, DL-GARCH, GARCH, The Chinese Yuan, volatility."
583,딥러닝 기법을 이용한 내일강수 예측,하지훈,2016,한국지능시스템학회,"정확한 강수예측을 위해서는 예측인자 선정과 예측방법에 대한 선택이 매우 중요하다. 최근에는 강수예측 방법으로기계학습 기법이 많이 사용되고 있으며, 그 중에서도 특히 인공신경망을 사용한 강수예측 방법은 좋은 성능을 보였다. 본논문에서는 딥러닝 기법 중 하나인 DBN(deep belief network)를 이용한 새로운 강수예측 방법을 제안한다. DBN는 비지도사전 학습을 통해 초기 가중치를 설정하여 기존 인공신경망의 문제점을 보완한다. 예측인자로는 기온, 전일-전주 강수일, 태양과 달 궤도 관련 자료를 선정하였다. 기온과 전일-전주 강수일은 서울에서의 1974년부터 2013년까지 총 40년간의AWS(automatic weather system) 관측 자료를 사용하였고, 태양과 달의 궤도 관련 자료는 서울을 중심으로 계산한 결과를사용하였다. 전체 기간에서 일부는 학습 자료로 사용하여 예측모델을 생성하였고, 나머지를 생성한 모델의 검증 자료로사용하였다. 모델 검증 결과로 나온 예측값들은 확률값을 가지며 임계치를 이용하여 강수유무를 판별하였다. 강수 정확도의척도로 양분예보기법 중 CSI(critical successive index)와 Bias(frequency bias)를 계산하였다. 이를 통해 DBN와 MLP(multilayer perceptron)의 성능을 비교한 결과 DBN의 강수 예측 정확도가 높았고, 수행속도 또한 2배 이상 빨랐다.For accurate precipitation forecasts the choice of weather faoctrs and prediction method is very important. Recently, machinele arning has been widely used for forecasting precipitation, and artificial neural network, one of machine learning techniques, showed good performance. In this paper, we suggest a new method for forecasting precipitation using DBN, one of deep learning techniques.
DBN has an advantage that initial weights are set by unsupervised learning, so this compensates for the defects of artificial neural networks. We used past precipitation, temperature, and the parameters of the sun and moon's motion as features for forecasting precipitation. The dataset consists of observation data which had been measured for 40 years from AWS in Seoul. Experiments were based on 8-fold cross validation. As a result of estimation, we got probabilities of test dataset, so threshold was used for the decision of precipitation. CSI and Bias were used for indicating the precision of precipitation. Our experimental results showed that DBN performed better than MLP.Deep learning, Deep belief network, Precipitation, Forecast"
584,딥러닝 신경망모형을 이용한 실시간 교통정보수집 연구,김대현 /KIM DAE HYON,2016,사단법인 인문사회과학기술융합학회,"비디오 기반의 실시간 교통정보수집 시스템은 자율주행을 포함한 지능형교통체계의 주요 서브시스템중 하나이며, 우수한 예측(추정)력 때문에 핵심 알고리즘으로 인공신경망 모형이 주로 사용되고 있다. 특히, 최근 과학 학술지 네이처에 구글 딥마인드의 알파고가 공개되면서 딥러닝과 인공지능에 대한 전 세계적인 관심이 집중되고 있다. 최근까지 가장 널리 사용된 인공신경망모형은 역전파 모형이었으나 최근 컴퓨터 하드웨어 및 알고리즘의 새로운 개발과 더불어 딥러닝 알고리즘을 이용한 딥인공신경망 모형에 대한 관심이 높아지고 있다. 그러나 아직 딥러닝 인공신경망에 대한 연구는 초기단계에 있는 실정이며 적용분야도 극히 한정되어 있다. 본 연구에서는 실시간 도로교통정보 수집을 위한 영상처리분야에서 딥러닝 알고리즘을 활용하고 실험결과를 통하여 향후 교통정보수집‧분석 분야에서의 딥러닝 인공신경망 모형의 적용가능성을 살펴보고자 한다. 특히, 본 연구에서 도출한 딥러닝 인공신경망모형의 실험결과를 기존 역전파 모형에 의한 실험결과와 비교하여 모형의 우수성을 비교‧평가하고자 한다.Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Normally, artificial neural networks have been used in real-time applications for collecting traffic information because of their remarkable performance in terms of predictive accuracy and real-time computing. Especially, the recent publication of a paper in the journal Nature and the success of Google DeepMind’s world-beating Go program AlphaGo caused significant increases of the public interest in artificial intelligence. Even though various types of learning models have been proposed, the Backpropagation has been the most popular neural network model. However, the Deep learning neural network model has attracted much interest recently. In this study, the Deep leaning neural network will be applied in image processing techniques for the real-time traffic information collection to investigate the performance of Deep learning algorithm in traffic engineering fields. In addition, the experimental results from Deep learing algorithm will be compared with the results from Backpropagation model to evaluate and propose better performing model.Intelligent Transportation Systems (ITS), artificial neural networks, Deep learning, traffic information, image processing"
585,딥러닝의 모형과 응용사례,안성만 /SungMahn Ahn,2016,한국지능정보시스템학회,"딥러닝은 인공신경망(neural network)이라는 인공지능분야의 모형이 발전된 형태로서, 계층구조로 이루어진 인공신경망의 내부계층(hidden layer)이 여러 단계로 이루어진 구조이다. 딥러닝에서의 주요 모형은 합성곱신경망(convolutional neural network), 순환신경망(recurrent neural network), 그리고 심층신뢰신경망(deep belief network)의 세가지라고 할 수있다. 그 중에서 현재 흥미로운 연구가 많이 발표되어서 관심이 집중되고 있는 모형은 지도학습(supervised learning)모형인 처음 두 개의 모형이다. 따라서 본 논문에서는 지도학습모형의 가중치를 최적화하는 기본적인 방법인 오류역전파 알고리즘을 살펴본 뒤에 합성곱신경망과 순환신경망의 구조와 응용사례 등을 살펴보고자 한다. 본문에서 다루지 않은 모형인 심층신뢰신경망은 아직까지는 합성곱신경망 이나 순환신경망보다는 상대적으로 주목을 덜 받고 있다. 그러나 심층신뢰신경망은 CNN이나 RNN과는 달리 비지도학습(unsupervised learning)모형이며, 사람이나 동물은 관찰을 통해서 스스로 학습한다는 점에서 궁극적으로는 비지도학습모형이 더 많이 연구되어야 할 주제가 될 것이다.Deep learning model is a kind of neural networks that allows multiple hidden layers. There are various deep learning architectures such as convolutional neural networks, deep belief networks and recurrent neural networks. Those have been applied to fields like computer vision, automatic speech recognition, natural language processing, audio recognition and bioinformatics where they have been shown to produce state-of-the-art results on various tasks. Among those architectures, convolutional neural networks and recurrent neural networks are classified as the supervised learning model. And in recent years, those supervised learning models have gained more popularity than unsupervised learning models such as deep belief networks, because supervised learning models have shown fashionable applications in such fields mentioned above.
Deep learning models can be trained with backpropagation algorithm. Backpropagation is an abbreviation for “backward propagation of errors” and a common method of training artificial neural networks used in conjunction with an optimization method such as gradient descent. The method calculates the gradient of an error function with respect to all the weights in the network. The gradient is fed to the optimization method which in turn uses it to update the weights, in an attempt to minimize the error function.
Convolutional neural networks use a special architecture which is particularly well-adapted to classify images. Using this architecture makes convolutional networks fast to train. This, in turn, helps us train deep, muti-layer networks, which are very good at classifying images. These days, deep convolutional networks are used in most neural networks for image recognition. Convolutional neural networks use three basic ideas: local receptive fields, shared weights, and pooling. By local receptive fields, we mean that each neuron in the first(or any) hidden layer will be connected to a small region of the input(or previous layer’s) neurons. Shared weights mean that we’re going to use the same weights and bias for each of the local receptive field. This means that all the neurons in the hidden layer detect exactly the same feature, just at different locations in the input image. In addition to the convolutional layers just described, convolutional neural networks also contain pooling layers. Pooling layers are usually used immediately after convolutional layers. What the pooling layers do is to simplify the information in the output from the convolutional layer.
Recent convolutional network architectures have 10 to 20 hidden layers and billions of connections between units. Training deep learning networks has taken weeks several years ago, but thanks to progress in GPU and algorithm enhancement, training time has reduced to several hours.
Neural networks with time-varying behavior are known as recurrent neural networks or RNNs. A recurrent neural network is a class of artificial neural network where connections between units form a directed cycle. This creates an internal state of the network which allows it to exhibit dynamic temporal behavior. Unlike feedforward neural networks, RNNs can use their internal memory to process arbitrary sequences of inputs. Early RNN models turned out to be very difficult to train, harder even than deep feedforward networks. The reason is the unstable gradient problem such as vanishing gradient and exploding gradient. The gradient can get smaller and smaller as it is propagated back through layers. This makes learning in early layers extremely slow. The problem actually gets worse in RNNs, since gradients aren’t just propagated backward through layers, they’re propagated backward through time. If the network runs for a long time, that can make the gradient extremely unstable and hard to learn from. It has been possible to incorporate an idea known as long short-term memory units (LSTMs) into RNNs. LSTMs make it much easier to get good results when training RNNs, and many recent papers make use of LSTMs or related ideas.Deep learning, Convolutional neural networks, Recurrent neural networks, Error backpropagation algorithm"
586,딥러닝분석과 기술적 분석 지표를 이용한 한국 코스피주가지수 방향성 예측,이우식 /Lee Woosik,2017,한국데이터정보과학회,"2016년 3월 구글 (Google)의 바둑인공지능 알파고 (AlphaGo)가 이세돌 9단과의 바둑대결에서승리한 이후 다양한 분야에서 인공지능 사용에 대한 관심이 높아지고 있는 가운데 금융투자 분야에서도 인공지능과 투자자문 전문가의 합성어인 로보어드바이저 (Robo-Advisor)에 대한 관심이 높아지고 있다. 인공지능 (artificial intelligence)기반의 의사결정은 비용 절감은 물론 효과적인 의사결정을가능하게 한다는 점에서 큰 장점이 있다. 본 연구에서는 기술적 분석 (technical analysis) 지표와 딥러닝 (deep learning) 모형을 결합하여 한국 코스피 지수를 예측하는 모형을 개발하고 제시한 모형들의 예측력을 비교, 분석한다. 분석 결과 기술적 분석 지표에 딥러닝 알고리즘을 결합한 모형이 주가지수 방향성 예측 문제에 응용될 수 있음을 확인하였다. 향후 본 연구에서 제안된 기술적 분석 지표와딥러닝모형을 결합한 기법은 로보어드바이저서비스에 응용할 수 있는 일반화 가능성을 보여준다Since Google’s AlphaGo defeated a world champion of Go players in 2016, there have been many interests in the deep learning. In the financial sector, a Robo-Advisor using deep learning gains a significant attention, which builds and manages portfolios of financial instruments for investors.In this paper, we have proposed the a deep learning algorithm geared toward identification and forecast of the KOSPI index direction,and we also have compared the accuracy of the prediction.In an application of forecasting the financial market index direction, we have shown that the Robo-Advisor using deep learning has a significant effect on finance industry. The Robo-Advisor collects a massive data such as earnings statements, news reports and regulatory filings, analyzes those and recommends investors how to view market trends and identify the best time to purchase financial assets. On the other hand, the Robo-Advisor allows businesses to learn more about their customers, develop better marketing strategies, increase sales and decrease costs.Artificial intelligence, deep learning, FinTech, Robo-Advisor, technical analysis."
587,딥러닝의 효율적인 학습을 위한 학습데이터 선별 기법에 관한 연구,주경돈 /Gyoung-don Joo,2017,한국정보과학회,"빅데이터의 증가와 병렬 하드웨어 처리, 학습 알고리즘의 개선은 딥러닝의 발전을 가져왔으며 인공지능이 적용 가능한 영역을 획기적으로 확장시키고 있다. 하지만 이러한 대규모의 학습 데이터에 기반한 딥러닝은 학습시간이 증가되고, 더 나아가 학습 데이터의 구축을 위한 레이블링(labeling) 비용이 데이터 수에 비례하여 증가하게 된다. 따라서 본 논문에서는, 딥러닝의 정확도를 최대한 유지하면서 필요한 학습 데이터의 수를 줄일수 있도록, 전체 데이터에서 제한된 수의 학습 데이터의 후보를 선별하는 기법에 대하여 연구한다. 특히 기존의 능동 학습에 기반한 기법들이 학습 데이터의 수가 매우 적어질 경우 딥러닝의 정확도가 급격히 떨어지는 문제를 해결할 수 있는 새로운 기법을 제안하였으며, 제안된 기법으로 선별된 학습 데이터는 그 수의 변화에 대해 딥러닝이 일관되게 경쟁력있는 정확도를 가짐을 다양한 실험을 통하여 검증하였다.The increase of big data, parallel hardware processing and enhanced learning algorithms have led to the development of deep learning and are dramatically expanding the domain of applications based on artificial intelligence. However, such large-scale training data extends the learning time, and furthermore, the labeling cost for building the training set increases in proportion to the volume of data. Therefore, in this paper, we study how to select a limited number of candidates for training set from given data so as to reduce the number of training set while maintaining the accuracy of deep learning. Especially, we found that, with the training set selected by the previous work, the accuracy of deep learning drastically decreases when the number of training set is very small and so proposed new methods to resolve this fall-off in accuracy. Through experiments, we showed that deep learning consistently has the competitive accuracy with the training set selected by the proposed method regardless of the size of training set.Deep Learning, Train Set Selection, Active Learning, Gamma distribution"
588,딥러닝을 이용한 범용적 스테그아날리시스,김현재 /Hyunjae Kim,2017,한국정보과학회,"스테그아날리시스(Steganalysis)란 이미지 등 일반적인 자료에 암호화된 정보를 은닉하는 스테가노그래피(Steganography)에 대한 검출 및 분석 방법으로, 기계학습 기반 방법론을 포함한다. 기존 기계학습 기반 스테그아날리시스는 영상(Image)의 특징(Feature) 추출 및 모델링에 기반하며, 최근 딥러닝(Deep Learning)의 적용으로 검출 정확도가 큰 폭으로 향상되었다. 하지만 현존하는 스테그아날리시스 모델은 단일 스테가노그래피 기법에 대해 국한되어 있어 학습에 사용되지 않은 스테고(Stego) 이미지의 경우 검출이 불가능한 결정적 한계를 가진다. 본 연구에서는 다양한 스테가노그래피 기법으로 생성된 스테고이미지에 딥러닝을 적용하여 스테그아날리시스를 학습하는 범용적 모델을 제안한다. 다양한 실험을 통해 제안 기법의 효용성 및 가능성을 확인하고, 범용적 스테그아날리시스 모델이 각각에 특화된 검출 기법과 유사한 정확도로 스테고 이미지를 검출할 수 있음을 보인다.Steganalysis is to detect information hidden by steganography inside general data such as images. There are stegoanalysis techniques that use machine learning (ML). Existing ML approaches to steganalysis are based on extracting features from stego images and modeling them.
Recently deep learning-based methodologies have shown significant improvements in detection accuracy. However, all the existing methods, including deep learning-based ones, have a critical limitation in that they can only detect stego images that are created by a specific steganography method. In this paper, we propose a generalized steganalysis method that can model multiple types of stego images using deep learning. Through various experiments, we confirm the effectiveness of our approach and envision directions for future research. In particular, we show that our method can detect each type of steganography with the same level of accuracy as that of a steganalysis method dedicated to that type of steganography, thereby demonstrating the general applicability of our approach to multiple types of stego images.steganalysis, steganography, deep learning, generalized steganalysis model"
589,"딥러닝 프레임워크의 비교: 티아노, 텐서플로, CNTK를 중심으로",정여진 /Yeojin Chung,2017,한국지능정보시스템학회,"딥러닝 프레임워크의 대표적인 기능으로는 ‘자동미분’과 ‘GPU의 활용’ 등을 들 수 있다. 본 논문은 파이썬의 라이브러리 형태로 사용 가능한 프레임워크 중에서 구글의 텐서플로와 마이크로소프트의 CNTK, 그리고 텐서 플로의 원조라고 할 수 있는 티아노를 비교하였다. 본문에서는 자동미분의 개념과 GPU의 활용형태를 간단히 설명하고, 그 다음에 logistic regression을 실행하는 예를 통하여 각 프레임워크의 문법을 알아본 뒤에, 마지막으로 대표적인 딥러닝 응용인 CNN의 예제를 실행시켜보고 코딩의 편의성과 실행속도 등을 확인해 보았다. 그 결과, 편의성의 관점에서 보면 티아노가 가장 코딩 하기가 어렵고, CNTK와 텐서플로는 많은 부분이 비슷하게 추상화 되어 있어서 코딩이 비슷하지만 가중치와 편향을 직접 정의하느냐의 여부에서 차이를 보였다. 그리고 각프레임워크의 실행속도에 대한 평가는 ‘큰 차이는 없다’는 것이다. 텐서플로는 티아노에 비하여 속도가 느리다는 평가가 있어왔는데, 본 연구의 실험에 의하면, 비록 CNN 모형에 국한되었지만, 텐서플로가 아주 조금이지만빠른 것으로 나타났다. CNTK의 경우에도, 비록 실험환경이 달랐지만, 실험환경의 차이에 의한 속도의 차이의 편차범위 이내에 있는 것으로 판단이 되었다. 본 연구에서는 세 종류의 딥러닝 프레임워크만을 살펴보았는데,위키피디아에 따르면 딥러닝 프레임워크의 종류는 12가지가 있으며, 각 프레임워크의 특징을 15가지 속성으로구분하여 차이를 특정하고 있다. 그 많은 속성 중에서 사용자의 입장에서 볼 때 중요한 속성은 어떤 언어(파이썬, C++, Java, 등)로 사용가능한지, 어떤 딥러닝 모형에 대한 라이브러리가 잘 구현되어 있는지 등일 것이다. 그리고 사용자가 대규모의 딥러닝 모형을 구축한다면, 다중 GPU 혹은 다중 서버를 지원하는지의 여부도 중요할 것이다. 또한 딥러닝 모형을 처음 학습하는 경우에는 사용설명서가 많은지 예제 프로그램이 많은지 여부도 중요한 기준이 될 것이다.The deep learning framework is software designed to help develop deep learning models. Some of its important functions include “automatic differentiation” and “utilization of GPU”. The list of popular deep learning framework includes Caffe (BVLC) and Theano (University of Montreal). And recently, Microsoft's deep learning framework, Microsoft Cognitive Toolkit, was released as open-source license, following Google’s Tensorflow a year earlier. The early deep learning frameworks have been developed mainly for research at universities. Beginning with the inception of Tensorflow, however, it seems that companies such as Microsoft and Facebook have started to join the competition of framework development. Given the trend, Google and other companies are expected to continue investing in the deep learning framework to bring forward the initiative in the artificial intelligence business. From this point of view, we think it is a good time to compare some of deep learning frameworks. So we compare three deep learning frameworks which can be used as a Python library. Those are Google's Tensorflow, Microsoft’s CNTK, and Theano which is sort of a predecessor of the preceding two.
The most common and important function of deep learning frameworks is the ability to perform automatic differentiation. Basically all the mathematical expressions of deep learning models can be represented as computational graphs, which consist of nodes and edges. Partial derivatives on each edge of a computational graph can then be obtained. With the partial derivatives, we can let software compute differentiation of any node with respect to any variable by utilizing chain rule of Calculus.
First of all, the convenience of coding is in the order of CNTK, Tensorflow, and Theano. The criterion is simply based on the lengths of the codes and the learning curve and the ease of coding are not the main concern. According to the criteria, Theano was the most difficult to implement with, and CNTK and Tensorflow were somewhat easier. With Tensorflow, we need to define weight variables and biases explicitly. The reason that CNTK and Tensorflow are easier to implement with is that those frameworks provide us with more abstraction than Theano. We, however, need to mention that low-level coding is not always bad. It gives us flexibility of coding. With the low-level coding such as in Theano, we can implement and test any new deep learning models or any new search methods that we can think of.
The assessment of the execution speed of each framework is that there is not meaningful difference. According to the experiment, execution speeds of Theano and Tensorflow are very similar, although the experiment was limited to a CNN model. In the case of CNTK, the experimental environment was not maintained as the same. The code written in CNTK has to be run in PC environment without GPU where codes execute as much as 50 times slower than with GPU. But we concluded that the difference of execution speed was within the range of variation caused by the different hardware setup.
In this study, we compared three types of deep learning framework: Theano, Tensorflow, and CNTK. According to Wikipedia, there are 12 available deep learning frameworks. And 15 different attributes differentiate each framework. Some of the important attributes would include interface language (Python, C ++, Java, etc.) and the availability of libraries on various deep learning models such as CNN, RNN, DBN, and etc. And if a user implements a large scale deep learning model, it will also be important to support multiple GPU or multiple servers. Also, if you are learning the deep learning model, it would also be important if there are enough examples and references.deep learning framework, Theano, TensorFlow, CNTK, computational graph, CIFAR-10"
590,딥러닝을 이용한 주변 무선단말 파악방안,이웅섭 /Lee， Woong-sup,2018,한국정보통신학회,"최근 단말-대-단말(Device-to-device, D2D) 통신기술이 차세대 무선통신시스템의 핵심기술로 큰 관심을 받고 있다. 이러한 단말간 통신에서는 자신의 주변에 어떠한 단말이 있는지 파악하는 주변단말 탐색(Neighbor discovery)이매우 중요하다. 본 논문에서는 최근 큰 관심을 받고 있는 딥러닝(Deep learning) 기술을 활용하여 단말간 통신에서 주변단말을 파악하는 방안에 대해서 제안한다. 제안 방안은 기존의 방안과 달리 무선채널의 공간적 연관성을 이용하여단말간의 신호 전송 없이 단말이 기지국으로 전송하는 상향링크 파일럿 신호를 기반으로 주변 단말을 찾고 따라서기존의 방식에 비해 신호전송 복잡도(signaling complexity)를 크게 줄일 수 있다. 또한 제안 방안에서는 떨어져 있는거리에 따라서 주변 단말을 분류 가능하여 기존 방안에 비해서 좀 더 세밀한 단말 탐색이 가능하다. 마지막으로 본 논문에서는 tensorflow를 이용한 컴퓨터 시뮬레이션을 통해 제안 방안의 성능을 검증하였다.Recently, the device-to-device (D2D) communication has been conceived as the key technology for the next-generationmobile communication systems. The neighbor discovery in which the nearby users are found, is essential for the properoperation of the D2D communication. In this paper, we propose new neighbor discovery scheme based on deep learningtechnology which has gained a lot of attention recently. In the proposed scheme, the neighboring users can be foundusing the uplink pilot transmission of users only, unlike conventional neighbor discovery schemes in which direct pilotcommunication among users is required, such that the signaling overhead can be greatly reduced in our proposed scheme.
Moreover, the neighbors with different proximity can also be classified accordingly which enables more accurate neighbordiscovery compared to the conventional schemes. The performance of our proposed scheme is verified through thetensorflow-based computer simulations.
키워드 : 딥러닝, 주변단말 탐색, 단말-대-단말 통신Deep learning, Neighbor discovery, Device-to-device communication, Wireless channel correlation"
591,신뢰성있는 딥러닝 기반 분석 모델을 참조하기 위한 딥러닝 기술 언어,문종혁 /Jong Hyeok Mun,2021,한국정보처리학회,"최근 딥러닝은 하드웨어 성능이 향상됨에 따라 자연어 처리, 영상 인식 등의 다양한 기술에 접목되어 활용되고 있다. 이러한 기술들을 활용해 지능형교통 시스템(ITS), 스마트홈, 헬스케어 등의 산업분야에서 데이터를 분석하여 고속도로 속도위반 차량 검출, 에너지 사용량 제어, 응급상황 등과 같은고품질의 서비스를 제공하며, 고품질의 서비스를 제공하기 위해서는 정확도가 향상된 딥러닝 모델이 적용되어야 한다. 이를 위해 서비스 환경의 데이터를분석하기 위한 딥러닝 모델을 개발할 때, 개발자는 신뢰성이 검증된 최신의 딥러닝 모델을 적용할 수 있어야 한다. 이는 개발자가 참조하는 딥러닝 모델에적용된 학습 데이터셋의 정확도를 측정하여 검증할 수 있다. 이러한 검증을 위해서 개발자는 학습 데이터셋, 딥러닝의 계층구조 및 개발 환경 등과 같은내용을 포함하는 딥러닝 모델을 문서화하여 적용하기 위한 구조적인 정보가 필요하다. 본 논문에서는 신뢰성있는 딥러닝 기반 데이터 분석 모델을 참조하기위한 딥러닝 기술 언어를 제안한다. 제안하는 기술 언어는 신뢰성 있는 딥러닝 모델을 개발하는데 필요한 학습데이터셋, 개발 환경 및 설정 등의 정보와더불어 딥러닝 모델의 계층구조를 표현할 수 있다. 제안하는 딥러닝 기술 언어를 이용하여 개발자는 지능형 교통 시스템에서 참조하는 분석 모델의정확도를 검증할 수 있다. 실험에서는 제안하는 언어의 유효성을 검증하기 위해, 번호판 인식 모델을 중심으로 딥러닝 기술 문서의 적용과정을 보인다With the recent advancements of deep learning, companies such as smart home, healthcare, and intelligent transportation systems areutilizing its functionality to provide high-quality services for vehicle detection, emergency situation detection, and controlling energyconsumption. To provide reliable services in such sensitive systems, deep learning models are required to have high accuracy. In orderto develop a deep learning model for analyzing previously mentioned services, developers should utilize the state of the art deep learningmodels that have already been verified for higher accuracy. The developers can verify the accuracy of the referenced model by validatingthe model on the dataset. For this validation, the developer needs structural information to document and apply deep learning models,including metadata such as learning dataset, network architecture, and development environments. In this paper, we propose a descriptionlanguage that represents the network architecture of the deep learning model along with its metadata that are necessary to develop adeep learning model. Through the proposed description language, developers can easily verify the accuracy of the referenced deep learningmodel. Our experiments demonstrate the application scenario of a deep learning description document that focuses on the license platerecognition for the detection of illegally parked vehicles.Trusted Deep Learning, Model Reference, Deep Learning Description Language, Traffic Situation Analysis Model"
592,딥러닝 분석을 이용한 중국 역내·외 위안화 변동성 예측,이우식 /Lee Woosik,2016,한국데이터정보과학회,"2008년 글로벌 금융위기 이후 중국은 위안화 국제화의 점진적 추진을 진행하면서 중국상하이 외환시장과 중국홍콩 외환시장에서 거래되는 통화인 역내위안화와 역외위안화를 형성시켰다. 본 연구는 위안화 국제화와 점진적인 중국 자본계정 개방에 따라 급변하는 외환시장상황의 변동성을 정확하게 파악하기 위해서 GARCH모형 (일반화된 자기회귀 조건부이분산성모형)에 다단계인공신경망을 결합한 MLP-GARCH 모형과 GARCH모형과 기계학습의 일종인 딥러닝 (deep learning)을 통합한 DL-GARCH을 가지고 위안화 변동성예측을 비교 실험과 분석을 하였다. 비교분석 결과 DLGARCH 모형은 MLP-GARCH보다 모형 위안화 역내·외 환율변동성 예측 면에서 더욱 더 개선된예측값을 제공하였다. 그래서 이분산시계열모형을 딥러닝과 결합한 DL-GARCH 모형은 시계열의환율 변동성 예측 문제에 딥러닝을 응용할 수 있음을 확인하였다. 향후 이분산시계열과 결합된 딥러닝 모형은 다른 금융시계열 데이터에 응용하여 그 일반화 가능성을 높일 수 있을 것이다.The People’s Republic of China has vigorously been pursuing the internationalization of the Chinese Yuan or Renminbi after the financial crisis of 2008. In this view, an abrupt increase of use of the Chinese Yuan in the onshore and offshore markets are important milestones to be one of important currencies. One of the most frequently used methods to forecast volatility is GARCH model. Since a prediction error of the GARCH model has been reported quite high, a lot of efforts have been made to improve forecasting capability of the GARCH model. In this paper, we have proposed MLPGARCH and a DL-GARCH by employing Artificial Neural Network to the GARCH. In an application to forecasting Chinese Yuan volatility, we have successfully shown their overall outperformance in forecasting over the GARCH.Deep learning, DL-GARCH, GARCH, The Chinese Yuan, volatility."
593,딥러닝 기법을 이용한 내일강수 예측,하지훈,2016,한국지능시스템학회,"정확한 강수예측을 위해서는 예측인자 선정과 예측방법에 대한 선택이 매우 중요하다. 최근에는 강수예측 방법으로기계학습 기법이 많이 사용되고 있으며, 그 중에서도 특히 인공신경망을 사용한 강수예측 방법은 좋은 성능을 보였다. 본논문에서는 딥러닝 기법 중 하나인 DBN(deep belief network)를 이용한 새로운 강수예측 방법을 제안한다. DBN는 비지도사전 학습을 통해 초기 가중치를 설정하여 기존 인공신경망의 문제점을 보완한다. 예측인자로는 기온, 전일-전주 강수일, 태양과 달 궤도 관련 자료를 선정하였다. 기온과 전일-전주 강수일은 서울에서의 1974년부터 2013년까지 총 40년간의AWS(automatic weather system) 관측 자료를 사용하였고, 태양과 달의 궤도 관련 자료는 서울을 중심으로 계산한 결과를사용하였다. 전체 기간에서 일부는 학습 자료로 사용하여 예측모델을 생성하였고, 나머지를 생성한 모델의 검증 자료로사용하였다. 모델 검증 결과로 나온 예측값들은 확률값을 가지며 임계치를 이용하여 강수유무를 판별하였다. 강수 정확도의척도로 양분예보기법 중 CSI(critical successive index)와 Bias(frequency bias)를 계산하였다. 이를 통해 DBN와 MLP(multilayer perceptron)의 성능을 비교한 결과 DBN의 강수 예측 정확도가 높았고, 수행속도 또한 2배 이상 빨랐다.For accurate precipitation forecasts the choice of weather faoctrs and prediction method is very important. Recently, machinele arning has been widely used for forecasting precipitation, and artificial neural network, one of machine learning techniques, showed good performance. In this paper, we suggest a new method for forecasting precipitation using DBN, one of deep learning techniques.
DBN has an advantage that initial weights are set by unsupervised learning, so this compensates for the defects of artificial neural networks. We used past precipitation, temperature, and the parameters of the sun and moon's motion as features for forecasting precipitation. The dataset consists of observation data which had been measured for 40 years from AWS in Seoul. Experiments were based on 8-fold cross validation. As a result of estimation, we got probabilities of test dataset, so threshold was used for the decision of precipitation. CSI and Bias were used for indicating the precision of precipitation. Our experimental results showed that DBN performed better than MLP.Deep learning, Deep belief network, Precipitation, Forecast"
594,딥러닝 신경망모형을 이용한 실시간 교통정보수집 연구,김대현 /KIM DAE HYON,2016,사단법인 인문사회과학기술융합학회,"비디오 기반의 실시간 교통정보수집 시스템은 자율주행을 포함한 지능형교통체계의 주요 서브시스템중 하나이며, 우수한 예측(추정)력 때문에 핵심 알고리즘으로 인공신경망 모형이 주로 사용되고 있다. 특히, 최근 과학 학술지 네이처에 구글 딥마인드의 알파고가 공개되면서 딥러닝과 인공지능에 대한 전 세계적인 관심이 집중되고 있다. 최근까지 가장 널리 사용된 인공신경망모형은 역전파 모형이었으나 최근 컴퓨터 하드웨어 및 알고리즘의 새로운 개발과 더불어 딥러닝 알고리즘을 이용한 딥인공신경망 모형에 대한 관심이 높아지고 있다. 그러나 아직 딥러닝 인공신경망에 대한 연구는 초기단계에 있는 실정이며 적용분야도 극히 한정되어 있다. 본 연구에서는 실시간 도로교통정보 수집을 위한 영상처리분야에서 딥러닝 알고리즘을 활용하고 실험결과를 통하여 향후 교통정보수집‧분석 분야에서의 딥러닝 인공신경망 모형의 적용가능성을 살펴보고자 한다. 특히, 본 연구에서 도출한 딥러닝 인공신경망모형의 실험결과를 기존 역전파 모형에 의한 실험결과와 비교하여 모형의 우수성을 비교‧평가하고자 한다.Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Normally, artificial neural networks have been used in real-time applications for collecting traffic information because of their remarkable performance in terms of predictive accuracy and real-time computing. Especially, the recent publication of a paper in the journal Nature and the success of Google DeepMind’s world-beating Go program AlphaGo caused significant increases of the public interest in artificial intelligence. Even though various types of learning models have been proposed, the Backpropagation has been the most popular neural network model. However, the Deep learning neural network model has attracted much interest recently. In this study, the Deep leaning neural network will be applied in image processing techniques for the real-time traffic information collection to investigate the performance of Deep learning algorithm in traffic engineering fields. In addition, the experimental results from Deep learing algorithm will be compared with the results from Backpropagation model to evaluate and propose better performing model.Intelligent Transportation Systems (ITS), artificial neural networks, Deep learning, traffic information, image processing"
595,딥러닝의 모형과 응용사례,안성만 /SungMahn Ahn,2016,한국지능정보시스템학회,"딥러닝은 인공신경망(neural network)이라는 인공지능분야의 모형이 발전된 형태로서, 계층구조로 이루어진 인공신경망의 내부계층(hidden layer)이 여러 단계로 이루어진 구조이다. 딥러닝에서의 주요 모형은 합성곱신경망(convolutional neural network), 순환신경망(recurrent neural network), 그리고 심층신뢰신경망(deep belief network)의 세가지라고 할 수있다. 그 중에서 현재 흥미로운 연구가 많이 발표되어서 관심이 집중되고 있는 모형은 지도학습(supervised learning)모형인 처음 두 개의 모형이다. 따라서 본 논문에서는 지도학습모형의 가중치를 최적화하는 기본적인 방법인 오류역전파 알고리즘을 살펴본 뒤에 합성곱신경망과 순환신경망의 구조와 응용사례 등을 살펴보고자 한다. 본문에서 다루지 않은 모형인 심층신뢰신경망은 아직까지는 합성곱신경망 이나 순환신경망보다는 상대적으로 주목을 덜 받고 있다. 그러나 심층신뢰신경망은 CNN이나 RNN과는 달리 비지도학습(unsupervised learning)모형이며, 사람이나 동물은 관찰을 통해서 스스로 학습한다는 점에서 궁극적으로는 비지도학습모형이 더 많이 연구되어야 할 주제가 될 것이다.Deep learning model is a kind of neural networks that allows multiple hidden layers. There are various deep learning architectures such as convolutional neural networks, deep belief networks and recurrent neural networks. Those have been applied to fields like computer vision, automatic speech recognition, natural language processing, audio recognition and bioinformatics where they have been shown to produce state-of-the-art results on various tasks. Among those architectures, convolutional neural networks and recurrent neural networks are classified as the supervised learning model. And in recent years, those supervised learning models have gained more popularity than unsupervised learning models such as deep belief networks, because supervised learning models have shown fashionable applications in such fields mentioned above.
Deep learning models can be trained with backpropagation algorithm. Backpropagation is an abbreviation for “backward propagation of errors” and a common method of training artificial neural networks used in conjunction with an optimization method such as gradient descent. The method calculates the gradient of an error function with respect to all the weights in the network. The gradient is fed to the optimization method which in turn uses it to update the weights, in an attempt to minimize the error function.
Convolutional neural networks use a special architecture which is particularly well-adapted to classify images. Using this architecture makes convolutional networks fast to train. This, in turn, helps us train deep, muti-layer networks, which are very good at classifying images. These days, deep convolutional networks are used in most neural networks for image recognition. Convolutional neural networks use three basic ideas: local receptive fields, shared weights, and pooling. By local receptive fields, we mean that each neuron in the first(or any) hidden layer will be connected to a small region of the input(or previous layer’s) neurons. Shared weights mean that we’re going to use the same weights and bias for each of the local receptive field. This means that all the neurons in the hidden layer detect exactly the same feature, just at different locations in the input image. In addition to the convolutional layers just described, convolutional neural networks also contain pooling layers. Pooling layers are usually used immediately after convolutional layers. What the pooling layers do is to simplify the information in the output from the convolutional layer.
Recent convolutional network architectures have 10 to 20 hidden layers and billions of connections between units. Training deep learning networks has taken weeks several years ago, but thanks to progress in GPU and algorithm enhancement, training time has reduced to several hours.
Neural networks with time-varying behavior are known as recurrent neural networks or RNNs. A recurrent neural network is a class of artificial neural network where connections between units form a directed cycle. This creates an internal state of the network which allows it to exhibit dynamic temporal behavior. Unlike feedforward neural networks, RNNs can use their internal memory to process arbitrary sequences of inputs. Early RNN models turned out to be very difficult to train, harder even than deep feedforward networks. The reason is the unstable gradient problem such as vanishing gradient and exploding gradient. The gradient can get smaller and smaller as it is propagated back through layers. This makes learning in early layers extremely slow. The problem actually gets worse in RNNs, since gradients aren’t just propagated backward through layers, they’re propagated backward through time. If the network runs for a long time, that can make the gradient extremely unstable and hard to learn from. It has been possible to incorporate an idea known as long short-term memory units (LSTMs) into RNNs. LSTMs make it much easier to get good results when training RNNs, and many recent papers make use of LSTMs or related ideas.Deep learning, Convolutional neural networks, Recurrent neural networks, Error backpropagation algorithm"
596,딥러닝분석과 기술적 분석 지표를 이용한 한국 코스피주가지수 방향성 예측,이우식 /Lee Woosik,2017,한국데이터정보과학회,"2016년 3월 구글 (Google)의 바둑인공지능 알파고 (AlphaGo)가 이세돌 9단과의 바둑대결에서승리한 이후 다양한 분야에서 인공지능 사용에 대한 관심이 높아지고 있는 가운데 금융투자 분야에서도 인공지능과 투자자문 전문가의 합성어인 로보어드바이저 (Robo-Advisor)에 대한 관심이 높아지고 있다. 인공지능 (artificial intelligence)기반의 의사결정은 비용 절감은 물론 효과적인 의사결정을가능하게 한다는 점에서 큰 장점이 있다. 본 연구에서는 기술적 분석 (technical analysis) 지표와 딥러닝 (deep learning) 모형을 결합하여 한국 코스피 지수를 예측하는 모형을 개발하고 제시한 모형들의 예측력을 비교, 분석한다. 분석 결과 기술적 분석 지표에 딥러닝 알고리즘을 결합한 모형이 주가지수 방향성 예측 문제에 응용될 수 있음을 확인하였다. 향후 본 연구에서 제안된 기술적 분석 지표와딥러닝모형을 결합한 기법은 로보어드바이저서비스에 응용할 수 있는 일반화 가능성을 보여준다Since Google’s AlphaGo defeated a world champion of Go players in 2016, there have been many interests in the deep learning. In the financial sector, a Robo-Advisor using deep learning gains a significant attention, which builds and manages portfolios of financial instruments for investors.In this paper, we have proposed the a deep learning algorithm geared toward identification and forecast of the KOSPI index direction,and we also have compared the accuracy of the prediction.In an application of forecasting the financial market index direction, we have shown that the Robo-Advisor using deep learning has a significant effect on finance industry. The Robo-Advisor collects a massive data such as earnings statements, news reports and regulatory filings, analyzes those and recommends investors how to view market trends and identify the best time to purchase financial assets. On the other hand, the Robo-Advisor allows businesses to learn more about their customers, develop better marketing strategies, increase sales and decrease costs.Artificial intelligence, deep learning, FinTech, Robo-Advisor, technical analysis."
597,딥러닝의 효율적인 학습을 위한 학습데이터 선별 기법에 관한 연구,주경돈 /Gyoung-don Joo,2017,한국정보과학회,"빅데이터의 증가와 병렬 하드웨어 처리, 학습 알고리즘의 개선은 딥러닝의 발전을 가져왔으며 인공지능이 적용 가능한 영역을 획기적으로 확장시키고 있다. 하지만 이러한 대규모의 학습 데이터에 기반한 딥러닝은 학습시간이 증가되고, 더 나아가 학습 데이터의 구축을 위한 레이블링(labeling) 비용이 데이터 수에 비례하여 증가하게 된다. 따라서 본 논문에서는, 딥러닝의 정확도를 최대한 유지하면서 필요한 학습 데이터의 수를 줄일수 있도록, 전체 데이터에서 제한된 수의 학습 데이터의 후보를 선별하는 기법에 대하여 연구한다. 특히 기존의 능동 학습에 기반한 기법들이 학습 데이터의 수가 매우 적어질 경우 딥러닝의 정확도가 급격히 떨어지는 문제를 해결할 수 있는 새로운 기법을 제안하였으며, 제안된 기법으로 선별된 학습 데이터는 그 수의 변화에 대해 딥러닝이 일관되게 경쟁력있는 정확도를 가짐을 다양한 실험을 통하여 검증하였다.The increase of big data, parallel hardware processing and enhanced learning algorithms have led to the development of deep learning and are dramatically expanding the domain of applications based on artificial intelligence. However, such large-scale training data extends the learning time, and furthermore, the labeling cost for building the training set increases in proportion to the volume of data. Therefore, in this paper, we study how to select a limited number of candidates for training set from given data so as to reduce the number of training set while maintaining the accuracy of deep learning. Especially, we found that, with the training set selected by the previous work, the accuracy of deep learning drastically decreases when the number of training set is very small and so proposed new methods to resolve this fall-off in accuracy. Through experiments, we showed that deep learning consistently has the competitive accuracy with the training set selected by the proposed method regardless of the size of training set.Deep Learning, Train Set Selection, Active Learning, Gamma distribution"
598,딥러닝을 이용한 범용적 스테그아날리시스,김현재 /Hyunjae Kim,2017,한국정보과학회,"스테그아날리시스(Steganalysis)란 이미지 등 일반적인 자료에 암호화된 정보를 은닉하는 스테가노그래피(Steganography)에 대한 검출 및 분석 방법으로, 기계학습 기반 방법론을 포함한다. 기존 기계학습 기반 스테그아날리시스는 영상(Image)의 특징(Feature) 추출 및 모델링에 기반하며, 최근 딥러닝(Deep Learning)의 적용으로 검출 정확도가 큰 폭으로 향상되었다. 하지만 현존하는 스테그아날리시스 모델은 단일 스테가노그래피 기법에 대해 국한되어 있어 학습에 사용되지 않은 스테고(Stego) 이미지의 경우 검출이 불가능한 결정적 한계를 가진다. 본 연구에서는 다양한 스테가노그래피 기법으로 생성된 스테고이미지에 딥러닝을 적용하여 스테그아날리시스를 학습하는 범용적 모델을 제안한다. 다양한 실험을 통해 제안 기법의 효용성 및 가능성을 확인하고, 범용적 스테그아날리시스 모델이 각각에 특화된 검출 기법과 유사한 정확도로 스테고 이미지를 검출할 수 있음을 보인다.Steganalysis is to detect information hidden by steganography inside general data such as images. There are stegoanalysis techniques that use machine learning (ML). Existing ML approaches to steganalysis are based on extracting features from stego images and modeling them.
Recently deep learning-based methodologies have shown significant improvements in detection accuracy. However, all the existing methods, including deep learning-based ones, have a critical limitation in that they can only detect stego images that are created by a specific steganography method. In this paper, we propose a generalized steganalysis method that can model multiple types of stego images using deep learning. Through various experiments, we confirm the effectiveness of our approach and envision directions for future research. In particular, we show that our method can detect each type of steganography with the same level of accuracy as that of a steganalysis method dedicated to that type of steganography, thereby demonstrating the general applicability of our approach to multiple types of stego images.steganalysis, steganography, deep learning, generalized steganalysis model"
599,"딥러닝 프레임워크의 비교: 티아노, 텐서플로, CNTK를 중심으로",정여진 /Yeojin Chung,2017,한국지능정보시스템학회,"딥러닝 프레임워크의 대표적인 기능으로는 ‘자동미분’과 ‘GPU의 활용’ 등을 들 수 있다. 본 논문은 파이썬의 라이브러리 형태로 사용 가능한 프레임워크 중에서 구글의 텐서플로와 마이크로소프트의 CNTK, 그리고 텐서 플로의 원조라고 할 수 있는 티아노를 비교하였다. 본문에서는 자동미분의 개념과 GPU의 활용형태를 간단히 설명하고, 그 다음에 logistic regression을 실행하는 예를 통하여 각 프레임워크의 문법을 알아본 뒤에, 마지막으로 대표적인 딥러닝 응용인 CNN의 예제를 실행시켜보고 코딩의 편의성과 실행속도 등을 확인해 보았다. 그 결과, 편의성의 관점에서 보면 티아노가 가장 코딩 하기가 어렵고, CNTK와 텐서플로는 많은 부분이 비슷하게 추상화 되어 있어서 코딩이 비슷하지만 가중치와 편향을 직접 정의하느냐의 여부에서 차이를 보였다. 그리고 각프레임워크의 실행속도에 대한 평가는 ‘큰 차이는 없다’는 것이다. 텐서플로는 티아노에 비하여 속도가 느리다는 평가가 있어왔는데, 본 연구의 실험에 의하면, 비록 CNN 모형에 국한되었지만, 텐서플로가 아주 조금이지만빠른 것으로 나타났다. CNTK의 경우에도, 비록 실험환경이 달랐지만, 실험환경의 차이에 의한 속도의 차이의 편차범위 이내에 있는 것으로 판단이 되었다. 본 연구에서는 세 종류의 딥러닝 프레임워크만을 살펴보았는데,위키피디아에 따르면 딥러닝 프레임워크의 종류는 12가지가 있으며, 각 프레임워크의 특징을 15가지 속성으로구분하여 차이를 특정하고 있다. 그 많은 속성 중에서 사용자의 입장에서 볼 때 중요한 속성은 어떤 언어(파이썬, C++, Java, 등)로 사용가능한지, 어떤 딥러닝 모형에 대한 라이브러리가 잘 구현되어 있는지 등일 것이다. 그리고 사용자가 대규모의 딥러닝 모형을 구축한다면, 다중 GPU 혹은 다중 서버를 지원하는지의 여부도 중요할 것이다. 또한 딥러닝 모형을 처음 학습하는 경우에는 사용설명서가 많은지 예제 프로그램이 많은지 여부도 중요한 기준이 될 것이다.The deep learning framework is software designed to help develop deep learning models. Some of its important functions include “automatic differentiation” and “utilization of GPU”. The list of popular deep learning framework includes Caffe (BVLC) and Theano (University of Montreal). And recently, Microsoft's deep learning framework, Microsoft Cognitive Toolkit, was released as open-source license, following Google’s Tensorflow a year earlier. The early deep learning frameworks have been developed mainly for research at universities. Beginning with the inception of Tensorflow, however, it seems that companies such as Microsoft and Facebook have started to join the competition of framework development. Given the trend, Google and other companies are expected to continue investing in the deep learning framework to bring forward the initiative in the artificial intelligence business. From this point of view, we think it is a good time to compare some of deep learning frameworks. So we compare three deep learning frameworks which can be used as a Python library. Those are Google's Tensorflow, Microsoft’s CNTK, and Theano which is sort of a predecessor of the preceding two.
The most common and important function of deep learning frameworks is the ability to perform automatic differentiation. Basically all the mathematical expressions of deep learning models can be represented as computational graphs, which consist of nodes and edges. Partial derivatives on each edge of a computational graph can then be obtained. With the partial derivatives, we can let software compute differentiation of any node with respect to any variable by utilizing chain rule of Calculus.
First of all, the convenience of coding is in the order of CNTK, Tensorflow, and Theano. The criterion is simply based on the lengths of the codes and the learning curve and the ease of coding are not the main concern. According to the criteria, Theano was the most difficult to implement with, and CNTK and Tensorflow were somewhat easier. With Tensorflow, we need to define weight variables and biases explicitly. The reason that CNTK and Tensorflow are easier to implement with is that those frameworks provide us with more abstraction than Theano. We, however, need to mention that low-level coding is not always bad. It gives us flexibility of coding. With the low-level coding such as in Theano, we can implement and test any new deep learning models or any new search methods that we can think of.
The assessment of the execution speed of each framework is that there is not meaningful difference. According to the experiment, execution speeds of Theano and Tensorflow are very similar, although the experiment was limited to a CNN model. In the case of CNTK, the experimental environment was not maintained as the same. The code written in CNTK has to be run in PC environment without GPU where codes execute as much as 50 times slower than with GPU. But we concluded that the difference of execution speed was within the range of variation caused by the different hardware setup.
In this study, we compared three types of deep learning framework: Theano, Tensorflow, and CNTK. According to Wikipedia, there are 12 available deep learning frameworks. And 15 different attributes differentiate each framework. Some of the important attributes would include interface language (Python, C ++, Java, etc.) and the availability of libraries on various deep learning models such as CNN, RNN, DBN, and etc. And if a user implements a large scale deep learning model, it will also be important to support multiple GPU or multiple servers. Also, if you are learning the deep learning model, it would also be important if there are enough examples and references.deep learning framework, Theano, TensorFlow, CNTK, computational graph, CIFAR-10"
600,딥러닝을 이용한 주변 무선단말 파악방안,이웅섭 /Lee， Woong-sup,2018,한국정보통신학회,"최근 단말-대-단말(Device-to-device, D2D) 통신기술이 차세대 무선통신시스템의 핵심기술로 큰 관심을 받고 있다. 이러한 단말간 통신에서는 자신의 주변에 어떠한 단말이 있는지 파악하는 주변단말 탐색(Neighbor discovery)이매우 중요하다. 본 논문에서는 최근 큰 관심을 받고 있는 딥러닝(Deep learning) 기술을 활용하여 단말간 통신에서 주변단말을 파악하는 방안에 대해서 제안한다. 제안 방안은 기존의 방안과 달리 무선채널의 공간적 연관성을 이용하여단말간의 신호 전송 없이 단말이 기지국으로 전송하는 상향링크 파일럿 신호를 기반으로 주변 단말을 찾고 따라서기존의 방식에 비해 신호전송 복잡도(signaling complexity)를 크게 줄일 수 있다. 또한 제안 방안에서는 떨어져 있는거리에 따라서 주변 단말을 분류 가능하여 기존 방안에 비해서 좀 더 세밀한 단말 탐색이 가능하다. 마지막으로 본 논문에서는 tensorflow를 이용한 컴퓨터 시뮬레이션을 통해 제안 방안의 성능을 검증하였다.Recently, the device-to-device (D2D) communication has been conceived as the key technology for the next-generationmobile communication systems. The neighbor discovery in which the nearby users are found, is essential for the properoperation of the D2D communication. In this paper, we propose new neighbor discovery scheme based on deep learningtechnology which has gained a lot of attention recently. In the proposed scheme, the neighboring users can be foundusing the uplink pilot transmission of users only, unlike conventional neighbor discovery schemes in which direct pilotcommunication among users is required, such that the signaling overhead can be greatly reduced in our proposed scheme.
Moreover, the neighbors with different proximity can also be classified accordingly which enables more accurate neighbordiscovery compared to the conventional schemes. The performance of our proposed scheme is verified through thetensorflow-based computer simulations.
키워드 : 딥러닝, 주변단말 탐색, 단말-대-단말 통신Deep learning, Neighbor discovery, Device-to-device communication, Wireless channel correlation"
601,신뢰성있는 딥러닝 기반 분석 모델을 참조하기 위한 딥러닝 기술 언어,문종혁 /Jong Hyeok Mun,2021,한국정보처리학회,"최근 딥러닝은 하드웨어 성능이 향상됨에 따라 자연어 처리, 영상 인식 등의 다양한 기술에 접목되어 활용되고 있다. 이러한 기술들을 활용해 지능형교통 시스템(ITS), 스마트홈, 헬스케어 등의 산업분야에서 데이터를 분석하여 고속도로 속도위반 차량 검출, 에너지 사용량 제어, 응급상황 등과 같은고품질의 서비스를 제공하며, 고품질의 서비스를 제공하기 위해서는 정확도가 향상된 딥러닝 모델이 적용되어야 한다. 이를 위해 서비스 환경의 데이터를분석하기 위한 딥러닝 모델을 개발할 때, 개발자는 신뢰성이 검증된 최신의 딥러닝 모델을 적용할 수 있어야 한다. 이는 개발자가 참조하는 딥러닝 모델에적용된 학습 데이터셋의 정확도를 측정하여 검증할 수 있다. 이러한 검증을 위해서 개발자는 학습 데이터셋, 딥러닝의 계층구조 및 개발 환경 등과 같은내용을 포함하는 딥러닝 모델을 문서화하여 적용하기 위한 구조적인 정보가 필요하다. 본 논문에서는 신뢰성있는 딥러닝 기반 데이터 분석 모델을 참조하기위한 딥러닝 기술 언어를 제안한다. 제안하는 기술 언어는 신뢰성 있는 딥러닝 모델을 개발하는데 필요한 학습데이터셋, 개발 환경 및 설정 등의 정보와더불어 딥러닝 모델의 계층구조를 표현할 수 있다. 제안하는 딥러닝 기술 언어를 이용하여 개발자는 지능형 교통 시스템에서 참조하는 분석 모델의정확도를 검증할 수 있다. 실험에서는 제안하는 언어의 유효성을 검증하기 위해, 번호판 인식 모델을 중심으로 딥러닝 기술 문서의 적용과정을 보인다With the recent advancements of deep learning, companies such as smart home, healthcare, and intelligent transportation systems areutilizing its functionality to provide high-quality services for vehicle detection, emergency situation detection, and controlling energyconsumption. To provide reliable services in such sensitive systems, deep learning models are required to have high accuracy. In orderto develop a deep learning model for analyzing previously mentioned services, developers should utilize the state of the art deep learningmodels that have already been verified for higher accuracy. The developers can verify the accuracy of the referenced model by validatingthe model on the dataset. For this validation, the developer needs structural information to document and apply deep learning models,including metadata such as learning dataset, network architecture, and development environments. In this paper, we propose a descriptionlanguage that represents the network architecture of the deep learning model along with its metadata that are necessary to develop adeep learning model. Through the proposed description language, developers can easily verify the accuracy of the referenced deep learningmodel. Our experiments demonstrate the application scenario of a deep learning description document that focuses on the license platerecognition for the detection of illegally parked vehicles.Trusted Deep Learning, Model Reference, Deep Learning Description Language, Traffic Situation Analysis Model"
602,딥러닝 분석을 이용한 중국 역내·외 위안화 변동성 예측,이우식 /Lee Woosik,2016,한국데이터정보과학회,"2008년 글로벌 금융위기 이후 중국은 위안화 국제화의 점진적 추진을 진행하면서 중국상하이 외환시장과 중국홍콩 외환시장에서 거래되는 통화인 역내위안화와 역외위안화를 형성시켰다. 본 연구는 위안화 국제화와 점진적인 중국 자본계정 개방에 따라 급변하는 외환시장상황의 변동성을 정확하게 파악하기 위해서 GARCH모형 (일반화된 자기회귀 조건부이분산성모형)에 다단계인공신경망을 결합한 MLP-GARCH 모형과 GARCH모형과 기계학습의 일종인 딥러닝 (deep learning)을 통합한 DL-GARCH을 가지고 위안화 변동성예측을 비교 실험과 분석을 하였다. 비교분석 결과 DLGARCH 모형은 MLP-GARCH보다 모형 위안화 역내·외 환율변동성 예측 면에서 더욱 더 개선된예측값을 제공하였다. 그래서 이분산시계열모형을 딥러닝과 결합한 DL-GARCH 모형은 시계열의환율 변동성 예측 문제에 딥러닝을 응용할 수 있음을 확인하였다. 향후 이분산시계열과 결합된 딥러닝 모형은 다른 금융시계열 데이터에 응용하여 그 일반화 가능성을 높일 수 있을 것이다.The People’s Republic of China has vigorously been pursuing the internationalization of the Chinese Yuan or Renminbi after the financial crisis of 2008. In this view, an abrupt increase of use of the Chinese Yuan in the onshore and offshore markets are important milestones to be one of important currencies. One of the most frequently used methods to forecast volatility is GARCH model. Since a prediction error of the GARCH model has been reported quite high, a lot of efforts have been made to improve forecasting capability of the GARCH model. In this paper, we have proposed MLPGARCH and a DL-GARCH by employing Artificial Neural Network to the GARCH. In an application to forecasting Chinese Yuan volatility, we have successfully shown their overall outperformance in forecasting over the GARCH.Deep learning, DL-GARCH, GARCH, The Chinese Yuan, volatility."
603,딥러닝 기법을 이용한 내일강수 예측,하지훈,2016,한국지능시스템학회,"정확한 강수예측을 위해서는 예측인자 선정과 예측방법에 대한 선택이 매우 중요하다. 최근에는 강수예측 방법으로기계학습 기법이 많이 사용되고 있으며, 그 중에서도 특히 인공신경망을 사용한 강수예측 방법은 좋은 성능을 보였다. 본논문에서는 딥러닝 기법 중 하나인 DBN(deep belief network)를 이용한 새로운 강수예측 방법을 제안한다. DBN는 비지도사전 학습을 통해 초기 가중치를 설정하여 기존 인공신경망의 문제점을 보완한다. 예측인자로는 기온, 전일-전주 강수일, 태양과 달 궤도 관련 자료를 선정하였다. 기온과 전일-전주 강수일은 서울에서의 1974년부터 2013년까지 총 40년간의AWS(automatic weather system) 관측 자료를 사용하였고, 태양과 달의 궤도 관련 자료는 서울을 중심으로 계산한 결과를사용하였다. 전체 기간에서 일부는 학습 자료로 사용하여 예측모델을 생성하였고, 나머지를 생성한 모델의 검증 자료로사용하였다. 모델 검증 결과로 나온 예측값들은 확률값을 가지며 임계치를 이용하여 강수유무를 판별하였다. 강수 정확도의척도로 양분예보기법 중 CSI(critical successive index)와 Bias(frequency bias)를 계산하였다. 이를 통해 DBN와 MLP(multilayer perceptron)의 성능을 비교한 결과 DBN의 강수 예측 정확도가 높았고, 수행속도 또한 2배 이상 빨랐다.For accurate precipitation forecasts the choice of weather faoctrs and prediction method is very important. Recently, machinele arning has been widely used for forecasting precipitation, and artificial neural network, one of machine learning techniques, showed good performance. In this paper, we suggest a new method for forecasting precipitation using DBN, one of deep learning techniques.
DBN has an advantage that initial weights are set by unsupervised learning, so this compensates for the defects of artificial neural networks. We used past precipitation, temperature, and the parameters of the sun and moon's motion as features for forecasting precipitation. The dataset consists of observation data which had been measured for 40 years from AWS in Seoul. Experiments were based on 8-fold cross validation. As a result of estimation, we got probabilities of test dataset, so threshold was used for the decision of precipitation. CSI and Bias were used for indicating the precision of precipitation. Our experimental results showed that DBN performed better than MLP.Deep learning, Deep belief network, Precipitation, Forecast"
604,딥러닝 신경망모형을 이용한 실시간 교통정보수집 연구,김대현 /KIM DAE HYON,2016,사단법인 인문사회과학기술융합학회,"비디오 기반의 실시간 교통정보수집 시스템은 자율주행을 포함한 지능형교통체계의 주요 서브시스템중 하나이며, 우수한 예측(추정)력 때문에 핵심 알고리즘으로 인공신경망 모형이 주로 사용되고 있다. 특히, 최근 과학 학술지 네이처에 구글 딥마인드의 알파고가 공개되면서 딥러닝과 인공지능에 대한 전 세계적인 관심이 집중되고 있다. 최근까지 가장 널리 사용된 인공신경망모형은 역전파 모형이었으나 최근 컴퓨터 하드웨어 및 알고리즘의 새로운 개발과 더불어 딥러닝 알고리즘을 이용한 딥인공신경망 모형에 대한 관심이 높아지고 있다. 그러나 아직 딥러닝 인공신경망에 대한 연구는 초기단계에 있는 실정이며 적용분야도 극히 한정되어 있다. 본 연구에서는 실시간 도로교통정보 수집을 위한 영상처리분야에서 딥러닝 알고리즘을 활용하고 실험결과를 통하여 향후 교통정보수집‧분석 분야에서의 딥러닝 인공신경망 모형의 적용가능성을 살펴보고자 한다. 특히, 본 연구에서 도출한 딥러닝 인공신경망모형의 실험결과를 기존 역전파 모형에 의한 실험결과와 비교하여 모형의 우수성을 비교‧평가하고자 한다.Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Normally, artificial neural networks have been used in real-time applications for collecting traffic information because of their remarkable performance in terms of predictive accuracy and real-time computing. Especially, the recent publication of a paper in the journal Nature and the success of Google DeepMind’s world-beating Go program AlphaGo caused significant increases of the public interest in artificial intelligence. Even though various types of learning models have been proposed, the Backpropagation has been the most popular neural network model. However, the Deep learning neural network model has attracted much interest recently. In this study, the Deep leaning neural network will be applied in image processing techniques for the real-time traffic information collection to investigate the performance of Deep learning algorithm in traffic engineering fields. In addition, the experimental results from Deep learing algorithm will be compared with the results from Backpropagation model to evaluate and propose better performing model.Intelligent Transportation Systems (ITS), artificial neural networks, Deep learning, traffic information, image processing"
605,딥러닝의 모형과 응용사례,안성만 /SungMahn Ahn,2016,한국지능정보시스템학회,"딥러닝은 인공신경망(neural network)이라는 인공지능분야의 모형이 발전된 형태로서, 계층구조로 이루어진 인공신경망의 내부계층(hidden layer)이 여러 단계로 이루어진 구조이다. 딥러닝에서의 주요 모형은 합성곱신경망(convolutional neural network), 순환신경망(recurrent neural network), 그리고 심층신뢰신경망(deep belief network)의 세가지라고 할 수있다. 그 중에서 현재 흥미로운 연구가 많이 발표되어서 관심이 집중되고 있는 모형은 지도학습(supervised learning)모형인 처음 두 개의 모형이다. 따라서 본 논문에서는 지도학습모형의 가중치를 최적화하는 기본적인 방법인 오류역전파 알고리즘을 살펴본 뒤에 합성곱신경망과 순환신경망의 구조와 응용사례 등을 살펴보고자 한다. 본문에서 다루지 않은 모형인 심층신뢰신경망은 아직까지는 합성곱신경망 이나 순환신경망보다는 상대적으로 주목을 덜 받고 있다. 그러나 심층신뢰신경망은 CNN이나 RNN과는 달리 비지도학습(unsupervised learning)모형이며, 사람이나 동물은 관찰을 통해서 스스로 학습한다는 점에서 궁극적으로는 비지도학습모형이 더 많이 연구되어야 할 주제가 될 것이다.Deep learning model is a kind of neural networks that allows multiple hidden layers. There are various deep learning architectures such as convolutional neural networks, deep belief networks and recurrent neural networks. Those have been applied to fields like computer vision, automatic speech recognition, natural language processing, audio recognition and bioinformatics where they have been shown to produce state-of-the-art results on various tasks. Among those architectures, convolutional neural networks and recurrent neural networks are classified as the supervised learning model. And in recent years, those supervised learning models have gained more popularity than unsupervised learning models such as deep belief networks, because supervised learning models have shown fashionable applications in such fields mentioned above.
Deep learning models can be trained with backpropagation algorithm. Backpropagation is an abbreviation for “backward propagation of errors” and a common method of training artificial neural networks used in conjunction with an optimization method such as gradient descent. The method calculates the gradient of an error function with respect to all the weights in the network. The gradient is fed to the optimization method which in turn uses it to update the weights, in an attempt to minimize the error function.
Convolutional neural networks use a special architecture which is particularly well-adapted to classify images. Using this architecture makes convolutional networks fast to train. This, in turn, helps us train deep, muti-layer networks, which are very good at classifying images. These days, deep convolutional networks are used in most neural networks for image recognition. Convolutional neural networks use three basic ideas: local receptive fields, shared weights, and pooling. By local receptive fields, we mean that each neuron in the first(or any) hidden layer will be connected to a small region of the input(or previous layer’s) neurons. Shared weights mean that we’re going to use the same weights and bias for each of the local receptive field. This means that all the neurons in the hidden layer detect exactly the same feature, just at different locations in the input image. In addition to the convolutional layers just described, convolutional neural networks also contain pooling layers. Pooling layers are usually used immediately after convolutional layers. What the pooling layers do is to simplify the information in the output from the convolutional layer.
Recent convolutional network architectures have 10 to 20 hidden layers and billions of connections between units. Training deep learning networks has taken weeks several years ago, but thanks to progress in GPU and algorithm enhancement, training time has reduced to several hours.
Neural networks with time-varying behavior are known as recurrent neural networks or RNNs. A recurrent neural network is a class of artificial neural network where connections between units form a directed cycle. This creates an internal state of the network which allows it to exhibit dynamic temporal behavior. Unlike feedforward neural networks, RNNs can use their internal memory to process arbitrary sequences of inputs. Early RNN models turned out to be very difficult to train, harder even than deep feedforward networks. The reason is the unstable gradient problem such as vanishing gradient and exploding gradient. The gradient can get smaller and smaller as it is propagated back through layers. This makes learning in early layers extremely slow. The problem actually gets worse in RNNs, since gradients aren’t just propagated backward through layers, they’re propagated backward through time. If the network runs for a long time, that can make the gradient extremely unstable and hard to learn from. It has been possible to incorporate an idea known as long short-term memory units (LSTMs) into RNNs. LSTMs make it much easier to get good results when training RNNs, and many recent papers make use of LSTMs or related ideas.Deep learning, Convolutional neural networks, Recurrent neural networks, Error backpropagation algorithm"
606,딥러닝분석과 기술적 분석 지표를 이용한 한국 코스피주가지수 방향성 예측,이우식 /Lee Woosik,2017,한국데이터정보과학회,"2016년 3월 구글 (Google)의 바둑인공지능 알파고 (AlphaGo)가 이세돌 9단과의 바둑대결에서승리한 이후 다양한 분야에서 인공지능 사용에 대한 관심이 높아지고 있는 가운데 금융투자 분야에서도 인공지능과 투자자문 전문가의 합성어인 로보어드바이저 (Robo-Advisor)에 대한 관심이 높아지고 있다. 인공지능 (artificial intelligence)기반의 의사결정은 비용 절감은 물론 효과적인 의사결정을가능하게 한다는 점에서 큰 장점이 있다. 본 연구에서는 기술적 분석 (technical analysis) 지표와 딥러닝 (deep learning) 모형을 결합하여 한국 코스피 지수를 예측하는 모형을 개발하고 제시한 모형들의 예측력을 비교, 분석한다. 분석 결과 기술적 분석 지표에 딥러닝 알고리즘을 결합한 모형이 주가지수 방향성 예측 문제에 응용될 수 있음을 확인하였다. 향후 본 연구에서 제안된 기술적 분석 지표와딥러닝모형을 결합한 기법은 로보어드바이저서비스에 응용할 수 있는 일반화 가능성을 보여준다Since Google’s AlphaGo defeated a world champion of Go players in 2016, there have been many interests in the deep learning. In the financial sector, a Robo-Advisor using deep learning gains a significant attention, which builds and manages portfolios of financial instruments for investors.In this paper, we have proposed the a deep learning algorithm geared toward identification and forecast of the KOSPI index direction,and we also have compared the accuracy of the prediction.In an application of forecasting the financial market index direction, we have shown that the Robo-Advisor using deep learning has a significant effect on finance industry. The Robo-Advisor collects a massive data such as earnings statements, news reports and regulatory filings, analyzes those and recommends investors how to view market trends and identify the best time to purchase financial assets. On the other hand, the Robo-Advisor allows businesses to learn more about their customers, develop better marketing strategies, increase sales and decrease costs.Artificial intelligence, deep learning, FinTech, Robo-Advisor, technical analysis."
607,딥러닝의 효율적인 학습을 위한 학습데이터 선별 기법에 관한 연구,주경돈 /Gyoung-don Joo,2017,한국정보과학회,"빅데이터의 증가와 병렬 하드웨어 처리, 학습 알고리즘의 개선은 딥러닝의 발전을 가져왔으며 인공지능이 적용 가능한 영역을 획기적으로 확장시키고 있다. 하지만 이러한 대규모의 학습 데이터에 기반한 딥러닝은 학습시간이 증가되고, 더 나아가 학습 데이터의 구축을 위한 레이블링(labeling) 비용이 데이터 수에 비례하여 증가하게 된다. 따라서 본 논문에서는, 딥러닝의 정확도를 최대한 유지하면서 필요한 학습 데이터의 수를 줄일수 있도록, 전체 데이터에서 제한된 수의 학습 데이터의 후보를 선별하는 기법에 대하여 연구한다. 특히 기존의 능동 학습에 기반한 기법들이 학습 데이터의 수가 매우 적어질 경우 딥러닝의 정확도가 급격히 떨어지는 문제를 해결할 수 있는 새로운 기법을 제안하였으며, 제안된 기법으로 선별된 학습 데이터는 그 수의 변화에 대해 딥러닝이 일관되게 경쟁력있는 정확도를 가짐을 다양한 실험을 통하여 검증하였다.The increase of big data, parallel hardware processing and enhanced learning algorithms have led to the development of deep learning and are dramatically expanding the domain of applications based on artificial intelligence. However, such large-scale training data extends the learning time, and furthermore, the labeling cost for building the training set increases in proportion to the volume of data. Therefore, in this paper, we study how to select a limited number of candidates for training set from given data so as to reduce the number of training set while maintaining the accuracy of deep learning. Especially, we found that, with the training set selected by the previous work, the accuracy of deep learning drastically decreases when the number of training set is very small and so proposed new methods to resolve this fall-off in accuracy. Through experiments, we showed that deep learning consistently has the competitive accuracy with the training set selected by the proposed method regardless of the size of training set.Deep Learning, Train Set Selection, Active Learning, Gamma distribution"
608,딥러닝을 이용한 범용적 스테그아날리시스,김현재 /Hyunjae Kim,2017,한국정보과학회,"스테그아날리시스(Steganalysis)란 이미지 등 일반적인 자료에 암호화된 정보를 은닉하는 스테가노그래피(Steganography)에 대한 검출 및 분석 방법으로, 기계학습 기반 방법론을 포함한다. 기존 기계학습 기반 스테그아날리시스는 영상(Image)의 특징(Feature) 추출 및 모델링에 기반하며, 최근 딥러닝(Deep Learning)의 적용으로 검출 정확도가 큰 폭으로 향상되었다. 하지만 현존하는 스테그아날리시스 모델은 단일 스테가노그래피 기법에 대해 국한되어 있어 학습에 사용되지 않은 스테고(Stego) 이미지의 경우 검출이 불가능한 결정적 한계를 가진다. 본 연구에서는 다양한 스테가노그래피 기법으로 생성된 스테고이미지에 딥러닝을 적용하여 스테그아날리시스를 학습하는 범용적 모델을 제안한다. 다양한 실험을 통해 제안 기법의 효용성 및 가능성을 확인하고, 범용적 스테그아날리시스 모델이 각각에 특화된 검출 기법과 유사한 정확도로 스테고 이미지를 검출할 수 있음을 보인다.Steganalysis is to detect information hidden by steganography inside general data such as images. There are stegoanalysis techniques that use machine learning (ML). Existing ML approaches to steganalysis are based on extracting features from stego images and modeling them.
Recently deep learning-based methodologies have shown significant improvements in detection accuracy. However, all the existing methods, including deep learning-based ones, have a critical limitation in that they can only detect stego images that are created by a specific steganography method. In this paper, we propose a generalized steganalysis method that can model multiple types of stego images using deep learning. Through various experiments, we confirm the effectiveness of our approach and envision directions for future research. In particular, we show that our method can detect each type of steganography with the same level of accuracy as that of a steganalysis method dedicated to that type of steganography, thereby demonstrating the general applicability of our approach to multiple types of stego images.steganalysis, steganography, deep learning, generalized steganalysis model"
609,"딥러닝 프레임워크의 비교: 티아노, 텐서플로, CNTK를 중심으로",정여진 /Yeojin Chung,2017,한국지능정보시스템학회,"딥러닝 프레임워크의 대표적인 기능으로는 ‘자동미분’과 ‘GPU의 활용’ 등을 들 수 있다. 본 논문은 파이썬의 라이브러리 형태로 사용 가능한 프레임워크 중에서 구글의 텐서플로와 마이크로소프트의 CNTK, 그리고 텐서 플로의 원조라고 할 수 있는 티아노를 비교하였다. 본문에서는 자동미분의 개념과 GPU의 활용형태를 간단히 설명하고, 그 다음에 logistic regression을 실행하는 예를 통하여 각 프레임워크의 문법을 알아본 뒤에, 마지막으로 대표적인 딥러닝 응용인 CNN의 예제를 실행시켜보고 코딩의 편의성과 실행속도 등을 확인해 보았다. 그 결과, 편의성의 관점에서 보면 티아노가 가장 코딩 하기가 어렵고, CNTK와 텐서플로는 많은 부분이 비슷하게 추상화 되어 있어서 코딩이 비슷하지만 가중치와 편향을 직접 정의하느냐의 여부에서 차이를 보였다. 그리고 각프레임워크의 실행속도에 대한 평가는 ‘큰 차이는 없다’는 것이다. 텐서플로는 티아노에 비하여 속도가 느리다는 평가가 있어왔는데, 본 연구의 실험에 의하면, 비록 CNN 모형에 국한되었지만, 텐서플로가 아주 조금이지만빠른 것으로 나타났다. CNTK의 경우에도, 비록 실험환경이 달랐지만, 실험환경의 차이에 의한 속도의 차이의 편차범위 이내에 있는 것으로 판단이 되었다. 본 연구에서는 세 종류의 딥러닝 프레임워크만을 살펴보았는데,위키피디아에 따르면 딥러닝 프레임워크의 종류는 12가지가 있으며, 각 프레임워크의 특징을 15가지 속성으로구분하여 차이를 특정하고 있다. 그 많은 속성 중에서 사용자의 입장에서 볼 때 중요한 속성은 어떤 언어(파이썬, C++, Java, 등)로 사용가능한지, 어떤 딥러닝 모형에 대한 라이브러리가 잘 구현되어 있는지 등일 것이다. 그리고 사용자가 대규모의 딥러닝 모형을 구축한다면, 다중 GPU 혹은 다중 서버를 지원하는지의 여부도 중요할 것이다. 또한 딥러닝 모형을 처음 학습하는 경우에는 사용설명서가 많은지 예제 프로그램이 많은지 여부도 중요한 기준이 될 것이다.The deep learning framework is software designed to help develop deep learning models. Some of its important functions include “automatic differentiation” and “utilization of GPU”. The list of popular deep learning framework includes Caffe (BVLC) and Theano (University of Montreal). And recently, Microsoft's deep learning framework, Microsoft Cognitive Toolkit, was released as open-source license, following Google’s Tensorflow a year earlier. The early deep learning frameworks have been developed mainly for research at universities. Beginning with the inception of Tensorflow, however, it seems that companies such as Microsoft and Facebook have started to join the competition of framework development. Given the trend, Google and other companies are expected to continue investing in the deep learning framework to bring forward the initiative in the artificial intelligence business. From this point of view, we think it is a good time to compare some of deep learning frameworks. So we compare three deep learning frameworks which can be used as a Python library. Those are Google's Tensorflow, Microsoft’s CNTK, and Theano which is sort of a predecessor of the preceding two.
The most common and important function of deep learning frameworks is the ability to perform automatic differentiation. Basically all the mathematical expressions of deep learning models can be represented as computational graphs, which consist of nodes and edges. Partial derivatives on each edge of a computational graph can then be obtained. With the partial derivatives, we can let software compute differentiation of any node with respect to any variable by utilizing chain rule of Calculus.
First of all, the convenience of coding is in the order of CNTK, Tensorflow, and Theano. The criterion is simply based on the lengths of the codes and the learning curve and the ease of coding are not the main concern. According to the criteria, Theano was the most difficult to implement with, and CNTK and Tensorflow were somewhat easier. With Tensorflow, we need to define weight variables and biases explicitly. The reason that CNTK and Tensorflow are easier to implement with is that those frameworks provide us with more abstraction than Theano. We, however, need to mention that low-level coding is not always bad. It gives us flexibility of coding. With the low-level coding such as in Theano, we can implement and test any new deep learning models or any new search methods that we can think of.
The assessment of the execution speed of each framework is that there is not meaningful difference. According to the experiment, execution speeds of Theano and Tensorflow are very similar, although the experiment was limited to a CNN model. In the case of CNTK, the experimental environment was not maintained as the same. The code written in CNTK has to be run in PC environment without GPU where codes execute as much as 50 times slower than with GPU. But we concluded that the difference of execution speed was within the range of variation caused by the different hardware setup.
In this study, we compared three types of deep learning framework: Theano, Tensorflow, and CNTK. According to Wikipedia, there are 12 available deep learning frameworks. And 15 different attributes differentiate each framework. Some of the important attributes would include interface language (Python, C ++, Java, etc.) and the availability of libraries on various deep learning models such as CNN, RNN, DBN, and etc. And if a user implements a large scale deep learning model, it will also be important to support multiple GPU or multiple servers. Also, if you are learning the deep learning model, it would also be important if there are enough examples and references.deep learning framework, Theano, TensorFlow, CNTK, computational graph, CIFAR-10"
610,딥러닝을 이용한 주변 무선단말 파악방안,이웅섭 /Lee， Woong-sup,2018,한국정보통신학회,"최근 단말-대-단말(Device-to-device, D2D) 통신기술이 차세대 무선통신시스템의 핵심기술로 큰 관심을 받고 있다. 이러한 단말간 통신에서는 자신의 주변에 어떠한 단말이 있는지 파악하는 주변단말 탐색(Neighbor discovery)이매우 중요하다. 본 논문에서는 최근 큰 관심을 받고 있는 딥러닝(Deep learning) 기술을 활용하여 단말간 통신에서 주변단말을 파악하는 방안에 대해서 제안한다. 제안 방안은 기존의 방안과 달리 무선채널의 공간적 연관성을 이용하여단말간의 신호 전송 없이 단말이 기지국으로 전송하는 상향링크 파일럿 신호를 기반으로 주변 단말을 찾고 따라서기존의 방식에 비해 신호전송 복잡도(signaling complexity)를 크게 줄일 수 있다. 또한 제안 방안에서는 떨어져 있는거리에 따라서 주변 단말을 분류 가능하여 기존 방안에 비해서 좀 더 세밀한 단말 탐색이 가능하다. 마지막으로 본 논문에서는 tensorflow를 이용한 컴퓨터 시뮬레이션을 통해 제안 방안의 성능을 검증하였다.Recently, the device-to-device (D2D) communication has been conceived as the key technology for the next-generationmobile communication systems. The neighbor discovery in which the nearby users are found, is essential for the properoperation of the D2D communication. In this paper, we propose new neighbor discovery scheme based on deep learningtechnology which has gained a lot of attention recently. In the proposed scheme, the neighboring users can be foundusing the uplink pilot transmission of users only, unlike conventional neighbor discovery schemes in which direct pilotcommunication among users is required, such that the signaling overhead can be greatly reduced in our proposed scheme.
Moreover, the neighbors with different proximity can also be classified accordingly which enables more accurate neighbordiscovery compared to the conventional schemes. The performance of our proposed scheme is verified through thetensorflow-based computer simulations.
키워드 : 딥러닝, 주변단말 탐색, 단말-대-단말 통신Deep learning, Neighbor discovery, Device-to-device communication, Wireless channel correlation"
611,신뢰성있는 딥러닝 기반 분석 모델을 참조하기 위한 딥러닝 기술 언어,문종혁 /Jong Hyeok Mun,2021,한국정보처리학회,"최근 딥러닝은 하드웨어 성능이 향상됨에 따라 자연어 처리, 영상 인식 등의 다양한 기술에 접목되어 활용되고 있다. 이러한 기술들을 활용해 지능형교통 시스템(ITS), 스마트홈, 헬스케어 등의 산업분야에서 데이터를 분석하여 고속도로 속도위반 차량 검출, 에너지 사용량 제어, 응급상황 등과 같은고품질의 서비스를 제공하며, 고품질의 서비스를 제공하기 위해서는 정확도가 향상된 딥러닝 모델이 적용되어야 한다. 이를 위해 서비스 환경의 데이터를분석하기 위한 딥러닝 모델을 개발할 때, 개발자는 신뢰성이 검증된 최신의 딥러닝 모델을 적용할 수 있어야 한다. 이는 개발자가 참조하는 딥러닝 모델에적용된 학습 데이터셋의 정확도를 측정하여 검증할 수 있다. 이러한 검증을 위해서 개발자는 학습 데이터셋, 딥러닝의 계층구조 및 개발 환경 등과 같은내용을 포함하는 딥러닝 모델을 문서화하여 적용하기 위한 구조적인 정보가 필요하다. 본 논문에서는 신뢰성있는 딥러닝 기반 데이터 분석 모델을 참조하기위한 딥러닝 기술 언어를 제안한다. 제안하는 기술 언어는 신뢰성 있는 딥러닝 모델을 개발하는데 필요한 학습데이터셋, 개발 환경 및 설정 등의 정보와더불어 딥러닝 모델의 계층구조를 표현할 수 있다. 제안하는 딥러닝 기술 언어를 이용하여 개발자는 지능형 교통 시스템에서 참조하는 분석 모델의정확도를 검증할 수 있다. 실험에서는 제안하는 언어의 유효성을 검증하기 위해, 번호판 인식 모델을 중심으로 딥러닝 기술 문서의 적용과정을 보인다With the recent advancements of deep learning, companies such as smart home, healthcare, and intelligent transportation systems areutilizing its functionality to provide high-quality services for vehicle detection, emergency situation detection, and controlling energyconsumption. To provide reliable services in such sensitive systems, deep learning models are required to have high accuracy. In orderto develop a deep learning model for analyzing previously mentioned services, developers should utilize the state of the art deep learningmodels that have already been verified for higher accuracy. The developers can verify the accuracy of the referenced model by validatingthe model on the dataset. For this validation, the developer needs structural information to document and apply deep learning models,including metadata such as learning dataset, network architecture, and development environments. In this paper, we propose a descriptionlanguage that represents the network architecture of the deep learning model along with its metadata that are necessary to develop adeep learning model. Through the proposed description language, developers can easily verify the accuracy of the referenced deep learningmodel. Our experiments demonstrate the application scenario of a deep learning description document that focuses on the license platerecognition for the detection of illegally parked vehicles.Trusted Deep Learning, Model Reference, Deep Learning Description Language, Traffic Situation Analysis Model"
612,딥러닝 분석을 이용한 중국 역내·외 위안화 변동성 예측,이우식 /Lee Woosik,2016,한국데이터정보과학회,"2008년 글로벌 금융위기 이후 중국은 위안화 국제화의 점진적 추진을 진행하면서 중국상하이 외환시장과 중국홍콩 외환시장에서 거래되는 통화인 역내위안화와 역외위안화를 형성시켰다. 본 연구는 위안화 국제화와 점진적인 중국 자본계정 개방에 따라 급변하는 외환시장상황의 변동성을 정확하게 파악하기 위해서 GARCH모형 (일반화된 자기회귀 조건부이분산성모형)에 다단계인공신경망을 결합한 MLP-GARCH 모형과 GARCH모형과 기계학습의 일종인 딥러닝 (deep learning)을 통합한 DL-GARCH을 가지고 위안화 변동성예측을 비교 실험과 분석을 하였다. 비교분석 결과 DLGARCH 모형은 MLP-GARCH보다 모형 위안화 역내·외 환율변동성 예측 면에서 더욱 더 개선된예측값을 제공하였다. 그래서 이분산시계열모형을 딥러닝과 결합한 DL-GARCH 모형은 시계열의환율 변동성 예측 문제에 딥러닝을 응용할 수 있음을 확인하였다. 향후 이분산시계열과 결합된 딥러닝 모형은 다른 금융시계열 데이터에 응용하여 그 일반화 가능성을 높일 수 있을 것이다.The People’s Republic of China has vigorously been pursuing the internationalization of the Chinese Yuan or Renminbi after the financial crisis of 2008. In this view, an abrupt increase of use of the Chinese Yuan in the onshore and offshore markets are important milestones to be one of important currencies. One of the most frequently used methods to forecast volatility is GARCH model. Since a prediction error of the GARCH model has been reported quite high, a lot of efforts have been made to improve forecasting capability of the GARCH model. In this paper, we have proposed MLPGARCH and a DL-GARCH by employing Artificial Neural Network to the GARCH. In an application to forecasting Chinese Yuan volatility, we have successfully shown their overall outperformance in forecasting over the GARCH.Deep learning, DL-GARCH, GARCH, The Chinese Yuan, volatility."
613,딥러닝 기법을 이용한 내일강수 예측,하지훈,2016,한국지능시스템학회,"정확한 강수예측을 위해서는 예측인자 선정과 예측방법에 대한 선택이 매우 중요하다. 최근에는 강수예측 방법으로기계학습 기법이 많이 사용되고 있으며, 그 중에서도 특히 인공신경망을 사용한 강수예측 방법은 좋은 성능을 보였다. 본논문에서는 딥러닝 기법 중 하나인 DBN(deep belief network)를 이용한 새로운 강수예측 방법을 제안한다. DBN는 비지도사전 학습을 통해 초기 가중치를 설정하여 기존 인공신경망의 문제점을 보완한다. 예측인자로는 기온, 전일-전주 강수일, 태양과 달 궤도 관련 자료를 선정하였다. 기온과 전일-전주 강수일은 서울에서의 1974년부터 2013년까지 총 40년간의AWS(automatic weather system) 관측 자료를 사용하였고, 태양과 달의 궤도 관련 자료는 서울을 중심으로 계산한 결과를사용하였다. 전체 기간에서 일부는 학습 자료로 사용하여 예측모델을 생성하였고, 나머지를 생성한 모델의 검증 자료로사용하였다. 모델 검증 결과로 나온 예측값들은 확률값을 가지며 임계치를 이용하여 강수유무를 판별하였다. 강수 정확도의척도로 양분예보기법 중 CSI(critical successive index)와 Bias(frequency bias)를 계산하였다. 이를 통해 DBN와 MLP(multilayer perceptron)의 성능을 비교한 결과 DBN의 강수 예측 정확도가 높았고, 수행속도 또한 2배 이상 빨랐다.For accurate precipitation forecasts the choice of weather faoctrs and prediction method is very important. Recently, machinele arning has been widely used for forecasting precipitation, and artificial neural network, one of machine learning techniques, showed good performance. In this paper, we suggest a new method for forecasting precipitation using DBN, one of deep learning techniques.
DBN has an advantage that initial weights are set by unsupervised learning, so this compensates for the defects of artificial neural networks. We used past precipitation, temperature, and the parameters of the sun and moon's motion as features for forecasting precipitation. The dataset consists of observation data which had been measured for 40 years from AWS in Seoul. Experiments were based on 8-fold cross validation. As a result of estimation, we got probabilities of test dataset, so threshold was used for the decision of precipitation. CSI and Bias were used for indicating the precision of precipitation. Our experimental results showed that DBN performed better than MLP.Deep learning, Deep belief network, Precipitation, Forecast"
614,딥러닝 신경망모형을 이용한 실시간 교통정보수집 연구,김대현 /KIM DAE HYON,2016,사단법인 인문사회과학기술융합학회,"비디오 기반의 실시간 교통정보수집 시스템은 자율주행을 포함한 지능형교통체계의 주요 서브시스템중 하나이며, 우수한 예측(추정)력 때문에 핵심 알고리즘으로 인공신경망 모형이 주로 사용되고 있다. 특히, 최근 과학 학술지 네이처에 구글 딥마인드의 알파고가 공개되면서 딥러닝과 인공지능에 대한 전 세계적인 관심이 집중되고 있다. 최근까지 가장 널리 사용된 인공신경망모형은 역전파 모형이었으나 최근 컴퓨터 하드웨어 및 알고리즘의 새로운 개발과 더불어 딥러닝 알고리즘을 이용한 딥인공신경망 모형에 대한 관심이 높아지고 있다. 그러나 아직 딥러닝 인공신경망에 대한 연구는 초기단계에 있는 실정이며 적용분야도 극히 한정되어 있다. 본 연구에서는 실시간 도로교통정보 수집을 위한 영상처리분야에서 딥러닝 알고리즘을 활용하고 실험결과를 통하여 향후 교통정보수집‧분석 분야에서의 딥러닝 인공신경망 모형의 적용가능성을 살펴보고자 한다. 특히, 본 연구에서 도출한 딥러닝 인공신경망모형의 실험결과를 기존 역전파 모형에 의한 실험결과와 비교하여 모형의 우수성을 비교‧평가하고자 한다.Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Normally, artificial neural networks have been used in real-time applications for collecting traffic information because of their remarkable performance in terms of predictive accuracy and real-time computing. Especially, the recent publication of a paper in the journal Nature and the success of Google DeepMind’s world-beating Go program AlphaGo caused significant increases of the public interest in artificial intelligence. Even though various types of learning models have been proposed, the Backpropagation has been the most popular neural network model. However, the Deep learning neural network model has attracted much interest recently. In this study, the Deep leaning neural network will be applied in image processing techniques for the real-time traffic information collection to investigate the performance of Deep learning algorithm in traffic engineering fields. In addition, the experimental results from Deep learing algorithm will be compared with the results from Backpropagation model to evaluate and propose better performing model.Intelligent Transportation Systems (ITS), artificial neural networks, Deep learning, traffic information, image processing"
615,딥러닝의 모형과 응용사례,안성만 /SungMahn Ahn,2016,한국지능정보시스템학회,"딥러닝은 인공신경망(neural network)이라는 인공지능분야의 모형이 발전된 형태로서, 계층구조로 이루어진 인공신경망의 내부계층(hidden layer)이 여러 단계로 이루어진 구조이다. 딥러닝에서의 주요 모형은 합성곱신경망(convolutional neural network), 순환신경망(recurrent neural network), 그리고 심층신뢰신경망(deep belief network)의 세가지라고 할 수있다. 그 중에서 현재 흥미로운 연구가 많이 발표되어서 관심이 집중되고 있는 모형은 지도학습(supervised learning)모형인 처음 두 개의 모형이다. 따라서 본 논문에서는 지도학습모형의 가중치를 최적화하는 기본적인 방법인 오류역전파 알고리즘을 살펴본 뒤에 합성곱신경망과 순환신경망의 구조와 응용사례 등을 살펴보고자 한다. 본문에서 다루지 않은 모형인 심층신뢰신경망은 아직까지는 합성곱신경망 이나 순환신경망보다는 상대적으로 주목을 덜 받고 있다. 그러나 심층신뢰신경망은 CNN이나 RNN과는 달리 비지도학습(unsupervised learning)모형이며, 사람이나 동물은 관찰을 통해서 스스로 학습한다는 점에서 궁극적으로는 비지도학습모형이 더 많이 연구되어야 할 주제가 될 것이다.Deep learning model is a kind of neural networks that allows multiple hidden layers. There are various deep learning architectures such as convolutional neural networks, deep belief networks and recurrent neural networks. Those have been applied to fields like computer vision, automatic speech recognition, natural language processing, audio recognition and bioinformatics where they have been shown to produce state-of-the-art results on various tasks. Among those architectures, convolutional neural networks and recurrent neural networks are classified as the supervised learning model. And in recent years, those supervised learning models have gained more popularity than unsupervised learning models such as deep belief networks, because supervised learning models have shown fashionable applications in such fields mentioned above.
Deep learning models can be trained with backpropagation algorithm. Backpropagation is an abbreviation for “backward propagation of errors” and a common method of training artificial neural networks used in conjunction with an optimization method such as gradient descent. The method calculates the gradient of an error function with respect to all the weights in the network. The gradient is fed to the optimization method which in turn uses it to update the weights, in an attempt to minimize the error function.
Convolutional neural networks use a special architecture which is particularly well-adapted to classify images. Using this architecture makes convolutional networks fast to train. This, in turn, helps us train deep, muti-layer networks, which are very good at classifying images. These days, deep convolutional networks are used in most neural networks for image recognition. Convolutional neural networks use three basic ideas: local receptive fields, shared weights, and pooling. By local receptive fields, we mean that each neuron in the first(or any) hidden layer will be connected to a small region of the input(or previous layer’s) neurons. Shared weights mean that we’re going to use the same weights and bias for each of the local receptive field. This means that all the neurons in the hidden layer detect exactly the same feature, just at different locations in the input image. In addition to the convolutional layers just described, convolutional neural networks also contain pooling layers. Pooling layers are usually used immediately after convolutional layers. What the pooling layers do is to simplify the information in the output from the convolutional layer.
Recent convolutional network architectures have 10 to 20 hidden layers and billions of connections between units. Training deep learning networks has taken weeks several years ago, but thanks to progress in GPU and algorithm enhancement, training time has reduced to several hours.
Neural networks with time-varying behavior are known as recurrent neural networks or RNNs. A recurrent neural network is a class of artificial neural network where connections between units form a directed cycle. This creates an internal state of the network which allows it to exhibit dynamic temporal behavior. Unlike feedforward neural networks, RNNs can use their internal memory to process arbitrary sequences of inputs. Early RNN models turned out to be very difficult to train, harder even than deep feedforward networks. The reason is the unstable gradient problem such as vanishing gradient and exploding gradient. The gradient can get smaller and smaller as it is propagated back through layers. This makes learning in early layers extremely slow. The problem actually gets worse in RNNs, since gradients aren’t just propagated backward through layers, they’re propagated backward through time. If the network runs for a long time, that can make the gradient extremely unstable and hard to learn from. It has been possible to incorporate an idea known as long short-term memory units (LSTMs) into RNNs. LSTMs make it much easier to get good results when training RNNs, and many recent papers make use of LSTMs or related ideas.Deep learning, Convolutional neural networks, Recurrent neural networks, Error backpropagation algorithm"
616,딥러닝분석과 기술적 분석 지표를 이용한 한국 코스피주가지수 방향성 예측,이우식 /Lee Woosik,2017,한국데이터정보과학회,"2016년 3월 구글 (Google)의 바둑인공지능 알파고 (AlphaGo)가 이세돌 9단과의 바둑대결에서승리한 이후 다양한 분야에서 인공지능 사용에 대한 관심이 높아지고 있는 가운데 금융투자 분야에서도 인공지능과 투자자문 전문가의 합성어인 로보어드바이저 (Robo-Advisor)에 대한 관심이 높아지고 있다. 인공지능 (artificial intelligence)기반의 의사결정은 비용 절감은 물론 효과적인 의사결정을가능하게 한다는 점에서 큰 장점이 있다. 본 연구에서는 기술적 분석 (technical analysis) 지표와 딥러닝 (deep learning) 모형을 결합하여 한국 코스피 지수를 예측하는 모형을 개발하고 제시한 모형들의 예측력을 비교, 분석한다. 분석 결과 기술적 분석 지표에 딥러닝 알고리즘을 결합한 모형이 주가지수 방향성 예측 문제에 응용될 수 있음을 확인하였다. 향후 본 연구에서 제안된 기술적 분석 지표와딥러닝모형을 결합한 기법은 로보어드바이저서비스에 응용할 수 있는 일반화 가능성을 보여준다Since Google’s AlphaGo defeated a world champion of Go players in 2016, there have been many interests in the deep learning. In the financial sector, a Robo-Advisor using deep learning gains a significant attention, which builds and manages portfolios of financial instruments for investors.In this paper, we have proposed the a deep learning algorithm geared toward identification and forecast of the KOSPI index direction,and we also have compared the accuracy of the prediction.In an application of forecasting the financial market index direction, we have shown that the Robo-Advisor using deep learning has a significant effect on finance industry. The Robo-Advisor collects a massive data such as earnings statements, news reports and regulatory filings, analyzes those and recommends investors how to view market trends and identify the best time to purchase financial assets. On the other hand, the Robo-Advisor allows businesses to learn more about their customers, develop better marketing strategies, increase sales and decrease costs.Artificial intelligence, deep learning, FinTech, Robo-Advisor, technical analysis."
617,딥러닝의 효율적인 학습을 위한 학습데이터 선별 기법에 관한 연구,주경돈 /Gyoung-don Joo,2017,한국정보과학회,"빅데이터의 증가와 병렬 하드웨어 처리, 학습 알고리즘의 개선은 딥러닝의 발전을 가져왔으며 인공지능이 적용 가능한 영역을 획기적으로 확장시키고 있다. 하지만 이러한 대규모의 학습 데이터에 기반한 딥러닝은 학습시간이 증가되고, 더 나아가 학습 데이터의 구축을 위한 레이블링(labeling) 비용이 데이터 수에 비례하여 증가하게 된다. 따라서 본 논문에서는, 딥러닝의 정확도를 최대한 유지하면서 필요한 학습 데이터의 수를 줄일수 있도록, 전체 데이터에서 제한된 수의 학습 데이터의 후보를 선별하는 기법에 대하여 연구한다. 특히 기존의 능동 학습에 기반한 기법들이 학습 데이터의 수가 매우 적어질 경우 딥러닝의 정확도가 급격히 떨어지는 문제를 해결할 수 있는 새로운 기법을 제안하였으며, 제안된 기법으로 선별된 학습 데이터는 그 수의 변화에 대해 딥러닝이 일관되게 경쟁력있는 정확도를 가짐을 다양한 실험을 통하여 검증하였다.The increase of big data, parallel hardware processing and enhanced learning algorithms have led to the development of deep learning and are dramatically expanding the domain of applications based on artificial intelligence. However, such large-scale training data extends the learning time, and furthermore, the labeling cost for building the training set increases in proportion to the volume of data. Therefore, in this paper, we study how to select a limited number of candidates for training set from given data so as to reduce the number of training set while maintaining the accuracy of deep learning. Especially, we found that, with the training set selected by the previous work, the accuracy of deep learning drastically decreases when the number of training set is very small and so proposed new methods to resolve this fall-off in accuracy. Through experiments, we showed that deep learning consistently has the competitive accuracy with the training set selected by the proposed method regardless of the size of training set.Deep Learning, Train Set Selection, Active Learning, Gamma distribution"
618,딥러닝을 이용한 범용적 스테그아날리시스,김현재 /Hyunjae Kim,2017,한국정보과학회,"스테그아날리시스(Steganalysis)란 이미지 등 일반적인 자료에 암호화된 정보를 은닉하는 스테가노그래피(Steganography)에 대한 검출 및 분석 방법으로, 기계학습 기반 방법론을 포함한다. 기존 기계학습 기반 스테그아날리시스는 영상(Image)의 특징(Feature) 추출 및 모델링에 기반하며, 최근 딥러닝(Deep Learning)의 적용으로 검출 정확도가 큰 폭으로 향상되었다. 하지만 현존하는 스테그아날리시스 모델은 단일 스테가노그래피 기법에 대해 국한되어 있어 학습에 사용되지 않은 스테고(Stego) 이미지의 경우 검출이 불가능한 결정적 한계를 가진다. 본 연구에서는 다양한 스테가노그래피 기법으로 생성된 스테고이미지에 딥러닝을 적용하여 스테그아날리시스를 학습하는 범용적 모델을 제안한다. 다양한 실험을 통해 제안 기법의 효용성 및 가능성을 확인하고, 범용적 스테그아날리시스 모델이 각각에 특화된 검출 기법과 유사한 정확도로 스테고 이미지를 검출할 수 있음을 보인다.Steganalysis is to detect information hidden by steganography inside general data such as images. There are stegoanalysis techniques that use machine learning (ML). Existing ML approaches to steganalysis are based on extracting features from stego images and modeling them.
Recently deep learning-based methodologies have shown significant improvements in detection accuracy. However, all the existing methods, including deep learning-based ones, have a critical limitation in that they can only detect stego images that are created by a specific steganography method. In this paper, we propose a generalized steganalysis method that can model multiple types of stego images using deep learning. Through various experiments, we confirm the effectiveness of our approach and envision directions for future research. In particular, we show that our method can detect each type of steganography with the same level of accuracy as that of a steganalysis method dedicated to that type of steganography, thereby demonstrating the general applicability of our approach to multiple types of stego images.steganalysis, steganography, deep learning, generalized steganalysis model"
619,"딥러닝 프레임워크의 비교: 티아노, 텐서플로, CNTK를 중심으로",정여진 /Yeojin Chung,2017,한국지능정보시스템학회,"딥러닝 프레임워크의 대표적인 기능으로는 ‘자동미분’과 ‘GPU의 활용’ 등을 들 수 있다. 본 논문은 파이썬의 라이브러리 형태로 사용 가능한 프레임워크 중에서 구글의 텐서플로와 마이크로소프트의 CNTK, 그리고 텐서 플로의 원조라고 할 수 있는 티아노를 비교하였다. 본문에서는 자동미분의 개념과 GPU의 활용형태를 간단히 설명하고, 그 다음에 logistic regression을 실행하는 예를 통하여 각 프레임워크의 문법을 알아본 뒤에, 마지막으로 대표적인 딥러닝 응용인 CNN의 예제를 실행시켜보고 코딩의 편의성과 실행속도 등을 확인해 보았다. 그 결과, 편의성의 관점에서 보면 티아노가 가장 코딩 하기가 어렵고, CNTK와 텐서플로는 많은 부분이 비슷하게 추상화 되어 있어서 코딩이 비슷하지만 가중치와 편향을 직접 정의하느냐의 여부에서 차이를 보였다. 그리고 각프레임워크의 실행속도에 대한 평가는 ‘큰 차이는 없다’는 것이다. 텐서플로는 티아노에 비하여 속도가 느리다는 평가가 있어왔는데, 본 연구의 실험에 의하면, 비록 CNN 모형에 국한되었지만, 텐서플로가 아주 조금이지만빠른 것으로 나타났다. CNTK의 경우에도, 비록 실험환경이 달랐지만, 실험환경의 차이에 의한 속도의 차이의 편차범위 이내에 있는 것으로 판단이 되었다. 본 연구에서는 세 종류의 딥러닝 프레임워크만을 살펴보았는데,위키피디아에 따르면 딥러닝 프레임워크의 종류는 12가지가 있으며, 각 프레임워크의 특징을 15가지 속성으로구분하여 차이를 특정하고 있다. 그 많은 속성 중에서 사용자의 입장에서 볼 때 중요한 속성은 어떤 언어(파이썬, C++, Java, 등)로 사용가능한지, 어떤 딥러닝 모형에 대한 라이브러리가 잘 구현되어 있는지 등일 것이다. 그리고 사용자가 대규모의 딥러닝 모형을 구축한다면, 다중 GPU 혹은 다중 서버를 지원하는지의 여부도 중요할 것이다. 또한 딥러닝 모형을 처음 학습하는 경우에는 사용설명서가 많은지 예제 프로그램이 많은지 여부도 중요한 기준이 될 것이다.The deep learning framework is software designed to help develop deep learning models. Some of its important functions include “automatic differentiation” and “utilization of GPU”. The list of popular deep learning framework includes Caffe (BVLC) and Theano (University of Montreal). And recently, Microsoft's deep learning framework, Microsoft Cognitive Toolkit, was released as open-source license, following Google’s Tensorflow a year earlier. The early deep learning frameworks have been developed mainly for research at universities. Beginning with the inception of Tensorflow, however, it seems that companies such as Microsoft and Facebook have started to join the competition of framework development. Given the trend, Google and other companies are expected to continue investing in the deep learning framework to bring forward the initiative in the artificial intelligence business. From this point of view, we think it is a good time to compare some of deep learning frameworks. So we compare three deep learning frameworks which can be used as a Python library. Those are Google's Tensorflow, Microsoft’s CNTK, and Theano which is sort of a predecessor of the preceding two.
The most common and important function of deep learning frameworks is the ability to perform automatic differentiation. Basically all the mathematical expressions of deep learning models can be represented as computational graphs, which consist of nodes and edges. Partial derivatives on each edge of a computational graph can then be obtained. With the partial derivatives, we can let software compute differentiation of any node with respect to any variable by utilizing chain rule of Calculus.
First of all, the convenience of coding is in the order of CNTK, Tensorflow, and Theano. The criterion is simply based on the lengths of the codes and the learning curve and the ease of coding are not the main concern. According to the criteria, Theano was the most difficult to implement with, and CNTK and Tensorflow were somewhat easier. With Tensorflow, we need to define weight variables and biases explicitly. The reason that CNTK and Tensorflow are easier to implement with is that those frameworks provide us with more abstraction than Theano. We, however, need to mention that low-level coding is not always bad. It gives us flexibility of coding. With the low-level coding such as in Theano, we can implement and test any new deep learning models or any new search methods that we can think of.
The assessment of the execution speed of each framework is that there is not meaningful difference. According to the experiment, execution speeds of Theano and Tensorflow are very similar, although the experiment was limited to a CNN model. In the case of CNTK, the experimental environment was not maintained as the same. The code written in CNTK has to be run in PC environment without GPU where codes execute as much as 50 times slower than with GPU. But we concluded that the difference of execution speed was within the range of variation caused by the different hardware setup.
In this study, we compared three types of deep learning framework: Theano, Tensorflow, and CNTK. According to Wikipedia, there are 12 available deep learning frameworks. And 15 different attributes differentiate each framework. Some of the important attributes would include interface language (Python, C ++, Java, etc.) and the availability of libraries on various deep learning models such as CNN, RNN, DBN, and etc. And if a user implements a large scale deep learning model, it will also be important to support multiple GPU or multiple servers. Also, if you are learning the deep learning model, it would also be important if there are enough examples and references.deep learning framework, Theano, TensorFlow, CNTK, computational graph, CIFAR-10"
620,딥러닝을 이용한 주변 무선단말 파악방안,이웅섭 /Lee， Woong-sup,2018,한국정보통신학회,"최근 단말-대-단말(Device-to-device, D2D) 통신기술이 차세대 무선통신시스템의 핵심기술로 큰 관심을 받고 있다. 이러한 단말간 통신에서는 자신의 주변에 어떠한 단말이 있는지 파악하는 주변단말 탐색(Neighbor discovery)이매우 중요하다. 본 논문에서는 최근 큰 관심을 받고 있는 딥러닝(Deep learning) 기술을 활용하여 단말간 통신에서 주변단말을 파악하는 방안에 대해서 제안한다. 제안 방안은 기존의 방안과 달리 무선채널의 공간적 연관성을 이용하여단말간의 신호 전송 없이 단말이 기지국으로 전송하는 상향링크 파일럿 신호를 기반으로 주변 단말을 찾고 따라서기존의 방식에 비해 신호전송 복잡도(signaling complexity)를 크게 줄일 수 있다. 또한 제안 방안에서는 떨어져 있는거리에 따라서 주변 단말을 분류 가능하여 기존 방안에 비해서 좀 더 세밀한 단말 탐색이 가능하다. 마지막으로 본 논문에서는 tensorflow를 이용한 컴퓨터 시뮬레이션을 통해 제안 방안의 성능을 검증하였다.Recently, the device-to-device (D2D) communication has been conceived as the key technology for the next-generationmobile communication systems. The neighbor discovery in which the nearby users are found, is essential for the properoperation of the D2D communication. In this paper, we propose new neighbor discovery scheme based on deep learningtechnology which has gained a lot of attention recently. In the proposed scheme, the neighboring users can be foundusing the uplink pilot transmission of users only, unlike conventional neighbor discovery schemes in which direct pilotcommunication among users is required, such that the signaling overhead can be greatly reduced in our proposed scheme.
Moreover, the neighbors with different proximity can also be classified accordingly which enables more accurate neighbordiscovery compared to the conventional schemes. The performance of our proposed scheme is verified through thetensorflow-based computer simulations.
키워드 : 딥러닝, 주변단말 탐색, 단말-대-단말 통신Deep learning, Neighbor discovery, Device-to-device communication, Wireless channel correlation"
621,신뢰성있는 딥러닝 기반 분석 모델을 참조하기 위한 딥러닝 기술 언어,문종혁 /Jong Hyeok Mun,2021,한국정보처리학회,"최근 딥러닝은 하드웨어 성능이 향상됨에 따라 자연어 처리, 영상 인식 등의 다양한 기술에 접목되어 활용되고 있다. 이러한 기술들을 활용해 지능형교통 시스템(ITS), 스마트홈, 헬스케어 등의 산업분야에서 데이터를 분석하여 고속도로 속도위반 차량 검출, 에너지 사용량 제어, 응급상황 등과 같은고품질의 서비스를 제공하며, 고품질의 서비스를 제공하기 위해서는 정확도가 향상된 딥러닝 모델이 적용되어야 한다. 이를 위해 서비스 환경의 데이터를분석하기 위한 딥러닝 모델을 개발할 때, 개발자는 신뢰성이 검증된 최신의 딥러닝 모델을 적용할 수 있어야 한다. 이는 개발자가 참조하는 딥러닝 모델에적용된 학습 데이터셋의 정확도를 측정하여 검증할 수 있다. 이러한 검증을 위해서 개발자는 학습 데이터셋, 딥러닝의 계층구조 및 개발 환경 등과 같은내용을 포함하는 딥러닝 모델을 문서화하여 적용하기 위한 구조적인 정보가 필요하다. 본 논문에서는 신뢰성있는 딥러닝 기반 데이터 분석 모델을 참조하기위한 딥러닝 기술 언어를 제안한다. 제안하는 기술 언어는 신뢰성 있는 딥러닝 모델을 개발하는데 필요한 학습데이터셋, 개발 환경 및 설정 등의 정보와더불어 딥러닝 모델의 계층구조를 표현할 수 있다. 제안하는 딥러닝 기술 언어를 이용하여 개발자는 지능형 교통 시스템에서 참조하는 분석 모델의정확도를 검증할 수 있다. 실험에서는 제안하는 언어의 유효성을 검증하기 위해, 번호판 인식 모델을 중심으로 딥러닝 기술 문서의 적용과정을 보인다With the recent advancements of deep learning, companies such as smart home, healthcare, and intelligent transportation systems areutilizing its functionality to provide high-quality services for vehicle detection, emergency situation detection, and controlling energyconsumption. To provide reliable services in such sensitive systems, deep learning models are required to have high accuracy. In orderto develop a deep learning model for analyzing previously mentioned services, developers should utilize the state of the art deep learningmodels that have already been verified for higher accuracy. The developers can verify the accuracy of the referenced model by validatingthe model on the dataset. For this validation, the developer needs structural information to document and apply deep learning models,including metadata such as learning dataset, network architecture, and development environments. In this paper, we propose a descriptionlanguage that represents the network architecture of the deep learning model along with its metadata that are necessary to develop adeep learning model. Through the proposed description language, developers can easily verify the accuracy of the referenced deep learningmodel. Our experiments demonstrate the application scenario of a deep learning description document that focuses on the license platerecognition for the detection of illegally parked vehicles.Trusted Deep Learning, Model Reference, Deep Learning Description Language, Traffic Situation Analysis Model"
622,딥러닝 분석을 이용한 중국 역내·외 위안화 변동성 예측,이우식 /Lee Woosik,2016,한국데이터정보과학회,"2008년 글로벌 금융위기 이후 중국은 위안화 국제화의 점진적 추진을 진행하면서 중국상하이 외환시장과 중국홍콩 외환시장에서 거래되는 통화인 역내위안화와 역외위안화를 형성시켰다. 본 연구는 위안화 국제화와 점진적인 중국 자본계정 개방에 따라 급변하는 외환시장상황의 변동성을 정확하게 파악하기 위해서 GARCH모형 (일반화된 자기회귀 조건부이분산성모형)에 다단계인공신경망을 결합한 MLP-GARCH 모형과 GARCH모형과 기계학습의 일종인 딥러닝 (deep learning)을 통합한 DL-GARCH을 가지고 위안화 변동성예측을 비교 실험과 분석을 하였다. 비교분석 결과 DLGARCH 모형은 MLP-GARCH보다 모형 위안화 역내·외 환율변동성 예측 면에서 더욱 더 개선된예측값을 제공하였다. 그래서 이분산시계열모형을 딥러닝과 결합한 DL-GARCH 모형은 시계열의환율 변동성 예측 문제에 딥러닝을 응용할 수 있음을 확인하였다. 향후 이분산시계열과 결합된 딥러닝 모형은 다른 금융시계열 데이터에 응용하여 그 일반화 가능성을 높일 수 있을 것이다.The People’s Republic of China has vigorously been pursuing the internationalization of the Chinese Yuan or Renminbi after the financial crisis of 2008. In this view, an abrupt increase of use of the Chinese Yuan in the onshore and offshore markets are important milestones to be one of important currencies. One of the most frequently used methods to forecast volatility is GARCH model. Since a prediction error of the GARCH model has been reported quite high, a lot of efforts have been made to improve forecasting capability of the GARCH model. In this paper, we have proposed MLPGARCH and a DL-GARCH by employing Artificial Neural Network to the GARCH. In an application to forecasting Chinese Yuan volatility, we have successfully shown their overall outperformance in forecasting over the GARCH.Deep learning, DL-GARCH, GARCH, The Chinese Yuan, volatility."
623,딥러닝 기법을 이용한 내일강수 예측,하지훈,2016,한국지능시스템학회,"정확한 강수예측을 위해서는 예측인자 선정과 예측방법에 대한 선택이 매우 중요하다. 최근에는 강수예측 방법으로기계학습 기법이 많이 사용되고 있으며, 그 중에서도 특히 인공신경망을 사용한 강수예측 방법은 좋은 성능을 보였다. 본논문에서는 딥러닝 기법 중 하나인 DBN(deep belief network)를 이용한 새로운 강수예측 방법을 제안한다. DBN는 비지도사전 학습을 통해 초기 가중치를 설정하여 기존 인공신경망의 문제점을 보완한다. 예측인자로는 기온, 전일-전주 강수일, 태양과 달 궤도 관련 자료를 선정하였다. 기온과 전일-전주 강수일은 서울에서의 1974년부터 2013년까지 총 40년간의AWS(automatic weather system) 관측 자료를 사용하였고, 태양과 달의 궤도 관련 자료는 서울을 중심으로 계산한 결과를사용하였다. 전체 기간에서 일부는 학습 자료로 사용하여 예측모델을 생성하였고, 나머지를 생성한 모델의 검증 자료로사용하였다. 모델 검증 결과로 나온 예측값들은 확률값을 가지며 임계치를 이용하여 강수유무를 판별하였다. 강수 정확도의척도로 양분예보기법 중 CSI(critical successive index)와 Bias(frequency bias)를 계산하였다. 이를 통해 DBN와 MLP(multilayer perceptron)의 성능을 비교한 결과 DBN의 강수 예측 정확도가 높았고, 수행속도 또한 2배 이상 빨랐다.For accurate precipitation forecasts the choice of weather faoctrs and prediction method is very important. Recently, machinele arning has been widely used for forecasting precipitation, and artificial neural network, one of machine learning techniques, showed good performance. In this paper, we suggest a new method for forecasting precipitation using DBN, one of deep learning techniques.
DBN has an advantage that initial weights are set by unsupervised learning, so this compensates for the defects of artificial neural networks. We used past precipitation, temperature, and the parameters of the sun and moon's motion as features for forecasting precipitation. The dataset consists of observation data which had been measured for 40 years from AWS in Seoul. Experiments were based on 8-fold cross validation. As a result of estimation, we got probabilities of test dataset, so threshold was used for the decision of precipitation. CSI and Bias were used for indicating the precision of precipitation. Our experimental results showed that DBN performed better than MLP.Deep learning, Deep belief network, Precipitation, Forecast"
624,딥러닝 신경망모형을 이용한 실시간 교통정보수집 연구,김대현 /KIM DAE HYON,2016,사단법인 인문사회과학기술융합학회,"비디오 기반의 실시간 교통정보수집 시스템은 자율주행을 포함한 지능형교통체계의 주요 서브시스템중 하나이며, 우수한 예측(추정)력 때문에 핵심 알고리즘으로 인공신경망 모형이 주로 사용되고 있다. 특히, 최근 과학 학술지 네이처에 구글 딥마인드의 알파고가 공개되면서 딥러닝과 인공지능에 대한 전 세계적인 관심이 집중되고 있다. 최근까지 가장 널리 사용된 인공신경망모형은 역전파 모형이었으나 최근 컴퓨터 하드웨어 및 알고리즘의 새로운 개발과 더불어 딥러닝 알고리즘을 이용한 딥인공신경망 모형에 대한 관심이 높아지고 있다. 그러나 아직 딥러닝 인공신경망에 대한 연구는 초기단계에 있는 실정이며 적용분야도 극히 한정되어 있다. 본 연구에서는 실시간 도로교통정보 수집을 위한 영상처리분야에서 딥러닝 알고리즘을 활용하고 실험결과를 통하여 향후 교통정보수집‧분석 분야에서의 딥러닝 인공신경망 모형의 적용가능성을 살펴보고자 한다. 특히, 본 연구에서 도출한 딥러닝 인공신경망모형의 실험결과를 기존 역전파 모형에 의한 실험결과와 비교하여 모형의 우수성을 비교‧평가하고자 한다.Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Normally, artificial neural networks have been used in real-time applications for collecting traffic information because of their remarkable performance in terms of predictive accuracy and real-time computing. Especially, the recent publication of a paper in the journal Nature and the success of Google DeepMind’s world-beating Go program AlphaGo caused significant increases of the public interest in artificial intelligence. Even though various types of learning models have been proposed, the Backpropagation has been the most popular neural network model. However, the Deep learning neural network model has attracted much interest recently. In this study, the Deep leaning neural network will be applied in image processing techniques for the real-time traffic information collection to investigate the performance of Deep learning algorithm in traffic engineering fields. In addition, the experimental results from Deep learing algorithm will be compared with the results from Backpropagation model to evaluate and propose better performing model.Intelligent Transportation Systems (ITS), artificial neural networks, Deep learning, traffic information, image processing"
625,딥러닝의 모형과 응용사례,안성만 /SungMahn Ahn,2016,한국지능정보시스템학회,"딥러닝은 인공신경망(neural network)이라는 인공지능분야의 모형이 발전된 형태로서, 계층구조로 이루어진 인공신경망의 내부계층(hidden layer)이 여러 단계로 이루어진 구조이다. 딥러닝에서의 주요 모형은 합성곱신경망(convolutional neural network), 순환신경망(recurrent neural network), 그리고 심층신뢰신경망(deep belief network)의 세가지라고 할 수있다. 그 중에서 현재 흥미로운 연구가 많이 발표되어서 관심이 집중되고 있는 모형은 지도학습(supervised learning)모형인 처음 두 개의 모형이다. 따라서 본 논문에서는 지도학습모형의 가중치를 최적화하는 기본적인 방법인 오류역전파 알고리즘을 살펴본 뒤에 합성곱신경망과 순환신경망의 구조와 응용사례 등을 살펴보고자 한다. 본문에서 다루지 않은 모형인 심층신뢰신경망은 아직까지는 합성곱신경망 이나 순환신경망보다는 상대적으로 주목을 덜 받고 있다. 그러나 심층신뢰신경망은 CNN이나 RNN과는 달리 비지도학습(unsupervised learning)모형이며, 사람이나 동물은 관찰을 통해서 스스로 학습한다는 점에서 궁극적으로는 비지도학습모형이 더 많이 연구되어야 할 주제가 될 것이다.Deep learning model is a kind of neural networks that allows multiple hidden layers. There are various deep learning architectures such as convolutional neural networks, deep belief networks and recurrent neural networks. Those have been applied to fields like computer vision, automatic speech recognition, natural language processing, audio recognition and bioinformatics where they have been shown to produce state-of-the-art results on various tasks. Among those architectures, convolutional neural networks and recurrent neural networks are classified as the supervised learning model. And in recent years, those supervised learning models have gained more popularity than unsupervised learning models such as deep belief networks, because supervised learning models have shown fashionable applications in such fields mentioned above.
Deep learning models can be trained with backpropagation algorithm. Backpropagation is an abbreviation for “backward propagation of errors” and a common method of training artificial neural networks used in conjunction with an optimization method such as gradient descent. The method calculates the gradient of an error function with respect to all the weights in the network. The gradient is fed to the optimization method which in turn uses it to update the weights, in an attempt to minimize the error function.
Convolutional neural networks use a special architecture which is particularly well-adapted to classify images. Using this architecture makes convolutional networks fast to train. This, in turn, helps us train deep, muti-layer networks, which are very good at classifying images. These days, deep convolutional networks are used in most neural networks for image recognition. Convolutional neural networks use three basic ideas: local receptive fields, shared weights, and pooling. By local receptive fields, we mean that each neuron in the first(or any) hidden layer will be connected to a small region of the input(or previous layer’s) neurons. Shared weights mean that we’re going to use the same weights and bias for each of the local receptive field. This means that all the neurons in the hidden layer detect exactly the same feature, just at different locations in the input image. In addition to the convolutional layers just described, convolutional neural networks also contain pooling layers. Pooling layers are usually used immediately after convolutional layers. What the pooling layers do is to simplify the information in the output from the convolutional layer.
Recent convolutional network architectures have 10 to 20 hidden layers and billions of connections between units. Training deep learning networks has taken weeks several years ago, but thanks to progress in GPU and algorithm enhancement, training time has reduced to several hours.
Neural networks with time-varying behavior are known as recurrent neural networks or RNNs. A recurrent neural network is a class of artificial neural network where connections between units form a directed cycle. This creates an internal state of the network which allows it to exhibit dynamic temporal behavior. Unlike feedforward neural networks, RNNs can use their internal memory to process arbitrary sequences of inputs. Early RNN models turned out to be very difficult to train, harder even than deep feedforward networks. The reason is the unstable gradient problem such as vanishing gradient and exploding gradient. The gradient can get smaller and smaller as it is propagated back through layers. This makes learning in early layers extremely slow. The problem actually gets worse in RNNs, since gradients aren’t just propagated backward through layers, they’re propagated backward through time. If the network runs for a long time, that can make the gradient extremely unstable and hard to learn from. It has been possible to incorporate an idea known as long short-term memory units (LSTMs) into RNNs. LSTMs make it much easier to get good results when training RNNs, and many recent papers make use of LSTMs or related ideas.Deep learning, Convolutional neural networks, Recurrent neural networks, Error backpropagation algorithm"
626,딥러닝분석과 기술적 분석 지표를 이용한 한국 코스피주가지수 방향성 예측,이우식 /Lee Woosik,2017,한국데이터정보과학회,"2016년 3월 구글 (Google)의 바둑인공지능 알파고 (AlphaGo)가 이세돌 9단과의 바둑대결에서승리한 이후 다양한 분야에서 인공지능 사용에 대한 관심이 높아지고 있는 가운데 금융투자 분야에서도 인공지능과 투자자문 전문가의 합성어인 로보어드바이저 (Robo-Advisor)에 대한 관심이 높아지고 있다. 인공지능 (artificial intelligence)기반의 의사결정은 비용 절감은 물론 효과적인 의사결정을가능하게 한다는 점에서 큰 장점이 있다. 본 연구에서는 기술적 분석 (technical analysis) 지표와 딥러닝 (deep learning) 모형을 결합하여 한국 코스피 지수를 예측하는 모형을 개발하고 제시한 모형들의 예측력을 비교, 분석한다. 분석 결과 기술적 분석 지표에 딥러닝 알고리즘을 결합한 모형이 주가지수 방향성 예측 문제에 응용될 수 있음을 확인하였다. 향후 본 연구에서 제안된 기술적 분석 지표와딥러닝모형을 결합한 기법은 로보어드바이저서비스에 응용할 수 있는 일반화 가능성을 보여준다Since Google’s AlphaGo defeated a world champion of Go players in 2016, there have been many interests in the deep learning. In the financial sector, a Robo-Advisor using deep learning gains a significant attention, which builds and manages portfolios of financial instruments for investors.In this paper, we have proposed the a deep learning algorithm geared toward identification and forecast of the KOSPI index direction,and we also have compared the accuracy of the prediction.In an application of forecasting the financial market index direction, we have shown that the Robo-Advisor using deep learning has a significant effect on finance industry. The Robo-Advisor collects a massive data such as earnings statements, news reports and regulatory filings, analyzes those and recommends investors how to view market trends and identify the best time to purchase financial assets. On the other hand, the Robo-Advisor allows businesses to learn more about their customers, develop better marketing strategies, increase sales and decrease costs.Artificial intelligence, deep learning, FinTech, Robo-Advisor, technical analysis."
627,딥러닝의 효율적인 학습을 위한 학습데이터 선별 기법에 관한 연구,주경돈 /Gyoung-don Joo,2017,한국정보과학회,"빅데이터의 증가와 병렬 하드웨어 처리, 학습 알고리즘의 개선은 딥러닝의 발전을 가져왔으며 인공지능이 적용 가능한 영역을 획기적으로 확장시키고 있다. 하지만 이러한 대규모의 학습 데이터에 기반한 딥러닝은 학습시간이 증가되고, 더 나아가 학습 데이터의 구축을 위한 레이블링(labeling) 비용이 데이터 수에 비례하여 증가하게 된다. 따라서 본 논문에서는, 딥러닝의 정확도를 최대한 유지하면서 필요한 학습 데이터의 수를 줄일수 있도록, 전체 데이터에서 제한된 수의 학습 데이터의 후보를 선별하는 기법에 대하여 연구한다. 특히 기존의 능동 학습에 기반한 기법들이 학습 데이터의 수가 매우 적어질 경우 딥러닝의 정확도가 급격히 떨어지는 문제를 해결할 수 있는 새로운 기법을 제안하였으며, 제안된 기법으로 선별된 학습 데이터는 그 수의 변화에 대해 딥러닝이 일관되게 경쟁력있는 정확도를 가짐을 다양한 실험을 통하여 검증하였다.The increase of big data, parallel hardware processing and enhanced learning algorithms have led to the development of deep learning and are dramatically expanding the domain of applications based on artificial intelligence. However, such large-scale training data extends the learning time, and furthermore, the labeling cost for building the training set increases in proportion to the volume of data. Therefore, in this paper, we study how to select a limited number of candidates for training set from given data so as to reduce the number of training set while maintaining the accuracy of deep learning. Especially, we found that, with the training set selected by the previous work, the accuracy of deep learning drastically decreases when the number of training set is very small and so proposed new methods to resolve this fall-off in accuracy. Through experiments, we showed that deep learning consistently has the competitive accuracy with the training set selected by the proposed method regardless of the size of training set.Deep Learning, Train Set Selection, Active Learning, Gamma distribution"
628,딥러닝을 이용한 범용적 스테그아날리시스,김현재 /Hyunjae Kim,2017,한국정보과학회,"스테그아날리시스(Steganalysis)란 이미지 등 일반적인 자료에 암호화된 정보를 은닉하는 스테가노그래피(Steganography)에 대한 검출 및 분석 방법으로, 기계학습 기반 방법론을 포함한다. 기존 기계학습 기반 스테그아날리시스는 영상(Image)의 특징(Feature) 추출 및 모델링에 기반하며, 최근 딥러닝(Deep Learning)의 적용으로 검출 정확도가 큰 폭으로 향상되었다. 하지만 현존하는 스테그아날리시스 모델은 단일 스테가노그래피 기법에 대해 국한되어 있어 학습에 사용되지 않은 스테고(Stego) 이미지의 경우 검출이 불가능한 결정적 한계를 가진다. 본 연구에서는 다양한 스테가노그래피 기법으로 생성된 스테고이미지에 딥러닝을 적용하여 스테그아날리시스를 학습하는 범용적 모델을 제안한다. 다양한 실험을 통해 제안 기법의 효용성 및 가능성을 확인하고, 범용적 스테그아날리시스 모델이 각각에 특화된 검출 기법과 유사한 정확도로 스테고 이미지를 검출할 수 있음을 보인다.Steganalysis is to detect information hidden by steganography inside general data such as images. There are stegoanalysis techniques that use machine learning (ML). Existing ML approaches to steganalysis are based on extracting features from stego images and modeling them.
Recently deep learning-based methodologies have shown significant improvements in detection accuracy. However, all the existing methods, including deep learning-based ones, have a critical limitation in that they can only detect stego images that are created by a specific steganography method. In this paper, we propose a generalized steganalysis method that can model multiple types of stego images using deep learning. Through various experiments, we confirm the effectiveness of our approach and envision directions for future research. In particular, we show that our method can detect each type of steganography with the same level of accuracy as that of a steganalysis method dedicated to that type of steganography, thereby demonstrating the general applicability of our approach to multiple types of stego images.steganalysis, steganography, deep learning, generalized steganalysis model"
629,"딥러닝 프레임워크의 비교: 티아노, 텐서플로, CNTK를 중심으로",정여진 /Yeojin Chung,2017,한국지능정보시스템학회,"딥러닝 프레임워크의 대표적인 기능으로는 ‘자동미분’과 ‘GPU의 활용’ 등을 들 수 있다. 본 논문은 파이썬의 라이브러리 형태로 사용 가능한 프레임워크 중에서 구글의 텐서플로와 마이크로소프트의 CNTK, 그리고 텐서 플로의 원조라고 할 수 있는 티아노를 비교하였다. 본문에서는 자동미분의 개념과 GPU의 활용형태를 간단히 설명하고, 그 다음에 logistic regression을 실행하는 예를 통하여 각 프레임워크의 문법을 알아본 뒤에, 마지막으로 대표적인 딥러닝 응용인 CNN의 예제를 실행시켜보고 코딩의 편의성과 실행속도 등을 확인해 보았다. 그 결과, 편의성의 관점에서 보면 티아노가 가장 코딩 하기가 어렵고, CNTK와 텐서플로는 많은 부분이 비슷하게 추상화 되어 있어서 코딩이 비슷하지만 가중치와 편향을 직접 정의하느냐의 여부에서 차이를 보였다. 그리고 각프레임워크의 실행속도에 대한 평가는 ‘큰 차이는 없다’는 것이다. 텐서플로는 티아노에 비하여 속도가 느리다는 평가가 있어왔는데, 본 연구의 실험에 의하면, 비록 CNN 모형에 국한되었지만, 텐서플로가 아주 조금이지만빠른 것으로 나타났다. CNTK의 경우에도, 비록 실험환경이 달랐지만, 실험환경의 차이에 의한 속도의 차이의 편차범위 이내에 있는 것으로 판단이 되었다. 본 연구에서는 세 종류의 딥러닝 프레임워크만을 살펴보았는데,위키피디아에 따르면 딥러닝 프레임워크의 종류는 12가지가 있으며, 각 프레임워크의 특징을 15가지 속성으로구분하여 차이를 특정하고 있다. 그 많은 속성 중에서 사용자의 입장에서 볼 때 중요한 속성은 어떤 언어(파이썬, C++, Java, 등)로 사용가능한지, 어떤 딥러닝 모형에 대한 라이브러리가 잘 구현되어 있는지 등일 것이다. 그리고 사용자가 대규모의 딥러닝 모형을 구축한다면, 다중 GPU 혹은 다중 서버를 지원하는지의 여부도 중요할 것이다. 또한 딥러닝 모형을 처음 학습하는 경우에는 사용설명서가 많은지 예제 프로그램이 많은지 여부도 중요한 기준이 될 것이다.The deep learning framework is software designed to help develop deep learning models. Some of its important functions include “automatic differentiation” and “utilization of GPU”. The list of popular deep learning framework includes Caffe (BVLC) and Theano (University of Montreal). And recently, Microsoft's deep learning framework, Microsoft Cognitive Toolkit, was released as open-source license, following Google’s Tensorflow a year earlier. The early deep learning frameworks have been developed mainly for research at universities. Beginning with the inception of Tensorflow, however, it seems that companies such as Microsoft and Facebook have started to join the competition of framework development. Given the trend, Google and other companies are expected to continue investing in the deep learning framework to bring forward the initiative in the artificial intelligence business. From this point of view, we think it is a good time to compare some of deep learning frameworks. So we compare three deep learning frameworks which can be used as a Python library. Those are Google's Tensorflow, Microsoft’s CNTK, and Theano which is sort of a predecessor of the preceding two.
The most common and important function of deep learning frameworks is the ability to perform automatic differentiation. Basically all the mathematical expressions of deep learning models can be represented as computational graphs, which consist of nodes and edges. Partial derivatives on each edge of a computational graph can then be obtained. With the partial derivatives, we can let software compute differentiation of any node with respect to any variable by utilizing chain rule of Calculus.
First of all, the convenience of coding is in the order of CNTK, Tensorflow, and Theano. The criterion is simply based on the lengths of the codes and the learning curve and the ease of coding are not the main concern. According to the criteria, Theano was the most difficult to implement with, and CNTK and Tensorflow were somewhat easier. With Tensorflow, we need to define weight variables and biases explicitly. The reason that CNTK and Tensorflow are easier to implement with is that those frameworks provide us with more abstraction than Theano. We, however, need to mention that low-level coding is not always bad. It gives us flexibility of coding. With the low-level coding such as in Theano, we can implement and test any new deep learning models or any new search methods that we can think of.
The assessment of the execution speed of each framework is that there is not meaningful difference. According to the experiment, execution speeds of Theano and Tensorflow are very similar, although the experiment was limited to a CNN model. In the case of CNTK, the experimental environment was not maintained as the same. The code written in CNTK has to be run in PC environment without GPU where codes execute as much as 50 times slower than with GPU. But we concluded that the difference of execution speed was within the range of variation caused by the different hardware setup.
In this study, we compared three types of deep learning framework: Theano, Tensorflow, and CNTK. According to Wikipedia, there are 12 available deep learning frameworks. And 15 different attributes differentiate each framework. Some of the important attributes would include interface language (Python, C ++, Java, etc.) and the availability of libraries on various deep learning models such as CNN, RNN, DBN, and etc. And if a user implements a large scale deep learning model, it will also be important to support multiple GPU or multiple servers. Also, if you are learning the deep learning model, it would also be important if there are enough examples and references.deep learning framework, Theano, TensorFlow, CNTK, computational graph, CIFAR-10"
630,딥러닝을 이용한 주변 무선단말 파악방안,이웅섭 /Lee， Woong-sup,2018,한국정보통신학회,"최근 단말-대-단말(Device-to-device, D2D) 통신기술이 차세대 무선통신시스템의 핵심기술로 큰 관심을 받고 있다. 이러한 단말간 통신에서는 자신의 주변에 어떠한 단말이 있는지 파악하는 주변단말 탐색(Neighbor discovery)이매우 중요하다. 본 논문에서는 최근 큰 관심을 받고 있는 딥러닝(Deep learning) 기술을 활용하여 단말간 통신에서 주변단말을 파악하는 방안에 대해서 제안한다. 제안 방안은 기존의 방안과 달리 무선채널의 공간적 연관성을 이용하여단말간의 신호 전송 없이 단말이 기지국으로 전송하는 상향링크 파일럿 신호를 기반으로 주변 단말을 찾고 따라서기존의 방식에 비해 신호전송 복잡도(signaling complexity)를 크게 줄일 수 있다. 또한 제안 방안에서는 떨어져 있는거리에 따라서 주변 단말을 분류 가능하여 기존 방안에 비해서 좀 더 세밀한 단말 탐색이 가능하다. 마지막으로 본 논문에서는 tensorflow를 이용한 컴퓨터 시뮬레이션을 통해 제안 방안의 성능을 검증하였다.Recently, the device-to-device (D2D) communication has been conceived as the key technology for the next-generationmobile communication systems. The neighbor discovery in which the nearby users are found, is essential for the properoperation of the D2D communication. In this paper, we propose new neighbor discovery scheme based on deep learningtechnology which has gained a lot of attention recently. In the proposed scheme, the neighboring users can be foundusing the uplink pilot transmission of users only, unlike conventional neighbor discovery schemes in which direct pilotcommunication among users is required, such that the signaling overhead can be greatly reduced in our proposed scheme.
Moreover, the neighbors with different proximity can also be classified accordingly which enables more accurate neighbordiscovery compared to the conventional schemes. The performance of our proposed scheme is verified through thetensorflow-based computer simulations.
키워드 : 딥러닝, 주변단말 탐색, 단말-대-단말 통신Deep learning, Neighbor discovery, Device-to-device communication, Wireless channel correlation"
631,신뢰성있는 딥러닝 기반 분석 모델을 참조하기 위한 딥러닝 기술 언어,문종혁 /Jong Hyeok Mun,2021,한국정보처리학회,"최근 딥러닝은 하드웨어 성능이 향상됨에 따라 자연어 처리, 영상 인식 등의 다양한 기술에 접목되어 활용되고 있다. 이러한 기술들을 활용해 지능형교통 시스템(ITS), 스마트홈, 헬스케어 등의 산업분야에서 데이터를 분석하여 고속도로 속도위반 차량 검출, 에너지 사용량 제어, 응급상황 등과 같은고품질의 서비스를 제공하며, 고품질의 서비스를 제공하기 위해서는 정확도가 향상된 딥러닝 모델이 적용되어야 한다. 이를 위해 서비스 환경의 데이터를분석하기 위한 딥러닝 모델을 개발할 때, 개발자는 신뢰성이 검증된 최신의 딥러닝 모델을 적용할 수 있어야 한다. 이는 개발자가 참조하는 딥러닝 모델에적용된 학습 데이터셋의 정확도를 측정하여 검증할 수 있다. 이러한 검증을 위해서 개발자는 학습 데이터셋, 딥러닝의 계층구조 및 개발 환경 등과 같은내용을 포함하는 딥러닝 모델을 문서화하여 적용하기 위한 구조적인 정보가 필요하다. 본 논문에서는 신뢰성있는 딥러닝 기반 데이터 분석 모델을 참조하기위한 딥러닝 기술 언어를 제안한다. 제안하는 기술 언어는 신뢰성 있는 딥러닝 모델을 개발하는데 필요한 학습데이터셋, 개발 환경 및 설정 등의 정보와더불어 딥러닝 모델의 계층구조를 표현할 수 있다. 제안하는 딥러닝 기술 언어를 이용하여 개발자는 지능형 교통 시스템에서 참조하는 분석 모델의정확도를 검증할 수 있다. 실험에서는 제안하는 언어의 유효성을 검증하기 위해, 번호판 인식 모델을 중심으로 딥러닝 기술 문서의 적용과정을 보인다With the recent advancements of deep learning, companies such as smart home, healthcare, and intelligent transportation systems areutilizing its functionality to provide high-quality services for vehicle detection, emergency situation detection, and controlling energyconsumption. To provide reliable services in such sensitive systems, deep learning models are required to have high accuracy. In orderto develop a deep learning model for analyzing previously mentioned services, developers should utilize the state of the art deep learningmodels that have already been verified for higher accuracy. The developers can verify the accuracy of the referenced model by validatingthe model on the dataset. For this validation, the developer needs structural information to document and apply deep learning models,including metadata such as learning dataset, network architecture, and development environments. In this paper, we propose a descriptionlanguage that represents the network architecture of the deep learning model along with its metadata that are necessary to develop adeep learning model. Through the proposed description language, developers can easily verify the accuracy of the referenced deep learningmodel. Our experiments demonstrate the application scenario of a deep learning description document that focuses on the license platerecognition for the detection of illegally parked vehicles.Trusted Deep Learning, Model Reference, Deep Learning Description Language, Traffic Situation Analysis Model"
632,딥러닝 분석을 이용한 중국 역내·외 위안화 변동성 예측,이우식 /Lee Woosik,2016,한국데이터정보과학회,"2008년 글로벌 금융위기 이후 중국은 위안화 국제화의 점진적 추진을 진행하면서 중국상하이 외환시장과 중국홍콩 외환시장에서 거래되는 통화인 역내위안화와 역외위안화를 형성시켰다. 본 연구는 위안화 국제화와 점진적인 중국 자본계정 개방에 따라 급변하는 외환시장상황의 변동성을 정확하게 파악하기 위해서 GARCH모형 (일반화된 자기회귀 조건부이분산성모형)에 다단계인공신경망을 결합한 MLP-GARCH 모형과 GARCH모형과 기계학습의 일종인 딥러닝 (deep learning)을 통합한 DL-GARCH을 가지고 위안화 변동성예측을 비교 실험과 분석을 하였다. 비교분석 결과 DLGARCH 모형은 MLP-GARCH보다 모형 위안화 역내·외 환율변동성 예측 면에서 더욱 더 개선된예측값을 제공하였다. 그래서 이분산시계열모형을 딥러닝과 결합한 DL-GARCH 모형은 시계열의환율 변동성 예측 문제에 딥러닝을 응용할 수 있음을 확인하였다. 향후 이분산시계열과 결합된 딥러닝 모형은 다른 금융시계열 데이터에 응용하여 그 일반화 가능성을 높일 수 있을 것이다.The People’s Republic of China has vigorously been pursuing the internationalization of the Chinese Yuan or Renminbi after the financial crisis of 2008. In this view, an abrupt increase of use of the Chinese Yuan in the onshore and offshore markets are important milestones to be one of important currencies. One of the most frequently used methods to forecast volatility is GARCH model. Since a prediction error of the GARCH model has been reported quite high, a lot of efforts have been made to improve forecasting capability of the GARCH model. In this paper, we have proposed MLPGARCH and a DL-GARCH by employing Artificial Neural Network to the GARCH. In an application to forecasting Chinese Yuan volatility, we have successfully shown their overall outperformance in forecasting over the GARCH.Deep learning, DL-GARCH, GARCH, The Chinese Yuan, volatility."
633,딥러닝 기법을 이용한 내일강수 예측,하지훈,2016,한국지능시스템학회,"정확한 강수예측을 위해서는 예측인자 선정과 예측방법에 대한 선택이 매우 중요하다. 최근에는 강수예측 방법으로기계학습 기법이 많이 사용되고 있으며, 그 중에서도 특히 인공신경망을 사용한 강수예측 방법은 좋은 성능을 보였다. 본논문에서는 딥러닝 기법 중 하나인 DBN(deep belief network)를 이용한 새로운 강수예측 방법을 제안한다. DBN는 비지도사전 학습을 통해 초기 가중치를 설정하여 기존 인공신경망의 문제점을 보완한다. 예측인자로는 기온, 전일-전주 강수일, 태양과 달 궤도 관련 자료를 선정하였다. 기온과 전일-전주 강수일은 서울에서의 1974년부터 2013년까지 총 40년간의AWS(automatic weather system) 관측 자료를 사용하였고, 태양과 달의 궤도 관련 자료는 서울을 중심으로 계산한 결과를사용하였다. 전체 기간에서 일부는 학습 자료로 사용하여 예측모델을 생성하였고, 나머지를 생성한 모델의 검증 자료로사용하였다. 모델 검증 결과로 나온 예측값들은 확률값을 가지며 임계치를 이용하여 강수유무를 판별하였다. 강수 정확도의척도로 양분예보기법 중 CSI(critical successive index)와 Bias(frequency bias)를 계산하였다. 이를 통해 DBN와 MLP(multilayer perceptron)의 성능을 비교한 결과 DBN의 강수 예측 정확도가 높았고, 수행속도 또한 2배 이상 빨랐다.For accurate precipitation forecasts the choice of weather faoctrs and prediction method is very important. Recently, machinele arning has been widely used for forecasting precipitation, and artificial neural network, one of machine learning techniques, showed good performance. In this paper, we suggest a new method for forecasting precipitation using DBN, one of deep learning techniques.
DBN has an advantage that initial weights are set by unsupervised learning, so this compensates for the defects of artificial neural networks. We used past precipitation, temperature, and the parameters of the sun and moon's motion as features for forecasting precipitation. The dataset consists of observation data which had been measured for 40 years from AWS in Seoul. Experiments were based on 8-fold cross validation. As a result of estimation, we got probabilities of test dataset, so threshold was used for the decision of precipitation. CSI and Bias were used for indicating the precision of precipitation. Our experimental results showed that DBN performed better than MLP.Deep learning, Deep belief network, Precipitation, Forecast"
634,딥러닝 신경망모형을 이용한 실시간 교통정보수집 연구,김대현 /KIM DAE HYON,2016,사단법인 인문사회과학기술융합학회,"비디오 기반의 실시간 교통정보수집 시스템은 자율주행을 포함한 지능형교통체계의 주요 서브시스템중 하나이며, 우수한 예측(추정)력 때문에 핵심 알고리즘으로 인공신경망 모형이 주로 사용되고 있다. 특히, 최근 과학 학술지 네이처에 구글 딥마인드의 알파고가 공개되면서 딥러닝과 인공지능에 대한 전 세계적인 관심이 집중되고 있다. 최근까지 가장 널리 사용된 인공신경망모형은 역전파 모형이었으나 최근 컴퓨터 하드웨어 및 알고리즘의 새로운 개발과 더불어 딥러닝 알고리즘을 이용한 딥인공신경망 모형에 대한 관심이 높아지고 있다. 그러나 아직 딥러닝 인공신경망에 대한 연구는 초기단계에 있는 실정이며 적용분야도 극히 한정되어 있다. 본 연구에서는 실시간 도로교통정보 수집을 위한 영상처리분야에서 딥러닝 알고리즘을 활용하고 실험결과를 통하여 향후 교통정보수집‧분석 분야에서의 딥러닝 인공신경망 모형의 적용가능성을 살펴보고자 한다. 특히, 본 연구에서 도출한 딥러닝 인공신경망모형의 실험결과를 기존 역전파 모형에 의한 실험결과와 비교하여 모형의 우수성을 비교‧평가하고자 한다.Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Normally, artificial neural networks have been used in real-time applications for collecting traffic information because of their remarkable performance in terms of predictive accuracy and real-time computing. Especially, the recent publication of a paper in the journal Nature and the success of Google DeepMind’s world-beating Go program AlphaGo caused significant increases of the public interest in artificial intelligence. Even though various types of learning models have been proposed, the Backpropagation has been the most popular neural network model. However, the Deep learning neural network model has attracted much interest recently. In this study, the Deep leaning neural network will be applied in image processing techniques for the real-time traffic information collection to investigate the performance of Deep learning algorithm in traffic engineering fields. In addition, the experimental results from Deep learing algorithm will be compared with the results from Backpropagation model to evaluate and propose better performing model.Intelligent Transportation Systems (ITS), artificial neural networks, Deep learning, traffic information, image processing"
635,딥러닝의 모형과 응용사례,안성만 /SungMahn Ahn,2016,한국지능정보시스템학회,"딥러닝은 인공신경망(neural network)이라는 인공지능분야의 모형이 발전된 형태로서, 계층구조로 이루어진 인공신경망의 내부계층(hidden layer)이 여러 단계로 이루어진 구조이다. 딥러닝에서의 주요 모형은 합성곱신경망(convolutional neural network), 순환신경망(recurrent neural network), 그리고 심층신뢰신경망(deep belief network)의 세가지라고 할 수있다. 그 중에서 현재 흥미로운 연구가 많이 발표되어서 관심이 집중되고 있는 모형은 지도학습(supervised learning)모형인 처음 두 개의 모형이다. 따라서 본 논문에서는 지도학습모형의 가중치를 최적화하는 기본적인 방법인 오류역전파 알고리즘을 살펴본 뒤에 합성곱신경망과 순환신경망의 구조와 응용사례 등을 살펴보고자 한다. 본문에서 다루지 않은 모형인 심층신뢰신경망은 아직까지는 합성곱신경망 이나 순환신경망보다는 상대적으로 주목을 덜 받고 있다. 그러나 심층신뢰신경망은 CNN이나 RNN과는 달리 비지도학습(unsupervised learning)모형이며, 사람이나 동물은 관찰을 통해서 스스로 학습한다는 점에서 궁극적으로는 비지도학습모형이 더 많이 연구되어야 할 주제가 될 것이다.Deep learning model is a kind of neural networks that allows multiple hidden layers. There are various deep learning architectures such as convolutional neural networks, deep belief networks and recurrent neural networks. Those have been applied to fields like computer vision, automatic speech recognition, natural language processing, audio recognition and bioinformatics where they have been shown to produce state-of-the-art results on various tasks. Among those architectures, convolutional neural networks and recurrent neural networks are classified as the supervised learning model. And in recent years, those supervised learning models have gained more popularity than unsupervised learning models such as deep belief networks, because supervised learning models have shown fashionable applications in such fields mentioned above.
Deep learning models can be trained with backpropagation algorithm. Backpropagation is an abbreviation for “backward propagation of errors” and a common method of training artificial neural networks used in conjunction with an optimization method such as gradient descent. The method calculates the gradient of an error function with respect to all the weights in the network. The gradient is fed to the optimization method which in turn uses it to update the weights, in an attempt to minimize the error function.
Convolutional neural networks use a special architecture which is particularly well-adapted to classify images. Using this architecture makes convolutional networks fast to train. This, in turn, helps us train deep, muti-layer networks, which are very good at classifying images. These days, deep convolutional networks are used in most neural networks for image recognition. Convolutional neural networks use three basic ideas: local receptive fields, shared weights, and pooling. By local receptive fields, we mean that each neuron in the first(or any) hidden layer will be connected to a small region of the input(or previous layer’s) neurons. Shared weights mean that we’re going to use the same weights and bias for each of the local receptive field. This means that all the neurons in the hidden layer detect exactly the same feature, just at different locations in the input image. In addition to the convolutional layers just described, convolutional neural networks also contain pooling layers. Pooling layers are usually used immediately after convolutional layers. What the pooling layers do is to simplify the information in the output from the convolutional layer.
Recent convolutional network architectures have 10 to 20 hidden layers and billions of connections between units. Training deep learning networks has taken weeks several years ago, but thanks to progress in GPU and algorithm enhancement, training time has reduced to several hours.
Neural networks with time-varying behavior are known as recurrent neural networks or RNNs. A recurrent neural network is a class of artificial neural network where connections between units form a directed cycle. This creates an internal state of the network which allows it to exhibit dynamic temporal behavior. Unlike feedforward neural networks, RNNs can use their internal memory to process arbitrary sequences of inputs. Early RNN models turned out to be very difficult to train, harder even than deep feedforward networks. The reason is the unstable gradient problem such as vanishing gradient and exploding gradient. The gradient can get smaller and smaller as it is propagated back through layers. This makes learning in early layers extremely slow. The problem actually gets worse in RNNs, since gradients aren’t just propagated backward through layers, they’re propagated backward through time. If the network runs for a long time, that can make the gradient extremely unstable and hard to learn from. It has been possible to incorporate an idea known as long short-term memory units (LSTMs) into RNNs. LSTMs make it much easier to get good results when training RNNs, and many recent papers make use of LSTMs or related ideas.Deep learning, Convolutional neural networks, Recurrent neural networks, Error backpropagation algorithm"
636,딥러닝분석과 기술적 분석 지표를 이용한 한국 코스피주가지수 방향성 예측,이우식 /Lee Woosik,2017,한국데이터정보과학회,"2016년 3월 구글 (Google)의 바둑인공지능 알파고 (AlphaGo)가 이세돌 9단과의 바둑대결에서승리한 이후 다양한 분야에서 인공지능 사용에 대한 관심이 높아지고 있는 가운데 금융투자 분야에서도 인공지능과 투자자문 전문가의 합성어인 로보어드바이저 (Robo-Advisor)에 대한 관심이 높아지고 있다. 인공지능 (artificial intelligence)기반의 의사결정은 비용 절감은 물론 효과적인 의사결정을가능하게 한다는 점에서 큰 장점이 있다. 본 연구에서는 기술적 분석 (technical analysis) 지표와 딥러닝 (deep learning) 모형을 결합하여 한국 코스피 지수를 예측하는 모형을 개발하고 제시한 모형들의 예측력을 비교, 분석한다. 분석 결과 기술적 분석 지표에 딥러닝 알고리즘을 결합한 모형이 주가지수 방향성 예측 문제에 응용될 수 있음을 확인하였다. 향후 본 연구에서 제안된 기술적 분석 지표와딥러닝모형을 결합한 기법은 로보어드바이저서비스에 응용할 수 있는 일반화 가능성을 보여준다Since Google’s AlphaGo defeated a world champion of Go players in 2016, there have been many interests in the deep learning. In the financial sector, a Robo-Advisor using deep learning gains a significant attention, which builds and manages portfolios of financial instruments for investors.In this paper, we have proposed the a deep learning algorithm geared toward identification and forecast of the KOSPI index direction,and we also have compared the accuracy of the prediction.In an application of forecasting the financial market index direction, we have shown that the Robo-Advisor using deep learning has a significant effect on finance industry. The Robo-Advisor collects a massive data such as earnings statements, news reports and regulatory filings, analyzes those and recommends investors how to view market trends and identify the best time to purchase financial assets. On the other hand, the Robo-Advisor allows businesses to learn more about their customers, develop better marketing strategies, increase sales and decrease costs.Artificial intelligence, deep learning, FinTech, Robo-Advisor, technical analysis."
637,딥러닝의 효율적인 학습을 위한 학습데이터 선별 기법에 관한 연구,주경돈 /Gyoung-don Joo,2017,한국정보과학회,"빅데이터의 증가와 병렬 하드웨어 처리, 학습 알고리즘의 개선은 딥러닝의 발전을 가져왔으며 인공지능이 적용 가능한 영역을 획기적으로 확장시키고 있다. 하지만 이러한 대규모의 학습 데이터에 기반한 딥러닝은 학습시간이 증가되고, 더 나아가 학습 데이터의 구축을 위한 레이블링(labeling) 비용이 데이터 수에 비례하여 증가하게 된다. 따라서 본 논문에서는, 딥러닝의 정확도를 최대한 유지하면서 필요한 학습 데이터의 수를 줄일수 있도록, 전체 데이터에서 제한된 수의 학습 데이터의 후보를 선별하는 기법에 대하여 연구한다. 특히 기존의 능동 학습에 기반한 기법들이 학습 데이터의 수가 매우 적어질 경우 딥러닝의 정확도가 급격히 떨어지는 문제를 해결할 수 있는 새로운 기법을 제안하였으며, 제안된 기법으로 선별된 학습 데이터는 그 수의 변화에 대해 딥러닝이 일관되게 경쟁력있는 정확도를 가짐을 다양한 실험을 통하여 검증하였다.The increase of big data, parallel hardware processing and enhanced learning algorithms have led to the development of deep learning and are dramatically expanding the domain of applications based on artificial intelligence. However, such large-scale training data extends the learning time, and furthermore, the labeling cost for building the training set increases in proportion to the volume of data. Therefore, in this paper, we study how to select a limited number of candidates for training set from given data so as to reduce the number of training set while maintaining the accuracy of deep learning. Especially, we found that, with the training set selected by the previous work, the accuracy of deep learning drastically decreases when the number of training set is very small and so proposed new methods to resolve this fall-off in accuracy. Through experiments, we showed that deep learning consistently has the competitive accuracy with the training set selected by the proposed method regardless of the size of training set.Deep Learning, Train Set Selection, Active Learning, Gamma distribution"
638,딥러닝을 이용한 범용적 스테그아날리시스,김현재 /Hyunjae Kim,2017,한국정보과학회,"스테그아날리시스(Steganalysis)란 이미지 등 일반적인 자료에 암호화된 정보를 은닉하는 스테가노그래피(Steganography)에 대한 검출 및 분석 방법으로, 기계학습 기반 방법론을 포함한다. 기존 기계학습 기반 스테그아날리시스는 영상(Image)의 특징(Feature) 추출 및 모델링에 기반하며, 최근 딥러닝(Deep Learning)의 적용으로 검출 정확도가 큰 폭으로 향상되었다. 하지만 현존하는 스테그아날리시스 모델은 단일 스테가노그래피 기법에 대해 국한되어 있어 학습에 사용되지 않은 스테고(Stego) 이미지의 경우 검출이 불가능한 결정적 한계를 가진다. 본 연구에서는 다양한 스테가노그래피 기법으로 생성된 스테고이미지에 딥러닝을 적용하여 스테그아날리시스를 학습하는 범용적 모델을 제안한다. 다양한 실험을 통해 제안 기법의 효용성 및 가능성을 확인하고, 범용적 스테그아날리시스 모델이 각각에 특화된 검출 기법과 유사한 정확도로 스테고 이미지를 검출할 수 있음을 보인다.Steganalysis is to detect information hidden by steganography inside general data such as images. There are stegoanalysis techniques that use machine learning (ML). Existing ML approaches to steganalysis are based on extracting features from stego images and modeling them.
Recently deep learning-based methodologies have shown significant improvements in detection accuracy. However, all the existing methods, including deep learning-based ones, have a critical limitation in that they can only detect stego images that are created by a specific steganography method. In this paper, we propose a generalized steganalysis method that can model multiple types of stego images using deep learning. Through various experiments, we confirm the effectiveness of our approach and envision directions for future research. In particular, we show that our method can detect each type of steganography with the same level of accuracy as that of a steganalysis method dedicated to that type of steganography, thereby demonstrating the general applicability of our approach to multiple types of stego images.steganalysis, steganography, deep learning, generalized steganalysis model"
639,"딥러닝 프레임워크의 비교: 티아노, 텐서플로, CNTK를 중심으로",정여진 /Yeojin Chung,2017,한국지능정보시스템학회,"딥러닝 프레임워크의 대표적인 기능으로는 ‘자동미분’과 ‘GPU의 활용’ 등을 들 수 있다. 본 논문은 파이썬의 라이브러리 형태로 사용 가능한 프레임워크 중에서 구글의 텐서플로와 마이크로소프트의 CNTK, 그리고 텐서 플로의 원조라고 할 수 있는 티아노를 비교하였다. 본문에서는 자동미분의 개념과 GPU의 활용형태를 간단히 설명하고, 그 다음에 logistic regression을 실행하는 예를 통하여 각 프레임워크의 문법을 알아본 뒤에, 마지막으로 대표적인 딥러닝 응용인 CNN의 예제를 실행시켜보고 코딩의 편의성과 실행속도 등을 확인해 보았다. 그 결과, 편의성의 관점에서 보면 티아노가 가장 코딩 하기가 어렵고, CNTK와 텐서플로는 많은 부분이 비슷하게 추상화 되어 있어서 코딩이 비슷하지만 가중치와 편향을 직접 정의하느냐의 여부에서 차이를 보였다. 그리고 각프레임워크의 실행속도에 대한 평가는 ‘큰 차이는 없다’는 것이다. 텐서플로는 티아노에 비하여 속도가 느리다는 평가가 있어왔는데, 본 연구의 실험에 의하면, 비록 CNN 모형에 국한되었지만, 텐서플로가 아주 조금이지만빠른 것으로 나타났다. CNTK의 경우에도, 비록 실험환경이 달랐지만, 실험환경의 차이에 의한 속도의 차이의 편차범위 이내에 있는 것으로 판단이 되었다. 본 연구에서는 세 종류의 딥러닝 프레임워크만을 살펴보았는데,위키피디아에 따르면 딥러닝 프레임워크의 종류는 12가지가 있으며, 각 프레임워크의 특징을 15가지 속성으로구분하여 차이를 특정하고 있다. 그 많은 속성 중에서 사용자의 입장에서 볼 때 중요한 속성은 어떤 언어(파이썬, C++, Java, 등)로 사용가능한지, 어떤 딥러닝 모형에 대한 라이브러리가 잘 구현되어 있는지 등일 것이다. 그리고 사용자가 대규모의 딥러닝 모형을 구축한다면, 다중 GPU 혹은 다중 서버를 지원하는지의 여부도 중요할 것이다. 또한 딥러닝 모형을 처음 학습하는 경우에는 사용설명서가 많은지 예제 프로그램이 많은지 여부도 중요한 기준이 될 것이다.The deep learning framework is software designed to help develop deep learning models. Some of its important functions include “automatic differentiation” and “utilization of GPU”. The list of popular deep learning framework includes Caffe (BVLC) and Theano (University of Montreal). And recently, Microsoft's deep learning framework, Microsoft Cognitive Toolkit, was released as open-source license, following Google’s Tensorflow a year earlier. The early deep learning frameworks have been developed mainly for research at universities. Beginning with the inception of Tensorflow, however, it seems that companies such as Microsoft and Facebook have started to join the competition of framework development. Given the trend, Google and other companies are expected to continue investing in the deep learning framework to bring forward the initiative in the artificial intelligence business. From this point of view, we think it is a good time to compare some of deep learning frameworks. So we compare three deep learning frameworks which can be used as a Python library. Those are Google's Tensorflow, Microsoft’s CNTK, and Theano which is sort of a predecessor of the preceding two.
The most common and important function of deep learning frameworks is the ability to perform automatic differentiation. Basically all the mathematical expressions of deep learning models can be represented as computational graphs, which consist of nodes and edges. Partial derivatives on each edge of a computational graph can then be obtained. With the partial derivatives, we can let software compute differentiation of any node with respect to any variable by utilizing chain rule of Calculus.
First of all, the convenience of coding is in the order of CNTK, Tensorflow, and Theano. The criterion is simply based on the lengths of the codes and the learning curve and the ease of coding are not the main concern. According to the criteria, Theano was the most difficult to implement with, and CNTK and Tensorflow were somewhat easier. With Tensorflow, we need to define weight variables and biases explicitly. The reason that CNTK and Tensorflow are easier to implement with is that those frameworks provide us with more abstraction than Theano. We, however, need to mention that low-level coding is not always bad. It gives us flexibility of coding. With the low-level coding such as in Theano, we can implement and test any new deep learning models or any new search methods that we can think of.
The assessment of the execution speed of each framework is that there is not meaningful difference. According to the experiment, execution speeds of Theano and Tensorflow are very similar, although the experiment was limited to a CNN model. In the case of CNTK, the experimental environment was not maintained as the same. The code written in CNTK has to be run in PC environment without GPU where codes execute as much as 50 times slower than with GPU. But we concluded that the difference of execution speed was within the range of variation caused by the different hardware setup.
In this study, we compared three types of deep learning framework: Theano, Tensorflow, and CNTK. According to Wikipedia, there are 12 available deep learning frameworks. And 15 different attributes differentiate each framework. Some of the important attributes would include interface language (Python, C ++, Java, etc.) and the availability of libraries on various deep learning models such as CNN, RNN, DBN, and etc. And if a user implements a large scale deep learning model, it will also be important to support multiple GPU or multiple servers. Also, if you are learning the deep learning model, it would also be important if there are enough examples and references.deep learning framework, Theano, TensorFlow, CNTK, computational graph, CIFAR-10"
640,딥러닝을 이용한 주변 무선단말 파악방안,이웅섭 /Lee， Woong-sup,2018,한국정보통신학회,"최근 단말-대-단말(Device-to-device, D2D) 통신기술이 차세대 무선통신시스템의 핵심기술로 큰 관심을 받고 있다. 이러한 단말간 통신에서는 자신의 주변에 어떠한 단말이 있는지 파악하는 주변단말 탐색(Neighbor discovery)이매우 중요하다. 본 논문에서는 최근 큰 관심을 받고 있는 딥러닝(Deep learning) 기술을 활용하여 단말간 통신에서 주변단말을 파악하는 방안에 대해서 제안한다. 제안 방안은 기존의 방안과 달리 무선채널의 공간적 연관성을 이용하여단말간의 신호 전송 없이 단말이 기지국으로 전송하는 상향링크 파일럿 신호를 기반으로 주변 단말을 찾고 따라서기존의 방식에 비해 신호전송 복잡도(signaling complexity)를 크게 줄일 수 있다. 또한 제안 방안에서는 떨어져 있는거리에 따라서 주변 단말을 분류 가능하여 기존 방안에 비해서 좀 더 세밀한 단말 탐색이 가능하다. 마지막으로 본 논문에서는 tensorflow를 이용한 컴퓨터 시뮬레이션을 통해 제안 방안의 성능을 검증하였다.Recently, the device-to-device (D2D) communication has been conceived as the key technology for the next-generationmobile communication systems. The neighbor discovery in which the nearby users are found, is essential for the properoperation of the D2D communication. In this paper, we propose new neighbor discovery scheme based on deep learningtechnology which has gained a lot of attention recently. In the proposed scheme, the neighboring users can be foundusing the uplink pilot transmission of users only, unlike conventional neighbor discovery schemes in which direct pilotcommunication among users is required, such that the signaling overhead can be greatly reduced in our proposed scheme.
Moreover, the neighbors with different proximity can also be classified accordingly which enables more accurate neighbordiscovery compared to the conventional schemes. The performance of our proposed scheme is verified through thetensorflow-based computer simulations.
키워드 : 딥러닝, 주변단말 탐색, 단말-대-단말 통신Deep learning, Neighbor discovery, Device-to-device communication, Wireless channel correlation"
641,신뢰성있는 딥러닝 기반 분석 모델을 참조하기 위한 딥러닝 기술 언어,문종혁 /Jong Hyeok Mun,2021,한국정보처리학회,"최근 딥러닝은 하드웨어 성능이 향상됨에 따라 자연어 처리, 영상 인식 등의 다양한 기술에 접목되어 활용되고 있다. 이러한 기술들을 활용해 지능형교통 시스템(ITS), 스마트홈, 헬스케어 등의 산업분야에서 데이터를 분석하여 고속도로 속도위반 차량 검출, 에너지 사용량 제어, 응급상황 등과 같은고품질의 서비스를 제공하며, 고품질의 서비스를 제공하기 위해서는 정확도가 향상된 딥러닝 모델이 적용되어야 한다. 이를 위해 서비스 환경의 데이터를분석하기 위한 딥러닝 모델을 개발할 때, 개발자는 신뢰성이 검증된 최신의 딥러닝 모델을 적용할 수 있어야 한다. 이는 개발자가 참조하는 딥러닝 모델에적용된 학습 데이터셋의 정확도를 측정하여 검증할 수 있다. 이러한 검증을 위해서 개발자는 학습 데이터셋, 딥러닝의 계층구조 및 개발 환경 등과 같은내용을 포함하는 딥러닝 모델을 문서화하여 적용하기 위한 구조적인 정보가 필요하다. 본 논문에서는 신뢰성있는 딥러닝 기반 데이터 분석 모델을 참조하기위한 딥러닝 기술 언어를 제안한다. 제안하는 기술 언어는 신뢰성 있는 딥러닝 모델을 개발하는데 필요한 학습데이터셋, 개발 환경 및 설정 등의 정보와더불어 딥러닝 모델의 계층구조를 표현할 수 있다. 제안하는 딥러닝 기술 언어를 이용하여 개발자는 지능형 교통 시스템에서 참조하는 분석 모델의정확도를 검증할 수 있다. 실험에서는 제안하는 언어의 유효성을 검증하기 위해, 번호판 인식 모델을 중심으로 딥러닝 기술 문서의 적용과정을 보인다With the recent advancements of deep learning, companies such as smart home, healthcare, and intelligent transportation systems areutilizing its functionality to provide high-quality services for vehicle detection, emergency situation detection, and controlling energyconsumption. To provide reliable services in such sensitive systems, deep learning models are required to have high accuracy. In orderto develop a deep learning model for analyzing previously mentioned services, developers should utilize the state of the art deep learningmodels that have already been verified for higher accuracy. The developers can verify the accuracy of the referenced model by validatingthe model on the dataset. For this validation, the developer needs structural information to document and apply deep learning models,including metadata such as learning dataset, network architecture, and development environments. In this paper, we propose a descriptionlanguage that represents the network architecture of the deep learning model along with its metadata that are necessary to develop adeep learning model. Through the proposed description language, developers can easily verify the accuracy of the referenced deep learningmodel. Our experiments demonstrate the application scenario of a deep learning description document that focuses on the license platerecognition for the detection of illegally parked vehicles.Trusted Deep Learning, Model Reference, Deep Learning Description Language, Traffic Situation Analysis Model"
642,딥러닝 분석을 이용한 중국 역내·외 위안화 변동성 예측,이우식 /Lee Woosik,2016,한국데이터정보과학회,"2008년 글로벌 금융위기 이후 중국은 위안화 국제화의 점진적 추진을 진행하면서 중국상하이 외환시장과 중국홍콩 외환시장에서 거래되는 통화인 역내위안화와 역외위안화를 형성시켰다. 본 연구는 위안화 국제화와 점진적인 중국 자본계정 개방에 따라 급변하는 외환시장상황의 변동성을 정확하게 파악하기 위해서 GARCH모형 (일반화된 자기회귀 조건부이분산성모형)에 다단계인공신경망을 결합한 MLP-GARCH 모형과 GARCH모형과 기계학습의 일종인 딥러닝 (deep learning)을 통합한 DL-GARCH을 가지고 위안화 변동성예측을 비교 실험과 분석을 하였다. 비교분석 결과 DLGARCH 모형은 MLP-GARCH보다 모형 위안화 역내·외 환율변동성 예측 면에서 더욱 더 개선된예측값을 제공하였다. 그래서 이분산시계열모형을 딥러닝과 결합한 DL-GARCH 모형은 시계열의환율 변동성 예측 문제에 딥러닝을 응용할 수 있음을 확인하였다. 향후 이분산시계열과 결합된 딥러닝 모형은 다른 금융시계열 데이터에 응용하여 그 일반화 가능성을 높일 수 있을 것이다.The People’s Republic of China has vigorously been pursuing the internationalization of the Chinese Yuan or Renminbi after the financial crisis of 2008. In this view, an abrupt increase of use of the Chinese Yuan in the onshore and offshore markets are important milestones to be one of important currencies. One of the most frequently used methods to forecast volatility is GARCH model. Since a prediction error of the GARCH model has been reported quite high, a lot of efforts have been made to improve forecasting capability of the GARCH model. In this paper, we have proposed MLPGARCH and a DL-GARCH by employing Artificial Neural Network to the GARCH. In an application to forecasting Chinese Yuan volatility, we have successfully shown their overall outperformance in forecasting over the GARCH.Deep learning, DL-GARCH, GARCH, The Chinese Yuan, volatility."
643,딥러닝 기법을 이용한 내일강수 예측,하지훈,2016,한국지능시스템학회,"정확한 강수예측을 위해서는 예측인자 선정과 예측방법에 대한 선택이 매우 중요하다. 최근에는 강수예측 방법으로기계학습 기법이 많이 사용되고 있으며, 그 중에서도 특히 인공신경망을 사용한 강수예측 방법은 좋은 성능을 보였다. 본논문에서는 딥러닝 기법 중 하나인 DBN(deep belief network)를 이용한 새로운 강수예측 방법을 제안한다. DBN는 비지도사전 학습을 통해 초기 가중치를 설정하여 기존 인공신경망의 문제점을 보완한다. 예측인자로는 기온, 전일-전주 강수일, 태양과 달 궤도 관련 자료를 선정하였다. 기온과 전일-전주 강수일은 서울에서의 1974년부터 2013년까지 총 40년간의AWS(automatic weather system) 관측 자료를 사용하였고, 태양과 달의 궤도 관련 자료는 서울을 중심으로 계산한 결과를사용하였다. 전체 기간에서 일부는 학습 자료로 사용하여 예측모델을 생성하였고, 나머지를 생성한 모델의 검증 자료로사용하였다. 모델 검증 결과로 나온 예측값들은 확률값을 가지며 임계치를 이용하여 강수유무를 판별하였다. 강수 정확도의척도로 양분예보기법 중 CSI(critical successive index)와 Bias(frequency bias)를 계산하였다. 이를 통해 DBN와 MLP(multilayer perceptron)의 성능을 비교한 결과 DBN의 강수 예측 정확도가 높았고, 수행속도 또한 2배 이상 빨랐다.For accurate precipitation forecasts the choice of weather faoctrs and prediction method is very important. Recently, machinele arning has been widely used for forecasting precipitation, and artificial neural network, one of machine learning techniques, showed good performance. In this paper, we suggest a new method for forecasting precipitation using DBN, one of deep learning techniques.
DBN has an advantage that initial weights are set by unsupervised learning, so this compensates for the defects of artificial neural networks. We used past precipitation, temperature, and the parameters of the sun and moon's motion as features for forecasting precipitation. The dataset consists of observation data which had been measured for 40 years from AWS in Seoul. Experiments were based on 8-fold cross validation. As a result of estimation, we got probabilities of test dataset, so threshold was used for the decision of precipitation. CSI and Bias were used for indicating the precision of precipitation. Our experimental results showed that DBN performed better than MLP.Deep learning, Deep belief network, Precipitation, Forecast"
644,딥러닝 신경망모형을 이용한 실시간 교통정보수집 연구,김대현 /KIM DAE HYON,2016,사단법인 인문사회과학기술융합학회,"비디오 기반의 실시간 교통정보수집 시스템은 자율주행을 포함한 지능형교통체계의 주요 서브시스템중 하나이며, 우수한 예측(추정)력 때문에 핵심 알고리즘으로 인공신경망 모형이 주로 사용되고 있다. 특히, 최근 과학 학술지 네이처에 구글 딥마인드의 알파고가 공개되면서 딥러닝과 인공지능에 대한 전 세계적인 관심이 집중되고 있다. 최근까지 가장 널리 사용된 인공신경망모형은 역전파 모형이었으나 최근 컴퓨터 하드웨어 및 알고리즘의 새로운 개발과 더불어 딥러닝 알고리즘을 이용한 딥인공신경망 모형에 대한 관심이 높아지고 있다. 그러나 아직 딥러닝 인공신경망에 대한 연구는 초기단계에 있는 실정이며 적용분야도 극히 한정되어 있다. 본 연구에서는 실시간 도로교통정보 수집을 위한 영상처리분야에서 딥러닝 알고리즘을 활용하고 실험결과를 통하여 향후 교통정보수집‧분석 분야에서의 딥러닝 인공신경망 모형의 적용가능성을 살펴보고자 한다. 특히, 본 연구에서 도출한 딥러닝 인공신경망모형의 실험결과를 기존 역전파 모형에 의한 실험결과와 비교하여 모형의 우수성을 비교‧평가하고자 한다.Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Normally, artificial neural networks have been used in real-time applications for collecting traffic information because of their remarkable performance in terms of predictive accuracy and real-time computing. Especially, the recent publication of a paper in the journal Nature and the success of Google DeepMind’s world-beating Go program AlphaGo caused significant increases of the public interest in artificial intelligence. Even though various types of learning models have been proposed, the Backpropagation has been the most popular neural network model. However, the Deep learning neural network model has attracted much interest recently. In this study, the Deep leaning neural network will be applied in image processing techniques for the real-time traffic information collection to investigate the performance of Deep learning algorithm in traffic engineering fields. In addition, the experimental results from Deep learing algorithm will be compared with the results from Backpropagation model to evaluate and propose better performing model.Intelligent Transportation Systems (ITS), artificial neural networks, Deep learning, traffic information, image processing"
645,딥러닝의 모형과 응용사례,안성만 /SungMahn Ahn,2016,한국지능정보시스템학회,"딥러닝은 인공신경망(neural network)이라는 인공지능분야의 모형이 발전된 형태로서, 계층구조로 이루어진 인공신경망의 내부계층(hidden layer)이 여러 단계로 이루어진 구조이다. 딥러닝에서의 주요 모형은 합성곱신경망(convolutional neural network), 순환신경망(recurrent neural network), 그리고 심층신뢰신경망(deep belief network)의 세가지라고 할 수있다. 그 중에서 현재 흥미로운 연구가 많이 발표되어서 관심이 집중되고 있는 모형은 지도학습(supervised learning)모형인 처음 두 개의 모형이다. 따라서 본 논문에서는 지도학습모형의 가중치를 최적화하는 기본적인 방법인 오류역전파 알고리즘을 살펴본 뒤에 합성곱신경망과 순환신경망의 구조와 응용사례 등을 살펴보고자 한다. 본문에서 다루지 않은 모형인 심층신뢰신경망은 아직까지는 합성곱신경망 이나 순환신경망보다는 상대적으로 주목을 덜 받고 있다. 그러나 심층신뢰신경망은 CNN이나 RNN과는 달리 비지도학습(unsupervised learning)모형이며, 사람이나 동물은 관찰을 통해서 스스로 학습한다는 점에서 궁극적으로는 비지도학습모형이 더 많이 연구되어야 할 주제가 될 것이다.Deep learning model is a kind of neural networks that allows multiple hidden layers. There are various deep learning architectures such as convolutional neural networks, deep belief networks and recurrent neural networks. Those have been applied to fields like computer vision, automatic speech recognition, natural language processing, audio recognition and bioinformatics where they have been shown to produce state-of-the-art results on various tasks. Among those architectures, convolutional neural networks and recurrent neural networks are classified as the supervised learning model. And in recent years, those supervised learning models have gained more popularity than unsupervised learning models such as deep belief networks, because supervised learning models have shown fashionable applications in such fields mentioned above.
Deep learning models can be trained with backpropagation algorithm. Backpropagation is an abbreviation for “backward propagation of errors” and a common method of training artificial neural networks used in conjunction with an optimization method such as gradient descent. The method calculates the gradient of an error function with respect to all the weights in the network. The gradient is fed to the optimization method which in turn uses it to update the weights, in an attempt to minimize the error function.
Convolutional neural networks use a special architecture which is particularly well-adapted to classify images. Using this architecture makes convolutional networks fast to train. This, in turn, helps us train deep, muti-layer networks, which are very good at classifying images. These days, deep convolutional networks are used in most neural networks for image recognition. Convolutional neural networks use three basic ideas: local receptive fields, shared weights, and pooling. By local receptive fields, we mean that each neuron in the first(or any) hidden layer will be connected to a small region of the input(or previous layer’s) neurons. Shared weights mean that we’re going to use the same weights and bias for each of the local receptive field. This means that all the neurons in the hidden layer detect exactly the same feature, just at different locations in the input image. In addition to the convolutional layers just described, convolutional neural networks also contain pooling layers. Pooling layers are usually used immediately after convolutional layers. What the pooling layers do is to simplify the information in the output from the convolutional layer.
Recent convolutional network architectures have 10 to 20 hidden layers and billions of connections between units. Training deep learning networks has taken weeks several years ago, but thanks to progress in GPU and algorithm enhancement, training time has reduced to several hours.
Neural networks with time-varying behavior are known as recurrent neural networks or RNNs. A recurrent neural network is a class of artificial neural network where connections between units form a directed cycle. This creates an internal state of the network which allows it to exhibit dynamic temporal behavior. Unlike feedforward neural networks, RNNs can use their internal memory to process arbitrary sequences of inputs. Early RNN models turned out to be very difficult to train, harder even than deep feedforward networks. The reason is the unstable gradient problem such as vanishing gradient and exploding gradient. The gradient can get smaller and smaller as it is propagated back through layers. This makes learning in early layers extremely slow. The problem actually gets worse in RNNs, since gradients aren’t just propagated backward through layers, they’re propagated backward through time. If the network runs for a long time, that can make the gradient extremely unstable and hard to learn from. It has been possible to incorporate an idea known as long short-term memory units (LSTMs) into RNNs. LSTMs make it much easier to get good results when training RNNs, and many recent papers make use of LSTMs or related ideas.Deep learning, Convolutional neural networks, Recurrent neural networks, Error backpropagation algorithm"
646,딥러닝분석과 기술적 분석 지표를 이용한 한국 코스피주가지수 방향성 예측,이우식 /Lee Woosik,2017,한국데이터정보과학회,"2016년 3월 구글 (Google)의 바둑인공지능 알파고 (AlphaGo)가 이세돌 9단과의 바둑대결에서승리한 이후 다양한 분야에서 인공지능 사용에 대한 관심이 높아지고 있는 가운데 금융투자 분야에서도 인공지능과 투자자문 전문가의 합성어인 로보어드바이저 (Robo-Advisor)에 대한 관심이 높아지고 있다. 인공지능 (artificial intelligence)기반의 의사결정은 비용 절감은 물론 효과적인 의사결정을가능하게 한다는 점에서 큰 장점이 있다. 본 연구에서는 기술적 분석 (technical analysis) 지표와 딥러닝 (deep learning) 모형을 결합하여 한국 코스피 지수를 예측하는 모형을 개발하고 제시한 모형들의 예측력을 비교, 분석한다. 분석 결과 기술적 분석 지표에 딥러닝 알고리즘을 결합한 모형이 주가지수 방향성 예측 문제에 응용될 수 있음을 확인하였다. 향후 본 연구에서 제안된 기술적 분석 지표와딥러닝모형을 결합한 기법은 로보어드바이저서비스에 응용할 수 있는 일반화 가능성을 보여준다Since Google’s AlphaGo defeated a world champion of Go players in 2016, there have been many interests in the deep learning. In the financial sector, a Robo-Advisor using deep learning gains a significant attention, which builds and manages portfolios of financial instruments for investors.In this paper, we have proposed the a deep learning algorithm geared toward identification and forecast of the KOSPI index direction,and we also have compared the accuracy of the prediction.In an application of forecasting the financial market index direction, we have shown that the Robo-Advisor using deep learning has a significant effect on finance industry. The Robo-Advisor collects a massive data such as earnings statements, news reports and regulatory filings, analyzes those and recommends investors how to view market trends and identify the best time to purchase financial assets. On the other hand, the Robo-Advisor allows businesses to learn more about their customers, develop better marketing strategies, increase sales and decrease costs.Artificial intelligence, deep learning, FinTech, Robo-Advisor, technical analysis."
647,딥러닝의 효율적인 학습을 위한 학습데이터 선별 기법에 관한 연구,주경돈 /Gyoung-don Joo,2017,한국정보과학회,"빅데이터의 증가와 병렬 하드웨어 처리, 학습 알고리즘의 개선은 딥러닝의 발전을 가져왔으며 인공지능이 적용 가능한 영역을 획기적으로 확장시키고 있다. 하지만 이러한 대규모의 학습 데이터에 기반한 딥러닝은 학습시간이 증가되고, 더 나아가 학습 데이터의 구축을 위한 레이블링(labeling) 비용이 데이터 수에 비례하여 증가하게 된다. 따라서 본 논문에서는, 딥러닝의 정확도를 최대한 유지하면서 필요한 학습 데이터의 수를 줄일수 있도록, 전체 데이터에서 제한된 수의 학습 데이터의 후보를 선별하는 기법에 대하여 연구한다. 특히 기존의 능동 학습에 기반한 기법들이 학습 데이터의 수가 매우 적어질 경우 딥러닝의 정확도가 급격히 떨어지는 문제를 해결할 수 있는 새로운 기법을 제안하였으며, 제안된 기법으로 선별된 학습 데이터는 그 수의 변화에 대해 딥러닝이 일관되게 경쟁력있는 정확도를 가짐을 다양한 실험을 통하여 검증하였다.The increase of big data, parallel hardware processing and enhanced learning algorithms have led to the development of deep learning and are dramatically expanding the domain of applications based on artificial intelligence. However, such large-scale training data extends the learning time, and furthermore, the labeling cost for building the training set increases in proportion to the volume of data. Therefore, in this paper, we study how to select a limited number of candidates for training set from given data so as to reduce the number of training set while maintaining the accuracy of deep learning. Especially, we found that, with the training set selected by the previous work, the accuracy of deep learning drastically decreases when the number of training set is very small and so proposed new methods to resolve this fall-off in accuracy. Through experiments, we showed that deep learning consistently has the competitive accuracy with the training set selected by the proposed method regardless of the size of training set.Deep Learning, Train Set Selection, Active Learning, Gamma distribution"
648,딥러닝을 이용한 범용적 스테그아날리시스,김현재 /Hyunjae Kim,2017,한국정보과학회,"스테그아날리시스(Steganalysis)란 이미지 등 일반적인 자료에 암호화된 정보를 은닉하는 스테가노그래피(Steganography)에 대한 검출 및 분석 방법으로, 기계학습 기반 방법론을 포함한다. 기존 기계학습 기반 스테그아날리시스는 영상(Image)의 특징(Feature) 추출 및 모델링에 기반하며, 최근 딥러닝(Deep Learning)의 적용으로 검출 정확도가 큰 폭으로 향상되었다. 하지만 현존하는 스테그아날리시스 모델은 단일 스테가노그래피 기법에 대해 국한되어 있어 학습에 사용되지 않은 스테고(Stego) 이미지의 경우 검출이 불가능한 결정적 한계를 가진다. 본 연구에서는 다양한 스테가노그래피 기법으로 생성된 스테고이미지에 딥러닝을 적용하여 스테그아날리시스를 학습하는 범용적 모델을 제안한다. 다양한 실험을 통해 제안 기법의 효용성 및 가능성을 확인하고, 범용적 스테그아날리시스 모델이 각각에 특화된 검출 기법과 유사한 정확도로 스테고 이미지를 검출할 수 있음을 보인다.Steganalysis is to detect information hidden by steganography inside general data such as images. There are stegoanalysis techniques that use machine learning (ML). Existing ML approaches to steganalysis are based on extracting features from stego images and modeling them.
Recently deep learning-based methodologies have shown significant improvements in detection accuracy. However, all the existing methods, including deep learning-based ones, have a critical limitation in that they can only detect stego images that are created by a specific steganography method. In this paper, we propose a generalized steganalysis method that can model multiple types of stego images using deep learning. Through various experiments, we confirm the effectiveness of our approach and envision directions for future research. In particular, we show that our method can detect each type of steganography with the same level of accuracy as that of a steganalysis method dedicated to that type of steganography, thereby demonstrating the general applicability of our approach to multiple types of stego images.steganalysis, steganography, deep learning, generalized steganalysis model"
649,"딥러닝 프레임워크의 비교: 티아노, 텐서플로, CNTK를 중심으로",정여진 /Yeojin Chung,2017,한국지능정보시스템학회,"딥러닝 프레임워크의 대표적인 기능으로는 ‘자동미분’과 ‘GPU의 활용’ 등을 들 수 있다. 본 논문은 파이썬의 라이브러리 형태로 사용 가능한 프레임워크 중에서 구글의 텐서플로와 마이크로소프트의 CNTK, 그리고 텐서 플로의 원조라고 할 수 있는 티아노를 비교하였다. 본문에서는 자동미분의 개념과 GPU의 활용형태를 간단히 설명하고, 그 다음에 logistic regression을 실행하는 예를 통하여 각 프레임워크의 문법을 알아본 뒤에, 마지막으로 대표적인 딥러닝 응용인 CNN의 예제를 실행시켜보고 코딩의 편의성과 실행속도 등을 확인해 보았다. 그 결과, 편의성의 관점에서 보면 티아노가 가장 코딩 하기가 어렵고, CNTK와 텐서플로는 많은 부분이 비슷하게 추상화 되어 있어서 코딩이 비슷하지만 가중치와 편향을 직접 정의하느냐의 여부에서 차이를 보였다. 그리고 각프레임워크의 실행속도에 대한 평가는 ‘큰 차이는 없다’는 것이다. 텐서플로는 티아노에 비하여 속도가 느리다는 평가가 있어왔는데, 본 연구의 실험에 의하면, 비록 CNN 모형에 국한되었지만, 텐서플로가 아주 조금이지만빠른 것으로 나타났다. CNTK의 경우에도, 비록 실험환경이 달랐지만, 실험환경의 차이에 의한 속도의 차이의 편차범위 이내에 있는 것으로 판단이 되었다. 본 연구에서는 세 종류의 딥러닝 프레임워크만을 살펴보았는데,위키피디아에 따르면 딥러닝 프레임워크의 종류는 12가지가 있으며, 각 프레임워크의 특징을 15가지 속성으로구분하여 차이를 특정하고 있다. 그 많은 속성 중에서 사용자의 입장에서 볼 때 중요한 속성은 어떤 언어(파이썬, C++, Java, 등)로 사용가능한지, 어떤 딥러닝 모형에 대한 라이브러리가 잘 구현되어 있는지 등일 것이다. 그리고 사용자가 대규모의 딥러닝 모형을 구축한다면, 다중 GPU 혹은 다중 서버를 지원하는지의 여부도 중요할 것이다. 또한 딥러닝 모형을 처음 학습하는 경우에는 사용설명서가 많은지 예제 프로그램이 많은지 여부도 중요한 기준이 될 것이다.The deep learning framework is software designed to help develop deep learning models. Some of its important functions include “automatic differentiation” and “utilization of GPU”. The list of popular deep learning framework includes Caffe (BVLC) and Theano (University of Montreal). And recently, Microsoft's deep learning framework, Microsoft Cognitive Toolkit, was released as open-source license, following Google’s Tensorflow a year earlier. The early deep learning frameworks have been developed mainly for research at universities. Beginning with the inception of Tensorflow, however, it seems that companies such as Microsoft and Facebook have started to join the competition of framework development. Given the trend, Google and other companies are expected to continue investing in the deep learning framework to bring forward the initiative in the artificial intelligence business. From this point of view, we think it is a good time to compare some of deep learning frameworks. So we compare three deep learning frameworks which can be used as a Python library. Those are Google's Tensorflow, Microsoft’s CNTK, and Theano which is sort of a predecessor of the preceding two.
The most common and important function of deep learning frameworks is the ability to perform automatic differentiation. Basically all the mathematical expressions of deep learning models can be represented as computational graphs, which consist of nodes and edges. Partial derivatives on each edge of a computational graph can then be obtained. With the partial derivatives, we can let software compute differentiation of any node with respect to any variable by utilizing chain rule of Calculus.
First of all, the convenience of coding is in the order of CNTK, Tensorflow, and Theano. The criterion is simply based on the lengths of the codes and the learning curve and the ease of coding are not the main concern. According to the criteria, Theano was the most difficult to implement with, and CNTK and Tensorflow were somewhat easier. With Tensorflow, we need to define weight variables and biases explicitly. The reason that CNTK and Tensorflow are easier to implement with is that those frameworks provide us with more abstraction than Theano. We, however, need to mention that low-level coding is not always bad. It gives us flexibility of coding. With the low-level coding such as in Theano, we can implement and test any new deep learning models or any new search methods that we can think of.
The assessment of the execution speed of each framework is that there is not meaningful difference. According to the experiment, execution speeds of Theano and Tensorflow are very similar, although the experiment was limited to a CNN model. In the case of CNTK, the experimental environment was not maintained as the same. The code written in CNTK has to be run in PC environment without GPU where codes execute as much as 50 times slower than with GPU. But we concluded that the difference of execution speed was within the range of variation caused by the different hardware setup.
In this study, we compared three types of deep learning framework: Theano, Tensorflow, and CNTK. According to Wikipedia, there are 12 available deep learning frameworks. And 15 different attributes differentiate each framework. Some of the important attributes would include interface language (Python, C ++, Java, etc.) and the availability of libraries on various deep learning models such as CNN, RNN, DBN, and etc. And if a user implements a large scale deep learning model, it will also be important to support multiple GPU or multiple servers. Also, if you are learning the deep learning model, it would also be important if there are enough examples and references.deep learning framework, Theano, TensorFlow, CNTK, computational graph, CIFAR-10"
650,딥러닝을 이용한 주변 무선단말 파악방안,이웅섭 /Lee， Woong-sup,2018,한국정보통신학회,"최근 단말-대-단말(Device-to-device, D2D) 통신기술이 차세대 무선통신시스템의 핵심기술로 큰 관심을 받고 있다. 이러한 단말간 통신에서는 자신의 주변에 어떠한 단말이 있는지 파악하는 주변단말 탐색(Neighbor discovery)이매우 중요하다. 본 논문에서는 최근 큰 관심을 받고 있는 딥러닝(Deep learning) 기술을 활용하여 단말간 통신에서 주변단말을 파악하는 방안에 대해서 제안한다. 제안 방안은 기존의 방안과 달리 무선채널의 공간적 연관성을 이용하여단말간의 신호 전송 없이 단말이 기지국으로 전송하는 상향링크 파일럿 신호를 기반으로 주변 단말을 찾고 따라서기존의 방식에 비해 신호전송 복잡도(signaling complexity)를 크게 줄일 수 있다. 또한 제안 방안에서는 떨어져 있는거리에 따라서 주변 단말을 분류 가능하여 기존 방안에 비해서 좀 더 세밀한 단말 탐색이 가능하다. 마지막으로 본 논문에서는 tensorflow를 이용한 컴퓨터 시뮬레이션을 통해 제안 방안의 성능을 검증하였다.Recently, the device-to-device (D2D) communication has been conceived as the key technology for the next-generationmobile communication systems. The neighbor discovery in which the nearby users are found, is essential for the properoperation of the D2D communication. In this paper, we propose new neighbor discovery scheme based on deep learningtechnology which has gained a lot of attention recently. In the proposed scheme, the neighboring users can be foundusing the uplink pilot transmission of users only, unlike conventional neighbor discovery schemes in which direct pilotcommunication among users is required, such that the signaling overhead can be greatly reduced in our proposed scheme.
Moreover, the neighbors with different proximity can also be classified accordingly which enables more accurate neighbordiscovery compared to the conventional schemes. The performance of our proposed scheme is verified through thetensorflow-based computer simulations.
키워드 : 딥러닝, 주변단말 탐색, 단말-대-단말 통신Deep learning, Neighbor discovery, Device-to-device communication, Wireless channel correlation"
651,신뢰성있는 딥러닝 기반 분석 모델을 참조하기 위한 딥러닝 기술 언어,문종혁 /Jong Hyeok Mun,2021,한국정보처리학회,"최근 딥러닝은 하드웨어 성능이 향상됨에 따라 자연어 처리, 영상 인식 등의 다양한 기술에 접목되어 활용되고 있다. 이러한 기술들을 활용해 지능형교통 시스템(ITS), 스마트홈, 헬스케어 등의 산업분야에서 데이터를 분석하여 고속도로 속도위반 차량 검출, 에너지 사용량 제어, 응급상황 등과 같은고품질의 서비스를 제공하며, 고품질의 서비스를 제공하기 위해서는 정확도가 향상된 딥러닝 모델이 적용되어야 한다. 이를 위해 서비스 환경의 데이터를분석하기 위한 딥러닝 모델을 개발할 때, 개발자는 신뢰성이 검증된 최신의 딥러닝 모델을 적용할 수 있어야 한다. 이는 개발자가 참조하는 딥러닝 모델에적용된 학습 데이터셋의 정확도를 측정하여 검증할 수 있다. 이러한 검증을 위해서 개발자는 학습 데이터셋, 딥러닝의 계층구조 및 개발 환경 등과 같은내용을 포함하는 딥러닝 모델을 문서화하여 적용하기 위한 구조적인 정보가 필요하다. 본 논문에서는 신뢰성있는 딥러닝 기반 데이터 분석 모델을 참조하기위한 딥러닝 기술 언어를 제안한다. 제안하는 기술 언어는 신뢰성 있는 딥러닝 모델을 개발하는데 필요한 학습데이터셋, 개발 환경 및 설정 등의 정보와더불어 딥러닝 모델의 계층구조를 표현할 수 있다. 제안하는 딥러닝 기술 언어를 이용하여 개발자는 지능형 교통 시스템에서 참조하는 분석 모델의정확도를 검증할 수 있다. 실험에서는 제안하는 언어의 유효성을 검증하기 위해, 번호판 인식 모델을 중심으로 딥러닝 기술 문서의 적용과정을 보인다With the recent advancements of deep learning, companies such as smart home, healthcare, and intelligent transportation systems areutilizing its functionality to provide high-quality services for vehicle detection, emergency situation detection, and controlling energyconsumption. To provide reliable services in such sensitive systems, deep learning models are required to have high accuracy. In orderto develop a deep learning model for analyzing previously mentioned services, developers should utilize the state of the art deep learningmodels that have already been verified for higher accuracy. The developers can verify the accuracy of the referenced model by validatingthe model on the dataset. For this validation, the developer needs structural information to document and apply deep learning models,including metadata such as learning dataset, network architecture, and development environments. In this paper, we propose a descriptionlanguage that represents the network architecture of the deep learning model along with its metadata that are necessary to develop adeep learning model. Through the proposed description language, developers can easily verify the accuracy of the referenced deep learningmodel. Our experiments demonstrate the application scenario of a deep learning description document that focuses on the license platerecognition for the detection of illegally parked vehicles.Trusted Deep Learning, Model Reference, Deep Learning Description Language, Traffic Situation Analysis Model"
652,딥러닝 분석을 이용한 중국 역내·외 위안화 변동성 예측,이우식 /Lee Woosik,2016,한국데이터정보과학회,"2008년 글로벌 금융위기 이후 중국은 위안화 국제화의 점진적 추진을 진행하면서 중국상하이 외환시장과 중국홍콩 외환시장에서 거래되는 통화인 역내위안화와 역외위안화를 형성시켰다. 본 연구는 위안화 국제화와 점진적인 중국 자본계정 개방에 따라 급변하는 외환시장상황의 변동성을 정확하게 파악하기 위해서 GARCH모형 (일반화된 자기회귀 조건부이분산성모형)에 다단계인공신경망을 결합한 MLP-GARCH 모형과 GARCH모형과 기계학습의 일종인 딥러닝 (deep learning)을 통합한 DL-GARCH을 가지고 위안화 변동성예측을 비교 실험과 분석을 하였다. 비교분석 결과 DLGARCH 모형은 MLP-GARCH보다 모형 위안화 역내·외 환율변동성 예측 면에서 더욱 더 개선된예측값을 제공하였다. 그래서 이분산시계열모형을 딥러닝과 결합한 DL-GARCH 모형은 시계열의환율 변동성 예측 문제에 딥러닝을 응용할 수 있음을 확인하였다. 향후 이분산시계열과 결합된 딥러닝 모형은 다른 금융시계열 데이터에 응용하여 그 일반화 가능성을 높일 수 있을 것이다.The People’s Republic of China has vigorously been pursuing the internationalization of the Chinese Yuan or Renminbi after the financial crisis of 2008. In this view, an abrupt increase of use of the Chinese Yuan in the onshore and offshore markets are important milestones to be one of important currencies. One of the most frequently used methods to forecast volatility is GARCH model. Since a prediction error of the GARCH model has been reported quite high, a lot of efforts have been made to improve forecasting capability of the GARCH model. In this paper, we have proposed MLPGARCH and a DL-GARCH by employing Artificial Neural Network to the GARCH. In an application to forecasting Chinese Yuan volatility, we have successfully shown their overall outperformance in forecasting over the GARCH.Deep learning, DL-GARCH, GARCH, The Chinese Yuan, volatility."
653,딥러닝 기법을 이용한 내일강수 예측,하지훈,2016,한국지능시스템학회,"정확한 강수예측을 위해서는 예측인자 선정과 예측방법에 대한 선택이 매우 중요하다. 최근에는 강수예측 방법으로기계학습 기법이 많이 사용되고 있으며, 그 중에서도 특히 인공신경망을 사용한 강수예측 방법은 좋은 성능을 보였다. 본논문에서는 딥러닝 기법 중 하나인 DBN(deep belief network)를 이용한 새로운 강수예측 방법을 제안한다. DBN는 비지도사전 학습을 통해 초기 가중치를 설정하여 기존 인공신경망의 문제점을 보완한다. 예측인자로는 기온, 전일-전주 강수일, 태양과 달 궤도 관련 자료를 선정하였다. 기온과 전일-전주 강수일은 서울에서의 1974년부터 2013년까지 총 40년간의AWS(automatic weather system) 관측 자료를 사용하였고, 태양과 달의 궤도 관련 자료는 서울을 중심으로 계산한 결과를사용하였다. 전체 기간에서 일부는 학습 자료로 사용하여 예측모델을 생성하였고, 나머지를 생성한 모델의 검증 자료로사용하였다. 모델 검증 결과로 나온 예측값들은 확률값을 가지며 임계치를 이용하여 강수유무를 판별하였다. 강수 정확도의척도로 양분예보기법 중 CSI(critical successive index)와 Bias(frequency bias)를 계산하였다. 이를 통해 DBN와 MLP(multilayer perceptron)의 성능을 비교한 결과 DBN의 강수 예측 정확도가 높았고, 수행속도 또한 2배 이상 빨랐다.For accurate precipitation forecasts the choice of weather faoctrs and prediction method is very important. Recently, machinele arning has been widely used for forecasting precipitation, and artificial neural network, one of machine learning techniques, showed good performance. In this paper, we suggest a new method for forecasting precipitation using DBN, one of deep learning techniques.
DBN has an advantage that initial weights are set by unsupervised learning, so this compensates for the defects of artificial neural networks. We used past precipitation, temperature, and the parameters of the sun and moon's motion as features for forecasting precipitation. The dataset consists of observation data which had been measured for 40 years from AWS in Seoul. Experiments were based on 8-fold cross validation. As a result of estimation, we got probabilities of test dataset, so threshold was used for the decision of precipitation. CSI and Bias were used for indicating the precision of precipitation. Our experimental results showed that DBN performed better than MLP.Deep learning, Deep belief network, Precipitation, Forecast"
654,딥러닝 신경망모형을 이용한 실시간 교통정보수집 연구,김대현 /KIM DAE HYON,2016,사단법인 인문사회과학기술융합학회,"비디오 기반의 실시간 교통정보수집 시스템은 자율주행을 포함한 지능형교통체계의 주요 서브시스템중 하나이며, 우수한 예측(추정)력 때문에 핵심 알고리즘으로 인공신경망 모형이 주로 사용되고 있다. 특히, 최근 과학 학술지 네이처에 구글 딥마인드의 알파고가 공개되면서 딥러닝과 인공지능에 대한 전 세계적인 관심이 집중되고 있다. 최근까지 가장 널리 사용된 인공신경망모형은 역전파 모형이었으나 최근 컴퓨터 하드웨어 및 알고리즘의 새로운 개발과 더불어 딥러닝 알고리즘을 이용한 딥인공신경망 모형에 대한 관심이 높아지고 있다. 그러나 아직 딥러닝 인공신경망에 대한 연구는 초기단계에 있는 실정이며 적용분야도 극히 한정되어 있다. 본 연구에서는 실시간 도로교통정보 수집을 위한 영상처리분야에서 딥러닝 알고리즘을 활용하고 실험결과를 통하여 향후 교통정보수집‧분석 분야에서의 딥러닝 인공신경망 모형의 적용가능성을 살펴보고자 한다. 특히, 본 연구에서 도출한 딥러닝 인공신경망모형의 실험결과를 기존 역전파 모형에 의한 실험결과와 비교하여 모형의 우수성을 비교‧평가하고자 한다.Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Normally, artificial neural networks have been used in real-time applications for collecting traffic information because of their remarkable performance in terms of predictive accuracy and real-time computing. Especially, the recent publication of a paper in the journal Nature and the success of Google DeepMind’s world-beating Go program AlphaGo caused significant increases of the public interest in artificial intelligence. Even though various types of learning models have been proposed, the Backpropagation has been the most popular neural network model. However, the Deep learning neural network model has attracted much interest recently. In this study, the Deep leaning neural network will be applied in image processing techniques for the real-time traffic information collection to investigate the performance of Deep learning algorithm in traffic engineering fields. In addition, the experimental results from Deep learing algorithm will be compared with the results from Backpropagation model to evaluate and propose better performing model.Intelligent Transportation Systems (ITS), artificial neural networks, Deep learning, traffic information, image processing"
655,딥러닝의 모형과 응용사례,안성만 /SungMahn Ahn,2016,한국지능정보시스템학회,"딥러닝은 인공신경망(neural network)이라는 인공지능분야의 모형이 발전된 형태로서, 계층구조로 이루어진 인공신경망의 내부계층(hidden layer)이 여러 단계로 이루어진 구조이다. 딥러닝에서의 주요 모형은 합성곱신경망(convolutional neural network), 순환신경망(recurrent neural network), 그리고 심층신뢰신경망(deep belief network)의 세가지라고 할 수있다. 그 중에서 현재 흥미로운 연구가 많이 발표되어서 관심이 집중되고 있는 모형은 지도학습(supervised learning)모형인 처음 두 개의 모형이다. 따라서 본 논문에서는 지도학습모형의 가중치를 최적화하는 기본적인 방법인 오류역전파 알고리즘을 살펴본 뒤에 합성곱신경망과 순환신경망의 구조와 응용사례 등을 살펴보고자 한다. 본문에서 다루지 않은 모형인 심층신뢰신경망은 아직까지는 합성곱신경망 이나 순환신경망보다는 상대적으로 주목을 덜 받고 있다. 그러나 심층신뢰신경망은 CNN이나 RNN과는 달리 비지도학습(unsupervised learning)모형이며, 사람이나 동물은 관찰을 통해서 스스로 학습한다는 점에서 궁극적으로는 비지도학습모형이 더 많이 연구되어야 할 주제가 될 것이다.Deep learning model is a kind of neural networks that allows multiple hidden layers. There are various deep learning architectures such as convolutional neural networks, deep belief networks and recurrent neural networks. Those have been applied to fields like computer vision, automatic speech recognition, natural language processing, audio recognition and bioinformatics where they have been shown to produce state-of-the-art results on various tasks. Among those architectures, convolutional neural networks and recurrent neural networks are classified as the supervised learning model. And in recent years, those supervised learning models have gained more popularity than unsupervised learning models such as deep belief networks, because supervised learning models have shown fashionable applications in such fields mentioned above.
Deep learning models can be trained with backpropagation algorithm. Backpropagation is an abbreviation for “backward propagation of errors” and a common method of training artificial neural networks used in conjunction with an optimization method such as gradient descent. The method calculates the gradient of an error function with respect to all the weights in the network. The gradient is fed to the optimization method which in turn uses it to update the weights, in an attempt to minimize the error function.
Convolutional neural networks use a special architecture which is particularly well-adapted to classify images. Using this architecture makes convolutional networks fast to train. This, in turn, helps us train deep, muti-layer networks, which are very good at classifying images. These days, deep convolutional networks are used in most neural networks for image recognition. Convolutional neural networks use three basic ideas: local receptive fields, shared weights, and pooling. By local receptive fields, we mean that each neuron in the first(or any) hidden layer will be connected to a small region of the input(or previous layer’s) neurons. Shared weights mean that we’re going to use the same weights and bias for each of the local receptive field. This means that all the neurons in the hidden layer detect exactly the same feature, just at different locations in the input image. In addition to the convolutional layers just described, convolutional neural networks also contain pooling layers. Pooling layers are usually used immediately after convolutional layers. What the pooling layers do is to simplify the information in the output from the convolutional layer.
Recent convolutional network architectures have 10 to 20 hidden layers and billions of connections between units. Training deep learning networks has taken weeks several years ago, but thanks to progress in GPU and algorithm enhancement, training time has reduced to several hours.
Neural networks with time-varying behavior are known as recurrent neural networks or RNNs. A recurrent neural network is a class of artificial neural network where connections between units form a directed cycle. This creates an internal state of the network which allows it to exhibit dynamic temporal behavior. Unlike feedforward neural networks, RNNs can use their internal memory to process arbitrary sequences of inputs. Early RNN models turned out to be very difficult to train, harder even than deep feedforward networks. The reason is the unstable gradient problem such as vanishing gradient and exploding gradient. The gradient can get smaller and smaller as it is propagated back through layers. This makes learning in early layers extremely slow. The problem actually gets worse in RNNs, since gradients aren’t just propagated backward through layers, they’re propagated backward through time. If the network runs for a long time, that can make the gradient extremely unstable and hard to learn from. It has been possible to incorporate an idea known as long short-term memory units (LSTMs) into RNNs. LSTMs make it much easier to get good results when training RNNs, and many recent papers make use of LSTMs or related ideas.Deep learning, Convolutional neural networks, Recurrent neural networks, Error backpropagation algorithm"
656,딥러닝분석과 기술적 분석 지표를 이용한 한국 코스피주가지수 방향성 예측,이우식 /Lee Woosik,2017,한국데이터정보과학회,"2016년 3월 구글 (Google)의 바둑인공지능 알파고 (AlphaGo)가 이세돌 9단과의 바둑대결에서승리한 이후 다양한 분야에서 인공지능 사용에 대한 관심이 높아지고 있는 가운데 금융투자 분야에서도 인공지능과 투자자문 전문가의 합성어인 로보어드바이저 (Robo-Advisor)에 대한 관심이 높아지고 있다. 인공지능 (artificial intelligence)기반의 의사결정은 비용 절감은 물론 효과적인 의사결정을가능하게 한다는 점에서 큰 장점이 있다. 본 연구에서는 기술적 분석 (technical analysis) 지표와 딥러닝 (deep learning) 모형을 결합하여 한국 코스피 지수를 예측하는 모형을 개발하고 제시한 모형들의 예측력을 비교, 분석한다. 분석 결과 기술적 분석 지표에 딥러닝 알고리즘을 결합한 모형이 주가지수 방향성 예측 문제에 응용될 수 있음을 확인하였다. 향후 본 연구에서 제안된 기술적 분석 지표와딥러닝모형을 결합한 기법은 로보어드바이저서비스에 응용할 수 있는 일반화 가능성을 보여준다Since Google’s AlphaGo defeated a world champion of Go players in 2016, there have been many interests in the deep learning. In the financial sector, a Robo-Advisor using deep learning gains a significant attention, which builds and manages portfolios of financial instruments for investors.In this paper, we have proposed the a deep learning algorithm geared toward identification and forecast of the KOSPI index direction,and we also have compared the accuracy of the prediction.In an application of forecasting the financial market index direction, we have shown that the Robo-Advisor using deep learning has a significant effect on finance industry. The Robo-Advisor collects a massive data such as earnings statements, news reports and regulatory filings, analyzes those and recommends investors how to view market trends and identify the best time to purchase financial assets. On the other hand, the Robo-Advisor allows businesses to learn more about their customers, develop better marketing strategies, increase sales and decrease costs.Artificial intelligence, deep learning, FinTech, Robo-Advisor, technical analysis."
657,딥러닝의 효율적인 학습을 위한 학습데이터 선별 기법에 관한 연구,주경돈 /Gyoung-don Joo,2017,한국정보과학회,"빅데이터의 증가와 병렬 하드웨어 처리, 학습 알고리즘의 개선은 딥러닝의 발전을 가져왔으며 인공지능이 적용 가능한 영역을 획기적으로 확장시키고 있다. 하지만 이러한 대규모의 학습 데이터에 기반한 딥러닝은 학습시간이 증가되고, 더 나아가 학습 데이터의 구축을 위한 레이블링(labeling) 비용이 데이터 수에 비례하여 증가하게 된다. 따라서 본 논문에서는, 딥러닝의 정확도를 최대한 유지하면서 필요한 학습 데이터의 수를 줄일수 있도록, 전체 데이터에서 제한된 수의 학습 데이터의 후보를 선별하는 기법에 대하여 연구한다. 특히 기존의 능동 학습에 기반한 기법들이 학습 데이터의 수가 매우 적어질 경우 딥러닝의 정확도가 급격히 떨어지는 문제를 해결할 수 있는 새로운 기법을 제안하였으며, 제안된 기법으로 선별된 학습 데이터는 그 수의 변화에 대해 딥러닝이 일관되게 경쟁력있는 정확도를 가짐을 다양한 실험을 통하여 검증하였다.The increase of big data, parallel hardware processing and enhanced learning algorithms have led to the development of deep learning and are dramatically expanding the domain of applications based on artificial intelligence. However, such large-scale training data extends the learning time, and furthermore, the labeling cost for building the training set increases in proportion to the volume of data. Therefore, in this paper, we study how to select a limited number of candidates for training set from given data so as to reduce the number of training set while maintaining the accuracy of deep learning. Especially, we found that, with the training set selected by the previous work, the accuracy of deep learning drastically decreases when the number of training set is very small and so proposed new methods to resolve this fall-off in accuracy. Through experiments, we showed that deep learning consistently has the competitive accuracy with the training set selected by the proposed method regardless of the size of training set.Deep Learning, Train Set Selection, Active Learning, Gamma distribution"
658,딥러닝을 이용한 범용적 스테그아날리시스,김현재 /Hyunjae Kim,2017,한국정보과학회,"스테그아날리시스(Steganalysis)란 이미지 등 일반적인 자료에 암호화된 정보를 은닉하는 스테가노그래피(Steganography)에 대한 검출 및 분석 방법으로, 기계학습 기반 방법론을 포함한다. 기존 기계학습 기반 스테그아날리시스는 영상(Image)의 특징(Feature) 추출 및 모델링에 기반하며, 최근 딥러닝(Deep Learning)의 적용으로 검출 정확도가 큰 폭으로 향상되었다. 하지만 현존하는 스테그아날리시스 모델은 단일 스테가노그래피 기법에 대해 국한되어 있어 학습에 사용되지 않은 스테고(Stego) 이미지의 경우 검출이 불가능한 결정적 한계를 가진다. 본 연구에서는 다양한 스테가노그래피 기법으로 생성된 스테고이미지에 딥러닝을 적용하여 스테그아날리시스를 학습하는 범용적 모델을 제안한다. 다양한 실험을 통해 제안 기법의 효용성 및 가능성을 확인하고, 범용적 스테그아날리시스 모델이 각각에 특화된 검출 기법과 유사한 정확도로 스테고 이미지를 검출할 수 있음을 보인다.Steganalysis is to detect information hidden by steganography inside general data such as images. There are stegoanalysis techniques that use machine learning (ML). Existing ML approaches to steganalysis are based on extracting features from stego images and modeling them.
Recently deep learning-based methodologies have shown significant improvements in detection accuracy. However, all the existing methods, including deep learning-based ones, have a critical limitation in that they can only detect stego images that are created by a specific steganography method. In this paper, we propose a generalized steganalysis method that can model multiple types of stego images using deep learning. Through various experiments, we confirm the effectiveness of our approach and envision directions for future research. In particular, we show that our method can detect each type of steganography with the same level of accuracy as that of a steganalysis method dedicated to that type of steganography, thereby demonstrating the general applicability of our approach to multiple types of stego images.steganalysis, steganography, deep learning, generalized steganalysis model"
659,"딥러닝 프레임워크의 비교: 티아노, 텐서플로, CNTK를 중심으로",정여진 /Yeojin Chung,2017,한국지능정보시스템학회,"딥러닝 프레임워크의 대표적인 기능으로는 ‘자동미분’과 ‘GPU의 활용’ 등을 들 수 있다. 본 논문은 파이썬의 라이브러리 형태로 사용 가능한 프레임워크 중에서 구글의 텐서플로와 마이크로소프트의 CNTK, 그리고 텐서 플로의 원조라고 할 수 있는 티아노를 비교하였다. 본문에서는 자동미분의 개념과 GPU의 활용형태를 간단히 설명하고, 그 다음에 logistic regression을 실행하는 예를 통하여 각 프레임워크의 문법을 알아본 뒤에, 마지막으로 대표적인 딥러닝 응용인 CNN의 예제를 실행시켜보고 코딩의 편의성과 실행속도 등을 확인해 보았다. 그 결과, 편의성의 관점에서 보면 티아노가 가장 코딩 하기가 어렵고, CNTK와 텐서플로는 많은 부분이 비슷하게 추상화 되어 있어서 코딩이 비슷하지만 가중치와 편향을 직접 정의하느냐의 여부에서 차이를 보였다. 그리고 각프레임워크의 실행속도에 대한 평가는 ‘큰 차이는 없다’는 것이다. 텐서플로는 티아노에 비하여 속도가 느리다는 평가가 있어왔는데, 본 연구의 실험에 의하면, 비록 CNN 모형에 국한되었지만, 텐서플로가 아주 조금이지만빠른 것으로 나타났다. CNTK의 경우에도, 비록 실험환경이 달랐지만, 실험환경의 차이에 의한 속도의 차이의 편차범위 이내에 있는 것으로 판단이 되었다. 본 연구에서는 세 종류의 딥러닝 프레임워크만을 살펴보았는데,위키피디아에 따르면 딥러닝 프레임워크의 종류는 12가지가 있으며, 각 프레임워크의 특징을 15가지 속성으로구분하여 차이를 특정하고 있다. 그 많은 속성 중에서 사용자의 입장에서 볼 때 중요한 속성은 어떤 언어(파이썬, C++, Java, 등)로 사용가능한지, 어떤 딥러닝 모형에 대한 라이브러리가 잘 구현되어 있는지 등일 것이다. 그리고 사용자가 대규모의 딥러닝 모형을 구축한다면, 다중 GPU 혹은 다중 서버를 지원하는지의 여부도 중요할 것이다. 또한 딥러닝 모형을 처음 학습하는 경우에는 사용설명서가 많은지 예제 프로그램이 많은지 여부도 중요한 기준이 될 것이다.The deep learning framework is software designed to help develop deep learning models. Some of its important functions include “automatic differentiation” and “utilization of GPU”. The list of popular deep learning framework includes Caffe (BVLC) and Theano (University of Montreal). And recently, Microsoft's deep learning framework, Microsoft Cognitive Toolkit, was released as open-source license, following Google’s Tensorflow a year earlier. The early deep learning frameworks have been developed mainly for research at universities. Beginning with the inception of Tensorflow, however, it seems that companies such as Microsoft and Facebook have started to join the competition of framework development. Given the trend, Google and other companies are expected to continue investing in the deep learning framework to bring forward the initiative in the artificial intelligence business. From this point of view, we think it is a good time to compare some of deep learning frameworks. So we compare three deep learning frameworks which can be used as a Python library. Those are Google's Tensorflow, Microsoft’s CNTK, and Theano which is sort of a predecessor of the preceding two.
The most common and important function of deep learning frameworks is the ability to perform automatic differentiation. Basically all the mathematical expressions of deep learning models can be represented as computational graphs, which consist of nodes and edges. Partial derivatives on each edge of a computational graph can then be obtained. With the partial derivatives, we can let software compute differentiation of any node with respect to any variable by utilizing chain rule of Calculus.
First of all, the convenience of coding is in the order of CNTK, Tensorflow, and Theano. The criterion is simply based on the lengths of the codes and the learning curve and the ease of coding are not the main concern. According to the criteria, Theano was the most difficult to implement with, and CNTK and Tensorflow were somewhat easier. With Tensorflow, we need to define weight variables and biases explicitly. The reason that CNTK and Tensorflow are easier to implement with is that those frameworks provide us with more abstraction than Theano. We, however, need to mention that low-level coding is not always bad. It gives us flexibility of coding. With the low-level coding such as in Theano, we can implement and test any new deep learning models or any new search methods that we can think of.
The assessment of the execution speed of each framework is that there is not meaningful difference. According to the experiment, execution speeds of Theano and Tensorflow are very similar, although the experiment was limited to a CNN model. In the case of CNTK, the experimental environment was not maintained as the same. The code written in CNTK has to be run in PC environment without GPU where codes execute as much as 50 times slower than with GPU. But we concluded that the difference of execution speed was within the range of variation caused by the different hardware setup.
In this study, we compared three types of deep learning framework: Theano, Tensorflow, and CNTK. According to Wikipedia, there are 12 available deep learning frameworks. And 15 different attributes differentiate each framework. Some of the important attributes would include interface language (Python, C ++, Java, etc.) and the availability of libraries on various deep learning models such as CNN, RNN, DBN, and etc. And if a user implements a large scale deep learning model, it will also be important to support multiple GPU or multiple servers. Also, if you are learning the deep learning model, it would also be important if there are enough examples and references.deep learning framework, Theano, TensorFlow, CNTK, computational graph, CIFAR-10"
660,딥러닝을 이용한 주변 무선단말 파악방안,이웅섭 /Lee， Woong-sup,2018,한국정보통신학회,"최근 단말-대-단말(Device-to-device, D2D) 통신기술이 차세대 무선통신시스템의 핵심기술로 큰 관심을 받고 있다. 이러한 단말간 통신에서는 자신의 주변에 어떠한 단말이 있는지 파악하는 주변단말 탐색(Neighbor discovery)이매우 중요하다. 본 논문에서는 최근 큰 관심을 받고 있는 딥러닝(Deep learning) 기술을 활용하여 단말간 통신에서 주변단말을 파악하는 방안에 대해서 제안한다. 제안 방안은 기존의 방안과 달리 무선채널의 공간적 연관성을 이용하여단말간의 신호 전송 없이 단말이 기지국으로 전송하는 상향링크 파일럿 신호를 기반으로 주변 단말을 찾고 따라서기존의 방식에 비해 신호전송 복잡도(signaling complexity)를 크게 줄일 수 있다. 또한 제안 방안에서는 떨어져 있는거리에 따라서 주변 단말을 분류 가능하여 기존 방안에 비해서 좀 더 세밀한 단말 탐색이 가능하다. 마지막으로 본 논문에서는 tensorflow를 이용한 컴퓨터 시뮬레이션을 통해 제안 방안의 성능을 검증하였다.Recently, the device-to-device (D2D) communication has been conceived as the key technology for the next-generationmobile communication systems. The neighbor discovery in which the nearby users are found, is essential for the properoperation of the D2D communication. In this paper, we propose new neighbor discovery scheme based on deep learningtechnology which has gained a lot of attention recently. In the proposed scheme, the neighboring users can be foundusing the uplink pilot transmission of users only, unlike conventional neighbor discovery schemes in which direct pilotcommunication among users is required, such that the signaling overhead can be greatly reduced in our proposed scheme.
Moreover, the neighbors with different proximity can also be classified accordingly which enables more accurate neighbordiscovery compared to the conventional schemes. The performance of our proposed scheme is verified through thetensorflow-based computer simulations.
키워드 : 딥러닝, 주변단말 탐색, 단말-대-단말 통신Deep learning, Neighbor discovery, Device-to-device communication, Wireless channel correlation"
661,신뢰성있는 딥러닝 기반 분석 모델을 참조하기 위한 딥러닝 기술 언어,문종혁 /Jong Hyeok Mun,2021,한국정보처리학회,"최근 딥러닝은 하드웨어 성능이 향상됨에 따라 자연어 처리, 영상 인식 등의 다양한 기술에 접목되어 활용되고 있다. 이러한 기술들을 활용해 지능형교통 시스템(ITS), 스마트홈, 헬스케어 등의 산업분야에서 데이터를 분석하여 고속도로 속도위반 차량 검출, 에너지 사용량 제어, 응급상황 등과 같은고품질의 서비스를 제공하며, 고품질의 서비스를 제공하기 위해서는 정확도가 향상된 딥러닝 모델이 적용되어야 한다. 이를 위해 서비스 환경의 데이터를분석하기 위한 딥러닝 모델을 개발할 때, 개발자는 신뢰성이 검증된 최신의 딥러닝 모델을 적용할 수 있어야 한다. 이는 개발자가 참조하는 딥러닝 모델에적용된 학습 데이터셋의 정확도를 측정하여 검증할 수 있다. 이러한 검증을 위해서 개발자는 학습 데이터셋, 딥러닝의 계층구조 및 개발 환경 등과 같은내용을 포함하는 딥러닝 모델을 문서화하여 적용하기 위한 구조적인 정보가 필요하다. 본 논문에서는 신뢰성있는 딥러닝 기반 데이터 분석 모델을 참조하기위한 딥러닝 기술 언어를 제안한다. 제안하는 기술 언어는 신뢰성 있는 딥러닝 모델을 개발하는데 필요한 학습데이터셋, 개발 환경 및 설정 등의 정보와더불어 딥러닝 모델의 계층구조를 표현할 수 있다. 제안하는 딥러닝 기술 언어를 이용하여 개발자는 지능형 교통 시스템에서 참조하는 분석 모델의정확도를 검증할 수 있다. 실험에서는 제안하는 언어의 유효성을 검증하기 위해, 번호판 인식 모델을 중심으로 딥러닝 기술 문서의 적용과정을 보인다With the recent advancements of deep learning, companies such as smart home, healthcare, and intelligent transportation systems areutilizing its functionality to provide high-quality services for vehicle detection, emergency situation detection, and controlling energyconsumption. To provide reliable services in such sensitive systems, deep learning models are required to have high accuracy. In orderto develop a deep learning model for analyzing previously mentioned services, developers should utilize the state of the art deep learningmodels that have already been verified for higher accuracy. The developers can verify the accuracy of the referenced model by validatingthe model on the dataset. For this validation, the developer needs structural information to document and apply deep learning models,including metadata such as learning dataset, network architecture, and development environments. In this paper, we propose a descriptionlanguage that represents the network architecture of the deep learning model along with its metadata that are necessary to develop adeep learning model. Through the proposed description language, developers can easily verify the accuracy of the referenced deep learningmodel. Our experiments demonstrate the application scenario of a deep learning description document that focuses on the license platerecognition for the detection of illegally parked vehicles.Trusted Deep Learning, Model Reference, Deep Learning Description Language, Traffic Situation Analysis Model"
662,딥러닝 분석을 이용한 중국 역내·외 위안화 변동성 예측,이우식 /Lee Woosik,2016,한국데이터정보과학회,"2008년 글로벌 금융위기 이후 중국은 위안화 국제화의 점진적 추진을 진행하면서 중국상하이 외환시장과 중국홍콩 외환시장에서 거래되는 통화인 역내위안화와 역외위안화를 형성시켰다. 본 연구는 위안화 국제화와 점진적인 중국 자본계정 개방에 따라 급변하는 외환시장상황의 변동성을 정확하게 파악하기 위해서 GARCH모형 (일반화된 자기회귀 조건부이분산성모형)에 다단계인공신경망을 결합한 MLP-GARCH 모형과 GARCH모형과 기계학습의 일종인 딥러닝 (deep learning)을 통합한 DL-GARCH을 가지고 위안화 변동성예측을 비교 실험과 분석을 하였다. 비교분석 결과 DLGARCH 모형은 MLP-GARCH보다 모형 위안화 역내·외 환율변동성 예측 면에서 더욱 더 개선된예측값을 제공하였다. 그래서 이분산시계열모형을 딥러닝과 결합한 DL-GARCH 모형은 시계열의환율 변동성 예측 문제에 딥러닝을 응용할 수 있음을 확인하였다. 향후 이분산시계열과 결합된 딥러닝 모형은 다른 금융시계열 데이터에 응용하여 그 일반화 가능성을 높일 수 있을 것이다.The People’s Republic of China has vigorously been pursuing the internationalization of the Chinese Yuan or Renminbi after the financial crisis of 2008. In this view, an abrupt increase of use of the Chinese Yuan in the onshore and offshore markets are important milestones to be one of important currencies. One of the most frequently used methods to forecast volatility is GARCH model. Since a prediction error of the GARCH model has been reported quite high, a lot of efforts have been made to improve forecasting capability of the GARCH model. In this paper, we have proposed MLPGARCH and a DL-GARCH by employing Artificial Neural Network to the GARCH. In an application to forecasting Chinese Yuan volatility, we have successfully shown their overall outperformance in forecasting over the GARCH.Deep learning, DL-GARCH, GARCH, The Chinese Yuan, volatility."
663,딥러닝 기법을 이용한 내일강수 예측,하지훈,2016,한국지능시스템학회,"정확한 강수예측을 위해서는 예측인자 선정과 예측방법에 대한 선택이 매우 중요하다. 최근에는 강수예측 방법으로기계학습 기법이 많이 사용되고 있으며, 그 중에서도 특히 인공신경망을 사용한 강수예측 방법은 좋은 성능을 보였다. 본논문에서는 딥러닝 기법 중 하나인 DBN(deep belief network)를 이용한 새로운 강수예측 방법을 제안한다. DBN는 비지도사전 학습을 통해 초기 가중치를 설정하여 기존 인공신경망의 문제점을 보완한다. 예측인자로는 기온, 전일-전주 강수일, 태양과 달 궤도 관련 자료를 선정하였다. 기온과 전일-전주 강수일은 서울에서의 1974년부터 2013년까지 총 40년간의AWS(automatic weather system) 관측 자료를 사용하였고, 태양과 달의 궤도 관련 자료는 서울을 중심으로 계산한 결과를사용하였다. 전체 기간에서 일부는 학습 자료로 사용하여 예측모델을 생성하였고, 나머지를 생성한 모델의 검증 자료로사용하였다. 모델 검증 결과로 나온 예측값들은 확률값을 가지며 임계치를 이용하여 강수유무를 판별하였다. 강수 정확도의척도로 양분예보기법 중 CSI(critical successive index)와 Bias(frequency bias)를 계산하였다. 이를 통해 DBN와 MLP(multilayer perceptron)의 성능을 비교한 결과 DBN의 강수 예측 정확도가 높았고, 수행속도 또한 2배 이상 빨랐다.For accurate precipitation forecasts the choice of weather faoctrs and prediction method is very important. Recently, machinele arning has been widely used for forecasting precipitation, and artificial neural network, one of machine learning techniques, showed good performance. In this paper, we suggest a new method for forecasting precipitation using DBN, one of deep learning techniques.
DBN has an advantage that initial weights are set by unsupervised learning, so this compensates for the defects of artificial neural networks. We used past precipitation, temperature, and the parameters of the sun and moon's motion as features for forecasting precipitation. The dataset consists of observation data which had been measured for 40 years from AWS in Seoul. Experiments were based on 8-fold cross validation. As a result of estimation, we got probabilities of test dataset, so threshold was used for the decision of precipitation. CSI and Bias were used for indicating the precision of precipitation. Our experimental results showed that DBN performed better than MLP.Deep learning, Deep belief network, Precipitation, Forecast"
664,딥러닝 신경망모형을 이용한 실시간 교통정보수집 연구,김대현 /KIM DAE HYON,2016,사단법인 인문사회과학기술융합학회,"비디오 기반의 실시간 교통정보수집 시스템은 자율주행을 포함한 지능형교통체계의 주요 서브시스템중 하나이며, 우수한 예측(추정)력 때문에 핵심 알고리즘으로 인공신경망 모형이 주로 사용되고 있다. 특히, 최근 과학 학술지 네이처에 구글 딥마인드의 알파고가 공개되면서 딥러닝과 인공지능에 대한 전 세계적인 관심이 집중되고 있다. 최근까지 가장 널리 사용된 인공신경망모형은 역전파 모형이었으나 최근 컴퓨터 하드웨어 및 알고리즘의 새로운 개발과 더불어 딥러닝 알고리즘을 이용한 딥인공신경망 모형에 대한 관심이 높아지고 있다. 그러나 아직 딥러닝 인공신경망에 대한 연구는 초기단계에 있는 실정이며 적용분야도 극히 한정되어 있다. 본 연구에서는 실시간 도로교통정보 수집을 위한 영상처리분야에서 딥러닝 알고리즘을 활용하고 실험결과를 통하여 향후 교통정보수집‧분석 분야에서의 딥러닝 인공신경망 모형의 적용가능성을 살펴보고자 한다. 특히, 본 연구에서 도출한 딥러닝 인공신경망모형의 실험결과를 기존 역전파 모형에 의한 실험결과와 비교하여 모형의 우수성을 비교‧평가하고자 한다.Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Normally, artificial neural networks have been used in real-time applications for collecting traffic information because of their remarkable performance in terms of predictive accuracy and real-time computing. Especially, the recent publication of a paper in the journal Nature and the success of Google DeepMind’s world-beating Go program AlphaGo caused significant increases of the public interest in artificial intelligence. Even though various types of learning models have been proposed, the Backpropagation has been the most popular neural network model. However, the Deep learning neural network model has attracted much interest recently. In this study, the Deep leaning neural network will be applied in image processing techniques for the real-time traffic information collection to investigate the performance of Deep learning algorithm in traffic engineering fields. In addition, the experimental results from Deep learing algorithm will be compared with the results from Backpropagation model to evaluate and propose better performing model.Intelligent Transportation Systems (ITS), artificial neural networks, Deep learning, traffic information, image processing"
665,딥러닝의 모형과 응용사례,안성만 /SungMahn Ahn,2016,한국지능정보시스템학회,"딥러닝은 인공신경망(neural network)이라는 인공지능분야의 모형이 발전된 형태로서, 계층구조로 이루어진 인공신경망의 내부계층(hidden layer)이 여러 단계로 이루어진 구조이다. 딥러닝에서의 주요 모형은 합성곱신경망(convolutional neural network), 순환신경망(recurrent neural network), 그리고 심층신뢰신경망(deep belief network)의 세가지라고 할 수있다. 그 중에서 현재 흥미로운 연구가 많이 발표되어서 관심이 집중되고 있는 모형은 지도학습(supervised learning)모형인 처음 두 개의 모형이다. 따라서 본 논문에서는 지도학습모형의 가중치를 최적화하는 기본적인 방법인 오류역전파 알고리즘을 살펴본 뒤에 합성곱신경망과 순환신경망의 구조와 응용사례 등을 살펴보고자 한다. 본문에서 다루지 않은 모형인 심층신뢰신경망은 아직까지는 합성곱신경망 이나 순환신경망보다는 상대적으로 주목을 덜 받고 있다. 그러나 심층신뢰신경망은 CNN이나 RNN과는 달리 비지도학습(unsupervised learning)모형이며, 사람이나 동물은 관찰을 통해서 스스로 학습한다는 점에서 궁극적으로는 비지도학습모형이 더 많이 연구되어야 할 주제가 될 것이다.Deep learning model is a kind of neural networks that allows multiple hidden layers. There are various deep learning architectures such as convolutional neural networks, deep belief networks and recurrent neural networks. Those have been applied to fields like computer vision, automatic speech recognition, natural language processing, audio recognition and bioinformatics where they have been shown to produce state-of-the-art results on various tasks. Among those architectures, convolutional neural networks and recurrent neural networks are classified as the supervised learning model. And in recent years, those supervised learning models have gained more popularity than unsupervised learning models such as deep belief networks, because supervised learning models have shown fashionable applications in such fields mentioned above.
Deep learning models can be trained with backpropagation algorithm. Backpropagation is an abbreviation for “backward propagation of errors” and a common method of training artificial neural networks used in conjunction with an optimization method such as gradient descent. The method calculates the gradient of an error function with respect to all the weights in the network. The gradient is fed to the optimization method which in turn uses it to update the weights, in an attempt to minimize the error function.
Convolutional neural networks use a special architecture which is particularly well-adapted to classify images. Using this architecture makes convolutional networks fast to train. This, in turn, helps us train deep, muti-layer networks, which are very good at classifying images. These days, deep convolutional networks are used in most neural networks for image recognition. Convolutional neural networks use three basic ideas: local receptive fields, shared weights, and pooling. By local receptive fields, we mean that each neuron in the first(or any) hidden layer will be connected to a small region of the input(or previous layer’s) neurons. Shared weights mean that we’re going to use the same weights and bias for each of the local receptive field. This means that all the neurons in the hidden layer detect exactly the same feature, just at different locations in the input image. In addition to the convolutional layers just described, convolutional neural networks also contain pooling layers. Pooling layers are usually used immediately after convolutional layers. What the pooling layers do is to simplify the information in the output from the convolutional layer.
Recent convolutional network architectures have 10 to 20 hidden layers and billions of connections between units. Training deep learning networks has taken weeks several years ago, but thanks to progress in GPU and algorithm enhancement, training time has reduced to several hours.
Neural networks with time-varying behavior are known as recurrent neural networks or RNNs. A recurrent neural network is a class of artificial neural network where connections between units form a directed cycle. This creates an internal state of the network which allows it to exhibit dynamic temporal behavior. Unlike feedforward neural networks, RNNs can use their internal memory to process arbitrary sequences of inputs. Early RNN models turned out to be very difficult to train, harder even than deep feedforward networks. The reason is the unstable gradient problem such as vanishing gradient and exploding gradient. The gradient can get smaller and smaller as it is propagated back through layers. This makes learning in early layers extremely slow. The problem actually gets worse in RNNs, since gradients aren’t just propagated backward through layers, they’re propagated backward through time. If the network runs for a long time, that can make the gradient extremely unstable and hard to learn from. It has been possible to incorporate an idea known as long short-term memory units (LSTMs) into RNNs. LSTMs make it much easier to get good results when training RNNs, and many recent papers make use of LSTMs or related ideas.Deep learning, Convolutional neural networks, Recurrent neural networks, Error backpropagation algorithm"
666,딥러닝분석과 기술적 분석 지표를 이용한 한국 코스피주가지수 방향성 예측,이우식 /Lee Woosik,2017,한국데이터정보과학회,"2016년 3월 구글 (Google)의 바둑인공지능 알파고 (AlphaGo)가 이세돌 9단과의 바둑대결에서승리한 이후 다양한 분야에서 인공지능 사용에 대한 관심이 높아지고 있는 가운데 금융투자 분야에서도 인공지능과 투자자문 전문가의 합성어인 로보어드바이저 (Robo-Advisor)에 대한 관심이 높아지고 있다. 인공지능 (artificial intelligence)기반의 의사결정은 비용 절감은 물론 효과적인 의사결정을가능하게 한다는 점에서 큰 장점이 있다. 본 연구에서는 기술적 분석 (technical analysis) 지표와 딥러닝 (deep learning) 모형을 결합하여 한국 코스피 지수를 예측하는 모형을 개발하고 제시한 모형들의 예측력을 비교, 분석한다. 분석 결과 기술적 분석 지표에 딥러닝 알고리즘을 결합한 모형이 주가지수 방향성 예측 문제에 응용될 수 있음을 확인하였다. 향후 본 연구에서 제안된 기술적 분석 지표와딥러닝모형을 결합한 기법은 로보어드바이저서비스에 응용할 수 있는 일반화 가능성을 보여준다Since Google’s AlphaGo defeated a world champion of Go players in 2016, there have been many interests in the deep learning. In the financial sector, a Robo-Advisor using deep learning gains a significant attention, which builds and manages portfolios of financial instruments for investors.In this paper, we have proposed the a deep learning algorithm geared toward identification and forecast of the KOSPI index direction,and we also have compared the accuracy of the prediction.In an application of forecasting the financial market index direction, we have shown that the Robo-Advisor using deep learning has a significant effect on finance industry. The Robo-Advisor collects a massive data such as earnings statements, news reports and regulatory filings, analyzes those and recommends investors how to view market trends and identify the best time to purchase financial assets. On the other hand, the Robo-Advisor allows businesses to learn more about their customers, develop better marketing strategies, increase sales and decrease costs.Artificial intelligence, deep learning, FinTech, Robo-Advisor, technical analysis."
667,딥러닝의 효율적인 학습을 위한 학습데이터 선별 기법에 관한 연구,주경돈 /Gyoung-don Joo,2017,한국정보과학회,"빅데이터의 증가와 병렬 하드웨어 처리, 학습 알고리즘의 개선은 딥러닝의 발전을 가져왔으며 인공지능이 적용 가능한 영역을 획기적으로 확장시키고 있다. 하지만 이러한 대규모의 학습 데이터에 기반한 딥러닝은 학습시간이 증가되고, 더 나아가 학습 데이터의 구축을 위한 레이블링(labeling) 비용이 데이터 수에 비례하여 증가하게 된다. 따라서 본 논문에서는, 딥러닝의 정확도를 최대한 유지하면서 필요한 학습 데이터의 수를 줄일수 있도록, 전체 데이터에서 제한된 수의 학습 데이터의 후보를 선별하는 기법에 대하여 연구한다. 특히 기존의 능동 학습에 기반한 기법들이 학습 데이터의 수가 매우 적어질 경우 딥러닝의 정확도가 급격히 떨어지는 문제를 해결할 수 있는 새로운 기법을 제안하였으며, 제안된 기법으로 선별된 학습 데이터는 그 수의 변화에 대해 딥러닝이 일관되게 경쟁력있는 정확도를 가짐을 다양한 실험을 통하여 검증하였다.The increase of big data, parallel hardware processing and enhanced learning algorithms have led to the development of deep learning and are dramatically expanding the domain of applications based on artificial intelligence. However, such large-scale training data extends the learning time, and furthermore, the labeling cost for building the training set increases in proportion to the volume of data. Therefore, in this paper, we study how to select a limited number of candidates for training set from given data so as to reduce the number of training set while maintaining the accuracy of deep learning. Especially, we found that, with the training set selected by the previous work, the accuracy of deep learning drastically decreases when the number of training set is very small and so proposed new methods to resolve this fall-off in accuracy. Through experiments, we showed that deep learning consistently has the competitive accuracy with the training set selected by the proposed method regardless of the size of training set.Deep Learning, Train Set Selection, Active Learning, Gamma distribution"
668,딥러닝을 이용한 범용적 스테그아날리시스,김현재 /Hyunjae Kim,2017,한국정보과학회,"스테그아날리시스(Steganalysis)란 이미지 등 일반적인 자료에 암호화된 정보를 은닉하는 스테가노그래피(Steganography)에 대한 검출 및 분석 방법으로, 기계학습 기반 방법론을 포함한다. 기존 기계학습 기반 스테그아날리시스는 영상(Image)의 특징(Feature) 추출 및 모델링에 기반하며, 최근 딥러닝(Deep Learning)의 적용으로 검출 정확도가 큰 폭으로 향상되었다. 하지만 현존하는 스테그아날리시스 모델은 단일 스테가노그래피 기법에 대해 국한되어 있어 학습에 사용되지 않은 스테고(Stego) 이미지의 경우 검출이 불가능한 결정적 한계를 가진다. 본 연구에서는 다양한 스테가노그래피 기법으로 생성된 스테고이미지에 딥러닝을 적용하여 스테그아날리시스를 학습하는 범용적 모델을 제안한다. 다양한 실험을 통해 제안 기법의 효용성 및 가능성을 확인하고, 범용적 스테그아날리시스 모델이 각각에 특화된 검출 기법과 유사한 정확도로 스테고 이미지를 검출할 수 있음을 보인다.Steganalysis is to detect information hidden by steganography inside general data such as images. There are stegoanalysis techniques that use machine learning (ML). Existing ML approaches to steganalysis are based on extracting features from stego images and modeling them.
Recently deep learning-based methodologies have shown significant improvements in detection accuracy. However, all the existing methods, including deep learning-based ones, have a critical limitation in that they can only detect stego images that are created by a specific steganography method. In this paper, we propose a generalized steganalysis method that can model multiple types of stego images using deep learning. Through various experiments, we confirm the effectiveness of our approach and envision directions for future research. In particular, we show that our method can detect each type of steganography with the same level of accuracy as that of a steganalysis method dedicated to that type of steganography, thereby demonstrating the general applicability of our approach to multiple types of stego images.steganalysis, steganography, deep learning, generalized steganalysis model"
669,"딥러닝 프레임워크의 비교: 티아노, 텐서플로, CNTK를 중심으로",정여진 /Yeojin Chung,2017,한국지능정보시스템학회,"딥러닝 프레임워크의 대표적인 기능으로는 ‘자동미분’과 ‘GPU의 활용’ 등을 들 수 있다. 본 논문은 파이썬의 라이브러리 형태로 사용 가능한 프레임워크 중에서 구글의 텐서플로와 마이크로소프트의 CNTK, 그리고 텐서 플로의 원조라고 할 수 있는 티아노를 비교하였다. 본문에서는 자동미분의 개념과 GPU의 활용형태를 간단히 설명하고, 그 다음에 logistic regression을 실행하는 예를 통하여 각 프레임워크의 문법을 알아본 뒤에, 마지막으로 대표적인 딥러닝 응용인 CNN의 예제를 실행시켜보고 코딩의 편의성과 실행속도 등을 확인해 보았다. 그 결과, 편의성의 관점에서 보면 티아노가 가장 코딩 하기가 어렵고, CNTK와 텐서플로는 많은 부분이 비슷하게 추상화 되어 있어서 코딩이 비슷하지만 가중치와 편향을 직접 정의하느냐의 여부에서 차이를 보였다. 그리고 각프레임워크의 실행속도에 대한 평가는 ‘큰 차이는 없다’는 것이다. 텐서플로는 티아노에 비하여 속도가 느리다는 평가가 있어왔는데, 본 연구의 실험에 의하면, 비록 CNN 모형에 국한되었지만, 텐서플로가 아주 조금이지만빠른 것으로 나타났다. CNTK의 경우에도, 비록 실험환경이 달랐지만, 실험환경의 차이에 의한 속도의 차이의 편차범위 이내에 있는 것으로 판단이 되었다. 본 연구에서는 세 종류의 딥러닝 프레임워크만을 살펴보았는데,위키피디아에 따르면 딥러닝 프레임워크의 종류는 12가지가 있으며, 각 프레임워크의 특징을 15가지 속성으로구분하여 차이를 특정하고 있다. 그 많은 속성 중에서 사용자의 입장에서 볼 때 중요한 속성은 어떤 언어(파이썬, C++, Java, 등)로 사용가능한지, 어떤 딥러닝 모형에 대한 라이브러리가 잘 구현되어 있는지 등일 것이다. 그리고 사용자가 대규모의 딥러닝 모형을 구축한다면, 다중 GPU 혹은 다중 서버를 지원하는지의 여부도 중요할 것이다. 또한 딥러닝 모형을 처음 학습하는 경우에는 사용설명서가 많은지 예제 프로그램이 많은지 여부도 중요한 기준이 될 것이다.The deep learning framework is software designed to help develop deep learning models. Some of its important functions include “automatic differentiation” and “utilization of GPU”. The list of popular deep learning framework includes Caffe (BVLC) and Theano (University of Montreal). And recently, Microsoft's deep learning framework, Microsoft Cognitive Toolkit, was released as open-source license, following Google’s Tensorflow a year earlier. The early deep learning frameworks have been developed mainly for research at universities. Beginning with the inception of Tensorflow, however, it seems that companies such as Microsoft and Facebook have started to join the competition of framework development. Given the trend, Google and other companies are expected to continue investing in the deep learning framework to bring forward the initiative in the artificial intelligence business. From this point of view, we think it is a good time to compare some of deep learning frameworks. So we compare three deep learning frameworks which can be used as a Python library. Those are Google's Tensorflow, Microsoft’s CNTK, and Theano which is sort of a predecessor of the preceding two.
The most common and important function of deep learning frameworks is the ability to perform automatic differentiation. Basically all the mathematical expressions of deep learning models can be represented as computational graphs, which consist of nodes and edges. Partial derivatives on each edge of a computational graph can then be obtained. With the partial derivatives, we can let software compute differentiation of any node with respect to any variable by utilizing chain rule of Calculus.
First of all, the convenience of coding is in the order of CNTK, Tensorflow, and Theano. The criterion is simply based on the lengths of the codes and the learning curve and the ease of coding are not the main concern. According to the criteria, Theano was the most difficult to implement with, and CNTK and Tensorflow were somewhat easier. With Tensorflow, we need to define weight variables and biases explicitly. The reason that CNTK and Tensorflow are easier to implement with is that those frameworks provide us with more abstraction than Theano. We, however, need to mention that low-level coding is not always bad. It gives us flexibility of coding. With the low-level coding such as in Theano, we can implement and test any new deep learning models or any new search methods that we can think of.
The assessment of the execution speed of each framework is that there is not meaningful difference. According to the experiment, execution speeds of Theano and Tensorflow are very similar, although the experiment was limited to a CNN model. In the case of CNTK, the experimental environment was not maintained as the same. The code written in CNTK has to be run in PC environment without GPU where codes execute as much as 50 times slower than with GPU. But we concluded that the difference of execution speed was within the range of variation caused by the different hardware setup.
In this study, we compared three types of deep learning framework: Theano, Tensorflow, and CNTK. According to Wikipedia, there are 12 available deep learning frameworks. And 15 different attributes differentiate each framework. Some of the important attributes would include interface language (Python, C ++, Java, etc.) and the availability of libraries on various deep learning models such as CNN, RNN, DBN, and etc. And if a user implements a large scale deep learning model, it will also be important to support multiple GPU or multiple servers. Also, if you are learning the deep learning model, it would also be important if there are enough examples and references.deep learning framework, Theano, TensorFlow, CNTK, computational graph, CIFAR-10"
670,딥러닝을 이용한 주변 무선단말 파악방안,이웅섭 /Lee， Woong-sup,2018,한국정보통신학회,"최근 단말-대-단말(Device-to-device, D2D) 통신기술이 차세대 무선통신시스템의 핵심기술로 큰 관심을 받고 있다. 이러한 단말간 통신에서는 자신의 주변에 어떠한 단말이 있는지 파악하는 주변단말 탐색(Neighbor discovery)이매우 중요하다. 본 논문에서는 최근 큰 관심을 받고 있는 딥러닝(Deep learning) 기술을 활용하여 단말간 통신에서 주변단말을 파악하는 방안에 대해서 제안한다. 제안 방안은 기존의 방안과 달리 무선채널의 공간적 연관성을 이용하여단말간의 신호 전송 없이 단말이 기지국으로 전송하는 상향링크 파일럿 신호를 기반으로 주변 단말을 찾고 따라서기존의 방식에 비해 신호전송 복잡도(signaling complexity)를 크게 줄일 수 있다. 또한 제안 방안에서는 떨어져 있는거리에 따라서 주변 단말을 분류 가능하여 기존 방안에 비해서 좀 더 세밀한 단말 탐색이 가능하다. 마지막으로 본 논문에서는 tensorflow를 이용한 컴퓨터 시뮬레이션을 통해 제안 방안의 성능을 검증하였다.Recently, the device-to-device (D2D) communication has been conceived as the key technology for the next-generationmobile communication systems. The neighbor discovery in which the nearby users are found, is essential for the properoperation of the D2D communication. In this paper, we propose new neighbor discovery scheme based on deep learningtechnology which has gained a lot of attention recently. In the proposed scheme, the neighboring users can be foundusing the uplink pilot transmission of users only, unlike conventional neighbor discovery schemes in which direct pilotcommunication among users is required, such that the signaling overhead can be greatly reduced in our proposed scheme.
Moreover, the neighbors with different proximity can also be classified accordingly which enables more accurate neighbordiscovery compared to the conventional schemes. The performance of our proposed scheme is verified through thetensorflow-based computer simulations.
키워드 : 딥러닝, 주변단말 탐색, 단말-대-단말 통신Deep learning, Neighbor discovery, Device-to-device communication, Wireless channel correlation"
671,신뢰성있는 딥러닝 기반 분석 모델을 참조하기 위한 딥러닝 기술 언어,문종혁 /Jong Hyeok Mun,2021,한국정보처리학회,"최근 딥러닝은 하드웨어 성능이 향상됨에 따라 자연어 처리, 영상 인식 등의 다양한 기술에 접목되어 활용되고 있다. 이러한 기술들을 활용해 지능형교통 시스템(ITS), 스마트홈, 헬스케어 등의 산업분야에서 데이터를 분석하여 고속도로 속도위반 차량 검출, 에너지 사용량 제어, 응급상황 등과 같은고품질의 서비스를 제공하며, 고품질의 서비스를 제공하기 위해서는 정확도가 향상된 딥러닝 모델이 적용되어야 한다. 이를 위해 서비스 환경의 데이터를분석하기 위한 딥러닝 모델을 개발할 때, 개발자는 신뢰성이 검증된 최신의 딥러닝 모델을 적용할 수 있어야 한다. 이는 개발자가 참조하는 딥러닝 모델에적용된 학습 데이터셋의 정확도를 측정하여 검증할 수 있다. 이러한 검증을 위해서 개발자는 학습 데이터셋, 딥러닝의 계층구조 및 개발 환경 등과 같은내용을 포함하는 딥러닝 모델을 문서화하여 적용하기 위한 구조적인 정보가 필요하다. 본 논문에서는 신뢰성있는 딥러닝 기반 데이터 분석 모델을 참조하기위한 딥러닝 기술 언어를 제안한다. 제안하는 기술 언어는 신뢰성 있는 딥러닝 모델을 개발하는데 필요한 학습데이터셋, 개발 환경 및 설정 등의 정보와더불어 딥러닝 모델의 계층구조를 표현할 수 있다. 제안하는 딥러닝 기술 언어를 이용하여 개발자는 지능형 교통 시스템에서 참조하는 분석 모델의정확도를 검증할 수 있다. 실험에서는 제안하는 언어의 유효성을 검증하기 위해, 번호판 인식 모델을 중심으로 딥러닝 기술 문서의 적용과정을 보인다With the recent advancements of deep learning, companies such as smart home, healthcare, and intelligent transportation systems areutilizing its functionality to provide high-quality services for vehicle detection, emergency situation detection, and controlling energyconsumption. To provide reliable services in such sensitive systems, deep learning models are required to have high accuracy. In orderto develop a deep learning model for analyzing previously mentioned services, developers should utilize the state of the art deep learningmodels that have already been verified for higher accuracy. The developers can verify the accuracy of the referenced model by validatingthe model on the dataset. For this validation, the developer needs structural information to document and apply deep learning models,including metadata such as learning dataset, network architecture, and development environments. In this paper, we propose a descriptionlanguage that represents the network architecture of the deep learning model along with its metadata that are necessary to develop adeep learning model. Through the proposed description language, developers can easily verify the accuracy of the referenced deep learningmodel. Our experiments demonstrate the application scenario of a deep learning description document that focuses on the license platerecognition for the detection of illegally parked vehicles.Trusted Deep Learning, Model Reference, Deep Learning Description Language, Traffic Situation Analysis Model"
672,딥러닝 분석을 이용한 중국 역내·외 위안화 변동성 예측,이우식 /Lee Woosik,2016,한국데이터정보과학회,"2008년 글로벌 금융위기 이후 중국은 위안화 국제화의 점진적 추진을 진행하면서 중국상하이 외환시장과 중국홍콩 외환시장에서 거래되는 통화인 역내위안화와 역외위안화를 형성시켰다. 본 연구는 위안화 국제화와 점진적인 중국 자본계정 개방에 따라 급변하는 외환시장상황의 변동성을 정확하게 파악하기 위해서 GARCH모형 (일반화된 자기회귀 조건부이분산성모형)에 다단계인공신경망을 결합한 MLP-GARCH 모형과 GARCH모형과 기계학습의 일종인 딥러닝 (deep learning)을 통합한 DL-GARCH을 가지고 위안화 변동성예측을 비교 실험과 분석을 하였다. 비교분석 결과 DLGARCH 모형은 MLP-GARCH보다 모형 위안화 역내·외 환율변동성 예측 면에서 더욱 더 개선된예측값을 제공하였다. 그래서 이분산시계열모형을 딥러닝과 결합한 DL-GARCH 모형은 시계열의환율 변동성 예측 문제에 딥러닝을 응용할 수 있음을 확인하였다. 향후 이분산시계열과 결합된 딥러닝 모형은 다른 금융시계열 데이터에 응용하여 그 일반화 가능성을 높일 수 있을 것이다.The People’s Republic of China has vigorously been pursuing the internationalization of the Chinese Yuan or Renminbi after the financial crisis of 2008. In this view, an abrupt increase of use of the Chinese Yuan in the onshore and offshore markets are important milestones to be one of important currencies. One of the most frequently used methods to forecast volatility is GARCH model. Since a prediction error of the GARCH model has been reported quite high, a lot of efforts have been made to improve forecasting capability of the GARCH model. In this paper, we have proposed MLPGARCH and a DL-GARCH by employing Artificial Neural Network to the GARCH. In an application to forecasting Chinese Yuan volatility, we have successfully shown their overall outperformance in forecasting over the GARCH.Deep learning, DL-GARCH, GARCH, The Chinese Yuan, volatility."
673,딥러닝 기법을 이용한 내일강수 예측,하지훈,2016,한국지능시스템학회,"정확한 강수예측을 위해서는 예측인자 선정과 예측방법에 대한 선택이 매우 중요하다. 최근에는 강수예측 방법으로기계학습 기법이 많이 사용되고 있으며, 그 중에서도 특히 인공신경망을 사용한 강수예측 방법은 좋은 성능을 보였다. 본논문에서는 딥러닝 기법 중 하나인 DBN(deep belief network)를 이용한 새로운 강수예측 방법을 제안한다. DBN는 비지도사전 학습을 통해 초기 가중치를 설정하여 기존 인공신경망의 문제점을 보완한다. 예측인자로는 기온, 전일-전주 강수일, 태양과 달 궤도 관련 자료를 선정하였다. 기온과 전일-전주 강수일은 서울에서의 1974년부터 2013년까지 총 40년간의AWS(automatic weather system) 관측 자료를 사용하였고, 태양과 달의 궤도 관련 자료는 서울을 중심으로 계산한 결과를사용하였다. 전체 기간에서 일부는 학습 자료로 사용하여 예측모델을 생성하였고, 나머지를 생성한 모델의 검증 자료로사용하였다. 모델 검증 결과로 나온 예측값들은 확률값을 가지며 임계치를 이용하여 강수유무를 판별하였다. 강수 정확도의척도로 양분예보기법 중 CSI(critical successive index)와 Bias(frequency bias)를 계산하였다. 이를 통해 DBN와 MLP(multilayer perceptron)의 성능을 비교한 결과 DBN의 강수 예측 정확도가 높았고, 수행속도 또한 2배 이상 빨랐다.For accurate precipitation forecasts the choice of weather faoctrs and prediction method is very important. Recently, machinele arning has been widely used for forecasting precipitation, and artificial neural network, one of machine learning techniques, showed good performance. In this paper, we suggest a new method for forecasting precipitation using DBN, one of deep learning techniques.
DBN has an advantage that initial weights are set by unsupervised learning, so this compensates for the defects of artificial neural networks. We used past precipitation, temperature, and the parameters of the sun and moon's motion as features for forecasting precipitation. The dataset consists of observation data which had been measured for 40 years from AWS in Seoul. Experiments were based on 8-fold cross validation. As a result of estimation, we got probabilities of test dataset, so threshold was used for the decision of precipitation. CSI and Bias were used for indicating the precision of precipitation. Our experimental results showed that DBN performed better than MLP.Deep learning, Deep belief network, Precipitation, Forecast"
674,딥러닝 신경망모형을 이용한 실시간 교통정보수집 연구,김대현 /KIM DAE HYON,2016,사단법인 인문사회과학기술융합학회,"비디오 기반의 실시간 교통정보수집 시스템은 자율주행을 포함한 지능형교통체계의 주요 서브시스템중 하나이며, 우수한 예측(추정)력 때문에 핵심 알고리즘으로 인공신경망 모형이 주로 사용되고 있다. 특히, 최근 과학 학술지 네이처에 구글 딥마인드의 알파고가 공개되면서 딥러닝과 인공지능에 대한 전 세계적인 관심이 집중되고 있다. 최근까지 가장 널리 사용된 인공신경망모형은 역전파 모형이었으나 최근 컴퓨터 하드웨어 및 알고리즘의 새로운 개발과 더불어 딥러닝 알고리즘을 이용한 딥인공신경망 모형에 대한 관심이 높아지고 있다. 그러나 아직 딥러닝 인공신경망에 대한 연구는 초기단계에 있는 실정이며 적용분야도 극히 한정되어 있다. 본 연구에서는 실시간 도로교통정보 수집을 위한 영상처리분야에서 딥러닝 알고리즘을 활용하고 실험결과를 통하여 향후 교통정보수집‧분석 분야에서의 딥러닝 인공신경망 모형의 적용가능성을 살펴보고자 한다. 특히, 본 연구에서 도출한 딥러닝 인공신경망모형의 실험결과를 기존 역전파 모형에 의한 실험결과와 비교하여 모형의 우수성을 비교‧평가하고자 한다.Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Normally, artificial neural networks have been used in real-time applications for collecting traffic information because of their remarkable performance in terms of predictive accuracy and real-time computing. Especially, the recent publication of a paper in the journal Nature and the success of Google DeepMind’s world-beating Go program AlphaGo caused significant increases of the public interest in artificial intelligence. Even though various types of learning models have been proposed, the Backpropagation has been the most popular neural network model. However, the Deep learning neural network model has attracted much interest recently. In this study, the Deep leaning neural network will be applied in image processing techniques for the real-time traffic information collection to investigate the performance of Deep learning algorithm in traffic engineering fields. In addition, the experimental results from Deep learing algorithm will be compared with the results from Backpropagation model to evaluate and propose better performing model.Intelligent Transportation Systems (ITS), artificial neural networks, Deep learning, traffic information, image processing"
675,딥러닝의 모형과 응용사례,안성만 /SungMahn Ahn,2016,한국지능정보시스템학회,"딥러닝은 인공신경망(neural network)이라는 인공지능분야의 모형이 발전된 형태로서, 계층구조로 이루어진 인공신경망의 내부계층(hidden layer)이 여러 단계로 이루어진 구조이다. 딥러닝에서의 주요 모형은 합성곱신경망(convolutional neural network), 순환신경망(recurrent neural network), 그리고 심층신뢰신경망(deep belief network)의 세가지라고 할 수있다. 그 중에서 현재 흥미로운 연구가 많이 발표되어서 관심이 집중되고 있는 모형은 지도학습(supervised learning)모형인 처음 두 개의 모형이다. 따라서 본 논문에서는 지도학습모형의 가중치를 최적화하는 기본적인 방법인 오류역전파 알고리즘을 살펴본 뒤에 합성곱신경망과 순환신경망의 구조와 응용사례 등을 살펴보고자 한다. 본문에서 다루지 않은 모형인 심층신뢰신경망은 아직까지는 합성곱신경망 이나 순환신경망보다는 상대적으로 주목을 덜 받고 있다. 그러나 심층신뢰신경망은 CNN이나 RNN과는 달리 비지도학습(unsupervised learning)모형이며, 사람이나 동물은 관찰을 통해서 스스로 학습한다는 점에서 궁극적으로는 비지도학습모형이 더 많이 연구되어야 할 주제가 될 것이다.Deep learning model is a kind of neural networks that allows multiple hidden layers. There are various deep learning architectures such as convolutional neural networks, deep belief networks and recurrent neural networks. Those have been applied to fields like computer vision, automatic speech recognition, natural language processing, audio recognition and bioinformatics where they have been shown to produce state-of-the-art results on various tasks. Among those architectures, convolutional neural networks and recurrent neural networks are classified as the supervised learning model. And in recent years, those supervised learning models have gained more popularity than unsupervised learning models such as deep belief networks, because supervised learning models have shown fashionable applications in such fields mentioned above.
Deep learning models can be trained with backpropagation algorithm. Backpropagation is an abbreviation for “backward propagation of errors” and a common method of training artificial neural networks used in conjunction with an optimization method such as gradient descent. The method calculates the gradient of an error function with respect to all the weights in the network. The gradient is fed to the optimization method which in turn uses it to update the weights, in an attempt to minimize the error function.
Convolutional neural networks use a special architecture which is particularly well-adapted to classify images. Using this architecture makes convolutional networks fast to train. This, in turn, helps us train deep, muti-layer networks, which are very good at classifying images. These days, deep convolutional networks are used in most neural networks for image recognition. Convolutional neural networks use three basic ideas: local receptive fields, shared weights, and pooling. By local receptive fields, we mean that each neuron in the first(or any) hidden layer will be connected to a small region of the input(or previous layer’s) neurons. Shared weights mean that we’re going to use the same weights and bias for each of the local receptive field. This means that all the neurons in the hidden layer detect exactly the same feature, just at different locations in the input image. In addition to the convolutional layers just described, convolutional neural networks also contain pooling layers. Pooling layers are usually used immediately after convolutional layers. What the pooling layers do is to simplify the information in the output from the convolutional layer.
Recent convolutional network architectures have 10 to 20 hidden layers and billions of connections between units. Training deep learning networks has taken weeks several years ago, but thanks to progress in GPU and algorithm enhancement, training time has reduced to several hours.
Neural networks with time-varying behavior are known as recurrent neural networks or RNNs. A recurrent neural network is a class of artificial neural network where connections between units form a directed cycle. This creates an internal state of the network which allows it to exhibit dynamic temporal behavior. Unlike feedforward neural networks, RNNs can use their internal memory to process arbitrary sequences of inputs. Early RNN models turned out to be very difficult to train, harder even than deep feedforward networks. The reason is the unstable gradient problem such as vanishing gradient and exploding gradient. The gradient can get smaller and smaller as it is propagated back through layers. This makes learning in early layers extremely slow. The problem actually gets worse in RNNs, since gradients aren’t just propagated backward through layers, they’re propagated backward through time. If the network runs for a long time, that can make the gradient extremely unstable and hard to learn from. It has been possible to incorporate an idea known as long short-term memory units (LSTMs) into RNNs. LSTMs make it much easier to get good results when training RNNs, and many recent papers make use of LSTMs or related ideas.Deep learning, Convolutional neural networks, Recurrent neural networks, Error backpropagation algorithm"
676,딥러닝분석과 기술적 분석 지표를 이용한 한국 코스피주가지수 방향성 예측,이우식 /Lee Woosik,2017,한국데이터정보과학회,"2016년 3월 구글 (Google)의 바둑인공지능 알파고 (AlphaGo)가 이세돌 9단과의 바둑대결에서승리한 이후 다양한 분야에서 인공지능 사용에 대한 관심이 높아지고 있는 가운데 금융투자 분야에서도 인공지능과 투자자문 전문가의 합성어인 로보어드바이저 (Robo-Advisor)에 대한 관심이 높아지고 있다. 인공지능 (artificial intelligence)기반의 의사결정은 비용 절감은 물론 효과적인 의사결정을가능하게 한다는 점에서 큰 장점이 있다. 본 연구에서는 기술적 분석 (technical analysis) 지표와 딥러닝 (deep learning) 모형을 결합하여 한국 코스피 지수를 예측하는 모형을 개발하고 제시한 모형들의 예측력을 비교, 분석한다. 분석 결과 기술적 분석 지표에 딥러닝 알고리즘을 결합한 모형이 주가지수 방향성 예측 문제에 응용될 수 있음을 확인하였다. 향후 본 연구에서 제안된 기술적 분석 지표와딥러닝모형을 결합한 기법은 로보어드바이저서비스에 응용할 수 있는 일반화 가능성을 보여준다Since Google’s AlphaGo defeated a world champion of Go players in 2016, there have been many interests in the deep learning. In the financial sector, a Robo-Advisor using deep learning gains a significant attention, which builds and manages portfolios of financial instruments for investors.In this paper, we have proposed the a deep learning algorithm geared toward identification and forecast of the KOSPI index direction,and we also have compared the accuracy of the prediction.In an application of forecasting the financial market index direction, we have shown that the Robo-Advisor using deep learning has a significant effect on finance industry. The Robo-Advisor collects a massive data such as earnings statements, news reports and regulatory filings, analyzes those and recommends investors how to view market trends and identify the best time to purchase financial assets. On the other hand, the Robo-Advisor allows businesses to learn more about their customers, develop better marketing strategies, increase sales and decrease costs.Artificial intelligence, deep learning, FinTech, Robo-Advisor, technical analysis."
677,딥러닝의 효율적인 학습을 위한 학습데이터 선별 기법에 관한 연구,주경돈 /Gyoung-don Joo,2017,한국정보과학회,"빅데이터의 증가와 병렬 하드웨어 처리, 학습 알고리즘의 개선은 딥러닝의 발전을 가져왔으며 인공지능이 적용 가능한 영역을 획기적으로 확장시키고 있다. 하지만 이러한 대규모의 학습 데이터에 기반한 딥러닝은 학습시간이 증가되고, 더 나아가 학습 데이터의 구축을 위한 레이블링(labeling) 비용이 데이터 수에 비례하여 증가하게 된다. 따라서 본 논문에서는, 딥러닝의 정확도를 최대한 유지하면서 필요한 학습 데이터의 수를 줄일수 있도록, 전체 데이터에서 제한된 수의 학습 데이터의 후보를 선별하는 기법에 대하여 연구한다. 특히 기존의 능동 학습에 기반한 기법들이 학습 데이터의 수가 매우 적어질 경우 딥러닝의 정확도가 급격히 떨어지는 문제를 해결할 수 있는 새로운 기법을 제안하였으며, 제안된 기법으로 선별된 학습 데이터는 그 수의 변화에 대해 딥러닝이 일관되게 경쟁력있는 정확도를 가짐을 다양한 실험을 통하여 검증하였다.The increase of big data, parallel hardware processing and enhanced learning algorithms have led to the development of deep learning and are dramatically expanding the domain of applications based on artificial intelligence. However, such large-scale training data extends the learning time, and furthermore, the labeling cost for building the training set increases in proportion to the volume of data. Therefore, in this paper, we study how to select a limited number of candidates for training set from given data so as to reduce the number of training set while maintaining the accuracy of deep learning. Especially, we found that, with the training set selected by the previous work, the accuracy of deep learning drastically decreases when the number of training set is very small and so proposed new methods to resolve this fall-off in accuracy. Through experiments, we showed that deep learning consistently has the competitive accuracy with the training set selected by the proposed method regardless of the size of training set.Deep Learning, Train Set Selection, Active Learning, Gamma distribution"
678,딥러닝을 이용한 범용적 스테그아날리시스,김현재 /Hyunjae Kim,2017,한국정보과학회,"스테그아날리시스(Steganalysis)란 이미지 등 일반적인 자료에 암호화된 정보를 은닉하는 스테가노그래피(Steganography)에 대한 검출 및 분석 방법으로, 기계학습 기반 방법론을 포함한다. 기존 기계학습 기반 스테그아날리시스는 영상(Image)의 특징(Feature) 추출 및 모델링에 기반하며, 최근 딥러닝(Deep Learning)의 적용으로 검출 정확도가 큰 폭으로 향상되었다. 하지만 현존하는 스테그아날리시스 모델은 단일 스테가노그래피 기법에 대해 국한되어 있어 학습에 사용되지 않은 스테고(Stego) 이미지의 경우 검출이 불가능한 결정적 한계를 가진다. 본 연구에서는 다양한 스테가노그래피 기법으로 생성된 스테고이미지에 딥러닝을 적용하여 스테그아날리시스를 학습하는 범용적 모델을 제안한다. 다양한 실험을 통해 제안 기법의 효용성 및 가능성을 확인하고, 범용적 스테그아날리시스 모델이 각각에 특화된 검출 기법과 유사한 정확도로 스테고 이미지를 검출할 수 있음을 보인다.Steganalysis is to detect information hidden by steganography inside general data such as images. There are stegoanalysis techniques that use machine learning (ML). Existing ML approaches to steganalysis are based on extracting features from stego images and modeling them.
Recently deep learning-based methodologies have shown significant improvements in detection accuracy. However, all the existing methods, including deep learning-based ones, have a critical limitation in that they can only detect stego images that are created by a specific steganography method. In this paper, we propose a generalized steganalysis method that can model multiple types of stego images using deep learning. Through various experiments, we confirm the effectiveness of our approach and envision directions for future research. In particular, we show that our method can detect each type of steganography with the same level of accuracy as that of a steganalysis method dedicated to that type of steganography, thereby demonstrating the general applicability of our approach to multiple types of stego images.steganalysis, steganography, deep learning, generalized steganalysis model"
679,"딥러닝 프레임워크의 비교: 티아노, 텐서플로, CNTK를 중심으로",정여진 /Yeojin Chung,2017,한국지능정보시스템학회,"딥러닝 프레임워크의 대표적인 기능으로는 ‘자동미분’과 ‘GPU의 활용’ 등을 들 수 있다. 본 논문은 파이썬의 라이브러리 형태로 사용 가능한 프레임워크 중에서 구글의 텐서플로와 마이크로소프트의 CNTK, 그리고 텐서 플로의 원조라고 할 수 있는 티아노를 비교하였다. 본문에서는 자동미분의 개념과 GPU의 활용형태를 간단히 설명하고, 그 다음에 logistic regression을 실행하는 예를 통하여 각 프레임워크의 문법을 알아본 뒤에, 마지막으로 대표적인 딥러닝 응용인 CNN의 예제를 실행시켜보고 코딩의 편의성과 실행속도 등을 확인해 보았다. 그 결과, 편의성의 관점에서 보면 티아노가 가장 코딩 하기가 어렵고, CNTK와 텐서플로는 많은 부분이 비슷하게 추상화 되어 있어서 코딩이 비슷하지만 가중치와 편향을 직접 정의하느냐의 여부에서 차이를 보였다. 그리고 각프레임워크의 실행속도에 대한 평가는 ‘큰 차이는 없다’는 것이다. 텐서플로는 티아노에 비하여 속도가 느리다는 평가가 있어왔는데, 본 연구의 실험에 의하면, 비록 CNN 모형에 국한되었지만, 텐서플로가 아주 조금이지만빠른 것으로 나타났다. CNTK의 경우에도, 비록 실험환경이 달랐지만, 실험환경의 차이에 의한 속도의 차이의 편차범위 이내에 있는 것으로 판단이 되었다. 본 연구에서는 세 종류의 딥러닝 프레임워크만을 살펴보았는데,위키피디아에 따르면 딥러닝 프레임워크의 종류는 12가지가 있으며, 각 프레임워크의 특징을 15가지 속성으로구분하여 차이를 특정하고 있다. 그 많은 속성 중에서 사용자의 입장에서 볼 때 중요한 속성은 어떤 언어(파이썬, C++, Java, 등)로 사용가능한지, 어떤 딥러닝 모형에 대한 라이브러리가 잘 구현되어 있는지 등일 것이다. 그리고 사용자가 대규모의 딥러닝 모형을 구축한다면, 다중 GPU 혹은 다중 서버를 지원하는지의 여부도 중요할 것이다. 또한 딥러닝 모형을 처음 학습하는 경우에는 사용설명서가 많은지 예제 프로그램이 많은지 여부도 중요한 기준이 될 것이다.The deep learning framework is software designed to help develop deep learning models. Some of its important functions include “automatic differentiation” and “utilization of GPU”. The list of popular deep learning framework includes Caffe (BVLC) and Theano (University of Montreal). And recently, Microsoft's deep learning framework, Microsoft Cognitive Toolkit, was released as open-source license, following Google’s Tensorflow a year earlier. The early deep learning frameworks have been developed mainly for research at universities. Beginning with the inception of Tensorflow, however, it seems that companies such as Microsoft and Facebook have started to join the competition of framework development. Given the trend, Google and other companies are expected to continue investing in the deep learning framework to bring forward the initiative in the artificial intelligence business. From this point of view, we think it is a good time to compare some of deep learning frameworks. So we compare three deep learning frameworks which can be used as a Python library. Those are Google's Tensorflow, Microsoft’s CNTK, and Theano which is sort of a predecessor of the preceding two.
The most common and important function of deep learning frameworks is the ability to perform automatic differentiation. Basically all the mathematical expressions of deep learning models can be represented as computational graphs, which consist of nodes and edges. Partial derivatives on each edge of a computational graph can then be obtained. With the partial derivatives, we can let software compute differentiation of any node with respect to any variable by utilizing chain rule of Calculus.
First of all, the convenience of coding is in the order of CNTK, Tensorflow, and Theano. The criterion is simply based on the lengths of the codes and the learning curve and the ease of coding are not the main concern. According to the criteria, Theano was the most difficult to implement with, and CNTK and Tensorflow were somewhat easier. With Tensorflow, we need to define weight variables and biases explicitly. The reason that CNTK and Tensorflow are easier to implement with is that those frameworks provide us with more abstraction than Theano. We, however, need to mention that low-level coding is not always bad. It gives us flexibility of coding. With the low-level coding such as in Theano, we can implement and test any new deep learning models or any new search methods that we can think of.
The assessment of the execution speed of each framework is that there is not meaningful difference. According to the experiment, execution speeds of Theano and Tensorflow are very similar, although the experiment was limited to a CNN model. In the case of CNTK, the experimental environment was not maintained as the same. The code written in CNTK has to be run in PC environment without GPU where codes execute as much as 50 times slower than with GPU. But we concluded that the difference of execution speed was within the range of variation caused by the different hardware setup.
In this study, we compared three types of deep learning framework: Theano, Tensorflow, and CNTK. According to Wikipedia, there are 12 available deep learning frameworks. And 15 different attributes differentiate each framework. Some of the important attributes would include interface language (Python, C ++, Java, etc.) and the availability of libraries on various deep learning models such as CNN, RNN, DBN, and etc. And if a user implements a large scale deep learning model, it will also be important to support multiple GPU or multiple servers. Also, if you are learning the deep learning model, it would also be important if there are enough examples and references.deep learning framework, Theano, TensorFlow, CNTK, computational graph, CIFAR-10"
680,딥러닝을 이용한 주변 무선단말 파악방안,이웅섭 /Lee， Woong-sup,2018,한국정보통신학회,"최근 단말-대-단말(Device-to-device, D2D) 통신기술이 차세대 무선통신시스템의 핵심기술로 큰 관심을 받고 있다. 이러한 단말간 통신에서는 자신의 주변에 어떠한 단말이 있는지 파악하는 주변단말 탐색(Neighbor discovery)이매우 중요하다. 본 논문에서는 최근 큰 관심을 받고 있는 딥러닝(Deep learning) 기술을 활용하여 단말간 통신에서 주변단말을 파악하는 방안에 대해서 제안한다. 제안 방안은 기존의 방안과 달리 무선채널의 공간적 연관성을 이용하여단말간의 신호 전송 없이 단말이 기지국으로 전송하는 상향링크 파일럿 신호를 기반으로 주변 단말을 찾고 따라서기존의 방식에 비해 신호전송 복잡도(signaling complexity)를 크게 줄일 수 있다. 또한 제안 방안에서는 떨어져 있는거리에 따라서 주변 단말을 분류 가능하여 기존 방안에 비해서 좀 더 세밀한 단말 탐색이 가능하다. 마지막으로 본 논문에서는 tensorflow를 이용한 컴퓨터 시뮬레이션을 통해 제안 방안의 성능을 검증하였다.Recently, the device-to-device (D2D) communication has been conceived as the key technology for the next-generationmobile communication systems. The neighbor discovery in which the nearby users are found, is essential for the properoperation of the D2D communication. In this paper, we propose new neighbor discovery scheme based on deep learningtechnology which has gained a lot of attention recently. In the proposed scheme, the neighboring users can be foundusing the uplink pilot transmission of users only, unlike conventional neighbor discovery schemes in which direct pilotcommunication among users is required, such that the signaling overhead can be greatly reduced in our proposed scheme.
Moreover, the neighbors with different proximity can also be classified accordingly which enables more accurate neighbordiscovery compared to the conventional schemes. The performance of our proposed scheme is verified through thetensorflow-based computer simulations.
키워드 : 딥러닝, 주변단말 탐색, 단말-대-단말 통신Deep learning, Neighbor discovery, Device-to-device communication, Wireless channel correlation"
681,신뢰성있는 딥러닝 기반 분석 모델을 참조하기 위한 딥러닝 기술 언어,문종혁 /Jong Hyeok Mun,2021,한국정보처리학회,"최근 딥러닝은 하드웨어 성능이 향상됨에 따라 자연어 처리, 영상 인식 등의 다양한 기술에 접목되어 활용되고 있다. 이러한 기술들을 활용해 지능형교통 시스템(ITS), 스마트홈, 헬스케어 등의 산업분야에서 데이터를 분석하여 고속도로 속도위반 차량 검출, 에너지 사용량 제어, 응급상황 등과 같은고품질의 서비스를 제공하며, 고품질의 서비스를 제공하기 위해서는 정확도가 향상된 딥러닝 모델이 적용되어야 한다. 이를 위해 서비스 환경의 데이터를분석하기 위한 딥러닝 모델을 개발할 때, 개발자는 신뢰성이 검증된 최신의 딥러닝 모델을 적용할 수 있어야 한다. 이는 개발자가 참조하는 딥러닝 모델에적용된 학습 데이터셋의 정확도를 측정하여 검증할 수 있다. 이러한 검증을 위해서 개발자는 학습 데이터셋, 딥러닝의 계층구조 및 개발 환경 등과 같은내용을 포함하는 딥러닝 모델을 문서화하여 적용하기 위한 구조적인 정보가 필요하다. 본 논문에서는 신뢰성있는 딥러닝 기반 데이터 분석 모델을 참조하기위한 딥러닝 기술 언어를 제안한다. 제안하는 기술 언어는 신뢰성 있는 딥러닝 모델을 개발하는데 필요한 학습데이터셋, 개발 환경 및 설정 등의 정보와더불어 딥러닝 모델의 계층구조를 표현할 수 있다. 제안하는 딥러닝 기술 언어를 이용하여 개발자는 지능형 교통 시스템에서 참조하는 분석 모델의정확도를 검증할 수 있다. 실험에서는 제안하는 언어의 유효성을 검증하기 위해, 번호판 인식 모델을 중심으로 딥러닝 기술 문서의 적용과정을 보인다With the recent advancements of deep learning, companies such as smart home, healthcare, and intelligent transportation systems areutilizing its functionality to provide high-quality services for vehicle detection, emergency situation detection, and controlling energyconsumption. To provide reliable services in such sensitive systems, deep learning models are required to have high accuracy. In orderto develop a deep learning model for analyzing previously mentioned services, developers should utilize the state of the art deep learningmodels that have already been verified for higher accuracy. The developers can verify the accuracy of the referenced model by validatingthe model on the dataset. For this validation, the developer needs structural information to document and apply deep learning models,including metadata such as learning dataset, network architecture, and development environments. In this paper, we propose a descriptionlanguage that represents the network architecture of the deep learning model along with its metadata that are necessary to develop adeep learning model. Through the proposed description language, developers can easily verify the accuracy of the referenced deep learningmodel. Our experiments demonstrate the application scenario of a deep learning description document that focuses on the license platerecognition for the detection of illegally parked vehicles.Trusted Deep Learning, Model Reference, Deep Learning Description Language, Traffic Situation Analysis Model"
682,딥러닝 분석을 이용한 중국 역내·외 위안화 변동성 예측,이우식 /Lee Woosik,2016,한국데이터정보과학회,"2008년 글로벌 금융위기 이후 중국은 위안화 국제화의 점진적 추진을 진행하면서 중국상하이 외환시장과 중국홍콩 외환시장에서 거래되는 통화인 역내위안화와 역외위안화를 형성시켰다. 본 연구는 위안화 국제화와 점진적인 중국 자본계정 개방에 따라 급변하는 외환시장상황의 변동성을 정확하게 파악하기 위해서 GARCH모형 (일반화된 자기회귀 조건부이분산성모형)에 다단계인공신경망을 결합한 MLP-GARCH 모형과 GARCH모형과 기계학습의 일종인 딥러닝 (deep learning)을 통합한 DL-GARCH을 가지고 위안화 변동성예측을 비교 실험과 분석을 하였다. 비교분석 결과 DLGARCH 모형은 MLP-GARCH보다 모형 위안화 역내·외 환율변동성 예측 면에서 더욱 더 개선된예측값을 제공하였다. 그래서 이분산시계열모형을 딥러닝과 결합한 DL-GARCH 모형은 시계열의환율 변동성 예측 문제에 딥러닝을 응용할 수 있음을 확인하였다. 향후 이분산시계열과 결합된 딥러닝 모형은 다른 금융시계열 데이터에 응용하여 그 일반화 가능성을 높일 수 있을 것이다.The People’s Republic of China has vigorously been pursuing the internationalization of the Chinese Yuan or Renminbi after the financial crisis of 2008. In this view, an abrupt increase of use of the Chinese Yuan in the onshore and offshore markets are important milestones to be one of important currencies. One of the most frequently used methods to forecast volatility is GARCH model. Since a prediction error of the GARCH model has been reported quite high, a lot of efforts have been made to improve forecasting capability of the GARCH model. In this paper, we have proposed MLPGARCH and a DL-GARCH by employing Artificial Neural Network to the GARCH. In an application to forecasting Chinese Yuan volatility, we have successfully shown their overall outperformance in forecasting over the GARCH.Deep learning, DL-GARCH, GARCH, The Chinese Yuan, volatility."
683,딥러닝 기법을 이용한 내일강수 예측,하지훈,2016,한국지능시스템학회,"정확한 강수예측을 위해서는 예측인자 선정과 예측방법에 대한 선택이 매우 중요하다. 최근에는 강수예측 방법으로기계학습 기법이 많이 사용되고 있으며, 그 중에서도 특히 인공신경망을 사용한 강수예측 방법은 좋은 성능을 보였다. 본논문에서는 딥러닝 기법 중 하나인 DBN(deep belief network)를 이용한 새로운 강수예측 방법을 제안한다. DBN는 비지도사전 학습을 통해 초기 가중치를 설정하여 기존 인공신경망의 문제점을 보완한다. 예측인자로는 기온, 전일-전주 강수일, 태양과 달 궤도 관련 자료를 선정하였다. 기온과 전일-전주 강수일은 서울에서의 1974년부터 2013년까지 총 40년간의AWS(automatic weather system) 관측 자료를 사용하였고, 태양과 달의 궤도 관련 자료는 서울을 중심으로 계산한 결과를사용하였다. 전체 기간에서 일부는 학습 자료로 사용하여 예측모델을 생성하였고, 나머지를 생성한 모델의 검증 자료로사용하였다. 모델 검증 결과로 나온 예측값들은 확률값을 가지며 임계치를 이용하여 강수유무를 판별하였다. 강수 정확도의척도로 양분예보기법 중 CSI(critical successive index)와 Bias(frequency bias)를 계산하였다. 이를 통해 DBN와 MLP(multilayer perceptron)의 성능을 비교한 결과 DBN의 강수 예측 정확도가 높았고, 수행속도 또한 2배 이상 빨랐다.For accurate precipitation forecasts the choice of weather faoctrs and prediction method is very important. Recently, machinele arning has been widely used for forecasting precipitation, and artificial neural network, one of machine learning techniques, showed good performance. In this paper, we suggest a new method for forecasting precipitation using DBN, one of deep learning techniques.
DBN has an advantage that initial weights are set by unsupervised learning, so this compensates for the defects of artificial neural networks. We used past precipitation, temperature, and the parameters of the sun and moon's motion as features for forecasting precipitation. The dataset consists of observation data which had been measured for 40 years from AWS in Seoul. Experiments were based on 8-fold cross validation. As a result of estimation, we got probabilities of test dataset, so threshold was used for the decision of precipitation. CSI and Bias were used for indicating the precision of precipitation. Our experimental results showed that DBN performed better than MLP.Deep learning, Deep belief network, Precipitation, Forecast"
684,딥러닝 신경망모형을 이용한 실시간 교통정보수집 연구,김대현 /KIM DAE HYON,2016,사단법인 인문사회과학기술융합학회,"비디오 기반의 실시간 교통정보수집 시스템은 자율주행을 포함한 지능형교통체계의 주요 서브시스템중 하나이며, 우수한 예측(추정)력 때문에 핵심 알고리즘으로 인공신경망 모형이 주로 사용되고 있다. 특히, 최근 과학 학술지 네이처에 구글 딥마인드의 알파고가 공개되면서 딥러닝과 인공지능에 대한 전 세계적인 관심이 집중되고 있다. 최근까지 가장 널리 사용된 인공신경망모형은 역전파 모형이었으나 최근 컴퓨터 하드웨어 및 알고리즘의 새로운 개발과 더불어 딥러닝 알고리즘을 이용한 딥인공신경망 모형에 대한 관심이 높아지고 있다. 그러나 아직 딥러닝 인공신경망에 대한 연구는 초기단계에 있는 실정이며 적용분야도 극히 한정되어 있다. 본 연구에서는 실시간 도로교통정보 수집을 위한 영상처리분야에서 딥러닝 알고리즘을 활용하고 실험결과를 통하여 향후 교통정보수집‧분석 분야에서의 딥러닝 인공신경망 모형의 적용가능성을 살펴보고자 한다. 특히, 본 연구에서 도출한 딥러닝 인공신경망모형의 실험결과를 기존 역전파 모형에 의한 실험결과와 비교하여 모형의 우수성을 비교‧평가하고자 한다.Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Normally, artificial neural networks have been used in real-time applications for collecting traffic information because of their remarkable performance in terms of predictive accuracy and real-time computing. Especially, the recent publication of a paper in the journal Nature and the success of Google DeepMind’s world-beating Go program AlphaGo caused significant increases of the public interest in artificial intelligence. Even though various types of learning models have been proposed, the Backpropagation has been the most popular neural network model. However, the Deep learning neural network model has attracted much interest recently. In this study, the Deep leaning neural network will be applied in image processing techniques for the real-time traffic information collection to investigate the performance of Deep learning algorithm in traffic engineering fields. In addition, the experimental results from Deep learing algorithm will be compared with the results from Backpropagation model to evaluate and propose better performing model.Intelligent Transportation Systems (ITS), artificial neural networks, Deep learning, traffic information, image processing"
685,딥러닝의 모형과 응용사례,안성만 /SungMahn Ahn,2016,한국지능정보시스템학회,"딥러닝은 인공신경망(neural network)이라는 인공지능분야의 모형이 발전된 형태로서, 계층구조로 이루어진 인공신경망의 내부계층(hidden layer)이 여러 단계로 이루어진 구조이다. 딥러닝에서의 주요 모형은 합성곱신경망(convolutional neural network), 순환신경망(recurrent neural network), 그리고 심층신뢰신경망(deep belief network)의 세가지라고 할 수있다. 그 중에서 현재 흥미로운 연구가 많이 발표되어서 관심이 집중되고 있는 모형은 지도학습(supervised learning)모형인 처음 두 개의 모형이다. 따라서 본 논문에서는 지도학습모형의 가중치를 최적화하는 기본적인 방법인 오류역전파 알고리즘을 살펴본 뒤에 합성곱신경망과 순환신경망의 구조와 응용사례 등을 살펴보고자 한다. 본문에서 다루지 않은 모형인 심층신뢰신경망은 아직까지는 합성곱신경망 이나 순환신경망보다는 상대적으로 주목을 덜 받고 있다. 그러나 심층신뢰신경망은 CNN이나 RNN과는 달리 비지도학습(unsupervised learning)모형이며, 사람이나 동물은 관찰을 통해서 스스로 학습한다는 점에서 궁극적으로는 비지도학습모형이 더 많이 연구되어야 할 주제가 될 것이다.Deep learning model is a kind of neural networks that allows multiple hidden layers. There are various deep learning architectures such as convolutional neural networks, deep belief networks and recurrent neural networks. Those have been applied to fields like computer vision, automatic speech recognition, natural language processing, audio recognition and bioinformatics where they have been shown to produce state-of-the-art results on various tasks. Among those architectures, convolutional neural networks and recurrent neural networks are classified as the supervised learning model. And in recent years, those supervised learning models have gained more popularity than unsupervised learning models such as deep belief networks, because supervised learning models have shown fashionable applications in such fields mentioned above.
Deep learning models can be trained with backpropagation algorithm. Backpropagation is an abbreviation for “backward propagation of errors” and a common method of training artificial neural networks used in conjunction with an optimization method such as gradient descent. The method calculates the gradient of an error function with respect to all the weights in the network. The gradient is fed to the optimization method which in turn uses it to update the weights, in an attempt to minimize the error function.
Convolutional neural networks use a special architecture which is particularly well-adapted to classify images. Using this architecture makes convolutional networks fast to train. This, in turn, helps us train deep, muti-layer networks, which are very good at classifying images. These days, deep convolutional networks are used in most neural networks for image recognition. Convolutional neural networks use three basic ideas: local receptive fields, shared weights, and pooling. By local receptive fields, we mean that each neuron in the first(or any) hidden layer will be connected to a small region of the input(or previous layer’s) neurons. Shared weights mean that we’re going to use the same weights and bias for each of the local receptive field. This means that all the neurons in the hidden layer detect exactly the same feature, just at different locations in the input image. In addition to the convolutional layers just described, convolutional neural networks also contain pooling layers. Pooling layers are usually used immediately after convolutional layers. What the pooling layers do is to simplify the information in the output from the convolutional layer.
Recent convolutional network architectures have 10 to 20 hidden layers and billions of connections between units. Training deep learning networks has taken weeks several years ago, but thanks to progress in GPU and algorithm enhancement, training time has reduced to several hours.
Neural networks with time-varying behavior are known as recurrent neural networks or RNNs. A recurrent neural network is a class of artificial neural network where connections between units form a directed cycle. This creates an internal state of the network which allows it to exhibit dynamic temporal behavior. Unlike feedforward neural networks, RNNs can use their internal memory to process arbitrary sequences of inputs. Early RNN models turned out to be very difficult to train, harder even than deep feedforward networks. The reason is the unstable gradient problem such as vanishing gradient and exploding gradient. The gradient can get smaller and smaller as it is propagated back through layers. This makes learning in early layers extremely slow. The problem actually gets worse in RNNs, since gradients aren’t just propagated backward through layers, they’re propagated backward through time. If the network runs for a long time, that can make the gradient extremely unstable and hard to learn from. It has been possible to incorporate an idea known as long short-term memory units (LSTMs) into RNNs. LSTMs make it much easier to get good results when training RNNs, and many recent papers make use of LSTMs or related ideas.Deep learning, Convolutional neural networks, Recurrent neural networks, Error backpropagation algorithm"
686,딥러닝분석과 기술적 분석 지표를 이용한 한국 코스피주가지수 방향성 예측,이우식 /Lee Woosik,2017,한국데이터정보과학회,"2016년 3월 구글 (Google)의 바둑인공지능 알파고 (AlphaGo)가 이세돌 9단과의 바둑대결에서승리한 이후 다양한 분야에서 인공지능 사용에 대한 관심이 높아지고 있는 가운데 금융투자 분야에서도 인공지능과 투자자문 전문가의 합성어인 로보어드바이저 (Robo-Advisor)에 대한 관심이 높아지고 있다. 인공지능 (artificial intelligence)기반의 의사결정은 비용 절감은 물론 효과적인 의사결정을가능하게 한다는 점에서 큰 장점이 있다. 본 연구에서는 기술적 분석 (technical analysis) 지표와 딥러닝 (deep learning) 모형을 결합하여 한국 코스피 지수를 예측하는 모형을 개발하고 제시한 모형들의 예측력을 비교, 분석한다. 분석 결과 기술적 분석 지표에 딥러닝 알고리즘을 결합한 모형이 주가지수 방향성 예측 문제에 응용될 수 있음을 확인하였다. 향후 본 연구에서 제안된 기술적 분석 지표와딥러닝모형을 결합한 기법은 로보어드바이저서비스에 응용할 수 있는 일반화 가능성을 보여준다Since Google’s AlphaGo defeated a world champion of Go players in 2016, there have been many interests in the deep learning. In the financial sector, a Robo-Advisor using deep learning gains a significant attention, which builds and manages portfolios of financial instruments for investors.In this paper, we have proposed the a deep learning algorithm geared toward identification and forecast of the KOSPI index direction,and we also have compared the accuracy of the prediction.In an application of forecasting the financial market index direction, we have shown that the Robo-Advisor using deep learning has a significant effect on finance industry. The Robo-Advisor collects a massive data such as earnings statements, news reports and regulatory filings, analyzes those and recommends investors how to view market trends and identify the best time to purchase financial assets. On the other hand, the Robo-Advisor allows businesses to learn more about their customers, develop better marketing strategies, increase sales and decrease costs.Artificial intelligence, deep learning, FinTech, Robo-Advisor, technical analysis."
687,딥러닝의 효율적인 학습을 위한 학습데이터 선별 기법에 관한 연구,주경돈 /Gyoung-don Joo,2017,한국정보과학회,"빅데이터의 증가와 병렬 하드웨어 처리, 학습 알고리즘의 개선은 딥러닝의 발전을 가져왔으며 인공지능이 적용 가능한 영역을 획기적으로 확장시키고 있다. 하지만 이러한 대규모의 학습 데이터에 기반한 딥러닝은 학습시간이 증가되고, 더 나아가 학습 데이터의 구축을 위한 레이블링(labeling) 비용이 데이터 수에 비례하여 증가하게 된다. 따라서 본 논문에서는, 딥러닝의 정확도를 최대한 유지하면서 필요한 학습 데이터의 수를 줄일수 있도록, 전체 데이터에서 제한된 수의 학습 데이터의 후보를 선별하는 기법에 대하여 연구한다. 특히 기존의 능동 학습에 기반한 기법들이 학습 데이터의 수가 매우 적어질 경우 딥러닝의 정확도가 급격히 떨어지는 문제를 해결할 수 있는 새로운 기법을 제안하였으며, 제안된 기법으로 선별된 학습 데이터는 그 수의 변화에 대해 딥러닝이 일관되게 경쟁력있는 정확도를 가짐을 다양한 실험을 통하여 검증하였다.The increase of big data, parallel hardware processing and enhanced learning algorithms have led to the development of deep learning and are dramatically expanding the domain of applications based on artificial intelligence. However, such large-scale training data extends the learning time, and furthermore, the labeling cost for building the training set increases in proportion to the volume of data. Therefore, in this paper, we study how to select a limited number of candidates for training set from given data so as to reduce the number of training set while maintaining the accuracy of deep learning. Especially, we found that, with the training set selected by the previous work, the accuracy of deep learning drastically decreases when the number of training set is very small and so proposed new methods to resolve this fall-off in accuracy. Through experiments, we showed that deep learning consistently has the competitive accuracy with the training set selected by the proposed method regardless of the size of training set.Deep Learning, Train Set Selection, Active Learning, Gamma distribution"
688,딥러닝을 이용한 범용적 스테그아날리시스,김현재 /Hyunjae Kim,2017,한국정보과학회,"스테그아날리시스(Steganalysis)란 이미지 등 일반적인 자료에 암호화된 정보를 은닉하는 스테가노그래피(Steganography)에 대한 검출 및 분석 방법으로, 기계학습 기반 방법론을 포함한다. 기존 기계학습 기반 스테그아날리시스는 영상(Image)의 특징(Feature) 추출 및 모델링에 기반하며, 최근 딥러닝(Deep Learning)의 적용으로 검출 정확도가 큰 폭으로 향상되었다. 하지만 현존하는 스테그아날리시스 모델은 단일 스테가노그래피 기법에 대해 국한되어 있어 학습에 사용되지 않은 스테고(Stego) 이미지의 경우 검출이 불가능한 결정적 한계를 가진다. 본 연구에서는 다양한 스테가노그래피 기법으로 생성된 스테고이미지에 딥러닝을 적용하여 스테그아날리시스를 학습하는 범용적 모델을 제안한다. 다양한 실험을 통해 제안 기법의 효용성 및 가능성을 확인하고, 범용적 스테그아날리시스 모델이 각각에 특화된 검출 기법과 유사한 정확도로 스테고 이미지를 검출할 수 있음을 보인다.Steganalysis is to detect information hidden by steganography inside general data such as images. There are stegoanalysis techniques that use machine learning (ML). Existing ML approaches to steganalysis are based on extracting features from stego images and modeling them.
Recently deep learning-based methodologies have shown significant improvements in detection accuracy. However, all the existing methods, including deep learning-based ones, have a critical limitation in that they can only detect stego images that are created by a specific steganography method. In this paper, we propose a generalized steganalysis method that can model multiple types of stego images using deep learning. Through various experiments, we confirm the effectiveness of our approach and envision directions for future research. In particular, we show that our method can detect each type of steganography with the same level of accuracy as that of a steganalysis method dedicated to that type of steganography, thereby demonstrating the general applicability of our approach to multiple types of stego images.steganalysis, steganography, deep learning, generalized steganalysis model"
689,"딥러닝 프레임워크의 비교: 티아노, 텐서플로, CNTK를 중심으로",정여진 /Yeojin Chung,2017,한국지능정보시스템학회,"딥러닝 프레임워크의 대표적인 기능으로는 ‘자동미분’과 ‘GPU의 활용’ 등을 들 수 있다. 본 논문은 파이썬의 라이브러리 형태로 사용 가능한 프레임워크 중에서 구글의 텐서플로와 마이크로소프트의 CNTK, 그리고 텐서 플로의 원조라고 할 수 있는 티아노를 비교하였다. 본문에서는 자동미분의 개념과 GPU의 활용형태를 간단히 설명하고, 그 다음에 logistic regression을 실행하는 예를 통하여 각 프레임워크의 문법을 알아본 뒤에, 마지막으로 대표적인 딥러닝 응용인 CNN의 예제를 실행시켜보고 코딩의 편의성과 실행속도 등을 확인해 보았다. 그 결과, 편의성의 관점에서 보면 티아노가 가장 코딩 하기가 어렵고, CNTK와 텐서플로는 많은 부분이 비슷하게 추상화 되어 있어서 코딩이 비슷하지만 가중치와 편향을 직접 정의하느냐의 여부에서 차이를 보였다. 그리고 각프레임워크의 실행속도에 대한 평가는 ‘큰 차이는 없다’는 것이다. 텐서플로는 티아노에 비하여 속도가 느리다는 평가가 있어왔는데, 본 연구의 실험에 의하면, 비록 CNN 모형에 국한되었지만, 텐서플로가 아주 조금이지만빠른 것으로 나타났다. CNTK의 경우에도, 비록 실험환경이 달랐지만, 실험환경의 차이에 의한 속도의 차이의 편차범위 이내에 있는 것으로 판단이 되었다. 본 연구에서는 세 종류의 딥러닝 프레임워크만을 살펴보았는데,위키피디아에 따르면 딥러닝 프레임워크의 종류는 12가지가 있으며, 각 프레임워크의 특징을 15가지 속성으로구분하여 차이를 특정하고 있다. 그 많은 속성 중에서 사용자의 입장에서 볼 때 중요한 속성은 어떤 언어(파이썬, C++, Java, 등)로 사용가능한지, 어떤 딥러닝 모형에 대한 라이브러리가 잘 구현되어 있는지 등일 것이다. 그리고 사용자가 대규모의 딥러닝 모형을 구축한다면, 다중 GPU 혹은 다중 서버를 지원하는지의 여부도 중요할 것이다. 또한 딥러닝 모형을 처음 학습하는 경우에는 사용설명서가 많은지 예제 프로그램이 많은지 여부도 중요한 기준이 될 것이다.The deep learning framework is software designed to help develop deep learning models. Some of its important functions include “automatic differentiation” and “utilization of GPU”. The list of popular deep learning framework includes Caffe (BVLC) and Theano (University of Montreal). And recently, Microsoft's deep learning framework, Microsoft Cognitive Toolkit, was released as open-source license, following Google’s Tensorflow a year earlier. The early deep learning frameworks have been developed mainly for research at universities. Beginning with the inception of Tensorflow, however, it seems that companies such as Microsoft and Facebook have started to join the competition of framework development. Given the trend, Google and other companies are expected to continue investing in the deep learning framework to bring forward the initiative in the artificial intelligence business. From this point of view, we think it is a good time to compare some of deep learning frameworks. So we compare three deep learning frameworks which can be used as a Python library. Those are Google's Tensorflow, Microsoft’s CNTK, and Theano which is sort of a predecessor of the preceding two.
The most common and important function of deep learning frameworks is the ability to perform automatic differentiation. Basically all the mathematical expressions of deep learning models can be represented as computational graphs, which consist of nodes and edges. Partial derivatives on each edge of a computational graph can then be obtained. With the partial derivatives, we can let software compute differentiation of any node with respect to any variable by utilizing chain rule of Calculus.
First of all, the convenience of coding is in the order of CNTK, Tensorflow, and Theano. The criterion is simply based on the lengths of the codes and the learning curve and the ease of coding are not the main concern. According to the criteria, Theano was the most difficult to implement with, and CNTK and Tensorflow were somewhat easier. With Tensorflow, we need to define weight variables and biases explicitly. The reason that CNTK and Tensorflow are easier to implement with is that those frameworks provide us with more abstraction than Theano. We, however, need to mention that low-level coding is not always bad. It gives us flexibility of coding. With the low-level coding such as in Theano, we can implement and test any new deep learning models or any new search methods that we can think of.
The assessment of the execution speed of each framework is that there is not meaningful difference. According to the experiment, execution speeds of Theano and Tensorflow are very similar, although the experiment was limited to a CNN model. In the case of CNTK, the experimental environment was not maintained as the same. The code written in CNTK has to be run in PC environment without GPU where codes execute as much as 50 times slower than with GPU. But we concluded that the difference of execution speed was within the range of variation caused by the different hardware setup.
In this study, we compared three types of deep learning framework: Theano, Tensorflow, and CNTK. According to Wikipedia, there are 12 available deep learning frameworks. And 15 different attributes differentiate each framework. Some of the important attributes would include interface language (Python, C ++, Java, etc.) and the availability of libraries on various deep learning models such as CNN, RNN, DBN, and etc. And if a user implements a large scale deep learning model, it will also be important to support multiple GPU or multiple servers. Also, if you are learning the deep learning model, it would also be important if there are enough examples and references.deep learning framework, Theano, TensorFlow, CNTK, computational graph, CIFAR-10"
690,딥러닝을 이용한 주변 무선단말 파악방안,이웅섭 /Lee， Woong-sup,2018,한국정보통신학회,"최근 단말-대-단말(Device-to-device, D2D) 통신기술이 차세대 무선통신시스템의 핵심기술로 큰 관심을 받고 있다. 이러한 단말간 통신에서는 자신의 주변에 어떠한 단말이 있는지 파악하는 주변단말 탐색(Neighbor discovery)이매우 중요하다. 본 논문에서는 최근 큰 관심을 받고 있는 딥러닝(Deep learning) 기술을 활용하여 단말간 통신에서 주변단말을 파악하는 방안에 대해서 제안한다. 제안 방안은 기존의 방안과 달리 무선채널의 공간적 연관성을 이용하여단말간의 신호 전송 없이 단말이 기지국으로 전송하는 상향링크 파일럿 신호를 기반으로 주변 단말을 찾고 따라서기존의 방식에 비해 신호전송 복잡도(signaling complexity)를 크게 줄일 수 있다. 또한 제안 방안에서는 떨어져 있는거리에 따라서 주변 단말을 분류 가능하여 기존 방안에 비해서 좀 더 세밀한 단말 탐색이 가능하다. 마지막으로 본 논문에서는 tensorflow를 이용한 컴퓨터 시뮬레이션을 통해 제안 방안의 성능을 검증하였다.Recently, the device-to-device (D2D) communication has been conceived as the key technology for the next-generationmobile communication systems. The neighbor discovery in which the nearby users are found, is essential for the properoperation of the D2D communication. In this paper, we propose new neighbor discovery scheme based on deep learningtechnology which has gained a lot of attention recently. In the proposed scheme, the neighboring users can be foundusing the uplink pilot transmission of users only, unlike conventional neighbor discovery schemes in which direct pilotcommunication among users is required, such that the signaling overhead can be greatly reduced in our proposed scheme.
Moreover, the neighbors with different proximity can also be classified accordingly which enables more accurate neighbordiscovery compared to the conventional schemes. The performance of our proposed scheme is verified through thetensorflow-based computer simulations.
키워드 : 딥러닝, 주변단말 탐색, 단말-대-단말 통신Deep learning, Neighbor discovery, Device-to-device communication, Wireless channel correlation"
691,신뢰성있는 딥러닝 기반 분석 모델을 참조하기 위한 딥러닝 기술 언어,문종혁 /Jong Hyeok Mun,2021,한국정보처리학회,"최근 딥러닝은 하드웨어 성능이 향상됨에 따라 자연어 처리, 영상 인식 등의 다양한 기술에 접목되어 활용되고 있다. 이러한 기술들을 활용해 지능형교통 시스템(ITS), 스마트홈, 헬스케어 등의 산업분야에서 데이터를 분석하여 고속도로 속도위반 차량 검출, 에너지 사용량 제어, 응급상황 등과 같은고품질의 서비스를 제공하며, 고품질의 서비스를 제공하기 위해서는 정확도가 향상된 딥러닝 모델이 적용되어야 한다. 이를 위해 서비스 환경의 데이터를분석하기 위한 딥러닝 모델을 개발할 때, 개발자는 신뢰성이 검증된 최신의 딥러닝 모델을 적용할 수 있어야 한다. 이는 개발자가 참조하는 딥러닝 모델에적용된 학습 데이터셋의 정확도를 측정하여 검증할 수 있다. 이러한 검증을 위해서 개발자는 학습 데이터셋, 딥러닝의 계층구조 및 개발 환경 등과 같은내용을 포함하는 딥러닝 모델을 문서화하여 적용하기 위한 구조적인 정보가 필요하다. 본 논문에서는 신뢰성있는 딥러닝 기반 데이터 분석 모델을 참조하기위한 딥러닝 기술 언어를 제안한다. 제안하는 기술 언어는 신뢰성 있는 딥러닝 모델을 개발하는데 필요한 학습데이터셋, 개발 환경 및 설정 등의 정보와더불어 딥러닝 모델의 계층구조를 표현할 수 있다. 제안하는 딥러닝 기술 언어를 이용하여 개발자는 지능형 교통 시스템에서 참조하는 분석 모델의정확도를 검증할 수 있다. 실험에서는 제안하는 언어의 유효성을 검증하기 위해, 번호판 인식 모델을 중심으로 딥러닝 기술 문서의 적용과정을 보인다With the recent advancements of deep learning, companies such as smart home, healthcare, and intelligent transportation systems areutilizing its functionality to provide high-quality services for vehicle detection, emergency situation detection, and controlling energyconsumption. To provide reliable services in such sensitive systems, deep learning models are required to have high accuracy. In orderto develop a deep learning model for analyzing previously mentioned services, developers should utilize the state of the art deep learningmodels that have already been verified for higher accuracy. The developers can verify the accuracy of the referenced model by validatingthe model on the dataset. For this validation, the developer needs structural information to document and apply deep learning models,including metadata such as learning dataset, network architecture, and development environments. In this paper, we propose a descriptionlanguage that represents the network architecture of the deep learning model along with its metadata that are necessary to develop adeep learning model. Through the proposed description language, developers can easily verify the accuracy of the referenced deep learningmodel. Our experiments demonstrate the application scenario of a deep learning description document that focuses on the license platerecognition for the detection of illegally parked vehicles.Trusted Deep Learning, Model Reference, Deep Learning Description Language, Traffic Situation Analysis Model"
692,딥러닝 분석을 이용한 중국 역내·외 위안화 변동성 예측,이우식 /Lee Woosik,2016,한국데이터정보과학회,"2008년 글로벌 금융위기 이후 중국은 위안화 국제화의 점진적 추진을 진행하면서 중국상하이 외환시장과 중국홍콩 외환시장에서 거래되는 통화인 역내위안화와 역외위안화를 형성시켰다. 본 연구는 위안화 국제화와 점진적인 중국 자본계정 개방에 따라 급변하는 외환시장상황의 변동성을 정확하게 파악하기 위해서 GARCH모형 (일반화된 자기회귀 조건부이분산성모형)에 다단계인공신경망을 결합한 MLP-GARCH 모형과 GARCH모형과 기계학습의 일종인 딥러닝 (deep learning)을 통합한 DL-GARCH을 가지고 위안화 변동성예측을 비교 실험과 분석을 하였다. 비교분석 결과 DLGARCH 모형은 MLP-GARCH보다 모형 위안화 역내·외 환율변동성 예측 면에서 더욱 더 개선된예측값을 제공하였다. 그래서 이분산시계열모형을 딥러닝과 결합한 DL-GARCH 모형은 시계열의환율 변동성 예측 문제에 딥러닝을 응용할 수 있음을 확인하였다. 향후 이분산시계열과 결합된 딥러닝 모형은 다른 금융시계열 데이터에 응용하여 그 일반화 가능성을 높일 수 있을 것이다.The People’s Republic of China has vigorously been pursuing the internationalization of the Chinese Yuan or Renminbi after the financial crisis of 2008. In this view, an abrupt increase of use of the Chinese Yuan in the onshore and offshore markets are important milestones to be one of important currencies. One of the most frequently used methods to forecast volatility is GARCH model. Since a prediction error of the GARCH model has been reported quite high, a lot of efforts have been made to improve forecasting capability of the GARCH model. In this paper, we have proposed MLPGARCH and a DL-GARCH by employing Artificial Neural Network to the GARCH. In an application to forecasting Chinese Yuan volatility, we have successfully shown their overall outperformance in forecasting over the GARCH.Deep learning, DL-GARCH, GARCH, The Chinese Yuan, volatility."
693,딥러닝 기법을 이용한 내일강수 예측,하지훈,2016,한국지능시스템학회,"정확한 강수예측을 위해서는 예측인자 선정과 예측방법에 대한 선택이 매우 중요하다. 최근에는 강수예측 방법으로기계학습 기법이 많이 사용되고 있으며, 그 중에서도 특히 인공신경망을 사용한 강수예측 방법은 좋은 성능을 보였다. 본논문에서는 딥러닝 기법 중 하나인 DBN(deep belief network)를 이용한 새로운 강수예측 방법을 제안한다. DBN는 비지도사전 학습을 통해 초기 가중치를 설정하여 기존 인공신경망의 문제점을 보완한다. 예측인자로는 기온, 전일-전주 강수일, 태양과 달 궤도 관련 자료를 선정하였다. 기온과 전일-전주 강수일은 서울에서의 1974년부터 2013년까지 총 40년간의AWS(automatic weather system) 관측 자료를 사용하였고, 태양과 달의 궤도 관련 자료는 서울을 중심으로 계산한 결과를사용하였다. 전체 기간에서 일부는 학습 자료로 사용하여 예측모델을 생성하였고, 나머지를 생성한 모델의 검증 자료로사용하였다. 모델 검증 결과로 나온 예측값들은 확률값을 가지며 임계치를 이용하여 강수유무를 판별하였다. 강수 정확도의척도로 양분예보기법 중 CSI(critical successive index)와 Bias(frequency bias)를 계산하였다. 이를 통해 DBN와 MLP(multilayer perceptron)의 성능을 비교한 결과 DBN의 강수 예측 정확도가 높았고, 수행속도 또한 2배 이상 빨랐다.For accurate precipitation forecasts the choice of weather faoctrs and prediction method is very important. Recently, machinele arning has been widely used for forecasting precipitation, and artificial neural network, one of machine learning techniques, showed good performance. In this paper, we suggest a new method for forecasting precipitation using DBN, one of deep learning techniques.
DBN has an advantage that initial weights are set by unsupervised learning, so this compensates for the defects of artificial neural networks. We used past precipitation, temperature, and the parameters of the sun and moon's motion as features for forecasting precipitation. The dataset consists of observation data which had been measured for 40 years from AWS in Seoul. Experiments were based on 8-fold cross validation. As a result of estimation, we got probabilities of test dataset, so threshold was used for the decision of precipitation. CSI and Bias were used for indicating the precision of precipitation. Our experimental results showed that DBN performed better than MLP.Deep learning, Deep belief network, Precipitation, Forecast"
694,딥러닝 신경망모형을 이용한 실시간 교통정보수집 연구,김대현 /KIM DAE HYON,2016,사단법인 인문사회과학기술융합학회,"비디오 기반의 실시간 교통정보수집 시스템은 자율주행을 포함한 지능형교통체계의 주요 서브시스템중 하나이며, 우수한 예측(추정)력 때문에 핵심 알고리즘으로 인공신경망 모형이 주로 사용되고 있다. 특히, 최근 과학 학술지 네이처에 구글 딥마인드의 알파고가 공개되면서 딥러닝과 인공지능에 대한 전 세계적인 관심이 집중되고 있다. 최근까지 가장 널리 사용된 인공신경망모형은 역전파 모형이었으나 최근 컴퓨터 하드웨어 및 알고리즘의 새로운 개발과 더불어 딥러닝 알고리즘을 이용한 딥인공신경망 모형에 대한 관심이 높아지고 있다. 그러나 아직 딥러닝 인공신경망에 대한 연구는 초기단계에 있는 실정이며 적용분야도 극히 한정되어 있다. 본 연구에서는 실시간 도로교통정보 수집을 위한 영상처리분야에서 딥러닝 알고리즘을 활용하고 실험결과를 통하여 향후 교통정보수집‧분석 분야에서의 딥러닝 인공신경망 모형의 적용가능성을 살펴보고자 한다. 특히, 본 연구에서 도출한 딥러닝 인공신경망모형의 실험결과를 기존 역전파 모형에 의한 실험결과와 비교하여 모형의 우수성을 비교‧평가하고자 한다.Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Normally, artificial neural networks have been used in real-time applications for collecting traffic information because of their remarkable performance in terms of predictive accuracy and real-time computing. Especially, the recent publication of a paper in the journal Nature and the success of Google DeepMind’s world-beating Go program AlphaGo caused significant increases of the public interest in artificial intelligence. Even though various types of learning models have been proposed, the Backpropagation has been the most popular neural network model. However, the Deep learning neural network model has attracted much interest recently. In this study, the Deep leaning neural network will be applied in image processing techniques for the real-time traffic information collection to investigate the performance of Deep learning algorithm in traffic engineering fields. In addition, the experimental results from Deep learing algorithm will be compared with the results from Backpropagation model to evaluate and propose better performing model.Intelligent Transportation Systems (ITS), artificial neural networks, Deep learning, traffic information, image processing"
695,딥러닝의 모형과 응용사례,안성만 /SungMahn Ahn,2016,한국지능정보시스템학회,"딥러닝은 인공신경망(neural network)이라는 인공지능분야의 모형이 발전된 형태로서, 계층구조로 이루어진 인공신경망의 내부계층(hidden layer)이 여러 단계로 이루어진 구조이다. 딥러닝에서의 주요 모형은 합성곱신경망(convolutional neural network), 순환신경망(recurrent neural network), 그리고 심층신뢰신경망(deep belief network)의 세가지라고 할 수있다. 그 중에서 현재 흥미로운 연구가 많이 발표되어서 관심이 집중되고 있는 모형은 지도학습(supervised learning)모형인 처음 두 개의 모형이다. 따라서 본 논문에서는 지도학습모형의 가중치를 최적화하는 기본적인 방법인 오류역전파 알고리즘을 살펴본 뒤에 합성곱신경망과 순환신경망의 구조와 응용사례 등을 살펴보고자 한다. 본문에서 다루지 않은 모형인 심층신뢰신경망은 아직까지는 합성곱신경망 이나 순환신경망보다는 상대적으로 주목을 덜 받고 있다. 그러나 심층신뢰신경망은 CNN이나 RNN과는 달리 비지도학습(unsupervised learning)모형이며, 사람이나 동물은 관찰을 통해서 스스로 학습한다는 점에서 궁극적으로는 비지도학습모형이 더 많이 연구되어야 할 주제가 될 것이다.Deep learning model is a kind of neural networks that allows multiple hidden layers. There are various deep learning architectures such as convolutional neural networks, deep belief networks and recurrent neural networks. Those have been applied to fields like computer vision, automatic speech recognition, natural language processing, audio recognition and bioinformatics where they have been shown to produce state-of-the-art results on various tasks. Among those architectures, convolutional neural networks and recurrent neural networks are classified as the supervised learning model. And in recent years, those supervised learning models have gained more popularity than unsupervised learning models such as deep belief networks, because supervised learning models have shown fashionable applications in such fields mentioned above.
Deep learning models can be trained with backpropagation algorithm. Backpropagation is an abbreviation for “backward propagation of errors” and a common method of training artificial neural networks used in conjunction with an optimization method such as gradient descent. The method calculates the gradient of an error function with respect to all the weights in the network. The gradient is fed to the optimization method which in turn uses it to update the weights, in an attempt to minimize the error function.
Convolutional neural networks use a special architecture which is particularly well-adapted to classify images. Using this architecture makes convolutional networks fast to train. This, in turn, helps us train deep, muti-layer networks, which are very good at classifying images. These days, deep convolutional networks are used in most neural networks for image recognition. Convolutional neural networks use three basic ideas: local receptive fields, shared weights, and pooling. By local receptive fields, we mean that each neuron in the first(or any) hidden layer will be connected to a small region of the input(or previous layer’s) neurons. Shared weights mean that we’re going to use the same weights and bias for each of the local receptive field. This means that all the neurons in the hidden layer detect exactly the same feature, just at different locations in the input image. In addition to the convolutional layers just described, convolutional neural networks also contain pooling layers. Pooling layers are usually used immediately after convolutional layers. What the pooling layers do is to simplify the information in the output from the convolutional layer.
Recent convolutional network architectures have 10 to 20 hidden layers and billions of connections between units. Training deep learning networks has taken weeks several years ago, but thanks to progress in GPU and algorithm enhancement, training time has reduced to several hours.
Neural networks with time-varying behavior are known as recurrent neural networks or RNNs. A recurrent neural network is a class of artificial neural network where connections between units form a directed cycle. This creates an internal state of the network which allows it to exhibit dynamic temporal behavior. Unlike feedforward neural networks, RNNs can use their internal memory to process arbitrary sequences of inputs. Early RNN models turned out to be very difficult to train, harder even than deep feedforward networks. The reason is the unstable gradient problem such as vanishing gradient and exploding gradient. The gradient can get smaller and smaller as it is propagated back through layers. This makes learning in early layers extremely slow. The problem actually gets worse in RNNs, since gradients aren’t just propagated backward through layers, they’re propagated backward through time. If the network runs for a long time, that can make the gradient extremely unstable and hard to learn from. It has been possible to incorporate an idea known as long short-term memory units (LSTMs) into RNNs. LSTMs make it much easier to get good results when training RNNs, and many recent papers make use of LSTMs or related ideas.Deep learning, Convolutional neural networks, Recurrent neural networks, Error backpropagation algorithm"
696,딥러닝분석과 기술적 분석 지표를 이용한 한국 코스피주가지수 방향성 예측,이우식 /Lee Woosik,2017,한국데이터정보과학회,"2016년 3월 구글 (Google)의 바둑인공지능 알파고 (AlphaGo)가 이세돌 9단과의 바둑대결에서승리한 이후 다양한 분야에서 인공지능 사용에 대한 관심이 높아지고 있는 가운데 금융투자 분야에서도 인공지능과 투자자문 전문가의 합성어인 로보어드바이저 (Robo-Advisor)에 대한 관심이 높아지고 있다. 인공지능 (artificial intelligence)기반의 의사결정은 비용 절감은 물론 효과적인 의사결정을가능하게 한다는 점에서 큰 장점이 있다. 본 연구에서는 기술적 분석 (technical analysis) 지표와 딥러닝 (deep learning) 모형을 결합하여 한국 코스피 지수를 예측하는 모형을 개발하고 제시한 모형들의 예측력을 비교, 분석한다. 분석 결과 기술적 분석 지표에 딥러닝 알고리즘을 결합한 모형이 주가지수 방향성 예측 문제에 응용될 수 있음을 확인하였다. 향후 본 연구에서 제안된 기술적 분석 지표와딥러닝모형을 결합한 기법은 로보어드바이저서비스에 응용할 수 있는 일반화 가능성을 보여준다Since Google’s AlphaGo defeated a world champion of Go players in 2016, there have been many interests in the deep learning. In the financial sector, a Robo-Advisor using deep learning gains a significant attention, which builds and manages portfolios of financial instruments for investors.In this paper, we have proposed the a deep learning algorithm geared toward identification and forecast of the KOSPI index direction,and we also have compared the accuracy of the prediction.In an application of forecasting the financial market index direction, we have shown that the Robo-Advisor using deep learning has a significant effect on finance industry. The Robo-Advisor collects a massive data such as earnings statements, news reports and regulatory filings, analyzes those and recommends investors how to view market trends and identify the best time to purchase financial assets. On the other hand, the Robo-Advisor allows businesses to learn more about their customers, develop better marketing strategies, increase sales and decrease costs.Artificial intelligence, deep learning, FinTech, Robo-Advisor, technical analysis."
697,딥러닝의 효율적인 학습을 위한 학습데이터 선별 기법에 관한 연구,주경돈 /Gyoung-don Joo,2017,한국정보과학회,"빅데이터의 증가와 병렬 하드웨어 처리, 학습 알고리즘의 개선은 딥러닝의 발전을 가져왔으며 인공지능이 적용 가능한 영역을 획기적으로 확장시키고 있다. 하지만 이러한 대규모의 학습 데이터에 기반한 딥러닝은 학습시간이 증가되고, 더 나아가 학습 데이터의 구축을 위한 레이블링(labeling) 비용이 데이터 수에 비례하여 증가하게 된다. 따라서 본 논문에서는, 딥러닝의 정확도를 최대한 유지하면서 필요한 학습 데이터의 수를 줄일수 있도록, 전체 데이터에서 제한된 수의 학습 데이터의 후보를 선별하는 기법에 대하여 연구한다. 특히 기존의 능동 학습에 기반한 기법들이 학습 데이터의 수가 매우 적어질 경우 딥러닝의 정확도가 급격히 떨어지는 문제를 해결할 수 있는 새로운 기법을 제안하였으며, 제안된 기법으로 선별된 학습 데이터는 그 수의 변화에 대해 딥러닝이 일관되게 경쟁력있는 정확도를 가짐을 다양한 실험을 통하여 검증하였다.The increase of big data, parallel hardware processing and enhanced learning algorithms have led to the development of deep learning and are dramatically expanding the domain of applications based on artificial intelligence. However, such large-scale training data extends the learning time, and furthermore, the labeling cost for building the training set increases in proportion to the volume of data. Therefore, in this paper, we study how to select a limited number of candidates for training set from given data so as to reduce the number of training set while maintaining the accuracy of deep learning. Especially, we found that, with the training set selected by the previous work, the accuracy of deep learning drastically decreases when the number of training set is very small and so proposed new methods to resolve this fall-off in accuracy. Through experiments, we showed that deep learning consistently has the competitive accuracy with the training set selected by the proposed method regardless of the size of training set.Deep Learning, Train Set Selection, Active Learning, Gamma distribution"
698,딥러닝을 이용한 범용적 스테그아날리시스,김현재 /Hyunjae Kim,2017,한국정보과학회,"스테그아날리시스(Steganalysis)란 이미지 등 일반적인 자료에 암호화된 정보를 은닉하는 스테가노그래피(Steganography)에 대한 검출 및 분석 방법으로, 기계학습 기반 방법론을 포함한다. 기존 기계학습 기반 스테그아날리시스는 영상(Image)의 특징(Feature) 추출 및 모델링에 기반하며, 최근 딥러닝(Deep Learning)의 적용으로 검출 정확도가 큰 폭으로 향상되었다. 하지만 현존하는 스테그아날리시스 모델은 단일 스테가노그래피 기법에 대해 국한되어 있어 학습에 사용되지 않은 스테고(Stego) 이미지의 경우 검출이 불가능한 결정적 한계를 가진다. 본 연구에서는 다양한 스테가노그래피 기법으로 생성된 스테고이미지에 딥러닝을 적용하여 스테그아날리시스를 학습하는 범용적 모델을 제안한다. 다양한 실험을 통해 제안 기법의 효용성 및 가능성을 확인하고, 범용적 스테그아날리시스 모델이 각각에 특화된 검출 기법과 유사한 정확도로 스테고 이미지를 검출할 수 있음을 보인다.Steganalysis is to detect information hidden by steganography inside general data such as images. There are stegoanalysis techniques that use machine learning (ML). Existing ML approaches to steganalysis are based on extracting features from stego images and modeling them.
Recently deep learning-based methodologies have shown significant improvements in detection accuracy. However, all the existing methods, including deep learning-based ones, have a critical limitation in that they can only detect stego images that are created by a specific steganography method. In this paper, we propose a generalized steganalysis method that can model multiple types of stego images using deep learning. Through various experiments, we confirm the effectiveness of our approach and envision directions for future research. In particular, we show that our method can detect each type of steganography with the same level of accuracy as that of a steganalysis method dedicated to that type of steganography, thereby demonstrating the general applicability of our approach to multiple types of stego images.steganalysis, steganography, deep learning, generalized steganalysis model"
699,"딥러닝 프레임워크의 비교: 티아노, 텐서플로, CNTK를 중심으로",정여진 /Yeojin Chung,2017,한국지능정보시스템학회,"딥러닝 프레임워크의 대표적인 기능으로는 ‘자동미분’과 ‘GPU의 활용’ 등을 들 수 있다. 본 논문은 파이썬의 라이브러리 형태로 사용 가능한 프레임워크 중에서 구글의 텐서플로와 마이크로소프트의 CNTK, 그리고 텐서 플로의 원조라고 할 수 있는 티아노를 비교하였다. 본문에서는 자동미분의 개념과 GPU의 활용형태를 간단히 설명하고, 그 다음에 logistic regression을 실행하는 예를 통하여 각 프레임워크의 문법을 알아본 뒤에, 마지막으로 대표적인 딥러닝 응용인 CNN의 예제를 실행시켜보고 코딩의 편의성과 실행속도 등을 확인해 보았다. 그 결과, 편의성의 관점에서 보면 티아노가 가장 코딩 하기가 어렵고, CNTK와 텐서플로는 많은 부분이 비슷하게 추상화 되어 있어서 코딩이 비슷하지만 가중치와 편향을 직접 정의하느냐의 여부에서 차이를 보였다. 그리고 각프레임워크의 실행속도에 대한 평가는 ‘큰 차이는 없다’는 것이다. 텐서플로는 티아노에 비하여 속도가 느리다는 평가가 있어왔는데, 본 연구의 실험에 의하면, 비록 CNN 모형에 국한되었지만, 텐서플로가 아주 조금이지만빠른 것으로 나타났다. CNTK의 경우에도, 비록 실험환경이 달랐지만, 실험환경의 차이에 의한 속도의 차이의 편차범위 이내에 있는 것으로 판단이 되었다. 본 연구에서는 세 종류의 딥러닝 프레임워크만을 살펴보았는데,위키피디아에 따르면 딥러닝 프레임워크의 종류는 12가지가 있으며, 각 프레임워크의 특징을 15가지 속성으로구분하여 차이를 특정하고 있다. 그 많은 속성 중에서 사용자의 입장에서 볼 때 중요한 속성은 어떤 언어(파이썬, C++, Java, 등)로 사용가능한지, 어떤 딥러닝 모형에 대한 라이브러리가 잘 구현되어 있는지 등일 것이다. 그리고 사용자가 대규모의 딥러닝 모형을 구축한다면, 다중 GPU 혹은 다중 서버를 지원하는지의 여부도 중요할 것이다. 또한 딥러닝 모형을 처음 학습하는 경우에는 사용설명서가 많은지 예제 프로그램이 많은지 여부도 중요한 기준이 될 것이다.The deep learning framework is software designed to help develop deep learning models. Some of its important functions include “automatic differentiation” and “utilization of GPU”. The list of popular deep learning framework includes Caffe (BVLC) and Theano (University of Montreal). And recently, Microsoft's deep learning framework, Microsoft Cognitive Toolkit, was released as open-source license, following Google’s Tensorflow a year earlier. The early deep learning frameworks have been developed mainly for research at universities. Beginning with the inception of Tensorflow, however, it seems that companies such as Microsoft and Facebook have started to join the competition of framework development. Given the trend, Google and other companies are expected to continue investing in the deep learning framework to bring forward the initiative in the artificial intelligence business. From this point of view, we think it is a good time to compare some of deep learning frameworks. So we compare three deep learning frameworks which can be used as a Python library. Those are Google's Tensorflow, Microsoft’s CNTK, and Theano which is sort of a predecessor of the preceding two.
The most common and important function of deep learning frameworks is the ability to perform automatic differentiation. Basically all the mathematical expressions of deep learning models can be represented as computational graphs, which consist of nodes and edges. Partial derivatives on each edge of a computational graph can then be obtained. With the partial derivatives, we can let software compute differentiation of any node with respect to any variable by utilizing chain rule of Calculus.
First of all, the convenience of coding is in the order of CNTK, Tensorflow, and Theano. The criterion is simply based on the lengths of the codes and the learning curve and the ease of coding are not the main concern. According to the criteria, Theano was the most difficult to implement with, and CNTK and Tensorflow were somewhat easier. With Tensorflow, we need to define weight variables and biases explicitly. The reason that CNTK and Tensorflow are easier to implement with is that those frameworks provide us with more abstraction than Theano. We, however, need to mention that low-level coding is not always bad. It gives us flexibility of coding. With the low-level coding such as in Theano, we can implement and test any new deep learning models or any new search methods that we can think of.
The assessment of the execution speed of each framework is that there is not meaningful difference. According to the experiment, execution speeds of Theano and Tensorflow are very similar, although the experiment was limited to a CNN model. In the case of CNTK, the experimental environment was not maintained as the same. The code written in CNTK has to be run in PC environment without GPU where codes execute as much as 50 times slower than with GPU. But we concluded that the difference of execution speed was within the range of variation caused by the different hardware setup.
In this study, we compared three types of deep learning framework: Theano, Tensorflow, and CNTK. According to Wikipedia, there are 12 available deep learning frameworks. And 15 different attributes differentiate each framework. Some of the important attributes would include interface language (Python, C ++, Java, etc.) and the availability of libraries on various deep learning models such as CNN, RNN, DBN, and etc. And if a user implements a large scale deep learning model, it will also be important to support multiple GPU or multiple servers. Also, if you are learning the deep learning model, it would also be important if there are enough examples and references.deep learning framework, Theano, TensorFlow, CNTK, computational graph, CIFAR-10"
700,딥러닝을 이용한 주변 무선단말 파악방안,이웅섭 /Lee， Woong-sup,2018,한국정보통신학회,"최근 단말-대-단말(Device-to-device, D2D) 통신기술이 차세대 무선통신시스템의 핵심기술로 큰 관심을 받고 있다. 이러한 단말간 통신에서는 자신의 주변에 어떠한 단말이 있는지 파악하는 주변단말 탐색(Neighbor discovery)이매우 중요하다. 본 논문에서는 최근 큰 관심을 받고 있는 딥러닝(Deep learning) 기술을 활용하여 단말간 통신에서 주변단말을 파악하는 방안에 대해서 제안한다. 제안 방안은 기존의 방안과 달리 무선채널의 공간적 연관성을 이용하여단말간의 신호 전송 없이 단말이 기지국으로 전송하는 상향링크 파일럿 신호를 기반으로 주변 단말을 찾고 따라서기존의 방식에 비해 신호전송 복잡도(signaling complexity)를 크게 줄일 수 있다. 또한 제안 방안에서는 떨어져 있는거리에 따라서 주변 단말을 분류 가능하여 기존 방안에 비해서 좀 더 세밀한 단말 탐색이 가능하다. 마지막으로 본 논문에서는 tensorflow를 이용한 컴퓨터 시뮬레이션을 통해 제안 방안의 성능을 검증하였다.Recently, the device-to-device (D2D) communication has been conceived as the key technology for the next-generationmobile communication systems. The neighbor discovery in which the nearby users are found, is essential for the properoperation of the D2D communication. In this paper, we propose new neighbor discovery scheme based on deep learningtechnology which has gained a lot of attention recently. In the proposed scheme, the neighboring users can be foundusing the uplink pilot transmission of users only, unlike conventional neighbor discovery schemes in which direct pilotcommunication among users is required, such that the signaling overhead can be greatly reduced in our proposed scheme.
Moreover, the neighbors with different proximity can also be classified accordingly which enables more accurate neighbordiscovery compared to the conventional schemes. The performance of our proposed scheme is verified through thetensorflow-based computer simulations.
키워드 : 딥러닝, 주변단말 탐색, 단말-대-단말 통신Deep learning, Neighbor discovery, Device-to-device communication, Wireless channel correlation"
701,신뢰성있는 딥러닝 기반 분석 모델을 참조하기 위한 딥러닝 기술 언어,문종혁 /Jong Hyeok Mun,2021,한국정보처리학회,"최근 딥러닝은 하드웨어 성능이 향상됨에 따라 자연어 처리, 영상 인식 등의 다양한 기술에 접목되어 활용되고 있다. 이러한 기술들을 활용해 지능형교통 시스템(ITS), 스마트홈, 헬스케어 등의 산업분야에서 데이터를 분석하여 고속도로 속도위반 차량 검출, 에너지 사용량 제어, 응급상황 등과 같은고품질의 서비스를 제공하며, 고품질의 서비스를 제공하기 위해서는 정확도가 향상된 딥러닝 모델이 적용되어야 한다. 이를 위해 서비스 환경의 데이터를분석하기 위한 딥러닝 모델을 개발할 때, 개발자는 신뢰성이 검증된 최신의 딥러닝 모델을 적용할 수 있어야 한다. 이는 개발자가 참조하는 딥러닝 모델에적용된 학습 데이터셋의 정확도를 측정하여 검증할 수 있다. 이러한 검증을 위해서 개발자는 학습 데이터셋, 딥러닝의 계층구조 및 개발 환경 등과 같은내용을 포함하는 딥러닝 모델을 문서화하여 적용하기 위한 구조적인 정보가 필요하다. 본 논문에서는 신뢰성있는 딥러닝 기반 데이터 분석 모델을 참조하기위한 딥러닝 기술 언어를 제안한다. 제안하는 기술 언어는 신뢰성 있는 딥러닝 모델을 개발하는데 필요한 학습데이터셋, 개발 환경 및 설정 등의 정보와더불어 딥러닝 모델의 계층구조를 표현할 수 있다. 제안하는 딥러닝 기술 언어를 이용하여 개발자는 지능형 교통 시스템에서 참조하는 분석 모델의정확도를 검증할 수 있다. 실험에서는 제안하는 언어의 유효성을 검증하기 위해, 번호판 인식 모델을 중심으로 딥러닝 기술 문서의 적용과정을 보인다With the recent advancements of deep learning, companies such as smart home, healthcare, and intelligent transportation systems areutilizing its functionality to provide high-quality services for vehicle detection, emergency situation detection, and controlling energyconsumption. To provide reliable services in such sensitive systems, deep learning models are required to have high accuracy. In orderto develop a deep learning model for analyzing previously mentioned services, developers should utilize the state of the art deep learningmodels that have already been verified for higher accuracy. The developers can verify the accuracy of the referenced model by validatingthe model on the dataset. For this validation, the developer needs structural information to document and apply deep learning models,including metadata such as learning dataset, network architecture, and development environments. In this paper, we propose a descriptionlanguage that represents the network architecture of the deep learning model along with its metadata that are necessary to develop adeep learning model. Through the proposed description language, developers can easily verify the accuracy of the referenced deep learningmodel. Our experiments demonstrate the application scenario of a deep learning description document that focuses on the license platerecognition for the detection of illegally parked vehicles.Trusted Deep Learning, Model Reference, Deep Learning Description Language, Traffic Situation Analysis Model"
702,딥러닝 분석을 이용한 중국 역내·외 위안화 변동성 예측,이우식 /Lee Woosik,2016,한국데이터정보과학회,"2008년 글로벌 금융위기 이후 중국은 위안화 국제화의 점진적 추진을 진행하면서 중국상하이 외환시장과 중국홍콩 외환시장에서 거래되는 통화인 역내위안화와 역외위안화를 형성시켰다. 본 연구는 위안화 국제화와 점진적인 중국 자본계정 개방에 따라 급변하는 외환시장상황의 변동성을 정확하게 파악하기 위해서 GARCH모형 (일반화된 자기회귀 조건부이분산성모형)에 다단계인공신경망을 결합한 MLP-GARCH 모형과 GARCH모형과 기계학습의 일종인 딥러닝 (deep learning)을 통합한 DL-GARCH을 가지고 위안화 변동성예측을 비교 실험과 분석을 하였다. 비교분석 결과 DLGARCH 모형은 MLP-GARCH보다 모형 위안화 역내·외 환율변동성 예측 면에서 더욱 더 개선된예측값을 제공하였다. 그래서 이분산시계열모형을 딥러닝과 결합한 DL-GARCH 모형은 시계열의환율 변동성 예측 문제에 딥러닝을 응용할 수 있음을 확인하였다. 향후 이분산시계열과 결합된 딥러닝 모형은 다른 금융시계열 데이터에 응용하여 그 일반화 가능성을 높일 수 있을 것이다.The People’s Republic of China has vigorously been pursuing the internationalization of the Chinese Yuan or Renminbi after the financial crisis of 2008. In this view, an abrupt increase of use of the Chinese Yuan in the onshore and offshore markets are important milestones to be one of important currencies. One of the most frequently used methods to forecast volatility is GARCH model. Since a prediction error of the GARCH model has been reported quite high, a lot of efforts have been made to improve forecasting capability of the GARCH model. In this paper, we have proposed MLPGARCH and a DL-GARCH by employing Artificial Neural Network to the GARCH. In an application to forecasting Chinese Yuan volatility, we have successfully shown their overall outperformance in forecasting over the GARCH.Deep learning, DL-GARCH, GARCH, The Chinese Yuan, volatility."
703,딥러닝 기법을 이용한 내일강수 예측,하지훈,2016,한국지능시스템학회,"정확한 강수예측을 위해서는 예측인자 선정과 예측방법에 대한 선택이 매우 중요하다. 최근에는 강수예측 방법으로기계학습 기법이 많이 사용되고 있으며, 그 중에서도 특히 인공신경망을 사용한 강수예측 방법은 좋은 성능을 보였다. 본논문에서는 딥러닝 기법 중 하나인 DBN(deep belief network)를 이용한 새로운 강수예측 방법을 제안한다. DBN는 비지도사전 학습을 통해 초기 가중치를 설정하여 기존 인공신경망의 문제점을 보완한다. 예측인자로는 기온, 전일-전주 강수일, 태양과 달 궤도 관련 자료를 선정하였다. 기온과 전일-전주 강수일은 서울에서의 1974년부터 2013년까지 총 40년간의AWS(automatic weather system) 관측 자료를 사용하였고, 태양과 달의 궤도 관련 자료는 서울을 중심으로 계산한 결과를사용하였다. 전체 기간에서 일부는 학습 자료로 사용하여 예측모델을 생성하였고, 나머지를 생성한 모델의 검증 자료로사용하였다. 모델 검증 결과로 나온 예측값들은 확률값을 가지며 임계치를 이용하여 강수유무를 판별하였다. 강수 정확도의척도로 양분예보기법 중 CSI(critical successive index)와 Bias(frequency bias)를 계산하였다. 이를 통해 DBN와 MLP(multilayer perceptron)의 성능을 비교한 결과 DBN의 강수 예측 정확도가 높았고, 수행속도 또한 2배 이상 빨랐다.For accurate precipitation forecasts the choice of weather faoctrs and prediction method is very important. Recently, machinele arning has been widely used for forecasting precipitation, and artificial neural network, one of machine learning techniques, showed good performance. In this paper, we suggest a new method for forecasting precipitation using DBN, one of deep learning techniques.
DBN has an advantage that initial weights are set by unsupervised learning, so this compensates for the defects of artificial neural networks. We used past precipitation, temperature, and the parameters of the sun and moon's motion as features for forecasting precipitation. The dataset consists of observation data which had been measured for 40 years from AWS in Seoul. Experiments were based on 8-fold cross validation. As a result of estimation, we got probabilities of test dataset, so threshold was used for the decision of precipitation. CSI and Bias were used for indicating the precision of precipitation. Our experimental results showed that DBN performed better than MLP.Deep learning, Deep belief network, Precipitation, Forecast"
704,딥러닝 신경망모형을 이용한 실시간 교통정보수집 연구,김대현 /KIM DAE HYON,2016,사단법인 인문사회과학기술융합학회,"비디오 기반의 실시간 교통정보수집 시스템은 자율주행을 포함한 지능형교통체계의 주요 서브시스템중 하나이며, 우수한 예측(추정)력 때문에 핵심 알고리즘으로 인공신경망 모형이 주로 사용되고 있다. 특히, 최근 과학 학술지 네이처에 구글 딥마인드의 알파고가 공개되면서 딥러닝과 인공지능에 대한 전 세계적인 관심이 집중되고 있다. 최근까지 가장 널리 사용된 인공신경망모형은 역전파 모형이었으나 최근 컴퓨터 하드웨어 및 알고리즘의 새로운 개발과 더불어 딥러닝 알고리즘을 이용한 딥인공신경망 모형에 대한 관심이 높아지고 있다. 그러나 아직 딥러닝 인공신경망에 대한 연구는 초기단계에 있는 실정이며 적용분야도 극히 한정되어 있다. 본 연구에서는 실시간 도로교통정보 수집을 위한 영상처리분야에서 딥러닝 알고리즘을 활용하고 실험결과를 통하여 향후 교통정보수집‧분석 분야에서의 딥러닝 인공신경망 모형의 적용가능성을 살펴보고자 한다. 특히, 본 연구에서 도출한 딥러닝 인공신경망모형의 실험결과를 기존 역전파 모형에 의한 실험결과와 비교하여 모형의 우수성을 비교‧평가하고자 한다.Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Normally, artificial neural networks have been used in real-time applications for collecting traffic information because of their remarkable performance in terms of predictive accuracy and real-time computing. Especially, the recent publication of a paper in the journal Nature and the success of Google DeepMind’s world-beating Go program AlphaGo caused significant increases of the public interest in artificial intelligence. Even though various types of learning models have been proposed, the Backpropagation has been the most popular neural network model. However, the Deep learning neural network model has attracted much interest recently. In this study, the Deep leaning neural network will be applied in image processing techniques for the real-time traffic information collection to investigate the performance of Deep learning algorithm in traffic engineering fields. In addition, the experimental results from Deep learing algorithm will be compared with the results from Backpropagation model to evaluate and propose better performing model.Intelligent Transportation Systems (ITS), artificial neural networks, Deep learning, traffic information, image processing"
705,딥러닝의 모형과 응용사례,안성만 /SungMahn Ahn,2016,한국지능정보시스템학회,"딥러닝은 인공신경망(neural network)이라는 인공지능분야의 모형이 발전된 형태로서, 계층구조로 이루어진 인공신경망의 내부계층(hidden layer)이 여러 단계로 이루어진 구조이다. 딥러닝에서의 주요 모형은 합성곱신경망(convolutional neural network), 순환신경망(recurrent neural network), 그리고 심층신뢰신경망(deep belief network)의 세가지라고 할 수있다. 그 중에서 현재 흥미로운 연구가 많이 발표되어서 관심이 집중되고 있는 모형은 지도학습(supervised learning)모형인 처음 두 개의 모형이다. 따라서 본 논문에서는 지도학습모형의 가중치를 최적화하는 기본적인 방법인 오류역전파 알고리즘을 살펴본 뒤에 합성곱신경망과 순환신경망의 구조와 응용사례 등을 살펴보고자 한다. 본문에서 다루지 않은 모형인 심층신뢰신경망은 아직까지는 합성곱신경망 이나 순환신경망보다는 상대적으로 주목을 덜 받고 있다. 그러나 심층신뢰신경망은 CNN이나 RNN과는 달리 비지도학습(unsupervised learning)모형이며, 사람이나 동물은 관찰을 통해서 스스로 학습한다는 점에서 궁극적으로는 비지도학습모형이 더 많이 연구되어야 할 주제가 될 것이다.Deep learning model is a kind of neural networks that allows multiple hidden layers. There are various deep learning architectures such as convolutional neural networks, deep belief networks and recurrent neural networks. Those have been applied to fields like computer vision, automatic speech recognition, natural language processing, audio recognition and bioinformatics where they have been shown to produce state-of-the-art results on various tasks. Among those architectures, convolutional neural networks and recurrent neural networks are classified as the supervised learning model. And in recent years, those supervised learning models have gained more popularity than unsupervised learning models such as deep belief networks, because supervised learning models have shown fashionable applications in such fields mentioned above.
Deep learning models can be trained with backpropagation algorithm. Backpropagation is an abbreviation for “backward propagation of errors” and a common method of training artificial neural networks used in conjunction with an optimization method such as gradient descent. The method calculates the gradient of an error function with respect to all the weights in the network. The gradient is fed to the optimization method which in turn uses it to update the weights, in an attempt to minimize the error function.
Convolutional neural networks use a special architecture which is particularly well-adapted to classify images. Using this architecture makes convolutional networks fast to train. This, in turn, helps us train deep, muti-layer networks, which are very good at classifying images. These days, deep convolutional networks are used in most neural networks for image recognition. Convolutional neural networks use three basic ideas: local receptive fields, shared weights, and pooling. By local receptive fields, we mean that each neuron in the first(or any) hidden layer will be connected to a small region of the input(or previous layer’s) neurons. Shared weights mean that we’re going to use the same weights and bias for each of the local receptive field. This means that all the neurons in the hidden layer detect exactly the same feature, just at different locations in the input image. In addition to the convolutional layers just described, convolutional neural networks also contain pooling layers. Pooling layers are usually used immediately after convolutional layers. What the pooling layers do is to simplify the information in the output from the convolutional layer.
Recent convolutional network architectures have 10 to 20 hidden layers and billions of connections between units. Training deep learning networks has taken weeks several years ago, but thanks to progress in GPU and algorithm enhancement, training time has reduced to several hours.
Neural networks with time-varying behavior are known as recurrent neural networks or RNNs. A recurrent neural network is a class of artificial neural network where connections between units form a directed cycle. This creates an internal state of the network which allows it to exhibit dynamic temporal behavior. Unlike feedforward neural networks, RNNs can use their internal memory to process arbitrary sequences of inputs. Early RNN models turned out to be very difficult to train, harder even than deep feedforward networks. The reason is the unstable gradient problem such as vanishing gradient and exploding gradient. The gradient can get smaller and smaller as it is propagated back through layers. This makes learning in early layers extremely slow. The problem actually gets worse in RNNs, since gradients aren’t just propagated backward through layers, they’re propagated backward through time. If the network runs for a long time, that can make the gradient extremely unstable and hard to learn from. It has been possible to incorporate an idea known as long short-term memory units (LSTMs) into RNNs. LSTMs make it much easier to get good results when training RNNs, and many recent papers make use of LSTMs or related ideas.Deep learning, Convolutional neural networks, Recurrent neural networks, Error backpropagation algorithm"
706,딥러닝분석과 기술적 분석 지표를 이용한 한국 코스피주가지수 방향성 예측,이우식 /Lee Woosik,2017,한국데이터정보과학회,"2016년 3월 구글 (Google)의 바둑인공지능 알파고 (AlphaGo)가 이세돌 9단과의 바둑대결에서승리한 이후 다양한 분야에서 인공지능 사용에 대한 관심이 높아지고 있는 가운데 금융투자 분야에서도 인공지능과 투자자문 전문가의 합성어인 로보어드바이저 (Robo-Advisor)에 대한 관심이 높아지고 있다. 인공지능 (artificial intelligence)기반의 의사결정은 비용 절감은 물론 효과적인 의사결정을가능하게 한다는 점에서 큰 장점이 있다. 본 연구에서는 기술적 분석 (technical analysis) 지표와 딥러닝 (deep learning) 모형을 결합하여 한국 코스피 지수를 예측하는 모형을 개발하고 제시한 모형들의 예측력을 비교, 분석한다. 분석 결과 기술적 분석 지표에 딥러닝 알고리즘을 결합한 모형이 주가지수 방향성 예측 문제에 응용될 수 있음을 확인하였다. 향후 본 연구에서 제안된 기술적 분석 지표와딥러닝모형을 결합한 기법은 로보어드바이저서비스에 응용할 수 있는 일반화 가능성을 보여준다Since Google’s AlphaGo defeated a world champion of Go players in 2016, there have been many interests in the deep learning. In the financial sector, a Robo-Advisor using deep learning gains a significant attention, which builds and manages portfolios of financial instruments for investors.In this paper, we have proposed the a deep learning algorithm geared toward identification and forecast of the KOSPI index direction,and we also have compared the accuracy of the prediction.In an application of forecasting the financial market index direction, we have shown that the Robo-Advisor using deep learning has a significant effect on finance industry. The Robo-Advisor collects a massive data such as earnings statements, news reports and regulatory filings, analyzes those and recommends investors how to view market trends and identify the best time to purchase financial assets. On the other hand, the Robo-Advisor allows businesses to learn more about their customers, develop better marketing strategies, increase sales and decrease costs.Artificial intelligence, deep learning, FinTech, Robo-Advisor, technical analysis."
707,딥러닝의 효율적인 학습을 위한 학습데이터 선별 기법에 관한 연구,주경돈 /Gyoung-don Joo,2017,한국정보과학회,"빅데이터의 증가와 병렬 하드웨어 처리, 학습 알고리즘의 개선은 딥러닝의 발전을 가져왔으며 인공지능이 적용 가능한 영역을 획기적으로 확장시키고 있다. 하지만 이러한 대규모의 학습 데이터에 기반한 딥러닝은 학습시간이 증가되고, 더 나아가 학습 데이터의 구축을 위한 레이블링(labeling) 비용이 데이터 수에 비례하여 증가하게 된다. 따라서 본 논문에서는, 딥러닝의 정확도를 최대한 유지하면서 필요한 학습 데이터의 수를 줄일수 있도록, 전체 데이터에서 제한된 수의 학습 데이터의 후보를 선별하는 기법에 대하여 연구한다. 특히 기존의 능동 학습에 기반한 기법들이 학습 데이터의 수가 매우 적어질 경우 딥러닝의 정확도가 급격히 떨어지는 문제를 해결할 수 있는 새로운 기법을 제안하였으며, 제안된 기법으로 선별된 학습 데이터는 그 수의 변화에 대해 딥러닝이 일관되게 경쟁력있는 정확도를 가짐을 다양한 실험을 통하여 검증하였다.The increase of big data, parallel hardware processing and enhanced learning algorithms have led to the development of deep learning and are dramatically expanding the domain of applications based on artificial intelligence. However, such large-scale training data extends the learning time, and furthermore, the labeling cost for building the training set increases in proportion to the volume of data. Therefore, in this paper, we study how to select a limited number of candidates for training set from given data so as to reduce the number of training set while maintaining the accuracy of deep learning. Especially, we found that, with the training set selected by the previous work, the accuracy of deep learning drastically decreases when the number of training set is very small and so proposed new methods to resolve this fall-off in accuracy. Through experiments, we showed that deep learning consistently has the competitive accuracy with the training set selected by the proposed method regardless of the size of training set.Deep Learning, Train Set Selection, Active Learning, Gamma distribution"
708,딥러닝을 이용한 범용적 스테그아날리시스,김현재 /Hyunjae Kim,2017,한국정보과학회,"스테그아날리시스(Steganalysis)란 이미지 등 일반적인 자료에 암호화된 정보를 은닉하는 스테가노그래피(Steganography)에 대한 검출 및 분석 방법으로, 기계학습 기반 방법론을 포함한다. 기존 기계학습 기반 스테그아날리시스는 영상(Image)의 특징(Feature) 추출 및 모델링에 기반하며, 최근 딥러닝(Deep Learning)의 적용으로 검출 정확도가 큰 폭으로 향상되었다. 하지만 현존하는 스테그아날리시스 모델은 단일 스테가노그래피 기법에 대해 국한되어 있어 학습에 사용되지 않은 스테고(Stego) 이미지의 경우 검출이 불가능한 결정적 한계를 가진다. 본 연구에서는 다양한 스테가노그래피 기법으로 생성된 스테고이미지에 딥러닝을 적용하여 스테그아날리시스를 학습하는 범용적 모델을 제안한다. 다양한 실험을 통해 제안 기법의 효용성 및 가능성을 확인하고, 범용적 스테그아날리시스 모델이 각각에 특화된 검출 기법과 유사한 정확도로 스테고 이미지를 검출할 수 있음을 보인다.Steganalysis is to detect information hidden by steganography inside general data such as images. There are stegoanalysis techniques that use machine learning (ML). Existing ML approaches to steganalysis are based on extracting features from stego images and modeling them.
Recently deep learning-based methodologies have shown significant improvements in detection accuracy. However, all the existing methods, including deep learning-based ones, have a critical limitation in that they can only detect stego images that are created by a specific steganography method. In this paper, we propose a generalized steganalysis method that can model multiple types of stego images using deep learning. Through various experiments, we confirm the effectiveness of our approach and envision directions for future research. In particular, we show that our method can detect each type of steganography with the same level of accuracy as that of a steganalysis method dedicated to that type of steganography, thereby demonstrating the general applicability of our approach to multiple types of stego images.steganalysis, steganography, deep learning, generalized steganalysis model"
709,"딥러닝 프레임워크의 비교: 티아노, 텐서플로, CNTK를 중심으로",정여진 /Yeojin Chung,2017,한국지능정보시스템학회,"딥러닝 프레임워크의 대표적인 기능으로는 ‘자동미분’과 ‘GPU의 활용’ 등을 들 수 있다. 본 논문은 파이썬의 라이브러리 형태로 사용 가능한 프레임워크 중에서 구글의 텐서플로와 마이크로소프트의 CNTK, 그리고 텐서 플로의 원조라고 할 수 있는 티아노를 비교하였다. 본문에서는 자동미분의 개념과 GPU의 활용형태를 간단히 설명하고, 그 다음에 logistic regression을 실행하는 예를 통하여 각 프레임워크의 문법을 알아본 뒤에, 마지막으로 대표적인 딥러닝 응용인 CNN의 예제를 실행시켜보고 코딩의 편의성과 실행속도 등을 확인해 보았다. 그 결과, 편의성의 관점에서 보면 티아노가 가장 코딩 하기가 어렵고, CNTK와 텐서플로는 많은 부분이 비슷하게 추상화 되어 있어서 코딩이 비슷하지만 가중치와 편향을 직접 정의하느냐의 여부에서 차이를 보였다. 그리고 각프레임워크의 실행속도에 대한 평가는 ‘큰 차이는 없다’는 것이다. 텐서플로는 티아노에 비하여 속도가 느리다는 평가가 있어왔는데, 본 연구의 실험에 의하면, 비록 CNN 모형에 국한되었지만, 텐서플로가 아주 조금이지만빠른 것으로 나타났다. CNTK의 경우에도, 비록 실험환경이 달랐지만, 실험환경의 차이에 의한 속도의 차이의 편차범위 이내에 있는 것으로 판단이 되었다. 본 연구에서는 세 종류의 딥러닝 프레임워크만을 살펴보았는데,위키피디아에 따르면 딥러닝 프레임워크의 종류는 12가지가 있으며, 각 프레임워크의 특징을 15가지 속성으로구분하여 차이를 특정하고 있다. 그 많은 속성 중에서 사용자의 입장에서 볼 때 중요한 속성은 어떤 언어(파이썬, C++, Java, 등)로 사용가능한지, 어떤 딥러닝 모형에 대한 라이브러리가 잘 구현되어 있는지 등일 것이다. 그리고 사용자가 대규모의 딥러닝 모형을 구축한다면, 다중 GPU 혹은 다중 서버를 지원하는지의 여부도 중요할 것이다. 또한 딥러닝 모형을 처음 학습하는 경우에는 사용설명서가 많은지 예제 프로그램이 많은지 여부도 중요한 기준이 될 것이다.The deep learning framework is software designed to help develop deep learning models. Some of its important functions include “automatic differentiation” and “utilization of GPU”. The list of popular deep learning framework includes Caffe (BVLC) and Theano (University of Montreal). And recently, Microsoft's deep learning framework, Microsoft Cognitive Toolkit, was released as open-source license, following Google’s Tensorflow a year earlier. The early deep learning frameworks have been developed mainly for research at universities. Beginning with the inception of Tensorflow, however, it seems that companies such as Microsoft and Facebook have started to join the competition of framework development. Given the trend, Google and other companies are expected to continue investing in the deep learning framework to bring forward the initiative in the artificial intelligence business. From this point of view, we think it is a good time to compare some of deep learning frameworks. So we compare three deep learning frameworks which can be used as a Python library. Those are Google's Tensorflow, Microsoft’s CNTK, and Theano which is sort of a predecessor of the preceding two.
The most common and important function of deep learning frameworks is the ability to perform automatic differentiation. Basically all the mathematical expressions of deep learning models can be represented as computational graphs, which consist of nodes and edges. Partial derivatives on each edge of a computational graph can then be obtained. With the partial derivatives, we can let software compute differentiation of any node with respect to any variable by utilizing chain rule of Calculus.
First of all, the convenience of coding is in the order of CNTK, Tensorflow, and Theano. The criterion is simply based on the lengths of the codes and the learning curve and the ease of coding are not the main concern. According to the criteria, Theano was the most difficult to implement with, and CNTK and Tensorflow were somewhat easier. With Tensorflow, we need to define weight variables and biases explicitly. The reason that CNTK and Tensorflow are easier to implement with is that those frameworks provide us with more abstraction than Theano. We, however, need to mention that low-level coding is not always bad. It gives us flexibility of coding. With the low-level coding such as in Theano, we can implement and test any new deep learning models or any new search methods that we can think of.
The assessment of the execution speed of each framework is that there is not meaningful difference. According to the experiment, execution speeds of Theano and Tensorflow are very similar, although the experiment was limited to a CNN model. In the case of CNTK, the experimental environment was not maintained as the same. The code written in CNTK has to be run in PC environment without GPU where codes execute as much as 50 times slower than with GPU. But we concluded that the difference of execution speed was within the range of variation caused by the different hardware setup.
In this study, we compared three types of deep learning framework: Theano, Tensorflow, and CNTK. According to Wikipedia, there are 12 available deep learning frameworks. And 15 different attributes differentiate each framework. Some of the important attributes would include interface language (Python, C ++, Java, etc.) and the availability of libraries on various deep learning models such as CNN, RNN, DBN, and etc. And if a user implements a large scale deep learning model, it will also be important to support multiple GPU or multiple servers. Also, if you are learning the deep learning model, it would also be important if there are enough examples and references.deep learning framework, Theano, TensorFlow, CNTK, computational graph, CIFAR-10"
710,딥러닝을 이용한 주변 무선단말 파악방안,이웅섭 /Lee， Woong-sup,2018,한국정보통신학회,"최근 단말-대-단말(Device-to-device, D2D) 통신기술이 차세대 무선통신시스템의 핵심기술로 큰 관심을 받고 있다. 이러한 단말간 통신에서는 자신의 주변에 어떠한 단말이 있는지 파악하는 주변단말 탐색(Neighbor discovery)이매우 중요하다. 본 논문에서는 최근 큰 관심을 받고 있는 딥러닝(Deep learning) 기술을 활용하여 단말간 통신에서 주변단말을 파악하는 방안에 대해서 제안한다. 제안 방안은 기존의 방안과 달리 무선채널의 공간적 연관성을 이용하여단말간의 신호 전송 없이 단말이 기지국으로 전송하는 상향링크 파일럿 신호를 기반으로 주변 단말을 찾고 따라서기존의 방식에 비해 신호전송 복잡도(signaling complexity)를 크게 줄일 수 있다. 또한 제안 방안에서는 떨어져 있는거리에 따라서 주변 단말을 분류 가능하여 기존 방안에 비해서 좀 더 세밀한 단말 탐색이 가능하다. 마지막으로 본 논문에서는 tensorflow를 이용한 컴퓨터 시뮬레이션을 통해 제안 방안의 성능을 검증하였다.Recently, the device-to-device (D2D) communication has been conceived as the key technology for the next-generationmobile communication systems. The neighbor discovery in which the nearby users are found, is essential for the properoperation of the D2D communication. In this paper, we propose new neighbor discovery scheme based on deep learningtechnology which has gained a lot of attention recently. In the proposed scheme, the neighboring users can be foundusing the uplink pilot transmission of users only, unlike conventional neighbor discovery schemes in which direct pilotcommunication among users is required, such that the signaling overhead can be greatly reduced in our proposed scheme.
Moreover, the neighbors with different proximity can also be classified accordingly which enables more accurate neighbordiscovery compared to the conventional schemes. The performance of our proposed scheme is verified through thetensorflow-based computer simulations.
키워드 : 딥러닝, 주변단말 탐색, 단말-대-단말 통신Deep learning, Neighbor discovery, Device-to-device communication, Wireless channel correlation"
711,신뢰성있는 딥러닝 기반 분석 모델을 참조하기 위한 딥러닝 기술 언어,문종혁 /Jong Hyeok Mun,2021,한국정보처리학회,"최근 딥러닝은 하드웨어 성능이 향상됨에 따라 자연어 처리, 영상 인식 등의 다양한 기술에 접목되어 활용되고 있다. 이러한 기술들을 활용해 지능형교통 시스템(ITS), 스마트홈, 헬스케어 등의 산업분야에서 데이터를 분석하여 고속도로 속도위반 차량 검출, 에너지 사용량 제어, 응급상황 등과 같은고품질의 서비스를 제공하며, 고품질의 서비스를 제공하기 위해서는 정확도가 향상된 딥러닝 모델이 적용되어야 한다. 이를 위해 서비스 환경의 데이터를분석하기 위한 딥러닝 모델을 개발할 때, 개발자는 신뢰성이 검증된 최신의 딥러닝 모델을 적용할 수 있어야 한다. 이는 개발자가 참조하는 딥러닝 모델에적용된 학습 데이터셋의 정확도를 측정하여 검증할 수 있다. 이러한 검증을 위해서 개발자는 학습 데이터셋, 딥러닝의 계층구조 및 개발 환경 등과 같은내용을 포함하는 딥러닝 모델을 문서화하여 적용하기 위한 구조적인 정보가 필요하다. 본 논문에서는 신뢰성있는 딥러닝 기반 데이터 분석 모델을 참조하기위한 딥러닝 기술 언어를 제안한다. 제안하는 기술 언어는 신뢰성 있는 딥러닝 모델을 개발하는데 필요한 학습데이터셋, 개발 환경 및 설정 등의 정보와더불어 딥러닝 모델의 계층구조를 표현할 수 있다. 제안하는 딥러닝 기술 언어를 이용하여 개발자는 지능형 교통 시스템에서 참조하는 분석 모델의정확도를 검증할 수 있다. 실험에서는 제안하는 언어의 유효성을 검증하기 위해, 번호판 인식 모델을 중심으로 딥러닝 기술 문서의 적용과정을 보인다With the recent advancements of deep learning, companies such as smart home, healthcare, and intelligent transportation systems areutilizing its functionality to provide high-quality services for vehicle detection, emergency situation detection, and controlling energyconsumption. To provide reliable services in such sensitive systems, deep learning models are required to have high accuracy. In orderto develop a deep learning model for analyzing previously mentioned services, developers should utilize the state of the art deep learningmodels that have already been verified for higher accuracy. The developers can verify the accuracy of the referenced model by validatingthe model on the dataset. For this validation, the developer needs structural information to document and apply deep learning models,including metadata such as learning dataset, network architecture, and development environments. In this paper, we propose a descriptionlanguage that represents the network architecture of the deep learning model along with its metadata that are necessary to develop adeep learning model. Through the proposed description language, developers can easily verify the accuracy of the referenced deep learningmodel. Our experiments demonstrate the application scenario of a deep learning description document that focuses on the license platerecognition for the detection of illegally parked vehicles.Trusted Deep Learning, Model Reference, Deep Learning Description Language, Traffic Situation Analysis Model"
712,딥러닝 분석을 이용한 중국 역내·외 위안화 변동성 예측,이우식 /Lee Woosik,2016,한국데이터정보과학회,"2008년 글로벌 금융위기 이후 중국은 위안화 국제화의 점진적 추진을 진행하면서 중국상하이 외환시장과 중국홍콩 외환시장에서 거래되는 통화인 역내위안화와 역외위안화를 형성시켰다. 본 연구는 위안화 국제화와 점진적인 중국 자본계정 개방에 따라 급변하는 외환시장상황의 변동성을 정확하게 파악하기 위해서 GARCH모형 (일반화된 자기회귀 조건부이분산성모형)에 다단계인공신경망을 결합한 MLP-GARCH 모형과 GARCH모형과 기계학습의 일종인 딥러닝 (deep learning)을 통합한 DL-GARCH을 가지고 위안화 변동성예측을 비교 실험과 분석을 하였다. 비교분석 결과 DLGARCH 모형은 MLP-GARCH보다 모형 위안화 역내·외 환율변동성 예측 면에서 더욱 더 개선된예측값을 제공하였다. 그래서 이분산시계열모형을 딥러닝과 결합한 DL-GARCH 모형은 시계열의환율 변동성 예측 문제에 딥러닝을 응용할 수 있음을 확인하였다. 향후 이분산시계열과 결합된 딥러닝 모형은 다른 금융시계열 데이터에 응용하여 그 일반화 가능성을 높일 수 있을 것이다.The People’s Republic of China has vigorously been pursuing the internationalization of the Chinese Yuan or Renminbi after the financial crisis of 2008. In this view, an abrupt increase of use of the Chinese Yuan in the onshore and offshore markets are important milestones to be one of important currencies. One of the most frequently used methods to forecast volatility is GARCH model. Since a prediction error of the GARCH model has been reported quite high, a lot of efforts have been made to improve forecasting capability of the GARCH model. In this paper, we have proposed MLPGARCH and a DL-GARCH by employing Artificial Neural Network to the GARCH. In an application to forecasting Chinese Yuan volatility, we have successfully shown their overall outperformance in forecasting over the GARCH.Deep learning, DL-GARCH, GARCH, The Chinese Yuan, volatility."
713,딥러닝 기법을 이용한 내일강수 예측,하지훈,2016,한국지능시스템학회,"정확한 강수예측을 위해서는 예측인자 선정과 예측방법에 대한 선택이 매우 중요하다. 최근에는 강수예측 방법으로기계학습 기법이 많이 사용되고 있으며, 그 중에서도 특히 인공신경망을 사용한 강수예측 방법은 좋은 성능을 보였다. 본논문에서는 딥러닝 기법 중 하나인 DBN(deep belief network)를 이용한 새로운 강수예측 방법을 제안한다. DBN는 비지도사전 학습을 통해 초기 가중치를 설정하여 기존 인공신경망의 문제점을 보완한다. 예측인자로는 기온, 전일-전주 강수일, 태양과 달 궤도 관련 자료를 선정하였다. 기온과 전일-전주 강수일은 서울에서의 1974년부터 2013년까지 총 40년간의AWS(automatic weather system) 관측 자료를 사용하였고, 태양과 달의 궤도 관련 자료는 서울을 중심으로 계산한 결과를사용하였다. 전체 기간에서 일부는 학습 자료로 사용하여 예측모델을 생성하였고, 나머지를 생성한 모델의 검증 자료로사용하였다. 모델 검증 결과로 나온 예측값들은 확률값을 가지며 임계치를 이용하여 강수유무를 판별하였다. 강수 정확도의척도로 양분예보기법 중 CSI(critical successive index)와 Bias(frequency bias)를 계산하였다. 이를 통해 DBN와 MLP(multilayer perceptron)의 성능을 비교한 결과 DBN의 강수 예측 정확도가 높았고, 수행속도 또한 2배 이상 빨랐다.For accurate precipitation forecasts the choice of weather faoctrs and prediction method is very important. Recently, machinele arning has been widely used for forecasting precipitation, and artificial neural network, one of machine learning techniques, showed good performance. In this paper, we suggest a new method for forecasting precipitation using DBN, one of deep learning techniques.
DBN has an advantage that initial weights are set by unsupervised learning, so this compensates for the defects of artificial neural networks. We used past precipitation, temperature, and the parameters of the sun and moon's motion as features for forecasting precipitation. The dataset consists of observation data which had been measured for 40 years from AWS in Seoul. Experiments were based on 8-fold cross validation. As a result of estimation, we got probabilities of test dataset, so threshold was used for the decision of precipitation. CSI and Bias were used for indicating the precision of precipitation. Our experimental results showed that DBN performed better than MLP.Deep learning, Deep belief network, Precipitation, Forecast"
714,딥러닝 신경망모형을 이용한 실시간 교통정보수집 연구,김대현 /KIM DAE HYON,2016,사단법인 인문사회과학기술융합학회,"비디오 기반의 실시간 교통정보수집 시스템은 자율주행을 포함한 지능형교통체계의 주요 서브시스템중 하나이며, 우수한 예측(추정)력 때문에 핵심 알고리즘으로 인공신경망 모형이 주로 사용되고 있다. 특히, 최근 과학 학술지 네이처에 구글 딥마인드의 알파고가 공개되면서 딥러닝과 인공지능에 대한 전 세계적인 관심이 집중되고 있다. 최근까지 가장 널리 사용된 인공신경망모형은 역전파 모형이었으나 최근 컴퓨터 하드웨어 및 알고리즘의 새로운 개발과 더불어 딥러닝 알고리즘을 이용한 딥인공신경망 모형에 대한 관심이 높아지고 있다. 그러나 아직 딥러닝 인공신경망에 대한 연구는 초기단계에 있는 실정이며 적용분야도 극히 한정되어 있다. 본 연구에서는 실시간 도로교통정보 수집을 위한 영상처리분야에서 딥러닝 알고리즘을 활용하고 실험결과를 통하여 향후 교통정보수집‧분석 분야에서의 딥러닝 인공신경망 모형의 적용가능성을 살펴보고자 한다. 특히, 본 연구에서 도출한 딥러닝 인공신경망모형의 실험결과를 기존 역전파 모형에 의한 실험결과와 비교하여 모형의 우수성을 비교‧평가하고자 한다.Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Normally, artificial neural networks have been used in real-time applications for collecting traffic information because of their remarkable performance in terms of predictive accuracy and real-time computing. Especially, the recent publication of a paper in the journal Nature and the success of Google DeepMind’s world-beating Go program AlphaGo caused significant increases of the public interest in artificial intelligence. Even though various types of learning models have been proposed, the Backpropagation has been the most popular neural network model. However, the Deep learning neural network model has attracted much interest recently. In this study, the Deep leaning neural network will be applied in image processing techniques for the real-time traffic information collection to investigate the performance of Deep learning algorithm in traffic engineering fields. In addition, the experimental results from Deep learing algorithm will be compared with the results from Backpropagation model to evaluate and propose better performing model.Intelligent Transportation Systems (ITS), artificial neural networks, Deep learning, traffic information, image processing"
715,딥러닝의 모형과 응용사례,안성만 /SungMahn Ahn,2016,한국지능정보시스템학회,"딥러닝은 인공신경망(neural network)이라는 인공지능분야의 모형이 발전된 형태로서, 계층구조로 이루어진 인공신경망의 내부계층(hidden layer)이 여러 단계로 이루어진 구조이다. 딥러닝에서의 주요 모형은 합성곱신경망(convolutional neural network), 순환신경망(recurrent neural network), 그리고 심층신뢰신경망(deep belief network)의 세가지라고 할 수있다. 그 중에서 현재 흥미로운 연구가 많이 발표되어서 관심이 집중되고 있는 모형은 지도학습(supervised learning)모형인 처음 두 개의 모형이다. 따라서 본 논문에서는 지도학습모형의 가중치를 최적화하는 기본적인 방법인 오류역전파 알고리즘을 살펴본 뒤에 합성곱신경망과 순환신경망의 구조와 응용사례 등을 살펴보고자 한다. 본문에서 다루지 않은 모형인 심층신뢰신경망은 아직까지는 합성곱신경망 이나 순환신경망보다는 상대적으로 주목을 덜 받고 있다. 그러나 심층신뢰신경망은 CNN이나 RNN과는 달리 비지도학습(unsupervised learning)모형이며, 사람이나 동물은 관찰을 통해서 스스로 학습한다는 점에서 궁극적으로는 비지도학습모형이 더 많이 연구되어야 할 주제가 될 것이다.Deep learning model is a kind of neural networks that allows multiple hidden layers. There are various deep learning architectures such as convolutional neural networks, deep belief networks and recurrent neural networks. Those have been applied to fields like computer vision, automatic speech recognition, natural language processing, audio recognition and bioinformatics where they have been shown to produce state-of-the-art results on various tasks. Among those architectures, convolutional neural networks and recurrent neural networks are classified as the supervised learning model. And in recent years, those supervised learning models have gained more popularity than unsupervised learning models such as deep belief networks, because supervised learning models have shown fashionable applications in such fields mentioned above.
Deep learning models can be trained with backpropagation algorithm. Backpropagation is an abbreviation for “backward propagation of errors” and a common method of training artificial neural networks used in conjunction with an optimization method such as gradient descent. The method calculates the gradient of an error function with respect to all the weights in the network. The gradient is fed to the optimization method which in turn uses it to update the weights, in an attempt to minimize the error function.
Convolutional neural networks use a special architecture which is particularly well-adapted to classify images. Using this architecture makes convolutional networks fast to train. This, in turn, helps us train deep, muti-layer networks, which are very good at classifying images. These days, deep convolutional networks are used in most neural networks for image recognition. Convolutional neural networks use three basic ideas: local receptive fields, shared weights, and pooling. By local receptive fields, we mean that each neuron in the first(or any) hidden layer will be connected to a small region of the input(or previous layer’s) neurons. Shared weights mean that we’re going to use the same weights and bias for each of the local receptive field. This means that all the neurons in the hidden layer detect exactly the same feature, just at different locations in the input image. In addition to the convolutional layers just described, convolutional neural networks also contain pooling layers. Pooling layers are usually used immediately after convolutional layers. What the pooling layers do is to simplify the information in the output from the convolutional layer.
Recent convolutional network architectures have 10 to 20 hidden layers and billions of connections between units. Training deep learning networks has taken weeks several years ago, but thanks to progress in GPU and algorithm enhancement, training time has reduced to several hours.
Neural networks with time-varying behavior are known as recurrent neural networks or RNNs. A recurrent neural network is a class of artificial neural network where connections between units form a directed cycle. This creates an internal state of the network which allows it to exhibit dynamic temporal behavior. Unlike feedforward neural networks, RNNs can use their internal memory to process arbitrary sequences of inputs. Early RNN models turned out to be very difficult to train, harder even than deep feedforward networks. The reason is the unstable gradient problem such as vanishing gradient and exploding gradient. The gradient can get smaller and smaller as it is propagated back through layers. This makes learning in early layers extremely slow. The problem actually gets worse in RNNs, since gradients aren’t just propagated backward through layers, they’re propagated backward through time. If the network runs for a long time, that can make the gradient extremely unstable and hard to learn from. It has been possible to incorporate an idea known as long short-term memory units (LSTMs) into RNNs. LSTMs make it much easier to get good results when training RNNs, and many recent papers make use of LSTMs or related ideas.Deep learning, Convolutional neural networks, Recurrent neural networks, Error backpropagation algorithm"
716,딥러닝분석과 기술적 분석 지표를 이용한 한국 코스피주가지수 방향성 예측,이우식 /Lee Woosik,2017,한국데이터정보과학회,"2016년 3월 구글 (Google)의 바둑인공지능 알파고 (AlphaGo)가 이세돌 9단과의 바둑대결에서승리한 이후 다양한 분야에서 인공지능 사용에 대한 관심이 높아지고 있는 가운데 금융투자 분야에서도 인공지능과 투자자문 전문가의 합성어인 로보어드바이저 (Robo-Advisor)에 대한 관심이 높아지고 있다. 인공지능 (artificial intelligence)기반의 의사결정은 비용 절감은 물론 효과적인 의사결정을가능하게 한다는 점에서 큰 장점이 있다. 본 연구에서는 기술적 분석 (technical analysis) 지표와 딥러닝 (deep learning) 모형을 결합하여 한국 코스피 지수를 예측하는 모형을 개발하고 제시한 모형들의 예측력을 비교, 분석한다. 분석 결과 기술적 분석 지표에 딥러닝 알고리즘을 결합한 모형이 주가지수 방향성 예측 문제에 응용될 수 있음을 확인하였다. 향후 본 연구에서 제안된 기술적 분석 지표와딥러닝모형을 결합한 기법은 로보어드바이저서비스에 응용할 수 있는 일반화 가능성을 보여준다Since Google’s AlphaGo defeated a world champion of Go players in 2016, there have been many interests in the deep learning. In the financial sector, a Robo-Advisor using deep learning gains a significant attention, which builds and manages portfolios of financial instruments for investors.In this paper, we have proposed the a deep learning algorithm geared toward identification and forecast of the KOSPI index direction,and we also have compared the accuracy of the prediction.In an application of forecasting the financial market index direction, we have shown that the Robo-Advisor using deep learning has a significant effect on finance industry. The Robo-Advisor collects a massive data such as earnings statements, news reports and regulatory filings, analyzes those and recommends investors how to view market trends and identify the best time to purchase financial assets. On the other hand, the Robo-Advisor allows businesses to learn more about their customers, develop better marketing strategies, increase sales and decrease costs.Artificial intelligence, deep learning, FinTech, Robo-Advisor, technical analysis."
717,딥러닝의 효율적인 학습을 위한 학습데이터 선별 기법에 관한 연구,주경돈 /Gyoung-don Joo,2017,한국정보과학회,"빅데이터의 증가와 병렬 하드웨어 처리, 학습 알고리즘의 개선은 딥러닝의 발전을 가져왔으며 인공지능이 적용 가능한 영역을 획기적으로 확장시키고 있다. 하지만 이러한 대규모의 학습 데이터에 기반한 딥러닝은 학습시간이 증가되고, 더 나아가 학습 데이터의 구축을 위한 레이블링(labeling) 비용이 데이터 수에 비례하여 증가하게 된다. 따라서 본 논문에서는, 딥러닝의 정확도를 최대한 유지하면서 필요한 학습 데이터의 수를 줄일수 있도록, 전체 데이터에서 제한된 수의 학습 데이터의 후보를 선별하는 기법에 대하여 연구한다. 특히 기존의 능동 학습에 기반한 기법들이 학습 데이터의 수가 매우 적어질 경우 딥러닝의 정확도가 급격히 떨어지는 문제를 해결할 수 있는 새로운 기법을 제안하였으며, 제안된 기법으로 선별된 학습 데이터는 그 수의 변화에 대해 딥러닝이 일관되게 경쟁력있는 정확도를 가짐을 다양한 실험을 통하여 검증하였다.The increase of big data, parallel hardware processing and enhanced learning algorithms have led to the development of deep learning and are dramatically expanding the domain of applications based on artificial intelligence. However, such large-scale training data extends the learning time, and furthermore, the labeling cost for building the training set increases in proportion to the volume of data. Therefore, in this paper, we study how to select a limited number of candidates for training set from given data so as to reduce the number of training set while maintaining the accuracy of deep learning. Especially, we found that, with the training set selected by the previous work, the accuracy of deep learning drastically decreases when the number of training set is very small and so proposed new methods to resolve this fall-off in accuracy. Through experiments, we showed that deep learning consistently has the competitive accuracy with the training set selected by the proposed method regardless of the size of training set.Deep Learning, Train Set Selection, Active Learning, Gamma distribution"
718,딥러닝을 이용한 범용적 스테그아날리시스,김현재 /Hyunjae Kim,2017,한국정보과학회,"스테그아날리시스(Steganalysis)란 이미지 등 일반적인 자료에 암호화된 정보를 은닉하는 스테가노그래피(Steganography)에 대한 검출 및 분석 방법으로, 기계학습 기반 방법론을 포함한다. 기존 기계학습 기반 스테그아날리시스는 영상(Image)의 특징(Feature) 추출 및 모델링에 기반하며, 최근 딥러닝(Deep Learning)의 적용으로 검출 정확도가 큰 폭으로 향상되었다. 하지만 현존하는 스테그아날리시스 모델은 단일 스테가노그래피 기법에 대해 국한되어 있어 학습에 사용되지 않은 스테고(Stego) 이미지의 경우 검출이 불가능한 결정적 한계를 가진다. 본 연구에서는 다양한 스테가노그래피 기법으로 생성된 스테고이미지에 딥러닝을 적용하여 스테그아날리시스를 학습하는 범용적 모델을 제안한다. 다양한 실험을 통해 제안 기법의 효용성 및 가능성을 확인하고, 범용적 스테그아날리시스 모델이 각각에 특화된 검출 기법과 유사한 정확도로 스테고 이미지를 검출할 수 있음을 보인다.Steganalysis is to detect information hidden by steganography inside general data such as images. There are stegoanalysis techniques that use machine learning (ML). Existing ML approaches to steganalysis are based on extracting features from stego images and modeling them.
Recently deep learning-based methodologies have shown significant improvements in detection accuracy. However, all the existing methods, including deep learning-based ones, have a critical limitation in that they can only detect stego images that are created by a specific steganography method. In this paper, we propose a generalized steganalysis method that can model multiple types of stego images using deep learning. Through various experiments, we confirm the effectiveness of our approach and envision directions for future research. In particular, we show that our method can detect each type of steganography with the same level of accuracy as that of a steganalysis method dedicated to that type of steganography, thereby demonstrating the general applicability of our approach to multiple types of stego images.steganalysis, steganography, deep learning, generalized steganalysis model"
719,"딥러닝 프레임워크의 비교: 티아노, 텐서플로, CNTK를 중심으로",정여진 /Yeojin Chung,2017,한국지능정보시스템학회,"딥러닝 프레임워크의 대표적인 기능으로는 ‘자동미분’과 ‘GPU의 활용’ 등을 들 수 있다. 본 논문은 파이썬의 라이브러리 형태로 사용 가능한 프레임워크 중에서 구글의 텐서플로와 마이크로소프트의 CNTK, 그리고 텐서 플로의 원조라고 할 수 있는 티아노를 비교하였다. 본문에서는 자동미분의 개념과 GPU의 활용형태를 간단히 설명하고, 그 다음에 logistic regression을 실행하는 예를 통하여 각 프레임워크의 문법을 알아본 뒤에, 마지막으로 대표적인 딥러닝 응용인 CNN의 예제를 실행시켜보고 코딩의 편의성과 실행속도 등을 확인해 보았다. 그 결과, 편의성의 관점에서 보면 티아노가 가장 코딩 하기가 어렵고, CNTK와 텐서플로는 많은 부분이 비슷하게 추상화 되어 있어서 코딩이 비슷하지만 가중치와 편향을 직접 정의하느냐의 여부에서 차이를 보였다. 그리고 각프레임워크의 실행속도에 대한 평가는 ‘큰 차이는 없다’는 것이다. 텐서플로는 티아노에 비하여 속도가 느리다는 평가가 있어왔는데, 본 연구의 실험에 의하면, 비록 CNN 모형에 국한되었지만, 텐서플로가 아주 조금이지만빠른 것으로 나타났다. CNTK의 경우에도, 비록 실험환경이 달랐지만, 실험환경의 차이에 의한 속도의 차이의 편차범위 이내에 있는 것으로 판단이 되었다. 본 연구에서는 세 종류의 딥러닝 프레임워크만을 살펴보았는데,위키피디아에 따르면 딥러닝 프레임워크의 종류는 12가지가 있으며, 각 프레임워크의 특징을 15가지 속성으로구분하여 차이를 특정하고 있다. 그 많은 속성 중에서 사용자의 입장에서 볼 때 중요한 속성은 어떤 언어(파이썬, C++, Java, 등)로 사용가능한지, 어떤 딥러닝 모형에 대한 라이브러리가 잘 구현되어 있는지 등일 것이다. 그리고 사용자가 대규모의 딥러닝 모형을 구축한다면, 다중 GPU 혹은 다중 서버를 지원하는지의 여부도 중요할 것이다. 또한 딥러닝 모형을 처음 학습하는 경우에는 사용설명서가 많은지 예제 프로그램이 많은지 여부도 중요한 기준이 될 것이다.The deep learning framework is software designed to help develop deep learning models. Some of its important functions include “automatic differentiation” and “utilization of GPU”. The list of popular deep learning framework includes Caffe (BVLC) and Theano (University of Montreal). And recently, Microsoft's deep learning framework, Microsoft Cognitive Toolkit, was released as open-source license, following Google’s Tensorflow a year earlier. The early deep learning frameworks have been developed mainly for research at universities. Beginning with the inception of Tensorflow, however, it seems that companies such as Microsoft and Facebook have started to join the competition of framework development. Given the trend, Google and other companies are expected to continue investing in the deep learning framework to bring forward the initiative in the artificial intelligence business. From this point of view, we think it is a good time to compare some of deep learning frameworks. So we compare three deep learning frameworks which can be used as a Python library. Those are Google's Tensorflow, Microsoft’s CNTK, and Theano which is sort of a predecessor of the preceding two.
The most common and important function of deep learning frameworks is the ability to perform automatic differentiation. Basically all the mathematical expressions of deep learning models can be represented as computational graphs, which consist of nodes and edges. Partial derivatives on each edge of a computational graph can then be obtained. With the partial derivatives, we can let software compute differentiation of any node with respect to any variable by utilizing chain rule of Calculus.
First of all, the convenience of coding is in the order of CNTK, Tensorflow, and Theano. The criterion is simply based on the lengths of the codes and the learning curve and the ease of coding are not the main concern. According to the criteria, Theano was the most difficult to implement with, and CNTK and Tensorflow were somewhat easier. With Tensorflow, we need to define weight variables and biases explicitly. The reason that CNTK and Tensorflow are easier to implement with is that those frameworks provide us with more abstraction than Theano. We, however, need to mention that low-level coding is not always bad. It gives us flexibility of coding. With the low-level coding such as in Theano, we can implement and test any new deep learning models or any new search methods that we can think of.
The assessment of the execution speed of each framework is that there is not meaningful difference. According to the experiment, execution speeds of Theano and Tensorflow are very similar, although the experiment was limited to a CNN model. In the case of CNTK, the experimental environment was not maintained as the same. The code written in CNTK has to be run in PC environment without GPU where codes execute as much as 50 times slower than with GPU. But we concluded that the difference of execution speed was within the range of variation caused by the different hardware setup.
In this study, we compared three types of deep learning framework: Theano, Tensorflow, and CNTK. According to Wikipedia, there are 12 available deep learning frameworks. And 15 different attributes differentiate each framework. Some of the important attributes would include interface language (Python, C ++, Java, etc.) and the availability of libraries on various deep learning models such as CNN, RNN, DBN, and etc. And if a user implements a large scale deep learning model, it will also be important to support multiple GPU or multiple servers. Also, if you are learning the deep learning model, it would also be important if there are enough examples and references.deep learning framework, Theano, TensorFlow, CNTK, computational graph, CIFAR-10"
720,딥러닝을 이용한 주변 무선단말 파악방안,이웅섭 /Lee， Woong-sup,2018,한국정보통신학회,"최근 단말-대-단말(Device-to-device, D2D) 통신기술이 차세대 무선통신시스템의 핵심기술로 큰 관심을 받고 있다. 이러한 단말간 통신에서는 자신의 주변에 어떠한 단말이 있는지 파악하는 주변단말 탐색(Neighbor discovery)이매우 중요하다. 본 논문에서는 최근 큰 관심을 받고 있는 딥러닝(Deep learning) 기술을 활용하여 단말간 통신에서 주변단말을 파악하는 방안에 대해서 제안한다. 제안 방안은 기존의 방안과 달리 무선채널의 공간적 연관성을 이용하여단말간의 신호 전송 없이 단말이 기지국으로 전송하는 상향링크 파일럿 신호를 기반으로 주변 단말을 찾고 따라서기존의 방식에 비해 신호전송 복잡도(signaling complexity)를 크게 줄일 수 있다. 또한 제안 방안에서는 떨어져 있는거리에 따라서 주변 단말을 분류 가능하여 기존 방안에 비해서 좀 더 세밀한 단말 탐색이 가능하다. 마지막으로 본 논문에서는 tensorflow를 이용한 컴퓨터 시뮬레이션을 통해 제안 방안의 성능을 검증하였다.Recently, the device-to-device (D2D) communication has been conceived as the key technology for the next-generationmobile communication systems. The neighbor discovery in which the nearby users are found, is essential for the properoperation of the D2D communication. In this paper, we propose new neighbor discovery scheme based on deep learningtechnology which has gained a lot of attention recently. In the proposed scheme, the neighboring users can be foundusing the uplink pilot transmission of users only, unlike conventional neighbor discovery schemes in which direct pilotcommunication among users is required, such that the signaling overhead can be greatly reduced in our proposed scheme.
Moreover, the neighbors with different proximity can also be classified accordingly which enables more accurate neighbordiscovery compared to the conventional schemes. The performance of our proposed scheme is verified through thetensorflow-based computer simulations.
키워드 : 딥러닝, 주변단말 탐색, 단말-대-단말 통신Deep learning, Neighbor discovery, Device-to-device communication, Wireless channel correlation"
721,신뢰성있는 딥러닝 기반 분석 모델을 참조하기 위한 딥러닝 기술 언어,문종혁 /Jong Hyeok Mun,2021,한국정보처리학회,"최근 딥러닝은 하드웨어 성능이 향상됨에 따라 자연어 처리, 영상 인식 등의 다양한 기술에 접목되어 활용되고 있다. 이러한 기술들을 활용해 지능형교통 시스템(ITS), 스마트홈, 헬스케어 등의 산업분야에서 데이터를 분석하여 고속도로 속도위반 차량 검출, 에너지 사용량 제어, 응급상황 등과 같은고품질의 서비스를 제공하며, 고품질의 서비스를 제공하기 위해서는 정확도가 향상된 딥러닝 모델이 적용되어야 한다. 이를 위해 서비스 환경의 데이터를분석하기 위한 딥러닝 모델을 개발할 때, 개발자는 신뢰성이 검증된 최신의 딥러닝 모델을 적용할 수 있어야 한다. 이는 개발자가 참조하는 딥러닝 모델에적용된 학습 데이터셋의 정확도를 측정하여 검증할 수 있다. 이러한 검증을 위해서 개발자는 학습 데이터셋, 딥러닝의 계층구조 및 개발 환경 등과 같은내용을 포함하는 딥러닝 모델을 문서화하여 적용하기 위한 구조적인 정보가 필요하다. 본 논문에서는 신뢰성있는 딥러닝 기반 데이터 분석 모델을 참조하기위한 딥러닝 기술 언어를 제안한다. 제안하는 기술 언어는 신뢰성 있는 딥러닝 모델을 개발하는데 필요한 학습데이터셋, 개발 환경 및 설정 등의 정보와더불어 딥러닝 모델의 계층구조를 표현할 수 있다. 제안하는 딥러닝 기술 언어를 이용하여 개발자는 지능형 교통 시스템에서 참조하는 분석 모델의정확도를 검증할 수 있다. 실험에서는 제안하는 언어의 유효성을 검증하기 위해, 번호판 인식 모델을 중심으로 딥러닝 기술 문서의 적용과정을 보인다With the recent advancements of deep learning, companies such as smart home, healthcare, and intelligent transportation systems areutilizing its functionality to provide high-quality services for vehicle detection, emergency situation detection, and controlling energyconsumption. To provide reliable services in such sensitive systems, deep learning models are required to have high accuracy. In orderto develop a deep learning model for analyzing previously mentioned services, developers should utilize the state of the art deep learningmodels that have already been verified for higher accuracy. The developers can verify the accuracy of the referenced model by validatingthe model on the dataset. For this validation, the developer needs structural information to document and apply deep learning models,including metadata such as learning dataset, network architecture, and development environments. In this paper, we propose a descriptionlanguage that represents the network architecture of the deep learning model along with its metadata that are necessary to develop adeep learning model. Through the proposed description language, developers can easily verify the accuracy of the referenced deep learningmodel. Our experiments demonstrate the application scenario of a deep learning description document that focuses on the license platerecognition for the detection of illegally parked vehicles.Trusted Deep Learning, Model Reference, Deep Learning Description Language, Traffic Situation Analysis Model"
722,딥러닝 분석을 이용한 중국 역내·외 위안화 변동성 예측,이우식 /Lee Woosik,2016,한국데이터정보과학회,"2008년 글로벌 금융위기 이후 중국은 위안화 국제화의 점진적 추진을 진행하면서 중국상하이 외환시장과 중국홍콩 외환시장에서 거래되는 통화인 역내위안화와 역외위안화를 형성시켰다. 본 연구는 위안화 국제화와 점진적인 중국 자본계정 개방에 따라 급변하는 외환시장상황의 변동성을 정확하게 파악하기 위해서 GARCH모형 (일반화된 자기회귀 조건부이분산성모형)에 다단계인공신경망을 결합한 MLP-GARCH 모형과 GARCH모형과 기계학습의 일종인 딥러닝 (deep learning)을 통합한 DL-GARCH을 가지고 위안화 변동성예측을 비교 실험과 분석을 하였다. 비교분석 결과 DLGARCH 모형은 MLP-GARCH보다 모형 위안화 역내·외 환율변동성 예측 면에서 더욱 더 개선된예측값을 제공하였다. 그래서 이분산시계열모형을 딥러닝과 결합한 DL-GARCH 모형은 시계열의환율 변동성 예측 문제에 딥러닝을 응용할 수 있음을 확인하였다. 향후 이분산시계열과 결합된 딥러닝 모형은 다른 금융시계열 데이터에 응용하여 그 일반화 가능성을 높일 수 있을 것이다.The People’s Republic of China has vigorously been pursuing the internationalization of the Chinese Yuan or Renminbi after the financial crisis of 2008. In this view, an abrupt increase of use of the Chinese Yuan in the onshore and offshore markets are important milestones to be one of important currencies. One of the most frequently used methods to forecast volatility is GARCH model. Since a prediction error of the GARCH model has been reported quite high, a lot of efforts have been made to improve forecasting capability of the GARCH model. In this paper, we have proposed MLPGARCH and a DL-GARCH by employing Artificial Neural Network to the GARCH. In an application to forecasting Chinese Yuan volatility, we have successfully shown their overall outperformance in forecasting over the GARCH.Deep learning, DL-GARCH, GARCH, The Chinese Yuan, volatility."
723,딥러닝 기법을 이용한 내일강수 예측,하지훈,2016,한국지능시스템학회,"정확한 강수예측을 위해서는 예측인자 선정과 예측방법에 대한 선택이 매우 중요하다. 최근에는 강수예측 방법으로기계학습 기법이 많이 사용되고 있으며, 그 중에서도 특히 인공신경망을 사용한 강수예측 방법은 좋은 성능을 보였다. 본논문에서는 딥러닝 기법 중 하나인 DBN(deep belief network)를 이용한 새로운 강수예측 방법을 제안한다. DBN는 비지도사전 학습을 통해 초기 가중치를 설정하여 기존 인공신경망의 문제점을 보완한다. 예측인자로는 기온, 전일-전주 강수일, 태양과 달 궤도 관련 자료를 선정하였다. 기온과 전일-전주 강수일은 서울에서의 1974년부터 2013년까지 총 40년간의AWS(automatic weather system) 관측 자료를 사용하였고, 태양과 달의 궤도 관련 자료는 서울을 중심으로 계산한 결과를사용하였다. 전체 기간에서 일부는 학습 자료로 사용하여 예측모델을 생성하였고, 나머지를 생성한 모델의 검증 자료로사용하였다. 모델 검증 결과로 나온 예측값들은 확률값을 가지며 임계치를 이용하여 강수유무를 판별하였다. 강수 정확도의척도로 양분예보기법 중 CSI(critical successive index)와 Bias(frequency bias)를 계산하였다. 이를 통해 DBN와 MLP(multilayer perceptron)의 성능을 비교한 결과 DBN의 강수 예측 정확도가 높았고, 수행속도 또한 2배 이상 빨랐다.For accurate precipitation forecasts the choice of weather faoctrs and prediction method is very important. Recently, machinele arning has been widely used for forecasting precipitation, and artificial neural network, one of machine learning techniques, showed good performance. In this paper, we suggest a new method for forecasting precipitation using DBN, one of deep learning techniques.
DBN has an advantage that initial weights are set by unsupervised learning, so this compensates for the defects of artificial neural networks. We used past precipitation, temperature, and the parameters of the sun and moon's motion as features for forecasting precipitation. The dataset consists of observation data which had been measured for 40 years from AWS in Seoul. Experiments were based on 8-fold cross validation. As a result of estimation, we got probabilities of test dataset, so threshold was used for the decision of precipitation. CSI and Bias were used for indicating the precision of precipitation. Our experimental results showed that DBN performed better than MLP.Deep learning, Deep belief network, Precipitation, Forecast"
724,딥러닝 신경망모형을 이용한 실시간 교통정보수집 연구,김대현 /KIM DAE HYON,2016,사단법인 인문사회과학기술융합학회,"비디오 기반의 실시간 교통정보수집 시스템은 자율주행을 포함한 지능형교통체계의 주요 서브시스템중 하나이며, 우수한 예측(추정)력 때문에 핵심 알고리즘으로 인공신경망 모형이 주로 사용되고 있다. 특히, 최근 과학 학술지 네이처에 구글 딥마인드의 알파고가 공개되면서 딥러닝과 인공지능에 대한 전 세계적인 관심이 집중되고 있다. 최근까지 가장 널리 사용된 인공신경망모형은 역전파 모형이었으나 최근 컴퓨터 하드웨어 및 알고리즘의 새로운 개발과 더불어 딥러닝 알고리즘을 이용한 딥인공신경망 모형에 대한 관심이 높아지고 있다. 그러나 아직 딥러닝 인공신경망에 대한 연구는 초기단계에 있는 실정이며 적용분야도 극히 한정되어 있다. 본 연구에서는 실시간 도로교통정보 수집을 위한 영상처리분야에서 딥러닝 알고리즘을 활용하고 실험결과를 통하여 향후 교통정보수집‧분석 분야에서의 딥러닝 인공신경망 모형의 적용가능성을 살펴보고자 한다. 특히, 본 연구에서 도출한 딥러닝 인공신경망모형의 실험결과를 기존 역전파 모형에 의한 실험결과와 비교하여 모형의 우수성을 비교‧평가하고자 한다.Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Normally, artificial neural networks have been used in real-time applications for collecting traffic information because of their remarkable performance in terms of predictive accuracy and real-time computing. Especially, the recent publication of a paper in the journal Nature and the success of Google DeepMind’s world-beating Go program AlphaGo caused significant increases of the public interest in artificial intelligence. Even though various types of learning models have been proposed, the Backpropagation has been the most popular neural network model. However, the Deep learning neural network model has attracted much interest recently. In this study, the Deep leaning neural network will be applied in image processing techniques for the real-time traffic information collection to investigate the performance of Deep learning algorithm in traffic engineering fields. In addition, the experimental results from Deep learing algorithm will be compared with the results from Backpropagation model to evaluate and propose better performing model.Intelligent Transportation Systems (ITS), artificial neural networks, Deep learning, traffic information, image processing"
725,딥러닝의 모형과 응용사례,안성만 /SungMahn Ahn,2016,한국지능정보시스템학회,"딥러닝은 인공신경망(neural network)이라는 인공지능분야의 모형이 발전된 형태로서, 계층구조로 이루어진 인공신경망의 내부계층(hidden layer)이 여러 단계로 이루어진 구조이다. 딥러닝에서의 주요 모형은 합성곱신경망(convolutional neural network), 순환신경망(recurrent neural network), 그리고 심층신뢰신경망(deep belief network)의 세가지라고 할 수있다. 그 중에서 현재 흥미로운 연구가 많이 발표되어서 관심이 집중되고 있는 모형은 지도학습(supervised learning)모형인 처음 두 개의 모형이다. 따라서 본 논문에서는 지도학습모형의 가중치를 최적화하는 기본적인 방법인 오류역전파 알고리즘을 살펴본 뒤에 합성곱신경망과 순환신경망의 구조와 응용사례 등을 살펴보고자 한다. 본문에서 다루지 않은 모형인 심층신뢰신경망은 아직까지는 합성곱신경망 이나 순환신경망보다는 상대적으로 주목을 덜 받고 있다. 그러나 심층신뢰신경망은 CNN이나 RNN과는 달리 비지도학습(unsupervised learning)모형이며, 사람이나 동물은 관찰을 통해서 스스로 학습한다는 점에서 궁극적으로는 비지도학습모형이 더 많이 연구되어야 할 주제가 될 것이다.Deep learning model is a kind of neural networks that allows multiple hidden layers. There are various deep learning architectures such as convolutional neural networks, deep belief networks and recurrent neural networks. Those have been applied to fields like computer vision, automatic speech recognition, natural language processing, audio recognition and bioinformatics where they have been shown to produce state-of-the-art results on various tasks. Among those architectures, convolutional neural networks and recurrent neural networks are classified as the supervised learning model. And in recent years, those supervised learning models have gained more popularity than unsupervised learning models such as deep belief networks, because supervised learning models have shown fashionable applications in such fields mentioned above.
Deep learning models can be trained with backpropagation algorithm. Backpropagation is an abbreviation for “backward propagation of errors” and a common method of training artificial neural networks used in conjunction with an optimization method such as gradient descent. The method calculates the gradient of an error function with respect to all the weights in the network. The gradient is fed to the optimization method which in turn uses it to update the weights, in an attempt to minimize the error function.
Convolutional neural networks use a special architecture which is particularly well-adapted to classify images. Using this architecture makes convolutional networks fast to train. This, in turn, helps us train deep, muti-layer networks, which are very good at classifying images. These days, deep convolutional networks are used in most neural networks for image recognition. Convolutional neural networks use three basic ideas: local receptive fields, shared weights, and pooling. By local receptive fields, we mean that each neuron in the first(or any) hidden layer will be connected to a small region of the input(or previous layer’s) neurons. Shared weights mean that we’re going to use the same weights and bias for each of the local receptive field. This means that all the neurons in the hidden layer detect exactly the same feature, just at different locations in the input image. In addition to the convolutional layers just described, convolutional neural networks also contain pooling layers. Pooling layers are usually used immediately after convolutional layers. What the pooling layers do is to simplify the information in the output from the convolutional layer.
Recent convolutional network architectures have 10 to 20 hidden layers and billions of connections between units. Training deep learning networks has taken weeks several years ago, but thanks to progress in GPU and algorithm enhancement, training time has reduced to several hours.
Neural networks with time-varying behavior are known as recurrent neural networks or RNNs. A recurrent neural network is a class of artificial neural network where connections between units form a directed cycle. This creates an internal state of the network which allows it to exhibit dynamic temporal behavior. Unlike feedforward neural networks, RNNs can use their internal memory to process arbitrary sequences of inputs. Early RNN models turned out to be very difficult to train, harder even than deep feedforward networks. The reason is the unstable gradient problem such as vanishing gradient and exploding gradient. The gradient can get smaller and smaller as it is propagated back through layers. This makes learning in early layers extremely slow. The problem actually gets worse in RNNs, since gradients aren’t just propagated backward through layers, they’re propagated backward through time. If the network runs for a long time, that can make the gradient extremely unstable and hard to learn from. It has been possible to incorporate an idea known as long short-term memory units (LSTMs) into RNNs. LSTMs make it much easier to get good results when training RNNs, and many recent papers make use of LSTMs or related ideas.Deep learning, Convolutional neural networks, Recurrent neural networks, Error backpropagation algorithm"
726,딥러닝분석과 기술적 분석 지표를 이용한 한국 코스피주가지수 방향성 예측,이우식 /Lee Woosik,2017,한국데이터정보과학회,"2016년 3월 구글 (Google)의 바둑인공지능 알파고 (AlphaGo)가 이세돌 9단과의 바둑대결에서승리한 이후 다양한 분야에서 인공지능 사용에 대한 관심이 높아지고 있는 가운데 금융투자 분야에서도 인공지능과 투자자문 전문가의 합성어인 로보어드바이저 (Robo-Advisor)에 대한 관심이 높아지고 있다. 인공지능 (artificial intelligence)기반의 의사결정은 비용 절감은 물론 효과적인 의사결정을가능하게 한다는 점에서 큰 장점이 있다. 본 연구에서는 기술적 분석 (technical analysis) 지표와 딥러닝 (deep learning) 모형을 결합하여 한국 코스피 지수를 예측하는 모형을 개발하고 제시한 모형들의 예측력을 비교, 분석한다. 분석 결과 기술적 분석 지표에 딥러닝 알고리즘을 결합한 모형이 주가지수 방향성 예측 문제에 응용될 수 있음을 확인하였다. 향후 본 연구에서 제안된 기술적 분석 지표와딥러닝모형을 결합한 기법은 로보어드바이저서비스에 응용할 수 있는 일반화 가능성을 보여준다Since Google’s AlphaGo defeated a world champion of Go players in 2016, there have been many interests in the deep learning. In the financial sector, a Robo-Advisor using deep learning gains a significant attention, which builds and manages portfolios of financial instruments for investors.In this paper, we have proposed the a deep learning algorithm geared toward identification and forecast of the KOSPI index direction,and we also have compared the accuracy of the prediction.In an application of forecasting the financial market index direction, we have shown that the Robo-Advisor using deep learning has a significant effect on finance industry. The Robo-Advisor collects a massive data such as earnings statements, news reports and regulatory filings, analyzes those and recommends investors how to view market trends and identify the best time to purchase financial assets. On the other hand, the Robo-Advisor allows businesses to learn more about their customers, develop better marketing strategies, increase sales and decrease costs.Artificial intelligence, deep learning, FinTech, Robo-Advisor, technical analysis."
727,딥러닝의 효율적인 학습을 위한 학습데이터 선별 기법에 관한 연구,주경돈 /Gyoung-don Joo,2017,한국정보과학회,"빅데이터의 증가와 병렬 하드웨어 처리, 학습 알고리즘의 개선은 딥러닝의 발전을 가져왔으며 인공지능이 적용 가능한 영역을 획기적으로 확장시키고 있다. 하지만 이러한 대규모의 학습 데이터에 기반한 딥러닝은 학습시간이 증가되고, 더 나아가 학습 데이터의 구축을 위한 레이블링(labeling) 비용이 데이터 수에 비례하여 증가하게 된다. 따라서 본 논문에서는, 딥러닝의 정확도를 최대한 유지하면서 필요한 학습 데이터의 수를 줄일수 있도록, 전체 데이터에서 제한된 수의 학습 데이터의 후보를 선별하는 기법에 대하여 연구한다. 특히 기존의 능동 학습에 기반한 기법들이 학습 데이터의 수가 매우 적어질 경우 딥러닝의 정확도가 급격히 떨어지는 문제를 해결할 수 있는 새로운 기법을 제안하였으며, 제안된 기법으로 선별된 학습 데이터는 그 수의 변화에 대해 딥러닝이 일관되게 경쟁력있는 정확도를 가짐을 다양한 실험을 통하여 검증하였다.The increase of big data, parallel hardware processing and enhanced learning algorithms have led to the development of deep learning and are dramatically expanding the domain of applications based on artificial intelligence. However, such large-scale training data extends the learning time, and furthermore, the labeling cost for building the training set increases in proportion to the volume of data. Therefore, in this paper, we study how to select a limited number of candidates for training set from given data so as to reduce the number of training set while maintaining the accuracy of deep learning. Especially, we found that, with the training set selected by the previous work, the accuracy of deep learning drastically decreases when the number of training set is very small and so proposed new methods to resolve this fall-off in accuracy. Through experiments, we showed that deep learning consistently has the competitive accuracy with the training set selected by the proposed method regardless of the size of training set.Deep Learning, Train Set Selection, Active Learning, Gamma distribution"
728,딥러닝을 이용한 범용적 스테그아날리시스,김현재 /Hyunjae Kim,2017,한국정보과학회,"스테그아날리시스(Steganalysis)란 이미지 등 일반적인 자료에 암호화된 정보를 은닉하는 스테가노그래피(Steganography)에 대한 검출 및 분석 방법으로, 기계학습 기반 방법론을 포함한다. 기존 기계학습 기반 스테그아날리시스는 영상(Image)의 특징(Feature) 추출 및 모델링에 기반하며, 최근 딥러닝(Deep Learning)의 적용으로 검출 정확도가 큰 폭으로 향상되었다. 하지만 현존하는 스테그아날리시스 모델은 단일 스테가노그래피 기법에 대해 국한되어 있어 학습에 사용되지 않은 스테고(Stego) 이미지의 경우 검출이 불가능한 결정적 한계를 가진다. 본 연구에서는 다양한 스테가노그래피 기법으로 생성된 스테고이미지에 딥러닝을 적용하여 스테그아날리시스를 학습하는 범용적 모델을 제안한다. 다양한 실험을 통해 제안 기법의 효용성 및 가능성을 확인하고, 범용적 스테그아날리시스 모델이 각각에 특화된 검출 기법과 유사한 정확도로 스테고 이미지를 검출할 수 있음을 보인다.Steganalysis is to detect information hidden by steganography inside general data such as images. There are stegoanalysis techniques that use machine learning (ML). Existing ML approaches to steganalysis are based on extracting features from stego images and modeling them.
Recently deep learning-based methodologies have shown significant improvements in detection accuracy. However, all the existing methods, including deep learning-based ones, have a critical limitation in that they can only detect stego images that are created by a specific steganography method. In this paper, we propose a generalized steganalysis method that can model multiple types of stego images using deep learning. Through various experiments, we confirm the effectiveness of our approach and envision directions for future research. In particular, we show that our method can detect each type of steganography with the same level of accuracy as that of a steganalysis method dedicated to that type of steganography, thereby demonstrating the general applicability of our approach to multiple types of stego images.steganalysis, steganography, deep learning, generalized steganalysis model"
729,"딥러닝 프레임워크의 비교: 티아노, 텐서플로, CNTK를 중심으로",정여진 /Yeojin Chung,2017,한국지능정보시스템학회,"딥러닝 프레임워크의 대표적인 기능으로는 ‘자동미분’과 ‘GPU의 활용’ 등을 들 수 있다. 본 논문은 파이썬의 라이브러리 형태로 사용 가능한 프레임워크 중에서 구글의 텐서플로와 마이크로소프트의 CNTK, 그리고 텐서 플로의 원조라고 할 수 있는 티아노를 비교하였다. 본문에서는 자동미분의 개념과 GPU의 활용형태를 간단히 설명하고, 그 다음에 logistic regression을 실행하는 예를 통하여 각 프레임워크의 문법을 알아본 뒤에, 마지막으로 대표적인 딥러닝 응용인 CNN의 예제를 실행시켜보고 코딩의 편의성과 실행속도 등을 확인해 보았다. 그 결과, 편의성의 관점에서 보면 티아노가 가장 코딩 하기가 어렵고, CNTK와 텐서플로는 많은 부분이 비슷하게 추상화 되어 있어서 코딩이 비슷하지만 가중치와 편향을 직접 정의하느냐의 여부에서 차이를 보였다. 그리고 각프레임워크의 실행속도에 대한 평가는 ‘큰 차이는 없다’는 것이다. 텐서플로는 티아노에 비하여 속도가 느리다는 평가가 있어왔는데, 본 연구의 실험에 의하면, 비록 CNN 모형에 국한되었지만, 텐서플로가 아주 조금이지만빠른 것으로 나타났다. CNTK의 경우에도, 비록 실험환경이 달랐지만, 실험환경의 차이에 의한 속도의 차이의 편차범위 이내에 있는 것으로 판단이 되었다. 본 연구에서는 세 종류의 딥러닝 프레임워크만을 살펴보았는데,위키피디아에 따르면 딥러닝 프레임워크의 종류는 12가지가 있으며, 각 프레임워크의 특징을 15가지 속성으로구분하여 차이를 특정하고 있다. 그 많은 속성 중에서 사용자의 입장에서 볼 때 중요한 속성은 어떤 언어(파이썬, C++, Java, 등)로 사용가능한지, 어떤 딥러닝 모형에 대한 라이브러리가 잘 구현되어 있는지 등일 것이다. 그리고 사용자가 대규모의 딥러닝 모형을 구축한다면, 다중 GPU 혹은 다중 서버를 지원하는지의 여부도 중요할 것이다. 또한 딥러닝 모형을 처음 학습하는 경우에는 사용설명서가 많은지 예제 프로그램이 많은지 여부도 중요한 기준이 될 것이다.The deep learning framework is software designed to help develop deep learning models. Some of its important functions include “automatic differentiation” and “utilization of GPU”. The list of popular deep learning framework includes Caffe (BVLC) and Theano (University of Montreal). And recently, Microsoft's deep learning framework, Microsoft Cognitive Toolkit, was released as open-source license, following Google’s Tensorflow a year earlier. The early deep learning frameworks have been developed mainly for research at universities. Beginning with the inception of Tensorflow, however, it seems that companies such as Microsoft and Facebook have started to join the competition of framework development. Given the trend, Google and other companies are expected to continue investing in the deep learning framework to bring forward the initiative in the artificial intelligence business. From this point of view, we think it is a good time to compare some of deep learning frameworks. So we compare three deep learning frameworks which can be used as a Python library. Those are Google's Tensorflow, Microsoft’s CNTK, and Theano which is sort of a predecessor of the preceding two.
The most common and important function of deep learning frameworks is the ability to perform automatic differentiation. Basically all the mathematical expressions of deep learning models can be represented as computational graphs, which consist of nodes and edges. Partial derivatives on each edge of a computational graph can then be obtained. With the partial derivatives, we can let software compute differentiation of any node with respect to any variable by utilizing chain rule of Calculus.
First of all, the convenience of coding is in the order of CNTK, Tensorflow, and Theano. The criterion is simply based on the lengths of the codes and the learning curve and the ease of coding are not the main concern. According to the criteria, Theano was the most difficult to implement with, and CNTK and Tensorflow were somewhat easier. With Tensorflow, we need to define weight variables and biases explicitly. The reason that CNTK and Tensorflow are easier to implement with is that those frameworks provide us with more abstraction than Theano. We, however, need to mention that low-level coding is not always bad. It gives us flexibility of coding. With the low-level coding such as in Theano, we can implement and test any new deep learning models or any new search methods that we can think of.
The assessment of the execution speed of each framework is that there is not meaningful difference. According to the experiment, execution speeds of Theano and Tensorflow are very similar, although the experiment was limited to a CNN model. In the case of CNTK, the experimental environment was not maintained as the same. The code written in CNTK has to be run in PC environment without GPU where codes execute as much as 50 times slower than with GPU. But we concluded that the difference of execution speed was within the range of variation caused by the different hardware setup.
In this study, we compared three types of deep learning framework: Theano, Tensorflow, and CNTK. According to Wikipedia, there are 12 available deep learning frameworks. And 15 different attributes differentiate each framework. Some of the important attributes would include interface language (Python, C ++, Java, etc.) and the availability of libraries on various deep learning models such as CNN, RNN, DBN, and etc. And if a user implements a large scale deep learning model, it will also be important to support multiple GPU or multiple servers. Also, if you are learning the deep learning model, it would also be important if there are enough examples and references.deep learning framework, Theano, TensorFlow, CNTK, computational graph, CIFAR-10"
730,딥러닝을 이용한 주변 무선단말 파악방안,이웅섭 /Lee， Woong-sup,2018,한국정보통신학회,"최근 단말-대-단말(Device-to-device, D2D) 통신기술이 차세대 무선통신시스템의 핵심기술로 큰 관심을 받고 있다. 이러한 단말간 통신에서는 자신의 주변에 어떠한 단말이 있는지 파악하는 주변단말 탐색(Neighbor discovery)이매우 중요하다. 본 논문에서는 최근 큰 관심을 받고 있는 딥러닝(Deep learning) 기술을 활용하여 단말간 통신에서 주변단말을 파악하는 방안에 대해서 제안한다. 제안 방안은 기존의 방안과 달리 무선채널의 공간적 연관성을 이용하여단말간의 신호 전송 없이 단말이 기지국으로 전송하는 상향링크 파일럿 신호를 기반으로 주변 단말을 찾고 따라서기존의 방식에 비해 신호전송 복잡도(signaling complexity)를 크게 줄일 수 있다. 또한 제안 방안에서는 떨어져 있는거리에 따라서 주변 단말을 분류 가능하여 기존 방안에 비해서 좀 더 세밀한 단말 탐색이 가능하다. 마지막으로 본 논문에서는 tensorflow를 이용한 컴퓨터 시뮬레이션을 통해 제안 방안의 성능을 검증하였다.Recently, the device-to-device (D2D) communication has been conceived as the key technology for the next-generationmobile communication systems. The neighbor discovery in which the nearby users are found, is essential for the properoperation of the D2D communication. In this paper, we propose new neighbor discovery scheme based on deep learningtechnology which has gained a lot of attention recently. In the proposed scheme, the neighboring users can be foundusing the uplink pilot transmission of users only, unlike conventional neighbor discovery schemes in which direct pilotcommunication among users is required, such that the signaling overhead can be greatly reduced in our proposed scheme.
Moreover, the neighbors with different proximity can also be classified accordingly which enables more accurate neighbordiscovery compared to the conventional schemes. The performance of our proposed scheme is verified through thetensorflow-based computer simulations.
키워드 : 딥러닝, 주변단말 탐색, 단말-대-단말 통신Deep learning, Neighbor discovery, Device-to-device communication, Wireless channel correlation"
731,신뢰성있는 딥러닝 기반 분석 모델을 참조하기 위한 딥러닝 기술 언어,문종혁 /Jong Hyeok Mun,2021,한국정보처리학회,"최근 딥러닝은 하드웨어 성능이 향상됨에 따라 자연어 처리, 영상 인식 등의 다양한 기술에 접목되어 활용되고 있다. 이러한 기술들을 활용해 지능형교통 시스템(ITS), 스마트홈, 헬스케어 등의 산업분야에서 데이터를 분석하여 고속도로 속도위반 차량 검출, 에너지 사용량 제어, 응급상황 등과 같은고품질의 서비스를 제공하며, 고품질의 서비스를 제공하기 위해서는 정확도가 향상된 딥러닝 모델이 적용되어야 한다. 이를 위해 서비스 환경의 데이터를분석하기 위한 딥러닝 모델을 개발할 때, 개발자는 신뢰성이 검증된 최신의 딥러닝 모델을 적용할 수 있어야 한다. 이는 개발자가 참조하는 딥러닝 모델에적용된 학습 데이터셋의 정확도를 측정하여 검증할 수 있다. 이러한 검증을 위해서 개발자는 학습 데이터셋, 딥러닝의 계층구조 및 개발 환경 등과 같은내용을 포함하는 딥러닝 모델을 문서화하여 적용하기 위한 구조적인 정보가 필요하다. 본 논문에서는 신뢰성있는 딥러닝 기반 데이터 분석 모델을 참조하기위한 딥러닝 기술 언어를 제안한다. 제안하는 기술 언어는 신뢰성 있는 딥러닝 모델을 개발하는데 필요한 학습데이터셋, 개발 환경 및 설정 등의 정보와더불어 딥러닝 모델의 계층구조를 표현할 수 있다. 제안하는 딥러닝 기술 언어를 이용하여 개발자는 지능형 교통 시스템에서 참조하는 분석 모델의정확도를 검증할 수 있다. 실험에서는 제안하는 언어의 유효성을 검증하기 위해, 번호판 인식 모델을 중심으로 딥러닝 기술 문서의 적용과정을 보인다With the recent advancements of deep learning, companies such as smart home, healthcare, and intelligent transportation systems areutilizing its functionality to provide high-quality services for vehicle detection, emergency situation detection, and controlling energyconsumption. To provide reliable services in such sensitive systems, deep learning models are required to have high accuracy. In orderto develop a deep learning model for analyzing previously mentioned services, developers should utilize the state of the art deep learningmodels that have already been verified for higher accuracy. The developers can verify the accuracy of the referenced model by validatingthe model on the dataset. For this validation, the developer needs structural information to document and apply deep learning models,including metadata such as learning dataset, network architecture, and development environments. In this paper, we propose a descriptionlanguage that represents the network architecture of the deep learning model along with its metadata that are necessary to develop adeep learning model. Through the proposed description language, developers can easily verify the accuracy of the referenced deep learningmodel. Our experiments demonstrate the application scenario of a deep learning description document that focuses on the license platerecognition for the detection of illegally parked vehicles.Trusted Deep Learning, Model Reference, Deep Learning Description Language, Traffic Situation Analysis Model"
732,딥러닝 분석을 이용한 중국 역내·외 위안화 변동성 예측,이우식 /Lee Woosik,2016,한국데이터정보과학회,"2008년 글로벌 금융위기 이후 중국은 위안화 국제화의 점진적 추진을 진행하면서 중국상하이 외환시장과 중국홍콩 외환시장에서 거래되는 통화인 역내위안화와 역외위안화를 형성시켰다. 본 연구는 위안화 국제화와 점진적인 중국 자본계정 개방에 따라 급변하는 외환시장상황의 변동성을 정확하게 파악하기 위해서 GARCH모형 (일반화된 자기회귀 조건부이분산성모형)에 다단계인공신경망을 결합한 MLP-GARCH 모형과 GARCH모형과 기계학습의 일종인 딥러닝 (deep learning)을 통합한 DL-GARCH을 가지고 위안화 변동성예측을 비교 실험과 분석을 하였다. 비교분석 결과 DLGARCH 모형은 MLP-GARCH보다 모형 위안화 역내·외 환율변동성 예측 면에서 더욱 더 개선된예측값을 제공하였다. 그래서 이분산시계열모형을 딥러닝과 결합한 DL-GARCH 모형은 시계열의환율 변동성 예측 문제에 딥러닝을 응용할 수 있음을 확인하였다. 향후 이분산시계열과 결합된 딥러닝 모형은 다른 금융시계열 데이터에 응용하여 그 일반화 가능성을 높일 수 있을 것이다.The People’s Republic of China has vigorously been pursuing the internationalization of the Chinese Yuan or Renminbi after the financial crisis of 2008. In this view, an abrupt increase of use of the Chinese Yuan in the onshore and offshore markets are important milestones to be one of important currencies. One of the most frequently used methods to forecast volatility is GARCH model. Since a prediction error of the GARCH model has been reported quite high, a lot of efforts have been made to improve forecasting capability of the GARCH model. In this paper, we have proposed MLPGARCH and a DL-GARCH by employing Artificial Neural Network to the GARCH. In an application to forecasting Chinese Yuan volatility, we have successfully shown their overall outperformance in forecasting over the GARCH.Deep learning, DL-GARCH, GARCH, The Chinese Yuan, volatility."
733,딥러닝 기법을 이용한 내일강수 예측,하지훈,2016,한국지능시스템학회,"정확한 강수예측을 위해서는 예측인자 선정과 예측방법에 대한 선택이 매우 중요하다. 최근에는 강수예측 방법으로기계학습 기법이 많이 사용되고 있으며, 그 중에서도 특히 인공신경망을 사용한 강수예측 방법은 좋은 성능을 보였다. 본논문에서는 딥러닝 기법 중 하나인 DBN(deep belief network)를 이용한 새로운 강수예측 방법을 제안한다. DBN는 비지도사전 학습을 통해 초기 가중치를 설정하여 기존 인공신경망의 문제점을 보완한다. 예측인자로는 기온, 전일-전주 강수일, 태양과 달 궤도 관련 자료를 선정하였다. 기온과 전일-전주 강수일은 서울에서의 1974년부터 2013년까지 총 40년간의AWS(automatic weather system) 관측 자료를 사용하였고, 태양과 달의 궤도 관련 자료는 서울을 중심으로 계산한 결과를사용하였다. 전체 기간에서 일부는 학습 자료로 사용하여 예측모델을 생성하였고, 나머지를 생성한 모델의 검증 자료로사용하였다. 모델 검증 결과로 나온 예측값들은 확률값을 가지며 임계치를 이용하여 강수유무를 판별하였다. 강수 정확도의척도로 양분예보기법 중 CSI(critical successive index)와 Bias(frequency bias)를 계산하였다. 이를 통해 DBN와 MLP(multilayer perceptron)의 성능을 비교한 결과 DBN의 강수 예측 정확도가 높았고, 수행속도 또한 2배 이상 빨랐다.For accurate precipitation forecasts the choice of weather faoctrs and prediction method is very important. Recently, machinele arning has been widely used for forecasting precipitation, and artificial neural network, one of machine learning techniques, showed good performance. In this paper, we suggest a new method for forecasting precipitation using DBN, one of deep learning techniques.
DBN has an advantage that initial weights are set by unsupervised learning, so this compensates for the defects of artificial neural networks. We used past precipitation, temperature, and the parameters of the sun and moon's motion as features for forecasting precipitation. The dataset consists of observation data which had been measured for 40 years from AWS in Seoul. Experiments were based on 8-fold cross validation. As a result of estimation, we got probabilities of test dataset, so threshold was used for the decision of precipitation. CSI and Bias were used for indicating the precision of precipitation. Our experimental results showed that DBN performed better than MLP.Deep learning, Deep belief network, Precipitation, Forecast"
734,딥러닝 신경망모형을 이용한 실시간 교통정보수집 연구,김대현 /KIM DAE HYON,2016,사단법인 인문사회과학기술융합학회,"비디오 기반의 실시간 교통정보수집 시스템은 자율주행을 포함한 지능형교통체계의 주요 서브시스템중 하나이며, 우수한 예측(추정)력 때문에 핵심 알고리즘으로 인공신경망 모형이 주로 사용되고 있다. 특히, 최근 과학 학술지 네이처에 구글 딥마인드의 알파고가 공개되면서 딥러닝과 인공지능에 대한 전 세계적인 관심이 집중되고 있다. 최근까지 가장 널리 사용된 인공신경망모형은 역전파 모형이었으나 최근 컴퓨터 하드웨어 및 알고리즘의 새로운 개발과 더불어 딥러닝 알고리즘을 이용한 딥인공신경망 모형에 대한 관심이 높아지고 있다. 그러나 아직 딥러닝 인공신경망에 대한 연구는 초기단계에 있는 실정이며 적용분야도 극히 한정되어 있다. 본 연구에서는 실시간 도로교통정보 수집을 위한 영상처리분야에서 딥러닝 알고리즘을 활용하고 실험결과를 통하여 향후 교통정보수집‧분석 분야에서의 딥러닝 인공신경망 모형의 적용가능성을 살펴보고자 한다. 특히, 본 연구에서 도출한 딥러닝 인공신경망모형의 실험결과를 기존 역전파 모형에 의한 실험결과와 비교하여 모형의 우수성을 비교‧평가하고자 한다.Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Normally, artificial neural networks have been used in real-time applications for collecting traffic information because of their remarkable performance in terms of predictive accuracy and real-time computing. Especially, the recent publication of a paper in the journal Nature and the success of Google DeepMind’s world-beating Go program AlphaGo caused significant increases of the public interest in artificial intelligence. Even though various types of learning models have been proposed, the Backpropagation has been the most popular neural network model. However, the Deep learning neural network model has attracted much interest recently. In this study, the Deep leaning neural network will be applied in image processing techniques for the real-time traffic information collection to investigate the performance of Deep learning algorithm in traffic engineering fields. In addition, the experimental results from Deep learing algorithm will be compared with the results from Backpropagation model to evaluate and propose better performing model.Intelligent Transportation Systems (ITS), artificial neural networks, Deep learning, traffic information, image processing"
735,딥러닝의 모형과 응용사례,안성만 /SungMahn Ahn,2016,한국지능정보시스템학회,"딥러닝은 인공신경망(neural network)이라는 인공지능분야의 모형이 발전된 형태로서, 계층구조로 이루어진 인공신경망의 내부계층(hidden layer)이 여러 단계로 이루어진 구조이다. 딥러닝에서의 주요 모형은 합성곱신경망(convolutional neural network), 순환신경망(recurrent neural network), 그리고 심층신뢰신경망(deep belief network)의 세가지라고 할 수있다. 그 중에서 현재 흥미로운 연구가 많이 발표되어서 관심이 집중되고 있는 모형은 지도학습(supervised learning)모형인 처음 두 개의 모형이다. 따라서 본 논문에서는 지도학습모형의 가중치를 최적화하는 기본적인 방법인 오류역전파 알고리즘을 살펴본 뒤에 합성곱신경망과 순환신경망의 구조와 응용사례 등을 살펴보고자 한다. 본문에서 다루지 않은 모형인 심층신뢰신경망은 아직까지는 합성곱신경망 이나 순환신경망보다는 상대적으로 주목을 덜 받고 있다. 그러나 심층신뢰신경망은 CNN이나 RNN과는 달리 비지도학습(unsupervised learning)모형이며, 사람이나 동물은 관찰을 통해서 스스로 학습한다는 점에서 궁극적으로는 비지도학습모형이 더 많이 연구되어야 할 주제가 될 것이다.Deep learning model is a kind of neural networks that allows multiple hidden layers. There are various deep learning architectures such as convolutional neural networks, deep belief networks and recurrent neural networks. Those have been applied to fields like computer vision, automatic speech recognition, natural language processing, audio recognition and bioinformatics where they have been shown to produce state-of-the-art results on various tasks. Among those architectures, convolutional neural networks and recurrent neural networks are classified as the supervised learning model. And in recent years, those supervised learning models have gained more popularity than unsupervised learning models such as deep belief networks, because supervised learning models have shown fashionable applications in such fields mentioned above.
Deep learning models can be trained with backpropagation algorithm. Backpropagation is an abbreviation for “backward propagation of errors” and a common method of training artificial neural networks used in conjunction with an optimization method such as gradient descent. The method calculates the gradient of an error function with respect to all the weights in the network. The gradient is fed to the optimization method which in turn uses it to update the weights, in an attempt to minimize the error function.
Convolutional neural networks use a special architecture which is particularly well-adapted to classify images. Using this architecture makes convolutional networks fast to train. This, in turn, helps us train deep, muti-layer networks, which are very good at classifying images. These days, deep convolutional networks are used in most neural networks for image recognition. Convolutional neural networks use three basic ideas: local receptive fields, shared weights, and pooling. By local receptive fields, we mean that each neuron in the first(or any) hidden layer will be connected to a small region of the input(or previous layer’s) neurons. Shared weights mean that we’re going to use the same weights and bias for each of the local receptive field. This means that all the neurons in the hidden layer detect exactly the same feature, just at different locations in the input image. In addition to the convolutional layers just described, convolutional neural networks also contain pooling layers. Pooling layers are usually used immediately after convolutional layers. What the pooling layers do is to simplify the information in the output from the convolutional layer.
Recent convolutional network architectures have 10 to 20 hidden layers and billions of connections between units. Training deep learning networks has taken weeks several years ago, but thanks to progress in GPU and algorithm enhancement, training time has reduced to several hours.
Neural networks with time-varying behavior are known as recurrent neural networks or RNNs. A recurrent neural network is a class of artificial neural network where connections between units form a directed cycle. This creates an internal state of the network which allows it to exhibit dynamic temporal behavior. Unlike feedforward neural networks, RNNs can use their internal memory to process arbitrary sequences of inputs. Early RNN models turned out to be very difficult to train, harder even than deep feedforward networks. The reason is the unstable gradient problem such as vanishing gradient and exploding gradient. The gradient can get smaller and smaller as it is propagated back through layers. This makes learning in early layers extremely slow. The problem actually gets worse in RNNs, since gradients aren’t just propagated backward through layers, they’re propagated backward through time. If the network runs for a long time, that can make the gradient extremely unstable and hard to learn from. It has been possible to incorporate an idea known as long short-term memory units (LSTMs) into RNNs. LSTMs make it much easier to get good results when training RNNs, and many recent papers make use of LSTMs or related ideas.Deep learning, Convolutional neural networks, Recurrent neural networks, Error backpropagation algorithm"
736,딥러닝분석과 기술적 분석 지표를 이용한 한국 코스피주가지수 방향성 예측,이우식 /Lee Woosik,2017,한국데이터정보과학회,"2016년 3월 구글 (Google)의 바둑인공지능 알파고 (AlphaGo)가 이세돌 9단과의 바둑대결에서승리한 이후 다양한 분야에서 인공지능 사용에 대한 관심이 높아지고 있는 가운데 금융투자 분야에서도 인공지능과 투자자문 전문가의 합성어인 로보어드바이저 (Robo-Advisor)에 대한 관심이 높아지고 있다. 인공지능 (artificial intelligence)기반의 의사결정은 비용 절감은 물론 효과적인 의사결정을가능하게 한다는 점에서 큰 장점이 있다. 본 연구에서는 기술적 분석 (technical analysis) 지표와 딥러닝 (deep learning) 모형을 결합하여 한국 코스피 지수를 예측하는 모형을 개발하고 제시한 모형들의 예측력을 비교, 분석한다. 분석 결과 기술적 분석 지표에 딥러닝 알고리즘을 결합한 모형이 주가지수 방향성 예측 문제에 응용될 수 있음을 확인하였다. 향후 본 연구에서 제안된 기술적 분석 지표와딥러닝모형을 결합한 기법은 로보어드바이저서비스에 응용할 수 있는 일반화 가능성을 보여준다Since Google’s AlphaGo defeated a world champion of Go players in 2016, there have been many interests in the deep learning. In the financial sector, a Robo-Advisor using deep learning gains a significant attention, which builds and manages portfolios of financial instruments for investors.In this paper, we have proposed the a deep learning algorithm geared toward identification and forecast of the KOSPI index direction,and we also have compared the accuracy of the prediction.In an application of forecasting the financial market index direction, we have shown that the Robo-Advisor using deep learning has a significant effect on finance industry. The Robo-Advisor collects a massive data such as earnings statements, news reports and regulatory filings, analyzes those and recommends investors how to view market trends and identify the best time to purchase financial assets. On the other hand, the Robo-Advisor allows businesses to learn more about their customers, develop better marketing strategies, increase sales and decrease costs.Artificial intelligence, deep learning, FinTech, Robo-Advisor, technical analysis."
737,딥러닝의 효율적인 학습을 위한 학습데이터 선별 기법에 관한 연구,주경돈 /Gyoung-don Joo,2017,한국정보과학회,"빅데이터의 증가와 병렬 하드웨어 처리, 학습 알고리즘의 개선은 딥러닝의 발전을 가져왔으며 인공지능이 적용 가능한 영역을 획기적으로 확장시키고 있다. 하지만 이러한 대규모의 학습 데이터에 기반한 딥러닝은 학습시간이 증가되고, 더 나아가 학습 데이터의 구축을 위한 레이블링(labeling) 비용이 데이터 수에 비례하여 증가하게 된다. 따라서 본 논문에서는, 딥러닝의 정확도를 최대한 유지하면서 필요한 학습 데이터의 수를 줄일수 있도록, 전체 데이터에서 제한된 수의 학습 데이터의 후보를 선별하는 기법에 대하여 연구한다. 특히 기존의 능동 학습에 기반한 기법들이 학습 데이터의 수가 매우 적어질 경우 딥러닝의 정확도가 급격히 떨어지는 문제를 해결할 수 있는 새로운 기법을 제안하였으며, 제안된 기법으로 선별된 학습 데이터는 그 수의 변화에 대해 딥러닝이 일관되게 경쟁력있는 정확도를 가짐을 다양한 실험을 통하여 검증하였다.The increase of big data, parallel hardware processing and enhanced learning algorithms have led to the development of deep learning and are dramatically expanding the domain of applications based on artificial intelligence. However, such large-scale training data extends the learning time, and furthermore, the labeling cost for building the training set increases in proportion to the volume of data. Therefore, in this paper, we study how to select a limited number of candidates for training set from given data so as to reduce the number of training set while maintaining the accuracy of deep learning. Especially, we found that, with the training set selected by the previous work, the accuracy of deep learning drastically decreases when the number of training set is very small and so proposed new methods to resolve this fall-off in accuracy. Through experiments, we showed that deep learning consistently has the competitive accuracy with the training set selected by the proposed method regardless of the size of training set.Deep Learning, Train Set Selection, Active Learning, Gamma distribution"
738,딥러닝을 이용한 범용적 스테그아날리시스,김현재 /Hyunjae Kim,2017,한국정보과학회,"스테그아날리시스(Steganalysis)란 이미지 등 일반적인 자료에 암호화된 정보를 은닉하는 스테가노그래피(Steganography)에 대한 검출 및 분석 방법으로, 기계학습 기반 방법론을 포함한다. 기존 기계학습 기반 스테그아날리시스는 영상(Image)의 특징(Feature) 추출 및 모델링에 기반하며, 최근 딥러닝(Deep Learning)의 적용으로 검출 정확도가 큰 폭으로 향상되었다. 하지만 현존하는 스테그아날리시스 모델은 단일 스테가노그래피 기법에 대해 국한되어 있어 학습에 사용되지 않은 스테고(Stego) 이미지의 경우 검출이 불가능한 결정적 한계를 가진다. 본 연구에서는 다양한 스테가노그래피 기법으로 생성된 스테고이미지에 딥러닝을 적용하여 스테그아날리시스를 학습하는 범용적 모델을 제안한다. 다양한 실험을 통해 제안 기법의 효용성 및 가능성을 확인하고, 범용적 스테그아날리시스 모델이 각각에 특화된 검출 기법과 유사한 정확도로 스테고 이미지를 검출할 수 있음을 보인다.Steganalysis is to detect information hidden by steganography inside general data such as images. There are stegoanalysis techniques that use machine learning (ML). Existing ML approaches to steganalysis are based on extracting features from stego images and modeling them.
Recently deep learning-based methodologies have shown significant improvements in detection accuracy. However, all the existing methods, including deep learning-based ones, have a critical limitation in that they can only detect stego images that are created by a specific steganography method. In this paper, we propose a generalized steganalysis method that can model multiple types of stego images using deep learning. Through various experiments, we confirm the effectiveness of our approach and envision directions for future research. In particular, we show that our method can detect each type of steganography with the same level of accuracy as that of a steganalysis method dedicated to that type of steganography, thereby demonstrating the general applicability of our approach to multiple types of stego images.steganalysis, steganography, deep learning, generalized steganalysis model"
739,"딥러닝 프레임워크의 비교: 티아노, 텐서플로, CNTK를 중심으로",정여진 /Yeojin Chung,2017,한국지능정보시스템학회,"딥러닝 프레임워크의 대표적인 기능으로는 ‘자동미분’과 ‘GPU의 활용’ 등을 들 수 있다. 본 논문은 파이썬의 라이브러리 형태로 사용 가능한 프레임워크 중에서 구글의 텐서플로와 마이크로소프트의 CNTK, 그리고 텐서 플로의 원조라고 할 수 있는 티아노를 비교하였다. 본문에서는 자동미분의 개념과 GPU의 활용형태를 간단히 설명하고, 그 다음에 logistic regression을 실행하는 예를 통하여 각 프레임워크의 문법을 알아본 뒤에, 마지막으로 대표적인 딥러닝 응용인 CNN의 예제를 실행시켜보고 코딩의 편의성과 실행속도 등을 확인해 보았다. 그 결과, 편의성의 관점에서 보면 티아노가 가장 코딩 하기가 어렵고, CNTK와 텐서플로는 많은 부분이 비슷하게 추상화 되어 있어서 코딩이 비슷하지만 가중치와 편향을 직접 정의하느냐의 여부에서 차이를 보였다. 그리고 각프레임워크의 실행속도에 대한 평가는 ‘큰 차이는 없다’는 것이다. 텐서플로는 티아노에 비하여 속도가 느리다는 평가가 있어왔는데, 본 연구의 실험에 의하면, 비록 CNN 모형에 국한되었지만, 텐서플로가 아주 조금이지만빠른 것으로 나타났다. CNTK의 경우에도, 비록 실험환경이 달랐지만, 실험환경의 차이에 의한 속도의 차이의 편차범위 이내에 있는 것으로 판단이 되었다. 본 연구에서는 세 종류의 딥러닝 프레임워크만을 살펴보았는데,위키피디아에 따르면 딥러닝 프레임워크의 종류는 12가지가 있으며, 각 프레임워크의 특징을 15가지 속성으로구분하여 차이를 특정하고 있다. 그 많은 속성 중에서 사용자의 입장에서 볼 때 중요한 속성은 어떤 언어(파이썬, C++, Java, 등)로 사용가능한지, 어떤 딥러닝 모형에 대한 라이브러리가 잘 구현되어 있는지 등일 것이다. 그리고 사용자가 대규모의 딥러닝 모형을 구축한다면, 다중 GPU 혹은 다중 서버를 지원하는지의 여부도 중요할 것이다. 또한 딥러닝 모형을 처음 학습하는 경우에는 사용설명서가 많은지 예제 프로그램이 많은지 여부도 중요한 기준이 될 것이다.The deep learning framework is software designed to help develop deep learning models. Some of its important functions include “automatic differentiation” and “utilization of GPU”. The list of popular deep learning framework includes Caffe (BVLC) and Theano (University of Montreal). And recently, Microsoft's deep learning framework, Microsoft Cognitive Toolkit, was released as open-source license, following Google’s Tensorflow a year earlier. The early deep learning frameworks have been developed mainly for research at universities. Beginning with the inception of Tensorflow, however, it seems that companies such as Microsoft and Facebook have started to join the competition of framework development. Given the trend, Google and other companies are expected to continue investing in the deep learning framework to bring forward the initiative in the artificial intelligence business. From this point of view, we think it is a good time to compare some of deep learning frameworks. So we compare three deep learning frameworks which can be used as a Python library. Those are Google's Tensorflow, Microsoft’s CNTK, and Theano which is sort of a predecessor of the preceding two.
The most common and important function of deep learning frameworks is the ability to perform automatic differentiation. Basically all the mathematical expressions of deep learning models can be represented as computational graphs, which consist of nodes and edges. Partial derivatives on each edge of a computational graph can then be obtained. With the partial derivatives, we can let software compute differentiation of any node with respect to any variable by utilizing chain rule of Calculus.
First of all, the convenience of coding is in the order of CNTK, Tensorflow, and Theano. The criterion is simply based on the lengths of the codes and the learning curve and the ease of coding are not the main concern. According to the criteria, Theano was the most difficult to implement with, and CNTK and Tensorflow were somewhat easier. With Tensorflow, we need to define weight variables and biases explicitly. The reason that CNTK and Tensorflow are easier to implement with is that those frameworks provide us with more abstraction than Theano. We, however, need to mention that low-level coding is not always bad. It gives us flexibility of coding. With the low-level coding such as in Theano, we can implement and test any new deep learning models or any new search methods that we can think of.
The assessment of the execution speed of each framework is that there is not meaningful difference. According to the experiment, execution speeds of Theano and Tensorflow are very similar, although the experiment was limited to a CNN model. In the case of CNTK, the experimental environment was not maintained as the same. The code written in CNTK has to be run in PC environment without GPU where codes execute as much as 50 times slower than with GPU. But we concluded that the difference of execution speed was within the range of variation caused by the different hardware setup.
In this study, we compared three types of deep learning framework: Theano, Tensorflow, and CNTK. According to Wikipedia, there are 12 available deep learning frameworks. And 15 different attributes differentiate each framework. Some of the important attributes would include interface language (Python, C ++, Java, etc.) and the availability of libraries on various deep learning models such as CNN, RNN, DBN, and etc. And if a user implements a large scale deep learning model, it will also be important to support multiple GPU or multiple servers. Also, if you are learning the deep learning model, it would also be important if there are enough examples and references.deep learning framework, Theano, TensorFlow, CNTK, computational graph, CIFAR-10"
740,딥러닝을 이용한 주변 무선단말 파악방안,이웅섭 /Lee， Woong-sup,2018,한국정보통신학회,"최근 단말-대-단말(Device-to-device, D2D) 통신기술이 차세대 무선통신시스템의 핵심기술로 큰 관심을 받고 있다. 이러한 단말간 통신에서는 자신의 주변에 어떠한 단말이 있는지 파악하는 주변단말 탐색(Neighbor discovery)이매우 중요하다. 본 논문에서는 최근 큰 관심을 받고 있는 딥러닝(Deep learning) 기술을 활용하여 단말간 통신에서 주변단말을 파악하는 방안에 대해서 제안한다. 제안 방안은 기존의 방안과 달리 무선채널의 공간적 연관성을 이용하여단말간의 신호 전송 없이 단말이 기지국으로 전송하는 상향링크 파일럿 신호를 기반으로 주변 단말을 찾고 따라서기존의 방식에 비해 신호전송 복잡도(signaling complexity)를 크게 줄일 수 있다. 또한 제안 방안에서는 떨어져 있는거리에 따라서 주변 단말을 분류 가능하여 기존 방안에 비해서 좀 더 세밀한 단말 탐색이 가능하다. 마지막으로 본 논문에서는 tensorflow를 이용한 컴퓨터 시뮬레이션을 통해 제안 방안의 성능을 검증하였다.Recently, the device-to-device (D2D) communication has been conceived as the key technology for the next-generationmobile communication systems. The neighbor discovery in which the nearby users are found, is essential for the properoperation of the D2D communication. In this paper, we propose new neighbor discovery scheme based on deep learningtechnology which has gained a lot of attention recently. In the proposed scheme, the neighboring users can be foundusing the uplink pilot transmission of users only, unlike conventional neighbor discovery schemes in which direct pilotcommunication among users is required, such that the signaling overhead can be greatly reduced in our proposed scheme.
Moreover, the neighbors with different proximity can also be classified accordingly which enables more accurate neighbordiscovery compared to the conventional schemes. The performance of our proposed scheme is verified through thetensorflow-based computer simulations.
키워드 : 딥러닝, 주변단말 탐색, 단말-대-단말 통신Deep learning, Neighbor discovery, Device-to-device communication, Wireless channel correlation"
741,신뢰성있는 딥러닝 기반 분석 모델을 참조하기 위한 딥러닝 기술 언어,문종혁 /Jong Hyeok Mun,2021,한국정보처리학회,"최근 딥러닝은 하드웨어 성능이 향상됨에 따라 자연어 처리, 영상 인식 등의 다양한 기술에 접목되어 활용되고 있다. 이러한 기술들을 활용해 지능형교통 시스템(ITS), 스마트홈, 헬스케어 등의 산업분야에서 데이터를 분석하여 고속도로 속도위반 차량 검출, 에너지 사용량 제어, 응급상황 등과 같은고품질의 서비스를 제공하며, 고품질의 서비스를 제공하기 위해서는 정확도가 향상된 딥러닝 모델이 적용되어야 한다. 이를 위해 서비스 환경의 데이터를분석하기 위한 딥러닝 모델을 개발할 때, 개발자는 신뢰성이 검증된 최신의 딥러닝 모델을 적용할 수 있어야 한다. 이는 개발자가 참조하는 딥러닝 모델에적용된 학습 데이터셋의 정확도를 측정하여 검증할 수 있다. 이러한 검증을 위해서 개발자는 학습 데이터셋, 딥러닝의 계층구조 및 개발 환경 등과 같은내용을 포함하는 딥러닝 모델을 문서화하여 적용하기 위한 구조적인 정보가 필요하다. 본 논문에서는 신뢰성있는 딥러닝 기반 데이터 분석 모델을 참조하기위한 딥러닝 기술 언어를 제안한다. 제안하는 기술 언어는 신뢰성 있는 딥러닝 모델을 개발하는데 필요한 학습데이터셋, 개발 환경 및 설정 등의 정보와더불어 딥러닝 모델의 계층구조를 표현할 수 있다. 제안하는 딥러닝 기술 언어를 이용하여 개발자는 지능형 교통 시스템에서 참조하는 분석 모델의정확도를 검증할 수 있다. 실험에서는 제안하는 언어의 유효성을 검증하기 위해, 번호판 인식 모델을 중심으로 딥러닝 기술 문서의 적용과정을 보인다With the recent advancements of deep learning, companies such as smart home, healthcare, and intelligent transportation systems areutilizing its functionality to provide high-quality services for vehicle detection, emergency situation detection, and controlling energyconsumption. To provide reliable services in such sensitive systems, deep learning models are required to have high accuracy. In orderto develop a deep learning model for analyzing previously mentioned services, developers should utilize the state of the art deep learningmodels that have already been verified for higher accuracy. The developers can verify the accuracy of the referenced model by validatingthe model on the dataset. For this validation, the developer needs structural information to document and apply deep learning models,including metadata such as learning dataset, network architecture, and development environments. In this paper, we propose a descriptionlanguage that represents the network architecture of the deep learning model along with its metadata that are necessary to develop adeep learning model. Through the proposed description language, developers can easily verify the accuracy of the referenced deep learningmodel. Our experiments demonstrate the application scenario of a deep learning description document that focuses on the license platerecognition for the detection of illegally parked vehicles.Trusted Deep Learning, Model Reference, Deep Learning Description Language, Traffic Situation Analysis Model"
742,딥러닝 분석을 이용한 중국 역내·외 위안화 변동성 예측,이우식 /Lee Woosik,2016,한국데이터정보과학회,"2008년 글로벌 금융위기 이후 중국은 위안화 국제화의 점진적 추진을 진행하면서 중국상하이 외환시장과 중국홍콩 외환시장에서 거래되는 통화인 역내위안화와 역외위안화를 형성시켰다. 본 연구는 위안화 국제화와 점진적인 중국 자본계정 개방에 따라 급변하는 외환시장상황의 변동성을 정확하게 파악하기 위해서 GARCH모형 (일반화된 자기회귀 조건부이분산성모형)에 다단계인공신경망을 결합한 MLP-GARCH 모형과 GARCH모형과 기계학습의 일종인 딥러닝 (deep learning)을 통합한 DL-GARCH을 가지고 위안화 변동성예측을 비교 실험과 분석을 하였다. 비교분석 결과 DLGARCH 모형은 MLP-GARCH보다 모형 위안화 역내·외 환율변동성 예측 면에서 더욱 더 개선된예측값을 제공하였다. 그래서 이분산시계열모형을 딥러닝과 결합한 DL-GARCH 모형은 시계열의환율 변동성 예측 문제에 딥러닝을 응용할 수 있음을 확인하였다. 향후 이분산시계열과 결합된 딥러닝 모형은 다른 금융시계열 데이터에 응용하여 그 일반화 가능성을 높일 수 있을 것이다.The People’s Republic of China has vigorously been pursuing the internationalization of the Chinese Yuan or Renminbi after the financial crisis of 2008. In this view, an abrupt increase of use of the Chinese Yuan in the onshore and offshore markets are important milestones to be one of important currencies. One of the most frequently used methods to forecast volatility is GARCH model. Since a prediction error of the GARCH model has been reported quite high, a lot of efforts have been made to improve forecasting capability of the GARCH model. In this paper, we have proposed MLPGARCH and a DL-GARCH by employing Artificial Neural Network to the GARCH. In an application to forecasting Chinese Yuan volatility, we have successfully shown their overall outperformance in forecasting over the GARCH.Deep learning, DL-GARCH, GARCH, The Chinese Yuan, volatility."
743,딥러닝 기법을 이용한 내일강수 예측,하지훈,2016,한국지능시스템학회,"정확한 강수예측을 위해서는 예측인자 선정과 예측방법에 대한 선택이 매우 중요하다. 최근에는 강수예측 방법으로기계학습 기법이 많이 사용되고 있으며, 그 중에서도 특히 인공신경망을 사용한 강수예측 방법은 좋은 성능을 보였다. 본논문에서는 딥러닝 기법 중 하나인 DBN(deep belief network)를 이용한 새로운 강수예측 방법을 제안한다. DBN는 비지도사전 학습을 통해 초기 가중치를 설정하여 기존 인공신경망의 문제점을 보완한다. 예측인자로는 기온, 전일-전주 강수일, 태양과 달 궤도 관련 자료를 선정하였다. 기온과 전일-전주 강수일은 서울에서의 1974년부터 2013년까지 총 40년간의AWS(automatic weather system) 관측 자료를 사용하였고, 태양과 달의 궤도 관련 자료는 서울을 중심으로 계산한 결과를사용하였다. 전체 기간에서 일부는 학습 자료로 사용하여 예측모델을 생성하였고, 나머지를 생성한 모델의 검증 자료로사용하였다. 모델 검증 결과로 나온 예측값들은 확률값을 가지며 임계치를 이용하여 강수유무를 판별하였다. 강수 정확도의척도로 양분예보기법 중 CSI(critical successive index)와 Bias(frequency bias)를 계산하였다. 이를 통해 DBN와 MLP(multilayer perceptron)의 성능을 비교한 결과 DBN의 강수 예측 정확도가 높았고, 수행속도 또한 2배 이상 빨랐다.For accurate precipitation forecasts the choice of weather faoctrs and prediction method is very important. Recently, machinele arning has been widely used for forecasting precipitation, and artificial neural network, one of machine learning techniques, showed good performance. In this paper, we suggest a new method for forecasting precipitation using DBN, one of deep learning techniques.
DBN has an advantage that initial weights are set by unsupervised learning, so this compensates for the defects of artificial neural networks. We used past precipitation, temperature, and the parameters of the sun and moon's motion as features for forecasting precipitation. The dataset consists of observation data which had been measured for 40 years from AWS in Seoul. Experiments were based on 8-fold cross validation. As a result of estimation, we got probabilities of test dataset, so threshold was used for the decision of precipitation. CSI and Bias were used for indicating the precision of precipitation. Our experimental results showed that DBN performed better than MLP.Deep learning, Deep belief network, Precipitation, Forecast"
744,딥러닝 신경망모형을 이용한 실시간 교통정보수집 연구,김대현 /KIM DAE HYON,2016,사단법인 인문사회과학기술융합학회,"비디오 기반의 실시간 교통정보수집 시스템은 자율주행을 포함한 지능형교통체계의 주요 서브시스템중 하나이며, 우수한 예측(추정)력 때문에 핵심 알고리즘으로 인공신경망 모형이 주로 사용되고 있다. 특히, 최근 과학 학술지 네이처에 구글 딥마인드의 알파고가 공개되면서 딥러닝과 인공지능에 대한 전 세계적인 관심이 집중되고 있다. 최근까지 가장 널리 사용된 인공신경망모형은 역전파 모형이었으나 최근 컴퓨터 하드웨어 및 알고리즘의 새로운 개발과 더불어 딥러닝 알고리즘을 이용한 딥인공신경망 모형에 대한 관심이 높아지고 있다. 그러나 아직 딥러닝 인공신경망에 대한 연구는 초기단계에 있는 실정이며 적용분야도 극히 한정되어 있다. 본 연구에서는 실시간 도로교통정보 수집을 위한 영상처리분야에서 딥러닝 알고리즘을 활용하고 실험결과를 통하여 향후 교통정보수집‧분석 분야에서의 딥러닝 인공신경망 모형의 적용가능성을 살펴보고자 한다. 특히, 본 연구에서 도출한 딥러닝 인공신경망모형의 실험결과를 기존 역전파 모형에 의한 실험결과와 비교하여 모형의 우수성을 비교‧평가하고자 한다.Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Normally, artificial neural networks have been used in real-time applications for collecting traffic information because of their remarkable performance in terms of predictive accuracy and real-time computing. Especially, the recent publication of a paper in the journal Nature and the success of Google DeepMind’s world-beating Go program AlphaGo caused significant increases of the public interest in artificial intelligence. Even though various types of learning models have been proposed, the Backpropagation has been the most popular neural network model. However, the Deep learning neural network model has attracted much interest recently. In this study, the Deep leaning neural network will be applied in image processing techniques for the real-time traffic information collection to investigate the performance of Deep learning algorithm in traffic engineering fields. In addition, the experimental results from Deep learing algorithm will be compared with the results from Backpropagation model to evaluate and propose better performing model.Intelligent Transportation Systems (ITS), artificial neural networks, Deep learning, traffic information, image processing"
745,딥러닝의 모형과 응용사례,안성만 /SungMahn Ahn,2016,한국지능정보시스템학회,"딥러닝은 인공신경망(neural network)이라는 인공지능분야의 모형이 발전된 형태로서, 계층구조로 이루어진 인공신경망의 내부계층(hidden layer)이 여러 단계로 이루어진 구조이다. 딥러닝에서의 주요 모형은 합성곱신경망(convolutional neural network), 순환신경망(recurrent neural network), 그리고 심층신뢰신경망(deep belief network)의 세가지라고 할 수있다. 그 중에서 현재 흥미로운 연구가 많이 발표되어서 관심이 집중되고 있는 모형은 지도학습(supervised learning)모형인 처음 두 개의 모형이다. 따라서 본 논문에서는 지도학습모형의 가중치를 최적화하는 기본적인 방법인 오류역전파 알고리즘을 살펴본 뒤에 합성곱신경망과 순환신경망의 구조와 응용사례 등을 살펴보고자 한다. 본문에서 다루지 않은 모형인 심층신뢰신경망은 아직까지는 합성곱신경망 이나 순환신경망보다는 상대적으로 주목을 덜 받고 있다. 그러나 심층신뢰신경망은 CNN이나 RNN과는 달리 비지도학습(unsupervised learning)모형이며, 사람이나 동물은 관찰을 통해서 스스로 학습한다는 점에서 궁극적으로는 비지도학습모형이 더 많이 연구되어야 할 주제가 될 것이다.Deep learning model is a kind of neural networks that allows multiple hidden layers. There are various deep learning architectures such as convolutional neural networks, deep belief networks and recurrent neural networks. Those have been applied to fields like computer vision, automatic speech recognition, natural language processing, audio recognition and bioinformatics where they have been shown to produce state-of-the-art results on various tasks. Among those architectures, convolutional neural networks and recurrent neural networks are classified as the supervised learning model. And in recent years, those supervised learning models have gained more popularity than unsupervised learning models such as deep belief networks, because supervised learning models have shown fashionable applications in such fields mentioned above.
Deep learning models can be trained with backpropagation algorithm. Backpropagation is an abbreviation for “backward propagation of errors” and a common method of training artificial neural networks used in conjunction with an optimization method such as gradient descent. The method calculates the gradient of an error function with respect to all the weights in the network. The gradient is fed to the optimization method which in turn uses it to update the weights, in an attempt to minimize the error function.
Convolutional neural networks use a special architecture which is particularly well-adapted to classify images. Using this architecture makes convolutional networks fast to train. This, in turn, helps us train deep, muti-layer networks, which are very good at classifying images. These days, deep convolutional networks are used in most neural networks for image recognition. Convolutional neural networks use three basic ideas: local receptive fields, shared weights, and pooling. By local receptive fields, we mean that each neuron in the first(or any) hidden layer will be connected to a small region of the input(or previous layer’s) neurons. Shared weights mean that we’re going to use the same weights and bias for each of the local receptive field. This means that all the neurons in the hidden layer detect exactly the same feature, just at different locations in the input image. In addition to the convolutional layers just described, convolutional neural networks also contain pooling layers. Pooling layers are usually used immediately after convolutional layers. What the pooling layers do is to simplify the information in the output from the convolutional layer.
Recent convolutional network architectures have 10 to 20 hidden layers and billions of connections between units. Training deep learning networks has taken weeks several years ago, but thanks to progress in GPU and algorithm enhancement, training time has reduced to several hours.
Neural networks with time-varying behavior are known as recurrent neural networks or RNNs. A recurrent neural network is a class of artificial neural network where connections between units form a directed cycle. This creates an internal state of the network which allows it to exhibit dynamic temporal behavior. Unlike feedforward neural networks, RNNs can use their internal memory to process arbitrary sequences of inputs. Early RNN models turned out to be very difficult to train, harder even than deep feedforward networks. The reason is the unstable gradient problem such as vanishing gradient and exploding gradient. The gradient can get smaller and smaller as it is propagated back through layers. This makes learning in early layers extremely slow. The problem actually gets worse in RNNs, since gradients aren’t just propagated backward through layers, they’re propagated backward through time. If the network runs for a long time, that can make the gradient extremely unstable and hard to learn from. It has been possible to incorporate an idea known as long short-term memory units (LSTMs) into RNNs. LSTMs make it much easier to get good results when training RNNs, and many recent papers make use of LSTMs or related ideas.Deep learning, Convolutional neural networks, Recurrent neural networks, Error backpropagation algorithm"
746,딥러닝분석과 기술적 분석 지표를 이용한 한국 코스피주가지수 방향성 예측,이우식 /Lee Woosik,2017,한국데이터정보과학회,"2016년 3월 구글 (Google)의 바둑인공지능 알파고 (AlphaGo)가 이세돌 9단과의 바둑대결에서승리한 이후 다양한 분야에서 인공지능 사용에 대한 관심이 높아지고 있는 가운데 금융투자 분야에서도 인공지능과 투자자문 전문가의 합성어인 로보어드바이저 (Robo-Advisor)에 대한 관심이 높아지고 있다. 인공지능 (artificial intelligence)기반의 의사결정은 비용 절감은 물론 효과적인 의사결정을가능하게 한다는 점에서 큰 장점이 있다. 본 연구에서는 기술적 분석 (technical analysis) 지표와 딥러닝 (deep learning) 모형을 결합하여 한국 코스피 지수를 예측하는 모형을 개발하고 제시한 모형들의 예측력을 비교, 분석한다. 분석 결과 기술적 분석 지표에 딥러닝 알고리즘을 결합한 모형이 주가지수 방향성 예측 문제에 응용될 수 있음을 확인하였다. 향후 본 연구에서 제안된 기술적 분석 지표와딥러닝모형을 결합한 기법은 로보어드바이저서비스에 응용할 수 있는 일반화 가능성을 보여준다Since Google’s AlphaGo defeated a world champion of Go players in 2016, there have been many interests in the deep learning. In the financial sector, a Robo-Advisor using deep learning gains a significant attention, which builds and manages portfolios of financial instruments for investors.In this paper, we have proposed the a deep learning algorithm geared toward identification and forecast of the KOSPI index direction,and we also have compared the accuracy of the prediction.In an application of forecasting the financial market index direction, we have shown that the Robo-Advisor using deep learning has a significant effect on finance industry. The Robo-Advisor collects a massive data such as earnings statements, news reports and regulatory filings, analyzes those and recommends investors how to view market trends and identify the best time to purchase financial assets. On the other hand, the Robo-Advisor allows businesses to learn more about their customers, develop better marketing strategies, increase sales and decrease costs.Artificial intelligence, deep learning, FinTech, Robo-Advisor, technical analysis."
747,딥러닝의 효율적인 학습을 위한 학습데이터 선별 기법에 관한 연구,주경돈 /Gyoung-don Joo,2017,한국정보과학회,"빅데이터의 증가와 병렬 하드웨어 처리, 학습 알고리즘의 개선은 딥러닝의 발전을 가져왔으며 인공지능이 적용 가능한 영역을 획기적으로 확장시키고 있다. 하지만 이러한 대규모의 학습 데이터에 기반한 딥러닝은 학습시간이 증가되고, 더 나아가 학습 데이터의 구축을 위한 레이블링(labeling) 비용이 데이터 수에 비례하여 증가하게 된다. 따라서 본 논문에서는, 딥러닝의 정확도를 최대한 유지하면서 필요한 학습 데이터의 수를 줄일수 있도록, 전체 데이터에서 제한된 수의 학습 데이터의 후보를 선별하는 기법에 대하여 연구한다. 특히 기존의 능동 학습에 기반한 기법들이 학습 데이터의 수가 매우 적어질 경우 딥러닝의 정확도가 급격히 떨어지는 문제를 해결할 수 있는 새로운 기법을 제안하였으며, 제안된 기법으로 선별된 학습 데이터는 그 수의 변화에 대해 딥러닝이 일관되게 경쟁력있는 정확도를 가짐을 다양한 실험을 통하여 검증하였다.The increase of big data, parallel hardware processing and enhanced learning algorithms have led to the development of deep learning and are dramatically expanding the domain of applications based on artificial intelligence. However, such large-scale training data extends the learning time, and furthermore, the labeling cost for building the training set increases in proportion to the volume of data. Therefore, in this paper, we study how to select a limited number of candidates for training set from given data so as to reduce the number of training set while maintaining the accuracy of deep learning. Especially, we found that, with the training set selected by the previous work, the accuracy of deep learning drastically decreases when the number of training set is very small and so proposed new methods to resolve this fall-off in accuracy. Through experiments, we showed that deep learning consistently has the competitive accuracy with the training set selected by the proposed method regardless of the size of training set.Deep Learning, Train Set Selection, Active Learning, Gamma distribution"
748,딥러닝을 이용한 범용적 스테그아날리시스,김현재 /Hyunjae Kim,2017,한국정보과학회,"스테그아날리시스(Steganalysis)란 이미지 등 일반적인 자료에 암호화된 정보를 은닉하는 스테가노그래피(Steganography)에 대한 검출 및 분석 방법으로, 기계학습 기반 방법론을 포함한다. 기존 기계학습 기반 스테그아날리시스는 영상(Image)의 특징(Feature) 추출 및 모델링에 기반하며, 최근 딥러닝(Deep Learning)의 적용으로 검출 정확도가 큰 폭으로 향상되었다. 하지만 현존하는 스테그아날리시스 모델은 단일 스테가노그래피 기법에 대해 국한되어 있어 학습에 사용되지 않은 스테고(Stego) 이미지의 경우 검출이 불가능한 결정적 한계를 가진다. 본 연구에서는 다양한 스테가노그래피 기법으로 생성된 스테고이미지에 딥러닝을 적용하여 스테그아날리시스를 학습하는 범용적 모델을 제안한다. 다양한 실험을 통해 제안 기법의 효용성 및 가능성을 확인하고, 범용적 스테그아날리시스 모델이 각각에 특화된 검출 기법과 유사한 정확도로 스테고 이미지를 검출할 수 있음을 보인다.Steganalysis is to detect information hidden by steganography inside general data such as images. There are stegoanalysis techniques that use machine learning (ML). Existing ML approaches to steganalysis are based on extracting features from stego images and modeling them.
Recently deep learning-based methodologies have shown significant improvements in detection accuracy. However, all the existing methods, including deep learning-based ones, have a critical limitation in that they can only detect stego images that are created by a specific steganography method. In this paper, we propose a generalized steganalysis method that can model multiple types of stego images using deep learning. Through various experiments, we confirm the effectiveness of our approach and envision directions for future research. In particular, we show that our method can detect each type of steganography with the same level of accuracy as that of a steganalysis method dedicated to that type of steganography, thereby demonstrating the general applicability of our approach to multiple types of stego images.steganalysis, steganography, deep learning, generalized steganalysis model"
749,"딥러닝 프레임워크의 비교: 티아노, 텐서플로, CNTK를 중심으로",정여진 /Yeojin Chung,2017,한국지능정보시스템학회,"딥러닝 프레임워크의 대표적인 기능으로는 ‘자동미분’과 ‘GPU의 활용’ 등을 들 수 있다. 본 논문은 파이썬의 라이브러리 형태로 사용 가능한 프레임워크 중에서 구글의 텐서플로와 마이크로소프트의 CNTK, 그리고 텐서 플로의 원조라고 할 수 있는 티아노를 비교하였다. 본문에서는 자동미분의 개념과 GPU의 활용형태를 간단히 설명하고, 그 다음에 logistic regression을 실행하는 예를 통하여 각 프레임워크의 문법을 알아본 뒤에, 마지막으로 대표적인 딥러닝 응용인 CNN의 예제를 실행시켜보고 코딩의 편의성과 실행속도 등을 확인해 보았다. 그 결과, 편의성의 관점에서 보면 티아노가 가장 코딩 하기가 어렵고, CNTK와 텐서플로는 많은 부분이 비슷하게 추상화 되어 있어서 코딩이 비슷하지만 가중치와 편향을 직접 정의하느냐의 여부에서 차이를 보였다. 그리고 각프레임워크의 실행속도에 대한 평가는 ‘큰 차이는 없다’는 것이다. 텐서플로는 티아노에 비하여 속도가 느리다는 평가가 있어왔는데, 본 연구의 실험에 의하면, 비록 CNN 모형에 국한되었지만, 텐서플로가 아주 조금이지만빠른 것으로 나타났다. CNTK의 경우에도, 비록 실험환경이 달랐지만, 실험환경의 차이에 의한 속도의 차이의 편차범위 이내에 있는 것으로 판단이 되었다. 본 연구에서는 세 종류의 딥러닝 프레임워크만을 살펴보았는데,위키피디아에 따르면 딥러닝 프레임워크의 종류는 12가지가 있으며, 각 프레임워크의 특징을 15가지 속성으로구분하여 차이를 특정하고 있다. 그 많은 속성 중에서 사용자의 입장에서 볼 때 중요한 속성은 어떤 언어(파이썬, C++, Java, 등)로 사용가능한지, 어떤 딥러닝 모형에 대한 라이브러리가 잘 구현되어 있는지 등일 것이다. 그리고 사용자가 대규모의 딥러닝 모형을 구축한다면, 다중 GPU 혹은 다중 서버를 지원하는지의 여부도 중요할 것이다. 또한 딥러닝 모형을 처음 학습하는 경우에는 사용설명서가 많은지 예제 프로그램이 많은지 여부도 중요한 기준이 될 것이다.The deep learning framework is software designed to help develop deep learning models. Some of its important functions include “automatic differentiation” and “utilization of GPU”. The list of popular deep learning framework includes Caffe (BVLC) and Theano (University of Montreal). And recently, Microsoft's deep learning framework, Microsoft Cognitive Toolkit, was released as open-source license, following Google’s Tensorflow a year earlier. The early deep learning frameworks have been developed mainly for research at universities. Beginning with the inception of Tensorflow, however, it seems that companies such as Microsoft and Facebook have started to join the competition of framework development. Given the trend, Google and other companies are expected to continue investing in the deep learning framework to bring forward the initiative in the artificial intelligence business. From this point of view, we think it is a good time to compare some of deep learning frameworks. So we compare three deep learning frameworks which can be used as a Python library. Those are Google's Tensorflow, Microsoft’s CNTK, and Theano which is sort of a predecessor of the preceding two.
The most common and important function of deep learning frameworks is the ability to perform automatic differentiation. Basically all the mathematical expressions of deep learning models can be represented as computational graphs, which consist of nodes and edges. Partial derivatives on each edge of a computational graph can then be obtained. With the partial derivatives, we can let software compute differentiation of any node with respect to any variable by utilizing chain rule of Calculus.
First of all, the convenience of coding is in the order of CNTK, Tensorflow, and Theano. The criterion is simply based on the lengths of the codes and the learning curve and the ease of coding are not the main concern. According to the criteria, Theano was the most difficult to implement with, and CNTK and Tensorflow were somewhat easier. With Tensorflow, we need to define weight variables and biases explicitly. The reason that CNTK and Tensorflow are easier to implement with is that those frameworks provide us with more abstraction than Theano. We, however, need to mention that low-level coding is not always bad. It gives us flexibility of coding. With the low-level coding such as in Theano, we can implement and test any new deep learning models or any new search methods that we can think of.
The assessment of the execution speed of each framework is that there is not meaningful difference. According to the experiment, execution speeds of Theano and Tensorflow are very similar, although the experiment was limited to a CNN model. In the case of CNTK, the experimental environment was not maintained as the same. The code written in CNTK has to be run in PC environment without GPU where codes execute as much as 50 times slower than with GPU. But we concluded that the difference of execution speed was within the range of variation caused by the different hardware setup.
In this study, we compared three types of deep learning framework: Theano, Tensorflow, and CNTK. According to Wikipedia, there are 12 available deep learning frameworks. And 15 different attributes differentiate each framework. Some of the important attributes would include interface language (Python, C ++, Java, etc.) and the availability of libraries on various deep learning models such as CNN, RNN, DBN, and etc. And if a user implements a large scale deep learning model, it will also be important to support multiple GPU or multiple servers. Also, if you are learning the deep learning model, it would also be important if there are enough examples and references.deep learning framework, Theano, TensorFlow, CNTK, computational graph, CIFAR-10"
750,딥러닝을 이용한 주변 무선단말 파악방안,이웅섭 /Lee， Woong-sup,2018,한국정보통신학회,"최근 단말-대-단말(Device-to-device, D2D) 통신기술이 차세대 무선통신시스템의 핵심기술로 큰 관심을 받고 있다. 이러한 단말간 통신에서는 자신의 주변에 어떠한 단말이 있는지 파악하는 주변단말 탐색(Neighbor discovery)이매우 중요하다. 본 논문에서는 최근 큰 관심을 받고 있는 딥러닝(Deep learning) 기술을 활용하여 단말간 통신에서 주변단말을 파악하는 방안에 대해서 제안한다. 제안 방안은 기존의 방안과 달리 무선채널의 공간적 연관성을 이용하여단말간의 신호 전송 없이 단말이 기지국으로 전송하는 상향링크 파일럿 신호를 기반으로 주변 단말을 찾고 따라서기존의 방식에 비해 신호전송 복잡도(signaling complexity)를 크게 줄일 수 있다. 또한 제안 방안에서는 떨어져 있는거리에 따라서 주변 단말을 분류 가능하여 기존 방안에 비해서 좀 더 세밀한 단말 탐색이 가능하다. 마지막으로 본 논문에서는 tensorflow를 이용한 컴퓨터 시뮬레이션을 통해 제안 방안의 성능을 검증하였다.Recently, the device-to-device (D2D) communication has been conceived as the key technology for the next-generationmobile communication systems. The neighbor discovery in which the nearby users are found, is essential for the properoperation of the D2D communication. In this paper, we propose new neighbor discovery scheme based on deep learningtechnology which has gained a lot of attention recently. In the proposed scheme, the neighboring users can be foundusing the uplink pilot transmission of users only, unlike conventional neighbor discovery schemes in which direct pilotcommunication among users is required, such that the signaling overhead can be greatly reduced in our proposed scheme.
Moreover, the neighbors with different proximity can also be classified accordingly which enables more accurate neighbordiscovery compared to the conventional schemes. The performance of our proposed scheme is verified through thetensorflow-based computer simulations.
키워드 : 딥러닝, 주변단말 탐색, 단말-대-단말 통신Deep learning, Neighbor discovery, Device-to-device communication, Wireless channel correlation"
751,신뢰성있는 딥러닝 기반 분석 모델을 참조하기 위한 딥러닝 기술 언어,문종혁 /Jong Hyeok Mun,2021,한국정보처리학회,"최근 딥러닝은 하드웨어 성능이 향상됨에 따라 자연어 처리, 영상 인식 등의 다양한 기술에 접목되어 활용되고 있다. 이러한 기술들을 활용해 지능형교통 시스템(ITS), 스마트홈, 헬스케어 등의 산업분야에서 데이터를 분석하여 고속도로 속도위반 차량 검출, 에너지 사용량 제어, 응급상황 등과 같은고품질의 서비스를 제공하며, 고품질의 서비스를 제공하기 위해서는 정확도가 향상된 딥러닝 모델이 적용되어야 한다. 이를 위해 서비스 환경의 데이터를분석하기 위한 딥러닝 모델을 개발할 때, 개발자는 신뢰성이 검증된 최신의 딥러닝 모델을 적용할 수 있어야 한다. 이는 개발자가 참조하는 딥러닝 모델에적용된 학습 데이터셋의 정확도를 측정하여 검증할 수 있다. 이러한 검증을 위해서 개발자는 학습 데이터셋, 딥러닝의 계층구조 및 개발 환경 등과 같은내용을 포함하는 딥러닝 모델을 문서화하여 적용하기 위한 구조적인 정보가 필요하다. 본 논문에서는 신뢰성있는 딥러닝 기반 데이터 분석 모델을 참조하기위한 딥러닝 기술 언어를 제안한다. 제안하는 기술 언어는 신뢰성 있는 딥러닝 모델을 개발하는데 필요한 학습데이터셋, 개발 환경 및 설정 등의 정보와더불어 딥러닝 모델의 계층구조를 표현할 수 있다. 제안하는 딥러닝 기술 언어를 이용하여 개발자는 지능형 교통 시스템에서 참조하는 분석 모델의정확도를 검증할 수 있다. 실험에서는 제안하는 언어의 유효성을 검증하기 위해, 번호판 인식 모델을 중심으로 딥러닝 기술 문서의 적용과정을 보인다With the recent advancements of deep learning, companies such as smart home, healthcare, and intelligent transportation systems areutilizing its functionality to provide high-quality services for vehicle detection, emergency situation detection, and controlling energyconsumption. To provide reliable services in such sensitive systems, deep learning models are required to have high accuracy. In orderto develop a deep learning model for analyzing previously mentioned services, developers should utilize the state of the art deep learningmodels that have already been verified for higher accuracy. The developers can verify the accuracy of the referenced model by validatingthe model on the dataset. For this validation, the developer needs structural information to document and apply deep learning models,including metadata such as learning dataset, network architecture, and development environments. In this paper, we propose a descriptionlanguage that represents the network architecture of the deep learning model along with its metadata that are necessary to develop adeep learning model. Through the proposed description language, developers can easily verify the accuracy of the referenced deep learningmodel. Our experiments demonstrate the application scenario of a deep learning description document that focuses on the license platerecognition for the detection of illegally parked vehicles.Trusted Deep Learning, Model Reference, Deep Learning Description Language, Traffic Situation Analysis Model"
752,딥러닝 분석을 이용한 중국 역내·외 위안화 변동성 예측,이우식 /Lee Woosik,2016,한국데이터정보과학회,"2008년 글로벌 금융위기 이후 중국은 위안화 국제화의 점진적 추진을 진행하면서 중국상하이 외환시장과 중국홍콩 외환시장에서 거래되는 통화인 역내위안화와 역외위안화를 형성시켰다. 본 연구는 위안화 국제화와 점진적인 중국 자본계정 개방에 따라 급변하는 외환시장상황의 변동성을 정확하게 파악하기 위해서 GARCH모형 (일반화된 자기회귀 조건부이분산성모형)에 다단계인공신경망을 결합한 MLP-GARCH 모형과 GARCH모형과 기계학습의 일종인 딥러닝 (deep learning)을 통합한 DL-GARCH을 가지고 위안화 변동성예측을 비교 실험과 분석을 하였다. 비교분석 결과 DLGARCH 모형은 MLP-GARCH보다 모형 위안화 역내·외 환율변동성 예측 면에서 더욱 더 개선된예측값을 제공하였다. 그래서 이분산시계열모형을 딥러닝과 결합한 DL-GARCH 모형은 시계열의환율 변동성 예측 문제에 딥러닝을 응용할 수 있음을 확인하였다. 향후 이분산시계열과 결합된 딥러닝 모형은 다른 금융시계열 데이터에 응용하여 그 일반화 가능성을 높일 수 있을 것이다.The People’s Republic of China has vigorously been pursuing the internationalization of the Chinese Yuan or Renminbi after the financial crisis of 2008. In this view, an abrupt increase of use of the Chinese Yuan in the onshore and offshore markets are important milestones to be one of important currencies. One of the most frequently used methods to forecast volatility is GARCH model. Since a prediction error of the GARCH model has been reported quite high, a lot of efforts have been made to improve forecasting capability of the GARCH model. In this paper, we have proposed MLPGARCH and a DL-GARCH by employing Artificial Neural Network to the GARCH. In an application to forecasting Chinese Yuan volatility, we have successfully shown their overall outperformance in forecasting over the GARCH.Deep learning, DL-GARCH, GARCH, The Chinese Yuan, volatility."
753,딥러닝 기법을 이용한 내일강수 예측,하지훈,2016,한국지능시스템학회,"정확한 강수예측을 위해서는 예측인자 선정과 예측방법에 대한 선택이 매우 중요하다. 최근에는 강수예측 방법으로기계학습 기법이 많이 사용되고 있으며, 그 중에서도 특히 인공신경망을 사용한 강수예측 방법은 좋은 성능을 보였다. 본논문에서는 딥러닝 기법 중 하나인 DBN(deep belief network)를 이용한 새로운 강수예측 방법을 제안한다. DBN는 비지도사전 학습을 통해 초기 가중치를 설정하여 기존 인공신경망의 문제점을 보완한다. 예측인자로는 기온, 전일-전주 강수일, 태양과 달 궤도 관련 자료를 선정하였다. 기온과 전일-전주 강수일은 서울에서의 1974년부터 2013년까지 총 40년간의AWS(automatic weather system) 관측 자료를 사용하였고, 태양과 달의 궤도 관련 자료는 서울을 중심으로 계산한 결과를사용하였다. 전체 기간에서 일부는 학습 자료로 사용하여 예측모델을 생성하였고, 나머지를 생성한 모델의 검증 자료로사용하였다. 모델 검증 결과로 나온 예측값들은 확률값을 가지며 임계치를 이용하여 강수유무를 판별하였다. 강수 정확도의척도로 양분예보기법 중 CSI(critical successive index)와 Bias(frequency bias)를 계산하였다. 이를 통해 DBN와 MLP(multilayer perceptron)의 성능을 비교한 결과 DBN의 강수 예측 정확도가 높았고, 수행속도 또한 2배 이상 빨랐다.For accurate precipitation forecasts the choice of weather faoctrs and prediction method is very important. Recently, machinele arning has been widely used for forecasting precipitation, and artificial neural network, one of machine learning techniques, showed good performance. In this paper, we suggest a new method for forecasting precipitation using DBN, one of deep learning techniques.
DBN has an advantage that initial weights are set by unsupervised learning, so this compensates for the defects of artificial neural networks. We used past precipitation, temperature, and the parameters of the sun and moon's motion as features for forecasting precipitation. The dataset consists of observation data which had been measured for 40 years from AWS in Seoul. Experiments were based on 8-fold cross validation. As a result of estimation, we got probabilities of test dataset, so threshold was used for the decision of precipitation. CSI and Bias were used for indicating the precision of precipitation. Our experimental results showed that DBN performed better than MLP.Deep learning, Deep belief network, Precipitation, Forecast"
754,딥러닝 신경망모형을 이용한 실시간 교통정보수집 연구,김대현 /KIM DAE HYON,2016,사단법인 인문사회과학기술융합학회,"비디오 기반의 실시간 교통정보수집 시스템은 자율주행을 포함한 지능형교통체계의 주요 서브시스템중 하나이며, 우수한 예측(추정)력 때문에 핵심 알고리즘으로 인공신경망 모형이 주로 사용되고 있다. 특히, 최근 과학 학술지 네이처에 구글 딥마인드의 알파고가 공개되면서 딥러닝과 인공지능에 대한 전 세계적인 관심이 집중되고 있다. 최근까지 가장 널리 사용된 인공신경망모형은 역전파 모형이었으나 최근 컴퓨터 하드웨어 및 알고리즘의 새로운 개발과 더불어 딥러닝 알고리즘을 이용한 딥인공신경망 모형에 대한 관심이 높아지고 있다. 그러나 아직 딥러닝 인공신경망에 대한 연구는 초기단계에 있는 실정이며 적용분야도 극히 한정되어 있다. 본 연구에서는 실시간 도로교통정보 수집을 위한 영상처리분야에서 딥러닝 알고리즘을 활용하고 실험결과를 통하여 향후 교통정보수집‧분석 분야에서의 딥러닝 인공신경망 모형의 적용가능성을 살펴보고자 한다. 특히, 본 연구에서 도출한 딥러닝 인공신경망모형의 실험결과를 기존 역전파 모형에 의한 실험결과와 비교하여 모형의 우수성을 비교‧평가하고자 한다.Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Normally, artificial neural networks have been used in real-time applications for collecting traffic information because of their remarkable performance in terms of predictive accuracy and real-time computing. Especially, the recent publication of a paper in the journal Nature and the success of Google DeepMind’s world-beating Go program AlphaGo caused significant increases of the public interest in artificial intelligence. Even though various types of learning models have been proposed, the Backpropagation has been the most popular neural network model. However, the Deep learning neural network model has attracted much interest recently. In this study, the Deep leaning neural network will be applied in image processing techniques for the real-time traffic information collection to investigate the performance of Deep learning algorithm in traffic engineering fields. In addition, the experimental results from Deep learing algorithm will be compared with the results from Backpropagation model to evaluate and propose better performing model.Intelligent Transportation Systems (ITS), artificial neural networks, Deep learning, traffic information, image processing"
755,딥러닝의 모형과 응용사례,안성만 /SungMahn Ahn,2016,한국지능정보시스템학회,"딥러닝은 인공신경망(neural network)이라는 인공지능분야의 모형이 발전된 형태로서, 계층구조로 이루어진 인공신경망의 내부계층(hidden layer)이 여러 단계로 이루어진 구조이다. 딥러닝에서의 주요 모형은 합성곱신경망(convolutional neural network), 순환신경망(recurrent neural network), 그리고 심층신뢰신경망(deep belief network)의 세가지라고 할 수있다. 그 중에서 현재 흥미로운 연구가 많이 발표되어서 관심이 집중되고 있는 모형은 지도학습(supervised learning)모형인 처음 두 개의 모형이다. 따라서 본 논문에서는 지도학습모형의 가중치를 최적화하는 기본적인 방법인 오류역전파 알고리즘을 살펴본 뒤에 합성곱신경망과 순환신경망의 구조와 응용사례 등을 살펴보고자 한다. 본문에서 다루지 않은 모형인 심층신뢰신경망은 아직까지는 합성곱신경망 이나 순환신경망보다는 상대적으로 주목을 덜 받고 있다. 그러나 심층신뢰신경망은 CNN이나 RNN과는 달리 비지도학습(unsupervised learning)모형이며, 사람이나 동물은 관찰을 통해서 스스로 학습한다는 점에서 궁극적으로는 비지도학습모형이 더 많이 연구되어야 할 주제가 될 것이다.Deep learning model is a kind of neural networks that allows multiple hidden layers. There are various deep learning architectures such as convolutional neural networks, deep belief networks and recurrent neural networks. Those have been applied to fields like computer vision, automatic speech recognition, natural language processing, audio recognition and bioinformatics where they have been shown to produce state-of-the-art results on various tasks. Among those architectures, convolutional neural networks and recurrent neural networks are classified as the supervised learning model. And in recent years, those supervised learning models have gained more popularity than unsupervised learning models such as deep belief networks, because supervised learning models have shown fashionable applications in such fields mentioned above.
Deep learning models can be trained with backpropagation algorithm. Backpropagation is an abbreviation for “backward propagation of errors” and a common method of training artificial neural networks used in conjunction with an optimization method such as gradient descent. The method calculates the gradient of an error function with respect to all the weights in the network. The gradient is fed to the optimization method which in turn uses it to update the weights, in an attempt to minimize the error function.
Convolutional neural networks use a special architecture which is particularly well-adapted to classify images. Using this architecture makes convolutional networks fast to train. This, in turn, helps us train deep, muti-layer networks, which are very good at classifying images. These days, deep convolutional networks are used in most neural networks for image recognition. Convolutional neural networks use three basic ideas: local receptive fields, shared weights, and pooling. By local receptive fields, we mean that each neuron in the first(or any) hidden layer will be connected to a small region of the input(or previous layer’s) neurons. Shared weights mean that we’re going to use the same weights and bias for each of the local receptive field. This means that all the neurons in the hidden layer detect exactly the same feature, just at different locations in the input image. In addition to the convolutional layers just described, convolutional neural networks also contain pooling layers. Pooling layers are usually used immediately after convolutional layers. What the pooling layers do is to simplify the information in the output from the convolutional layer.
Recent convolutional network architectures have 10 to 20 hidden layers and billions of connections between units. Training deep learning networks has taken weeks several years ago, but thanks to progress in GPU and algorithm enhancement, training time has reduced to several hours.
Neural networks with time-varying behavior are known as recurrent neural networks or RNNs. A recurrent neural network is a class of artificial neural network where connections between units form a directed cycle. This creates an internal state of the network which allows it to exhibit dynamic temporal behavior. Unlike feedforward neural networks, RNNs can use their internal memory to process arbitrary sequences of inputs. Early RNN models turned out to be very difficult to train, harder even than deep feedforward networks. The reason is the unstable gradient problem such as vanishing gradient and exploding gradient. The gradient can get smaller and smaller as it is propagated back through layers. This makes learning in early layers extremely slow. The problem actually gets worse in RNNs, since gradients aren’t just propagated backward through layers, they’re propagated backward through time. If the network runs for a long time, that can make the gradient extremely unstable and hard to learn from. It has been possible to incorporate an idea known as long short-term memory units (LSTMs) into RNNs. LSTMs make it much easier to get good results when training RNNs, and many recent papers make use of LSTMs or related ideas.Deep learning, Convolutional neural networks, Recurrent neural networks, Error backpropagation algorithm"
756,딥러닝분석과 기술적 분석 지표를 이용한 한국 코스피주가지수 방향성 예측,이우식 /Lee Woosik,2017,한국데이터정보과학회,"2016년 3월 구글 (Google)의 바둑인공지능 알파고 (AlphaGo)가 이세돌 9단과의 바둑대결에서승리한 이후 다양한 분야에서 인공지능 사용에 대한 관심이 높아지고 있는 가운데 금융투자 분야에서도 인공지능과 투자자문 전문가의 합성어인 로보어드바이저 (Robo-Advisor)에 대한 관심이 높아지고 있다. 인공지능 (artificial intelligence)기반의 의사결정은 비용 절감은 물론 효과적인 의사결정을가능하게 한다는 점에서 큰 장점이 있다. 본 연구에서는 기술적 분석 (technical analysis) 지표와 딥러닝 (deep learning) 모형을 결합하여 한국 코스피 지수를 예측하는 모형을 개발하고 제시한 모형들의 예측력을 비교, 분석한다. 분석 결과 기술적 분석 지표에 딥러닝 알고리즘을 결합한 모형이 주가지수 방향성 예측 문제에 응용될 수 있음을 확인하였다. 향후 본 연구에서 제안된 기술적 분석 지표와딥러닝모형을 결합한 기법은 로보어드바이저서비스에 응용할 수 있는 일반화 가능성을 보여준다Since Google’s AlphaGo defeated a world champion of Go players in 2016, there have been many interests in the deep learning. In the financial sector, a Robo-Advisor using deep learning gains a significant attention, which builds and manages portfolios of financial instruments for investors.In this paper, we have proposed the a deep learning algorithm geared toward identification and forecast of the KOSPI index direction,and we also have compared the accuracy of the prediction.In an application of forecasting the financial market index direction, we have shown that the Robo-Advisor using deep learning has a significant effect on finance industry. The Robo-Advisor collects a massive data such as earnings statements, news reports and regulatory filings, analyzes those and recommends investors how to view market trends and identify the best time to purchase financial assets. On the other hand, the Robo-Advisor allows businesses to learn more about their customers, develop better marketing strategies, increase sales and decrease costs.Artificial intelligence, deep learning, FinTech, Robo-Advisor, technical analysis."
757,딥러닝의 효율적인 학습을 위한 학습데이터 선별 기법에 관한 연구,주경돈 /Gyoung-don Joo,2017,한국정보과학회,"빅데이터의 증가와 병렬 하드웨어 처리, 학습 알고리즘의 개선은 딥러닝의 발전을 가져왔으며 인공지능이 적용 가능한 영역을 획기적으로 확장시키고 있다. 하지만 이러한 대규모의 학습 데이터에 기반한 딥러닝은 학습시간이 증가되고, 더 나아가 학습 데이터의 구축을 위한 레이블링(labeling) 비용이 데이터 수에 비례하여 증가하게 된다. 따라서 본 논문에서는, 딥러닝의 정확도를 최대한 유지하면서 필요한 학습 데이터의 수를 줄일수 있도록, 전체 데이터에서 제한된 수의 학습 데이터의 후보를 선별하는 기법에 대하여 연구한다. 특히 기존의 능동 학습에 기반한 기법들이 학습 데이터의 수가 매우 적어질 경우 딥러닝의 정확도가 급격히 떨어지는 문제를 해결할 수 있는 새로운 기법을 제안하였으며, 제안된 기법으로 선별된 학습 데이터는 그 수의 변화에 대해 딥러닝이 일관되게 경쟁력있는 정확도를 가짐을 다양한 실험을 통하여 검증하였다.The increase of big data, parallel hardware processing and enhanced learning algorithms have led to the development of deep learning and are dramatically expanding the domain of applications based on artificial intelligence. However, such large-scale training data extends the learning time, and furthermore, the labeling cost for building the training set increases in proportion to the volume of data. Therefore, in this paper, we study how to select a limited number of candidates for training set from given data so as to reduce the number of training set while maintaining the accuracy of deep learning. Especially, we found that, with the training set selected by the previous work, the accuracy of deep learning drastically decreases when the number of training set is very small and so proposed new methods to resolve this fall-off in accuracy. Through experiments, we showed that deep learning consistently has the competitive accuracy with the training set selected by the proposed method regardless of the size of training set.Deep Learning, Train Set Selection, Active Learning, Gamma distribution"
758,딥러닝을 이용한 범용적 스테그아날리시스,김현재 /Hyunjae Kim,2017,한국정보과학회,"스테그아날리시스(Steganalysis)란 이미지 등 일반적인 자료에 암호화된 정보를 은닉하는 스테가노그래피(Steganography)에 대한 검출 및 분석 방법으로, 기계학습 기반 방법론을 포함한다. 기존 기계학습 기반 스테그아날리시스는 영상(Image)의 특징(Feature) 추출 및 모델링에 기반하며, 최근 딥러닝(Deep Learning)의 적용으로 검출 정확도가 큰 폭으로 향상되었다. 하지만 현존하는 스테그아날리시스 모델은 단일 스테가노그래피 기법에 대해 국한되어 있어 학습에 사용되지 않은 스테고(Stego) 이미지의 경우 검출이 불가능한 결정적 한계를 가진다. 본 연구에서는 다양한 스테가노그래피 기법으로 생성된 스테고이미지에 딥러닝을 적용하여 스테그아날리시스를 학습하는 범용적 모델을 제안한다. 다양한 실험을 통해 제안 기법의 효용성 및 가능성을 확인하고, 범용적 스테그아날리시스 모델이 각각에 특화된 검출 기법과 유사한 정확도로 스테고 이미지를 검출할 수 있음을 보인다.Steganalysis is to detect information hidden by steganography inside general data such as images. There are stegoanalysis techniques that use machine learning (ML). Existing ML approaches to steganalysis are based on extracting features from stego images and modeling them.
Recently deep learning-based methodologies have shown significant improvements in detection accuracy. However, all the existing methods, including deep learning-based ones, have a critical limitation in that they can only detect stego images that are created by a specific steganography method. In this paper, we propose a generalized steganalysis method that can model multiple types of stego images using deep learning. Through various experiments, we confirm the effectiveness of our approach and envision directions for future research. In particular, we show that our method can detect each type of steganography with the same level of accuracy as that of a steganalysis method dedicated to that type of steganography, thereby demonstrating the general applicability of our approach to multiple types of stego images.steganalysis, steganography, deep learning, generalized steganalysis model"
759,"딥러닝 프레임워크의 비교: 티아노, 텐서플로, CNTK를 중심으로",정여진 /Yeojin Chung,2017,한국지능정보시스템학회,"딥러닝 프레임워크의 대표적인 기능으로는 ‘자동미분’과 ‘GPU의 활용’ 등을 들 수 있다. 본 논문은 파이썬의 라이브러리 형태로 사용 가능한 프레임워크 중에서 구글의 텐서플로와 마이크로소프트의 CNTK, 그리고 텐서 플로의 원조라고 할 수 있는 티아노를 비교하였다. 본문에서는 자동미분의 개념과 GPU의 활용형태를 간단히 설명하고, 그 다음에 logistic regression을 실행하는 예를 통하여 각 프레임워크의 문법을 알아본 뒤에, 마지막으로 대표적인 딥러닝 응용인 CNN의 예제를 실행시켜보고 코딩의 편의성과 실행속도 등을 확인해 보았다. 그 결과, 편의성의 관점에서 보면 티아노가 가장 코딩 하기가 어렵고, CNTK와 텐서플로는 많은 부분이 비슷하게 추상화 되어 있어서 코딩이 비슷하지만 가중치와 편향을 직접 정의하느냐의 여부에서 차이를 보였다. 그리고 각프레임워크의 실행속도에 대한 평가는 ‘큰 차이는 없다’는 것이다. 텐서플로는 티아노에 비하여 속도가 느리다는 평가가 있어왔는데, 본 연구의 실험에 의하면, 비록 CNN 모형에 국한되었지만, 텐서플로가 아주 조금이지만빠른 것으로 나타났다. CNTK의 경우에도, 비록 실험환경이 달랐지만, 실험환경의 차이에 의한 속도의 차이의 편차범위 이내에 있는 것으로 판단이 되었다. 본 연구에서는 세 종류의 딥러닝 프레임워크만을 살펴보았는데,위키피디아에 따르면 딥러닝 프레임워크의 종류는 12가지가 있으며, 각 프레임워크의 특징을 15가지 속성으로구분하여 차이를 특정하고 있다. 그 많은 속성 중에서 사용자의 입장에서 볼 때 중요한 속성은 어떤 언어(파이썬, C++, Java, 등)로 사용가능한지, 어떤 딥러닝 모형에 대한 라이브러리가 잘 구현되어 있는지 등일 것이다. 그리고 사용자가 대규모의 딥러닝 모형을 구축한다면, 다중 GPU 혹은 다중 서버를 지원하는지의 여부도 중요할 것이다. 또한 딥러닝 모형을 처음 학습하는 경우에는 사용설명서가 많은지 예제 프로그램이 많은지 여부도 중요한 기준이 될 것이다.The deep learning framework is software designed to help develop deep learning models. Some of its important functions include “automatic differentiation” and “utilization of GPU”. The list of popular deep learning framework includes Caffe (BVLC) and Theano (University of Montreal). And recently, Microsoft's deep learning framework, Microsoft Cognitive Toolkit, was released as open-source license, following Google’s Tensorflow a year earlier. The early deep learning frameworks have been developed mainly for research at universities. Beginning with the inception of Tensorflow, however, it seems that companies such as Microsoft and Facebook have started to join the competition of framework development. Given the trend, Google and other companies are expected to continue investing in the deep learning framework to bring forward the initiative in the artificial intelligence business. From this point of view, we think it is a good time to compare some of deep learning frameworks. So we compare three deep learning frameworks which can be used as a Python library. Those are Google's Tensorflow, Microsoft’s CNTK, and Theano which is sort of a predecessor of the preceding two.
The most common and important function of deep learning frameworks is the ability to perform automatic differentiation. Basically all the mathematical expressions of deep learning models can be represented as computational graphs, which consist of nodes and edges. Partial derivatives on each edge of a computational graph can then be obtained. With the partial derivatives, we can let software compute differentiation of any node with respect to any variable by utilizing chain rule of Calculus.
First of all, the convenience of coding is in the order of CNTK, Tensorflow, and Theano. The criterion is simply based on the lengths of the codes and the learning curve and the ease of coding are not the main concern. According to the criteria, Theano was the most difficult to implement with, and CNTK and Tensorflow were somewhat easier. With Tensorflow, we need to define weight variables and biases explicitly. The reason that CNTK and Tensorflow are easier to implement with is that those frameworks provide us with more abstraction than Theano. We, however, need to mention that low-level coding is not always bad. It gives us flexibility of coding. With the low-level coding such as in Theano, we can implement and test any new deep learning models or any new search methods that we can think of.
The assessment of the execution speed of each framework is that there is not meaningful difference. According to the experiment, execution speeds of Theano and Tensorflow are very similar, although the experiment was limited to a CNN model. In the case of CNTK, the experimental environment was not maintained as the same. The code written in CNTK has to be run in PC environment without GPU where codes execute as much as 50 times slower than with GPU. But we concluded that the difference of execution speed was within the range of variation caused by the different hardware setup.
In this study, we compared three types of deep learning framework: Theano, Tensorflow, and CNTK. According to Wikipedia, there are 12 available deep learning frameworks. And 15 different attributes differentiate each framework. Some of the important attributes would include interface language (Python, C ++, Java, etc.) and the availability of libraries on various deep learning models such as CNN, RNN, DBN, and etc. And if a user implements a large scale deep learning model, it will also be important to support multiple GPU or multiple servers. Also, if you are learning the deep learning model, it would also be important if there are enough examples and references.deep learning framework, Theano, TensorFlow, CNTK, computational graph, CIFAR-10"
760,딥러닝을 이용한 주변 무선단말 파악방안,이웅섭 /Lee， Woong-sup,2018,한국정보통신학회,"최근 단말-대-단말(Device-to-device, D2D) 통신기술이 차세대 무선통신시스템의 핵심기술로 큰 관심을 받고 있다. 이러한 단말간 통신에서는 자신의 주변에 어떠한 단말이 있는지 파악하는 주변단말 탐색(Neighbor discovery)이매우 중요하다. 본 논문에서는 최근 큰 관심을 받고 있는 딥러닝(Deep learning) 기술을 활용하여 단말간 통신에서 주변단말을 파악하는 방안에 대해서 제안한다. 제안 방안은 기존의 방안과 달리 무선채널의 공간적 연관성을 이용하여단말간의 신호 전송 없이 단말이 기지국으로 전송하는 상향링크 파일럿 신호를 기반으로 주변 단말을 찾고 따라서기존의 방식에 비해 신호전송 복잡도(signaling complexity)를 크게 줄일 수 있다. 또한 제안 방안에서는 떨어져 있는거리에 따라서 주변 단말을 분류 가능하여 기존 방안에 비해서 좀 더 세밀한 단말 탐색이 가능하다. 마지막으로 본 논문에서는 tensorflow를 이용한 컴퓨터 시뮬레이션을 통해 제안 방안의 성능을 검증하였다.Recently, the device-to-device (D2D) communication has been conceived as the key technology for the next-generationmobile communication systems. The neighbor discovery in which the nearby users are found, is essential for the properoperation of the D2D communication. In this paper, we propose new neighbor discovery scheme based on deep learningtechnology which has gained a lot of attention recently. In the proposed scheme, the neighboring users can be foundusing the uplink pilot transmission of users only, unlike conventional neighbor discovery schemes in which direct pilotcommunication among users is required, such that the signaling overhead can be greatly reduced in our proposed scheme.
Moreover, the neighbors with different proximity can also be classified accordingly which enables more accurate neighbordiscovery compared to the conventional schemes. The performance of our proposed scheme is verified through thetensorflow-based computer simulations.
키워드 : 딥러닝, 주변단말 탐색, 단말-대-단말 통신Deep learning, Neighbor discovery, Device-to-device communication, Wireless channel correlation"
761,신뢰성있는 딥러닝 기반 분석 모델을 참조하기 위한 딥러닝 기술 언어,문종혁 /Jong Hyeok Mun,2021,한국정보처리학회,"최근 딥러닝은 하드웨어 성능이 향상됨에 따라 자연어 처리, 영상 인식 등의 다양한 기술에 접목되어 활용되고 있다. 이러한 기술들을 활용해 지능형교통 시스템(ITS), 스마트홈, 헬스케어 등의 산업분야에서 데이터를 분석하여 고속도로 속도위반 차량 검출, 에너지 사용량 제어, 응급상황 등과 같은고품질의 서비스를 제공하며, 고품질의 서비스를 제공하기 위해서는 정확도가 향상된 딥러닝 모델이 적용되어야 한다. 이를 위해 서비스 환경의 데이터를분석하기 위한 딥러닝 모델을 개발할 때, 개발자는 신뢰성이 검증된 최신의 딥러닝 모델을 적용할 수 있어야 한다. 이는 개발자가 참조하는 딥러닝 모델에적용된 학습 데이터셋의 정확도를 측정하여 검증할 수 있다. 이러한 검증을 위해서 개발자는 학습 데이터셋, 딥러닝의 계층구조 및 개발 환경 등과 같은내용을 포함하는 딥러닝 모델을 문서화하여 적용하기 위한 구조적인 정보가 필요하다. 본 논문에서는 신뢰성있는 딥러닝 기반 데이터 분석 모델을 참조하기위한 딥러닝 기술 언어를 제안한다. 제안하는 기술 언어는 신뢰성 있는 딥러닝 모델을 개발하는데 필요한 학습데이터셋, 개발 환경 및 설정 등의 정보와더불어 딥러닝 모델의 계층구조를 표현할 수 있다. 제안하는 딥러닝 기술 언어를 이용하여 개발자는 지능형 교통 시스템에서 참조하는 분석 모델의정확도를 검증할 수 있다. 실험에서는 제안하는 언어의 유효성을 검증하기 위해, 번호판 인식 모델을 중심으로 딥러닝 기술 문서의 적용과정을 보인다With the recent advancements of deep learning, companies such as smart home, healthcare, and intelligent transportation systems areutilizing its functionality to provide high-quality services for vehicle detection, emergency situation detection, and controlling energyconsumption. To provide reliable services in such sensitive systems, deep learning models are required to have high accuracy. In orderto develop a deep learning model for analyzing previously mentioned services, developers should utilize the state of the art deep learningmodels that have already been verified for higher accuracy. The developers can verify the accuracy of the referenced model by validatingthe model on the dataset. For this validation, the developer needs structural information to document and apply deep learning models,including metadata such as learning dataset, network architecture, and development environments. In this paper, we propose a descriptionlanguage that represents the network architecture of the deep learning model along with its metadata that are necessary to develop adeep learning model. Through the proposed description language, developers can easily verify the accuracy of the referenced deep learningmodel. Our experiments demonstrate the application scenario of a deep learning description document that focuses on the license platerecognition for the detection of illegally parked vehicles.Trusted Deep Learning, Model Reference, Deep Learning Description Language, Traffic Situation Analysis Model"
762,딥러닝 분석을 이용한 중국 역내·외 위안화 변동성 예측,이우식 /Lee Woosik,2016,한국데이터정보과학회,"2008년 글로벌 금융위기 이후 중국은 위안화 국제화의 점진적 추진을 진행하면서 중국상하이 외환시장과 중국홍콩 외환시장에서 거래되는 통화인 역내위안화와 역외위안화를 형성시켰다. 본 연구는 위안화 국제화와 점진적인 중국 자본계정 개방에 따라 급변하는 외환시장상황의 변동성을 정확하게 파악하기 위해서 GARCH모형 (일반화된 자기회귀 조건부이분산성모형)에 다단계인공신경망을 결합한 MLP-GARCH 모형과 GARCH모형과 기계학습의 일종인 딥러닝 (deep learning)을 통합한 DL-GARCH을 가지고 위안화 변동성예측을 비교 실험과 분석을 하였다. 비교분석 결과 DLGARCH 모형은 MLP-GARCH보다 모형 위안화 역내·외 환율변동성 예측 면에서 더욱 더 개선된예측값을 제공하였다. 그래서 이분산시계열모형을 딥러닝과 결합한 DL-GARCH 모형은 시계열의환율 변동성 예측 문제에 딥러닝을 응용할 수 있음을 확인하였다. 향후 이분산시계열과 결합된 딥러닝 모형은 다른 금융시계열 데이터에 응용하여 그 일반화 가능성을 높일 수 있을 것이다.The People’s Republic of China has vigorously been pursuing the internationalization of the Chinese Yuan or Renminbi after the financial crisis of 2008. In this view, an abrupt increase of use of the Chinese Yuan in the onshore and offshore markets are important milestones to be one of important currencies. One of the most frequently used methods to forecast volatility is GARCH model. Since a prediction error of the GARCH model has been reported quite high, a lot of efforts have been made to improve forecasting capability of the GARCH model. In this paper, we have proposed MLPGARCH and a DL-GARCH by employing Artificial Neural Network to the GARCH. In an application to forecasting Chinese Yuan volatility, we have successfully shown their overall outperformance in forecasting over the GARCH.Deep learning, DL-GARCH, GARCH, The Chinese Yuan, volatility."
763,딥러닝 기법을 이용한 내일강수 예측,하지훈,2016,한국지능시스템학회,"정확한 강수예측을 위해서는 예측인자 선정과 예측방법에 대한 선택이 매우 중요하다. 최근에는 강수예측 방법으로기계학습 기법이 많이 사용되고 있으며, 그 중에서도 특히 인공신경망을 사용한 강수예측 방법은 좋은 성능을 보였다. 본논문에서는 딥러닝 기법 중 하나인 DBN(deep belief network)를 이용한 새로운 강수예측 방법을 제안한다. DBN는 비지도사전 학습을 통해 초기 가중치를 설정하여 기존 인공신경망의 문제점을 보완한다. 예측인자로는 기온, 전일-전주 강수일, 태양과 달 궤도 관련 자료를 선정하였다. 기온과 전일-전주 강수일은 서울에서의 1974년부터 2013년까지 총 40년간의AWS(automatic weather system) 관측 자료를 사용하였고, 태양과 달의 궤도 관련 자료는 서울을 중심으로 계산한 결과를사용하였다. 전체 기간에서 일부는 학습 자료로 사용하여 예측모델을 생성하였고, 나머지를 생성한 모델의 검증 자료로사용하였다. 모델 검증 결과로 나온 예측값들은 확률값을 가지며 임계치를 이용하여 강수유무를 판별하였다. 강수 정확도의척도로 양분예보기법 중 CSI(critical successive index)와 Bias(frequency bias)를 계산하였다. 이를 통해 DBN와 MLP(multilayer perceptron)의 성능을 비교한 결과 DBN의 강수 예측 정확도가 높았고, 수행속도 또한 2배 이상 빨랐다.For accurate precipitation forecasts the choice of weather faoctrs and prediction method is very important. Recently, machinele arning has been widely used for forecasting precipitation, and artificial neural network, one of machine learning techniques, showed good performance. In this paper, we suggest a new method for forecasting precipitation using DBN, one of deep learning techniques.
DBN has an advantage that initial weights are set by unsupervised learning, so this compensates for the defects of artificial neural networks. We used past precipitation, temperature, and the parameters of the sun and moon's motion as features for forecasting precipitation. The dataset consists of observation data which had been measured for 40 years from AWS in Seoul. Experiments were based on 8-fold cross validation. As a result of estimation, we got probabilities of test dataset, so threshold was used for the decision of precipitation. CSI and Bias were used for indicating the precision of precipitation. Our experimental results showed that DBN performed better than MLP.Deep learning, Deep belief network, Precipitation, Forecast"
764,딥러닝 신경망모형을 이용한 실시간 교통정보수집 연구,김대현 /KIM DAE HYON,2016,사단법인 인문사회과학기술융합학회,"비디오 기반의 실시간 교통정보수집 시스템은 자율주행을 포함한 지능형교통체계의 주요 서브시스템중 하나이며, 우수한 예측(추정)력 때문에 핵심 알고리즘으로 인공신경망 모형이 주로 사용되고 있다. 특히, 최근 과학 학술지 네이처에 구글 딥마인드의 알파고가 공개되면서 딥러닝과 인공지능에 대한 전 세계적인 관심이 집중되고 있다. 최근까지 가장 널리 사용된 인공신경망모형은 역전파 모형이었으나 최근 컴퓨터 하드웨어 및 알고리즘의 새로운 개발과 더불어 딥러닝 알고리즘을 이용한 딥인공신경망 모형에 대한 관심이 높아지고 있다. 그러나 아직 딥러닝 인공신경망에 대한 연구는 초기단계에 있는 실정이며 적용분야도 극히 한정되어 있다. 본 연구에서는 실시간 도로교통정보 수집을 위한 영상처리분야에서 딥러닝 알고리즘을 활용하고 실험결과를 통하여 향후 교통정보수집‧분석 분야에서의 딥러닝 인공신경망 모형의 적용가능성을 살펴보고자 한다. 특히, 본 연구에서 도출한 딥러닝 인공신경망모형의 실험결과를 기존 역전파 모형에 의한 실험결과와 비교하여 모형의 우수성을 비교‧평가하고자 한다.Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Normally, artificial neural networks have been used in real-time applications for collecting traffic information because of their remarkable performance in terms of predictive accuracy and real-time computing. Especially, the recent publication of a paper in the journal Nature and the success of Google DeepMind’s world-beating Go program AlphaGo caused significant increases of the public interest in artificial intelligence. Even though various types of learning models have been proposed, the Backpropagation has been the most popular neural network model. However, the Deep learning neural network model has attracted much interest recently. In this study, the Deep leaning neural network will be applied in image processing techniques for the real-time traffic information collection to investigate the performance of Deep learning algorithm in traffic engineering fields. In addition, the experimental results from Deep learing algorithm will be compared with the results from Backpropagation model to evaluate and propose better performing model.Intelligent Transportation Systems (ITS), artificial neural networks, Deep learning, traffic information, image processing"
765,딥러닝의 모형과 응용사례,안성만 /SungMahn Ahn,2016,한국지능정보시스템학회,"딥러닝은 인공신경망(neural network)이라는 인공지능분야의 모형이 발전된 형태로서, 계층구조로 이루어진 인공신경망의 내부계층(hidden layer)이 여러 단계로 이루어진 구조이다. 딥러닝에서의 주요 모형은 합성곱신경망(convolutional neural network), 순환신경망(recurrent neural network), 그리고 심층신뢰신경망(deep belief network)의 세가지라고 할 수있다. 그 중에서 현재 흥미로운 연구가 많이 발표되어서 관심이 집중되고 있는 모형은 지도학습(supervised learning)모형인 처음 두 개의 모형이다. 따라서 본 논문에서는 지도학습모형의 가중치를 최적화하는 기본적인 방법인 오류역전파 알고리즘을 살펴본 뒤에 합성곱신경망과 순환신경망의 구조와 응용사례 등을 살펴보고자 한다. 본문에서 다루지 않은 모형인 심층신뢰신경망은 아직까지는 합성곱신경망 이나 순환신경망보다는 상대적으로 주목을 덜 받고 있다. 그러나 심층신뢰신경망은 CNN이나 RNN과는 달리 비지도학습(unsupervised learning)모형이며, 사람이나 동물은 관찰을 통해서 스스로 학습한다는 점에서 궁극적으로는 비지도학습모형이 더 많이 연구되어야 할 주제가 될 것이다.Deep learning model is a kind of neural networks that allows multiple hidden layers. There are various deep learning architectures such as convolutional neural networks, deep belief networks and recurrent neural networks. Those have been applied to fields like computer vision, automatic speech recognition, natural language processing, audio recognition and bioinformatics where they have been shown to produce state-of-the-art results on various tasks. Among those architectures, convolutional neural networks and recurrent neural networks are classified as the supervised learning model. And in recent years, those supervised learning models have gained more popularity than unsupervised learning models such as deep belief networks, because supervised learning models have shown fashionable applications in such fields mentioned above.
Deep learning models can be trained with backpropagation algorithm. Backpropagation is an abbreviation for “backward propagation of errors” and a common method of training artificial neural networks used in conjunction with an optimization method such as gradient descent. The method calculates the gradient of an error function with respect to all the weights in the network. The gradient is fed to the optimization method which in turn uses it to update the weights, in an attempt to minimize the error function.
Convolutional neural networks use a special architecture which is particularly well-adapted to classify images. Using this architecture makes convolutional networks fast to train. This, in turn, helps us train deep, muti-layer networks, which are very good at classifying images. These days, deep convolutional networks are used in most neural networks for image recognition. Convolutional neural networks use three basic ideas: local receptive fields, shared weights, and pooling. By local receptive fields, we mean that each neuron in the first(or any) hidden layer will be connected to a small region of the input(or previous layer’s) neurons. Shared weights mean that we’re going to use the same weights and bias for each of the local receptive field. This means that all the neurons in the hidden layer detect exactly the same feature, just at different locations in the input image. In addition to the convolutional layers just described, convolutional neural networks also contain pooling layers. Pooling layers are usually used immediately after convolutional layers. What the pooling layers do is to simplify the information in the output from the convolutional layer.
Recent convolutional network architectures have 10 to 20 hidden layers and billions of connections between units. Training deep learning networks has taken weeks several years ago, but thanks to progress in GPU and algorithm enhancement, training time has reduced to several hours.
Neural networks with time-varying behavior are known as recurrent neural networks or RNNs. A recurrent neural network is a class of artificial neural network where connections between units form a directed cycle. This creates an internal state of the network which allows it to exhibit dynamic temporal behavior. Unlike feedforward neural networks, RNNs can use their internal memory to process arbitrary sequences of inputs. Early RNN models turned out to be very difficult to train, harder even than deep feedforward networks. The reason is the unstable gradient problem such as vanishing gradient and exploding gradient. The gradient can get smaller and smaller as it is propagated back through layers. This makes learning in early layers extremely slow. The problem actually gets worse in RNNs, since gradients aren’t just propagated backward through layers, they’re propagated backward through time. If the network runs for a long time, that can make the gradient extremely unstable and hard to learn from. It has been possible to incorporate an idea known as long short-term memory units (LSTMs) into RNNs. LSTMs make it much easier to get good results when training RNNs, and many recent papers make use of LSTMs or related ideas.Deep learning, Convolutional neural networks, Recurrent neural networks, Error backpropagation algorithm"
766,딥러닝분석과 기술적 분석 지표를 이용한 한국 코스피주가지수 방향성 예측,이우식 /Lee Woosik,2017,한국데이터정보과학회,"2016년 3월 구글 (Google)의 바둑인공지능 알파고 (AlphaGo)가 이세돌 9단과의 바둑대결에서승리한 이후 다양한 분야에서 인공지능 사용에 대한 관심이 높아지고 있는 가운데 금융투자 분야에서도 인공지능과 투자자문 전문가의 합성어인 로보어드바이저 (Robo-Advisor)에 대한 관심이 높아지고 있다. 인공지능 (artificial intelligence)기반의 의사결정은 비용 절감은 물론 효과적인 의사결정을가능하게 한다는 점에서 큰 장점이 있다. 본 연구에서는 기술적 분석 (technical analysis) 지표와 딥러닝 (deep learning) 모형을 결합하여 한국 코스피 지수를 예측하는 모형을 개발하고 제시한 모형들의 예측력을 비교, 분석한다. 분석 결과 기술적 분석 지표에 딥러닝 알고리즘을 결합한 모형이 주가지수 방향성 예측 문제에 응용될 수 있음을 확인하였다. 향후 본 연구에서 제안된 기술적 분석 지표와딥러닝모형을 결합한 기법은 로보어드바이저서비스에 응용할 수 있는 일반화 가능성을 보여준다Since Google’s AlphaGo defeated a world champion of Go players in 2016, there have been many interests in the deep learning. In the financial sector, a Robo-Advisor using deep learning gains a significant attention, which builds and manages portfolios of financial instruments for investors.In this paper, we have proposed the a deep learning algorithm geared toward identification and forecast of the KOSPI index direction,and we also have compared the accuracy of the prediction.In an application of forecasting the financial market index direction, we have shown that the Robo-Advisor using deep learning has a significant effect on finance industry. The Robo-Advisor collects a massive data such as earnings statements, news reports and regulatory filings, analyzes those and recommends investors how to view market trends and identify the best time to purchase financial assets. On the other hand, the Robo-Advisor allows businesses to learn more about their customers, develop better marketing strategies, increase sales and decrease costs.Artificial intelligence, deep learning, FinTech, Robo-Advisor, technical analysis."
767,딥러닝의 효율적인 학습을 위한 학습데이터 선별 기법에 관한 연구,주경돈 /Gyoung-don Joo,2017,한국정보과학회,"빅데이터의 증가와 병렬 하드웨어 처리, 학습 알고리즘의 개선은 딥러닝의 발전을 가져왔으며 인공지능이 적용 가능한 영역을 획기적으로 확장시키고 있다. 하지만 이러한 대규모의 학습 데이터에 기반한 딥러닝은 학습시간이 증가되고, 더 나아가 학습 데이터의 구축을 위한 레이블링(labeling) 비용이 데이터 수에 비례하여 증가하게 된다. 따라서 본 논문에서는, 딥러닝의 정확도를 최대한 유지하면서 필요한 학습 데이터의 수를 줄일수 있도록, 전체 데이터에서 제한된 수의 학습 데이터의 후보를 선별하는 기법에 대하여 연구한다. 특히 기존의 능동 학습에 기반한 기법들이 학습 데이터의 수가 매우 적어질 경우 딥러닝의 정확도가 급격히 떨어지는 문제를 해결할 수 있는 새로운 기법을 제안하였으며, 제안된 기법으로 선별된 학습 데이터는 그 수의 변화에 대해 딥러닝이 일관되게 경쟁력있는 정확도를 가짐을 다양한 실험을 통하여 검증하였다.The increase of big data, parallel hardware processing and enhanced learning algorithms have led to the development of deep learning and are dramatically expanding the domain of applications based on artificial intelligence. However, such large-scale training data extends the learning time, and furthermore, the labeling cost for building the training set increases in proportion to the volume of data. Therefore, in this paper, we study how to select a limited number of candidates for training set from given data so as to reduce the number of training set while maintaining the accuracy of deep learning. Especially, we found that, with the training set selected by the previous work, the accuracy of deep learning drastically decreases when the number of training set is very small and so proposed new methods to resolve this fall-off in accuracy. Through experiments, we showed that deep learning consistently has the competitive accuracy with the training set selected by the proposed method regardless of the size of training set.Deep Learning, Train Set Selection, Active Learning, Gamma distribution"
768,딥러닝을 이용한 범용적 스테그아날리시스,김현재 /Hyunjae Kim,2017,한국정보과학회,"스테그아날리시스(Steganalysis)란 이미지 등 일반적인 자료에 암호화된 정보를 은닉하는 스테가노그래피(Steganography)에 대한 검출 및 분석 방법으로, 기계학습 기반 방법론을 포함한다. 기존 기계학습 기반 스테그아날리시스는 영상(Image)의 특징(Feature) 추출 및 모델링에 기반하며, 최근 딥러닝(Deep Learning)의 적용으로 검출 정확도가 큰 폭으로 향상되었다. 하지만 현존하는 스테그아날리시스 모델은 단일 스테가노그래피 기법에 대해 국한되어 있어 학습에 사용되지 않은 스테고(Stego) 이미지의 경우 검출이 불가능한 결정적 한계를 가진다. 본 연구에서는 다양한 스테가노그래피 기법으로 생성된 스테고이미지에 딥러닝을 적용하여 스테그아날리시스를 학습하는 범용적 모델을 제안한다. 다양한 실험을 통해 제안 기법의 효용성 및 가능성을 확인하고, 범용적 스테그아날리시스 모델이 각각에 특화된 검출 기법과 유사한 정확도로 스테고 이미지를 검출할 수 있음을 보인다.Steganalysis is to detect information hidden by steganography inside general data such as images. There are stegoanalysis techniques that use machine learning (ML). Existing ML approaches to steganalysis are based on extracting features from stego images and modeling them.
Recently deep learning-based methodologies have shown significant improvements in detection accuracy. However, all the existing methods, including deep learning-based ones, have a critical limitation in that they can only detect stego images that are created by a specific steganography method. In this paper, we propose a generalized steganalysis method that can model multiple types of stego images using deep learning. Through various experiments, we confirm the effectiveness of our approach and envision directions for future research. In particular, we show that our method can detect each type of steganography with the same level of accuracy as that of a steganalysis method dedicated to that type of steganography, thereby demonstrating the general applicability of our approach to multiple types of stego images.steganalysis, steganography, deep learning, generalized steganalysis model"
769,"딥러닝 프레임워크의 비교: 티아노, 텐서플로, CNTK를 중심으로",정여진 /Yeojin Chung,2017,한국지능정보시스템학회,"딥러닝 프레임워크의 대표적인 기능으로는 ‘자동미분’과 ‘GPU의 활용’ 등을 들 수 있다. 본 논문은 파이썬의 라이브러리 형태로 사용 가능한 프레임워크 중에서 구글의 텐서플로와 마이크로소프트의 CNTK, 그리고 텐서 플로의 원조라고 할 수 있는 티아노를 비교하였다. 본문에서는 자동미분의 개념과 GPU의 활용형태를 간단히 설명하고, 그 다음에 logistic regression을 실행하는 예를 통하여 각 프레임워크의 문법을 알아본 뒤에, 마지막으로 대표적인 딥러닝 응용인 CNN의 예제를 실행시켜보고 코딩의 편의성과 실행속도 등을 확인해 보았다. 그 결과, 편의성의 관점에서 보면 티아노가 가장 코딩 하기가 어렵고, CNTK와 텐서플로는 많은 부분이 비슷하게 추상화 되어 있어서 코딩이 비슷하지만 가중치와 편향을 직접 정의하느냐의 여부에서 차이를 보였다. 그리고 각프레임워크의 실행속도에 대한 평가는 ‘큰 차이는 없다’는 것이다. 텐서플로는 티아노에 비하여 속도가 느리다는 평가가 있어왔는데, 본 연구의 실험에 의하면, 비록 CNN 모형에 국한되었지만, 텐서플로가 아주 조금이지만빠른 것으로 나타났다. CNTK의 경우에도, 비록 실험환경이 달랐지만, 실험환경의 차이에 의한 속도의 차이의 편차범위 이내에 있는 것으로 판단이 되었다. 본 연구에서는 세 종류의 딥러닝 프레임워크만을 살펴보았는데,위키피디아에 따르면 딥러닝 프레임워크의 종류는 12가지가 있으며, 각 프레임워크의 특징을 15가지 속성으로구분하여 차이를 특정하고 있다. 그 많은 속성 중에서 사용자의 입장에서 볼 때 중요한 속성은 어떤 언어(파이썬, C++, Java, 등)로 사용가능한지, 어떤 딥러닝 모형에 대한 라이브러리가 잘 구현되어 있는지 등일 것이다. 그리고 사용자가 대규모의 딥러닝 모형을 구축한다면, 다중 GPU 혹은 다중 서버를 지원하는지의 여부도 중요할 것이다. 또한 딥러닝 모형을 처음 학습하는 경우에는 사용설명서가 많은지 예제 프로그램이 많은지 여부도 중요한 기준이 될 것이다.The deep learning framework is software designed to help develop deep learning models. Some of its important functions include “automatic differentiation” and “utilization of GPU”. The list of popular deep learning framework includes Caffe (BVLC) and Theano (University of Montreal). And recently, Microsoft's deep learning framework, Microsoft Cognitive Toolkit, was released as open-source license, following Google’s Tensorflow a year earlier. The early deep learning frameworks have been developed mainly for research at universities. Beginning with the inception of Tensorflow, however, it seems that companies such as Microsoft and Facebook have started to join the competition of framework development. Given the trend, Google and other companies are expected to continue investing in the deep learning framework to bring forward the initiative in the artificial intelligence business. From this point of view, we think it is a good time to compare some of deep learning frameworks. So we compare three deep learning frameworks which can be used as a Python library. Those are Google's Tensorflow, Microsoft’s CNTK, and Theano which is sort of a predecessor of the preceding two.
The most common and important function of deep learning frameworks is the ability to perform automatic differentiation. Basically all the mathematical expressions of deep learning models can be represented as computational graphs, which consist of nodes and edges. Partial derivatives on each edge of a computational graph can then be obtained. With the partial derivatives, we can let software compute differentiation of any node with respect to any variable by utilizing chain rule of Calculus.
First of all, the convenience of coding is in the order of CNTK, Tensorflow, and Theano. The criterion is simply based on the lengths of the codes and the learning curve and the ease of coding are not the main concern. According to the criteria, Theano was the most difficult to implement with, and CNTK and Tensorflow were somewhat easier. With Tensorflow, we need to define weight variables and biases explicitly. The reason that CNTK and Tensorflow are easier to implement with is that those frameworks provide us with more abstraction than Theano. We, however, need to mention that low-level coding is not always bad. It gives us flexibility of coding. With the low-level coding such as in Theano, we can implement and test any new deep learning models or any new search methods that we can think of.
The assessment of the execution speed of each framework is that there is not meaningful difference. According to the experiment, execution speeds of Theano and Tensorflow are very similar, although the experiment was limited to a CNN model. In the case of CNTK, the experimental environment was not maintained as the same. The code written in CNTK has to be run in PC environment without GPU where codes execute as much as 50 times slower than with GPU. But we concluded that the difference of execution speed was within the range of variation caused by the different hardware setup.
In this study, we compared three types of deep learning framework: Theano, Tensorflow, and CNTK. According to Wikipedia, there are 12 available deep learning frameworks. And 15 different attributes differentiate each framework. Some of the important attributes would include interface language (Python, C ++, Java, etc.) and the availability of libraries on various deep learning models such as CNN, RNN, DBN, and etc. And if a user implements a large scale deep learning model, it will also be important to support multiple GPU or multiple servers. Also, if you are learning the deep learning model, it would also be important if there are enough examples and references.deep learning framework, Theano, TensorFlow, CNTK, computational graph, CIFAR-10"
770,딥러닝을 이용한 주변 무선단말 파악방안,이웅섭 /Lee， Woong-sup,2018,한국정보통신학회,"최근 단말-대-단말(Device-to-device, D2D) 통신기술이 차세대 무선통신시스템의 핵심기술로 큰 관심을 받고 있다. 이러한 단말간 통신에서는 자신의 주변에 어떠한 단말이 있는지 파악하는 주변단말 탐색(Neighbor discovery)이매우 중요하다. 본 논문에서는 최근 큰 관심을 받고 있는 딥러닝(Deep learning) 기술을 활용하여 단말간 통신에서 주변단말을 파악하는 방안에 대해서 제안한다. 제안 방안은 기존의 방안과 달리 무선채널의 공간적 연관성을 이용하여단말간의 신호 전송 없이 단말이 기지국으로 전송하는 상향링크 파일럿 신호를 기반으로 주변 단말을 찾고 따라서기존의 방식에 비해 신호전송 복잡도(signaling complexity)를 크게 줄일 수 있다. 또한 제안 방안에서는 떨어져 있는거리에 따라서 주변 단말을 분류 가능하여 기존 방안에 비해서 좀 더 세밀한 단말 탐색이 가능하다. 마지막으로 본 논문에서는 tensorflow를 이용한 컴퓨터 시뮬레이션을 통해 제안 방안의 성능을 검증하였다.Recently, the device-to-device (D2D) communication has been conceived as the key technology for the next-generationmobile communication systems. The neighbor discovery in which the nearby users are found, is essential for the properoperation of the D2D communication. In this paper, we propose new neighbor discovery scheme based on deep learningtechnology which has gained a lot of attention recently. In the proposed scheme, the neighboring users can be foundusing the uplink pilot transmission of users only, unlike conventional neighbor discovery schemes in which direct pilotcommunication among users is required, such that the signaling overhead can be greatly reduced in our proposed scheme.
Moreover, the neighbors with different proximity can also be classified accordingly which enables more accurate neighbordiscovery compared to the conventional schemes. The performance of our proposed scheme is verified through thetensorflow-based computer simulations.
키워드 : 딥러닝, 주변단말 탐색, 단말-대-단말 통신Deep learning, Neighbor discovery, Device-to-device communication, Wireless channel correlation"
771,신뢰성있는 딥러닝 기반 분석 모델을 참조하기 위한 딥러닝 기술 언어,문종혁 /Jong Hyeok Mun,2021,한국정보처리학회,"최근 딥러닝은 하드웨어 성능이 향상됨에 따라 자연어 처리, 영상 인식 등의 다양한 기술에 접목되어 활용되고 있다. 이러한 기술들을 활용해 지능형교통 시스템(ITS), 스마트홈, 헬스케어 등의 산업분야에서 데이터를 분석하여 고속도로 속도위반 차량 검출, 에너지 사용량 제어, 응급상황 등과 같은고품질의 서비스를 제공하며, 고품질의 서비스를 제공하기 위해서는 정확도가 향상된 딥러닝 모델이 적용되어야 한다. 이를 위해 서비스 환경의 데이터를분석하기 위한 딥러닝 모델을 개발할 때, 개발자는 신뢰성이 검증된 최신의 딥러닝 모델을 적용할 수 있어야 한다. 이는 개발자가 참조하는 딥러닝 모델에적용된 학습 데이터셋의 정확도를 측정하여 검증할 수 있다. 이러한 검증을 위해서 개발자는 학습 데이터셋, 딥러닝의 계층구조 및 개발 환경 등과 같은내용을 포함하는 딥러닝 모델을 문서화하여 적용하기 위한 구조적인 정보가 필요하다. 본 논문에서는 신뢰성있는 딥러닝 기반 데이터 분석 모델을 참조하기위한 딥러닝 기술 언어를 제안한다. 제안하는 기술 언어는 신뢰성 있는 딥러닝 모델을 개발하는데 필요한 학습데이터셋, 개발 환경 및 설정 등의 정보와더불어 딥러닝 모델의 계층구조를 표현할 수 있다. 제안하는 딥러닝 기술 언어를 이용하여 개발자는 지능형 교통 시스템에서 참조하는 분석 모델의정확도를 검증할 수 있다. 실험에서는 제안하는 언어의 유효성을 검증하기 위해, 번호판 인식 모델을 중심으로 딥러닝 기술 문서의 적용과정을 보인다With the recent advancements of deep learning, companies such as smart home, healthcare, and intelligent transportation systems areutilizing its functionality to provide high-quality services for vehicle detection, emergency situation detection, and controlling energyconsumption. To provide reliable services in such sensitive systems, deep learning models are required to have high accuracy. In orderto develop a deep learning model for analyzing previously mentioned services, developers should utilize the state of the art deep learningmodels that have already been verified for higher accuracy. The developers can verify the accuracy of the referenced model by validatingthe model on the dataset. For this validation, the developer needs structural information to document and apply deep learning models,including metadata such as learning dataset, network architecture, and development environments. In this paper, we propose a descriptionlanguage that represents the network architecture of the deep learning model along with its metadata that are necessary to develop adeep learning model. Through the proposed description language, developers can easily verify the accuracy of the referenced deep learningmodel. Our experiments demonstrate the application scenario of a deep learning description document that focuses on the license platerecognition for the detection of illegally parked vehicles.Trusted Deep Learning, Model Reference, Deep Learning Description Language, Traffic Situation Analysis Model"
772,딥러닝 분석을 이용한 중국 역내·외 위안화 변동성 예측,이우식 /Lee Woosik,2016,한국데이터정보과학회,"2008년 글로벌 금융위기 이후 중국은 위안화 국제화의 점진적 추진을 진행하면서 중국상하이 외환시장과 중국홍콩 외환시장에서 거래되는 통화인 역내위안화와 역외위안화를 형성시켰다. 본 연구는 위안화 국제화와 점진적인 중국 자본계정 개방에 따라 급변하는 외환시장상황의 변동성을 정확하게 파악하기 위해서 GARCH모형 (일반화된 자기회귀 조건부이분산성모형)에 다단계인공신경망을 결합한 MLP-GARCH 모형과 GARCH모형과 기계학습의 일종인 딥러닝 (deep learning)을 통합한 DL-GARCH을 가지고 위안화 변동성예측을 비교 실험과 분석을 하였다. 비교분석 결과 DLGARCH 모형은 MLP-GARCH보다 모형 위안화 역내·외 환율변동성 예측 면에서 더욱 더 개선된예측값을 제공하였다. 그래서 이분산시계열모형을 딥러닝과 결합한 DL-GARCH 모형은 시계열의환율 변동성 예측 문제에 딥러닝을 응용할 수 있음을 확인하였다. 향후 이분산시계열과 결합된 딥러닝 모형은 다른 금융시계열 데이터에 응용하여 그 일반화 가능성을 높일 수 있을 것이다.The People’s Republic of China has vigorously been pursuing the internationalization of the Chinese Yuan or Renminbi after the financial crisis of 2008. In this view, an abrupt increase of use of the Chinese Yuan in the onshore and offshore markets are important milestones to be one of important currencies. One of the most frequently used methods to forecast volatility is GARCH model. Since a prediction error of the GARCH model has been reported quite high, a lot of efforts have been made to improve forecasting capability of the GARCH model. In this paper, we have proposed MLPGARCH and a DL-GARCH by employing Artificial Neural Network to the GARCH. In an application to forecasting Chinese Yuan volatility, we have successfully shown their overall outperformance in forecasting over the GARCH.Deep learning, DL-GARCH, GARCH, The Chinese Yuan, volatility."
773,딥러닝 기법을 이용한 내일강수 예측,하지훈,2016,한국지능시스템학회,"정확한 강수예측을 위해서는 예측인자 선정과 예측방법에 대한 선택이 매우 중요하다. 최근에는 강수예측 방법으로기계학습 기법이 많이 사용되고 있으며, 그 중에서도 특히 인공신경망을 사용한 강수예측 방법은 좋은 성능을 보였다. 본논문에서는 딥러닝 기법 중 하나인 DBN(deep belief network)를 이용한 새로운 강수예측 방법을 제안한다. DBN는 비지도사전 학습을 통해 초기 가중치를 설정하여 기존 인공신경망의 문제점을 보완한다. 예측인자로는 기온, 전일-전주 강수일, 태양과 달 궤도 관련 자료를 선정하였다. 기온과 전일-전주 강수일은 서울에서의 1974년부터 2013년까지 총 40년간의AWS(automatic weather system) 관측 자료를 사용하였고, 태양과 달의 궤도 관련 자료는 서울을 중심으로 계산한 결과를사용하였다. 전체 기간에서 일부는 학습 자료로 사용하여 예측모델을 생성하였고, 나머지를 생성한 모델의 검증 자료로사용하였다. 모델 검증 결과로 나온 예측값들은 확률값을 가지며 임계치를 이용하여 강수유무를 판별하였다. 강수 정확도의척도로 양분예보기법 중 CSI(critical successive index)와 Bias(frequency bias)를 계산하였다. 이를 통해 DBN와 MLP(multilayer perceptron)의 성능을 비교한 결과 DBN의 강수 예측 정확도가 높았고, 수행속도 또한 2배 이상 빨랐다.For accurate precipitation forecasts the choice of weather faoctrs and prediction method is very important. Recently, machinele arning has been widely used for forecasting precipitation, and artificial neural network, one of machine learning techniques, showed good performance. In this paper, we suggest a new method for forecasting precipitation using DBN, one of deep learning techniques.
DBN has an advantage that initial weights are set by unsupervised learning, so this compensates for the defects of artificial neural networks. We used past precipitation, temperature, and the parameters of the sun and moon's motion as features for forecasting precipitation. The dataset consists of observation data which had been measured for 40 years from AWS in Seoul. Experiments were based on 8-fold cross validation. As a result of estimation, we got probabilities of test dataset, so threshold was used for the decision of precipitation. CSI and Bias were used for indicating the precision of precipitation. Our experimental results showed that DBN performed better than MLP.Deep learning, Deep belief network, Precipitation, Forecast"
774,딥러닝 신경망모형을 이용한 실시간 교통정보수집 연구,김대현 /KIM DAE HYON,2016,사단법인 인문사회과학기술융합학회,"비디오 기반의 실시간 교통정보수집 시스템은 자율주행을 포함한 지능형교통체계의 주요 서브시스템중 하나이며, 우수한 예측(추정)력 때문에 핵심 알고리즘으로 인공신경망 모형이 주로 사용되고 있다. 특히, 최근 과학 학술지 네이처에 구글 딥마인드의 알파고가 공개되면서 딥러닝과 인공지능에 대한 전 세계적인 관심이 집중되고 있다. 최근까지 가장 널리 사용된 인공신경망모형은 역전파 모형이었으나 최근 컴퓨터 하드웨어 및 알고리즘의 새로운 개발과 더불어 딥러닝 알고리즘을 이용한 딥인공신경망 모형에 대한 관심이 높아지고 있다. 그러나 아직 딥러닝 인공신경망에 대한 연구는 초기단계에 있는 실정이며 적용분야도 극히 한정되어 있다. 본 연구에서는 실시간 도로교통정보 수집을 위한 영상처리분야에서 딥러닝 알고리즘을 활용하고 실험결과를 통하여 향후 교통정보수집‧분석 분야에서의 딥러닝 인공신경망 모형의 적용가능성을 살펴보고자 한다. 특히, 본 연구에서 도출한 딥러닝 인공신경망모형의 실험결과를 기존 역전파 모형에 의한 실험결과와 비교하여 모형의 우수성을 비교‧평가하고자 한다.Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Normally, artificial neural networks have been used in real-time applications for collecting traffic information because of their remarkable performance in terms of predictive accuracy and real-time computing. Especially, the recent publication of a paper in the journal Nature and the success of Google DeepMind’s world-beating Go program AlphaGo caused significant increases of the public interest in artificial intelligence. Even though various types of learning models have been proposed, the Backpropagation has been the most popular neural network model. However, the Deep learning neural network model has attracted much interest recently. In this study, the Deep leaning neural network will be applied in image processing techniques for the real-time traffic information collection to investigate the performance of Deep learning algorithm in traffic engineering fields. In addition, the experimental results from Deep learing algorithm will be compared with the results from Backpropagation model to evaluate and propose better performing model.Intelligent Transportation Systems (ITS), artificial neural networks, Deep learning, traffic information, image processing"
775,딥러닝의 모형과 응용사례,안성만 /SungMahn Ahn,2016,한국지능정보시스템학회,"딥러닝은 인공신경망(neural network)이라는 인공지능분야의 모형이 발전된 형태로서, 계층구조로 이루어진 인공신경망의 내부계층(hidden layer)이 여러 단계로 이루어진 구조이다. 딥러닝에서의 주요 모형은 합성곱신경망(convolutional neural network), 순환신경망(recurrent neural network), 그리고 심층신뢰신경망(deep belief network)의 세가지라고 할 수있다. 그 중에서 현재 흥미로운 연구가 많이 발표되어서 관심이 집중되고 있는 모형은 지도학습(supervised learning)모형인 처음 두 개의 모형이다. 따라서 본 논문에서는 지도학습모형의 가중치를 최적화하는 기본적인 방법인 오류역전파 알고리즘을 살펴본 뒤에 합성곱신경망과 순환신경망의 구조와 응용사례 등을 살펴보고자 한다. 본문에서 다루지 않은 모형인 심층신뢰신경망은 아직까지는 합성곱신경망 이나 순환신경망보다는 상대적으로 주목을 덜 받고 있다. 그러나 심층신뢰신경망은 CNN이나 RNN과는 달리 비지도학습(unsupervised learning)모형이며, 사람이나 동물은 관찰을 통해서 스스로 학습한다는 점에서 궁극적으로는 비지도학습모형이 더 많이 연구되어야 할 주제가 될 것이다.Deep learning model is a kind of neural networks that allows multiple hidden layers. There are various deep learning architectures such as convolutional neural networks, deep belief networks and recurrent neural networks. Those have been applied to fields like computer vision, automatic speech recognition, natural language processing, audio recognition and bioinformatics where they have been shown to produce state-of-the-art results on various tasks. Among those architectures, convolutional neural networks and recurrent neural networks are classified as the supervised learning model. And in recent years, those supervised learning models have gained more popularity than unsupervised learning models such as deep belief networks, because supervised learning models have shown fashionable applications in such fields mentioned above.
Deep learning models can be trained with backpropagation algorithm. Backpropagation is an abbreviation for “backward propagation of errors” and a common method of training artificial neural networks used in conjunction with an optimization method such as gradient descent. The method calculates the gradient of an error function with respect to all the weights in the network. The gradient is fed to the optimization method which in turn uses it to update the weights, in an attempt to minimize the error function.
Convolutional neural networks use a special architecture which is particularly well-adapted to classify images. Using this architecture makes convolutional networks fast to train. This, in turn, helps us train deep, muti-layer networks, which are very good at classifying images. These days, deep convolutional networks are used in most neural networks for image recognition. Convolutional neural networks use three basic ideas: local receptive fields, shared weights, and pooling. By local receptive fields, we mean that each neuron in the first(or any) hidden layer will be connected to a small region of the input(or previous layer’s) neurons. Shared weights mean that we’re going to use the same weights and bias for each of the local receptive field. This means that all the neurons in the hidden layer detect exactly the same feature, just at different locations in the input image. In addition to the convolutional layers just described, convolutional neural networks also contain pooling layers. Pooling layers are usually used immediately after convolutional layers. What the pooling layers do is to simplify the information in the output from the convolutional layer.
Recent convolutional network architectures have 10 to 20 hidden layers and billions of connections between units. Training deep learning networks has taken weeks several years ago, but thanks to progress in GPU and algorithm enhancement, training time has reduced to several hours.
Neural networks with time-varying behavior are known as recurrent neural networks or RNNs. A recurrent neural network is a class of artificial neural network where connections between units form a directed cycle. This creates an internal state of the network which allows it to exhibit dynamic temporal behavior. Unlike feedforward neural networks, RNNs can use their internal memory to process arbitrary sequences of inputs. Early RNN models turned out to be very difficult to train, harder even than deep feedforward networks. The reason is the unstable gradient problem such as vanishing gradient and exploding gradient. The gradient can get smaller and smaller as it is propagated back through layers. This makes learning in early layers extremely slow. The problem actually gets worse in RNNs, since gradients aren’t just propagated backward through layers, they’re propagated backward through time. If the network runs for a long time, that can make the gradient extremely unstable and hard to learn from. It has been possible to incorporate an idea known as long short-term memory units (LSTMs) into RNNs. LSTMs make it much easier to get good results when training RNNs, and many recent papers make use of LSTMs or related ideas.Deep learning, Convolutional neural networks, Recurrent neural networks, Error backpropagation algorithm"
776,딥러닝분석과 기술적 분석 지표를 이용한 한국 코스피주가지수 방향성 예측,이우식 /Lee Woosik,2017,한국데이터정보과학회,"2016년 3월 구글 (Google)의 바둑인공지능 알파고 (AlphaGo)가 이세돌 9단과의 바둑대결에서승리한 이후 다양한 분야에서 인공지능 사용에 대한 관심이 높아지고 있는 가운데 금융투자 분야에서도 인공지능과 투자자문 전문가의 합성어인 로보어드바이저 (Robo-Advisor)에 대한 관심이 높아지고 있다. 인공지능 (artificial intelligence)기반의 의사결정은 비용 절감은 물론 효과적인 의사결정을가능하게 한다는 점에서 큰 장점이 있다. 본 연구에서는 기술적 분석 (technical analysis) 지표와 딥러닝 (deep learning) 모형을 결합하여 한국 코스피 지수를 예측하는 모형을 개발하고 제시한 모형들의 예측력을 비교, 분석한다. 분석 결과 기술적 분석 지표에 딥러닝 알고리즘을 결합한 모형이 주가지수 방향성 예측 문제에 응용될 수 있음을 확인하였다. 향후 본 연구에서 제안된 기술적 분석 지표와딥러닝모형을 결합한 기법은 로보어드바이저서비스에 응용할 수 있는 일반화 가능성을 보여준다Since Google’s AlphaGo defeated a world champion of Go players in 2016, there have been many interests in the deep learning. In the financial sector, a Robo-Advisor using deep learning gains a significant attention, which builds and manages portfolios of financial instruments for investors.In this paper, we have proposed the a deep learning algorithm geared toward identification and forecast of the KOSPI index direction,and we also have compared the accuracy of the prediction.In an application of forecasting the financial market index direction, we have shown that the Robo-Advisor using deep learning has a significant effect on finance industry. The Robo-Advisor collects a massive data such as earnings statements, news reports and regulatory filings, analyzes those and recommends investors how to view market trends and identify the best time to purchase financial assets. On the other hand, the Robo-Advisor allows businesses to learn more about their customers, develop better marketing strategies, increase sales and decrease costs.Artificial intelligence, deep learning, FinTech, Robo-Advisor, technical analysis."
777,딥러닝의 효율적인 학습을 위한 학습데이터 선별 기법에 관한 연구,주경돈 /Gyoung-don Joo,2017,한국정보과학회,"빅데이터의 증가와 병렬 하드웨어 처리, 학습 알고리즘의 개선은 딥러닝의 발전을 가져왔으며 인공지능이 적용 가능한 영역을 획기적으로 확장시키고 있다. 하지만 이러한 대규모의 학습 데이터에 기반한 딥러닝은 학습시간이 증가되고, 더 나아가 학습 데이터의 구축을 위한 레이블링(labeling) 비용이 데이터 수에 비례하여 증가하게 된다. 따라서 본 논문에서는, 딥러닝의 정확도를 최대한 유지하면서 필요한 학습 데이터의 수를 줄일수 있도록, 전체 데이터에서 제한된 수의 학습 데이터의 후보를 선별하는 기법에 대하여 연구한다. 특히 기존의 능동 학습에 기반한 기법들이 학습 데이터의 수가 매우 적어질 경우 딥러닝의 정확도가 급격히 떨어지는 문제를 해결할 수 있는 새로운 기법을 제안하였으며, 제안된 기법으로 선별된 학습 데이터는 그 수의 변화에 대해 딥러닝이 일관되게 경쟁력있는 정확도를 가짐을 다양한 실험을 통하여 검증하였다.The increase of big data, parallel hardware processing and enhanced learning algorithms have led to the development of deep learning and are dramatically expanding the domain of applications based on artificial intelligence. However, such large-scale training data extends the learning time, and furthermore, the labeling cost for building the training set increases in proportion to the volume of data. Therefore, in this paper, we study how to select a limited number of candidates for training set from given data so as to reduce the number of training set while maintaining the accuracy of deep learning. Especially, we found that, with the training set selected by the previous work, the accuracy of deep learning drastically decreases when the number of training set is very small and so proposed new methods to resolve this fall-off in accuracy. Through experiments, we showed that deep learning consistently has the competitive accuracy with the training set selected by the proposed method regardless of the size of training set.Deep Learning, Train Set Selection, Active Learning, Gamma distribution"
778,딥러닝을 이용한 범용적 스테그아날리시스,김현재 /Hyunjae Kim,2017,한국정보과학회,"스테그아날리시스(Steganalysis)란 이미지 등 일반적인 자료에 암호화된 정보를 은닉하는 스테가노그래피(Steganography)에 대한 검출 및 분석 방법으로, 기계학습 기반 방법론을 포함한다. 기존 기계학습 기반 스테그아날리시스는 영상(Image)의 특징(Feature) 추출 및 모델링에 기반하며, 최근 딥러닝(Deep Learning)의 적용으로 검출 정확도가 큰 폭으로 향상되었다. 하지만 현존하는 스테그아날리시스 모델은 단일 스테가노그래피 기법에 대해 국한되어 있어 학습에 사용되지 않은 스테고(Stego) 이미지의 경우 검출이 불가능한 결정적 한계를 가진다. 본 연구에서는 다양한 스테가노그래피 기법으로 생성된 스테고이미지에 딥러닝을 적용하여 스테그아날리시스를 학습하는 범용적 모델을 제안한다. 다양한 실험을 통해 제안 기법의 효용성 및 가능성을 확인하고, 범용적 스테그아날리시스 모델이 각각에 특화된 검출 기법과 유사한 정확도로 스테고 이미지를 검출할 수 있음을 보인다.Steganalysis is to detect information hidden by steganography inside general data such as images. There are stegoanalysis techniques that use machine learning (ML). Existing ML approaches to steganalysis are based on extracting features from stego images and modeling them.
Recently deep learning-based methodologies have shown significant improvements in detection accuracy. However, all the existing methods, including deep learning-based ones, have a critical limitation in that they can only detect stego images that are created by a specific steganography method. In this paper, we propose a generalized steganalysis method that can model multiple types of stego images using deep learning. Through various experiments, we confirm the effectiveness of our approach and envision directions for future research. In particular, we show that our method can detect each type of steganography with the same level of accuracy as that of a steganalysis method dedicated to that type of steganography, thereby demonstrating the general applicability of our approach to multiple types of stego images.steganalysis, steganography, deep learning, generalized steganalysis model"
779,"딥러닝 프레임워크의 비교: 티아노, 텐서플로, CNTK를 중심으로",정여진 /Yeojin Chung,2017,한국지능정보시스템학회,"딥러닝 프레임워크의 대표적인 기능으로는 ‘자동미분’과 ‘GPU의 활용’ 등을 들 수 있다. 본 논문은 파이썬의 라이브러리 형태로 사용 가능한 프레임워크 중에서 구글의 텐서플로와 마이크로소프트의 CNTK, 그리고 텐서 플로의 원조라고 할 수 있는 티아노를 비교하였다. 본문에서는 자동미분의 개념과 GPU의 활용형태를 간단히 설명하고, 그 다음에 logistic regression을 실행하는 예를 통하여 각 프레임워크의 문법을 알아본 뒤에, 마지막으로 대표적인 딥러닝 응용인 CNN의 예제를 실행시켜보고 코딩의 편의성과 실행속도 등을 확인해 보았다. 그 결과, 편의성의 관점에서 보면 티아노가 가장 코딩 하기가 어렵고, CNTK와 텐서플로는 많은 부분이 비슷하게 추상화 되어 있어서 코딩이 비슷하지만 가중치와 편향을 직접 정의하느냐의 여부에서 차이를 보였다. 그리고 각프레임워크의 실행속도에 대한 평가는 ‘큰 차이는 없다’는 것이다. 텐서플로는 티아노에 비하여 속도가 느리다는 평가가 있어왔는데, 본 연구의 실험에 의하면, 비록 CNN 모형에 국한되었지만, 텐서플로가 아주 조금이지만빠른 것으로 나타났다. CNTK의 경우에도, 비록 실험환경이 달랐지만, 실험환경의 차이에 의한 속도의 차이의 편차범위 이내에 있는 것으로 판단이 되었다. 본 연구에서는 세 종류의 딥러닝 프레임워크만을 살펴보았는데,위키피디아에 따르면 딥러닝 프레임워크의 종류는 12가지가 있으며, 각 프레임워크의 특징을 15가지 속성으로구분하여 차이를 특정하고 있다. 그 많은 속성 중에서 사용자의 입장에서 볼 때 중요한 속성은 어떤 언어(파이썬, C++, Java, 등)로 사용가능한지, 어떤 딥러닝 모형에 대한 라이브러리가 잘 구현되어 있는지 등일 것이다. 그리고 사용자가 대규모의 딥러닝 모형을 구축한다면, 다중 GPU 혹은 다중 서버를 지원하는지의 여부도 중요할 것이다. 또한 딥러닝 모형을 처음 학습하는 경우에는 사용설명서가 많은지 예제 프로그램이 많은지 여부도 중요한 기준이 될 것이다.The deep learning framework is software designed to help develop deep learning models. Some of its important functions include “automatic differentiation” and “utilization of GPU”. The list of popular deep learning framework includes Caffe (BVLC) and Theano (University of Montreal). And recently, Microsoft's deep learning framework, Microsoft Cognitive Toolkit, was released as open-source license, following Google’s Tensorflow a year earlier. The early deep learning frameworks have been developed mainly for research at universities. Beginning with the inception of Tensorflow, however, it seems that companies such as Microsoft and Facebook have started to join the competition of framework development. Given the trend, Google and other companies are expected to continue investing in the deep learning framework to bring forward the initiative in the artificial intelligence business. From this point of view, we think it is a good time to compare some of deep learning frameworks. So we compare three deep learning frameworks which can be used as a Python library. Those are Google's Tensorflow, Microsoft’s CNTK, and Theano which is sort of a predecessor of the preceding two.
The most common and important function of deep learning frameworks is the ability to perform automatic differentiation. Basically all the mathematical expressions of deep learning models can be represented as computational graphs, which consist of nodes and edges. Partial derivatives on each edge of a computational graph can then be obtained. With the partial derivatives, we can let software compute differentiation of any node with respect to any variable by utilizing chain rule of Calculus.
First of all, the convenience of coding is in the order of CNTK, Tensorflow, and Theano. The criterion is simply based on the lengths of the codes and the learning curve and the ease of coding are not the main concern. According to the criteria, Theano was the most difficult to implement with, and CNTK and Tensorflow were somewhat easier. With Tensorflow, we need to define weight variables and biases explicitly. The reason that CNTK and Tensorflow are easier to implement with is that those frameworks provide us with more abstraction than Theano. We, however, need to mention that low-level coding is not always bad. It gives us flexibility of coding. With the low-level coding such as in Theano, we can implement and test any new deep learning models or any new search methods that we can think of.
The assessment of the execution speed of each framework is that there is not meaningful difference. According to the experiment, execution speeds of Theano and Tensorflow are very similar, although the experiment was limited to a CNN model. In the case of CNTK, the experimental environment was not maintained as the same. The code written in CNTK has to be run in PC environment without GPU where codes execute as much as 50 times slower than with GPU. But we concluded that the difference of execution speed was within the range of variation caused by the different hardware setup.
In this study, we compared three types of deep learning framework: Theano, Tensorflow, and CNTK. According to Wikipedia, there are 12 available deep learning frameworks. And 15 different attributes differentiate each framework. Some of the important attributes would include interface language (Python, C ++, Java, etc.) and the availability of libraries on various deep learning models such as CNN, RNN, DBN, and etc. And if a user implements a large scale deep learning model, it will also be important to support multiple GPU or multiple servers. Also, if you are learning the deep learning model, it would also be important if there are enough examples and references.deep learning framework, Theano, TensorFlow, CNTK, computational graph, CIFAR-10"
780,딥러닝을 이용한 주변 무선단말 파악방안,이웅섭 /Lee， Woong-sup,2018,한국정보통신학회,"최근 단말-대-단말(Device-to-device, D2D) 통신기술이 차세대 무선통신시스템의 핵심기술로 큰 관심을 받고 있다. 이러한 단말간 통신에서는 자신의 주변에 어떠한 단말이 있는지 파악하는 주변단말 탐색(Neighbor discovery)이매우 중요하다. 본 논문에서는 최근 큰 관심을 받고 있는 딥러닝(Deep learning) 기술을 활용하여 단말간 통신에서 주변단말을 파악하는 방안에 대해서 제안한다. 제안 방안은 기존의 방안과 달리 무선채널의 공간적 연관성을 이용하여단말간의 신호 전송 없이 단말이 기지국으로 전송하는 상향링크 파일럿 신호를 기반으로 주변 단말을 찾고 따라서기존의 방식에 비해 신호전송 복잡도(signaling complexity)를 크게 줄일 수 있다. 또한 제안 방안에서는 떨어져 있는거리에 따라서 주변 단말을 분류 가능하여 기존 방안에 비해서 좀 더 세밀한 단말 탐색이 가능하다. 마지막으로 본 논문에서는 tensorflow를 이용한 컴퓨터 시뮬레이션을 통해 제안 방안의 성능을 검증하였다.Recently, the device-to-device (D2D) communication has been conceived as the key technology for the next-generationmobile communication systems. The neighbor discovery in which the nearby users are found, is essential for the properoperation of the D2D communication. In this paper, we propose new neighbor discovery scheme based on deep learningtechnology which has gained a lot of attention recently. In the proposed scheme, the neighboring users can be foundusing the uplink pilot transmission of users only, unlike conventional neighbor discovery schemes in which direct pilotcommunication among users is required, such that the signaling overhead can be greatly reduced in our proposed scheme.
Moreover, the neighbors with different proximity can also be classified accordingly which enables more accurate neighbordiscovery compared to the conventional schemes. The performance of our proposed scheme is verified through thetensorflow-based computer simulations.
키워드 : 딥러닝, 주변단말 탐색, 단말-대-단말 통신Deep learning, Neighbor discovery, Device-to-device communication, Wireless channel correlation"
781,신뢰성있는 딥러닝 기반 분석 모델을 참조하기 위한 딥러닝 기술 언어,문종혁 /Jong Hyeok Mun,2021,한국정보처리학회,"최근 딥러닝은 하드웨어 성능이 향상됨에 따라 자연어 처리, 영상 인식 등의 다양한 기술에 접목되어 활용되고 있다. 이러한 기술들을 활용해 지능형교통 시스템(ITS), 스마트홈, 헬스케어 등의 산업분야에서 데이터를 분석하여 고속도로 속도위반 차량 검출, 에너지 사용량 제어, 응급상황 등과 같은고품질의 서비스를 제공하며, 고품질의 서비스를 제공하기 위해서는 정확도가 향상된 딥러닝 모델이 적용되어야 한다. 이를 위해 서비스 환경의 데이터를분석하기 위한 딥러닝 모델을 개발할 때, 개발자는 신뢰성이 검증된 최신의 딥러닝 모델을 적용할 수 있어야 한다. 이는 개발자가 참조하는 딥러닝 모델에적용된 학습 데이터셋의 정확도를 측정하여 검증할 수 있다. 이러한 검증을 위해서 개발자는 학습 데이터셋, 딥러닝의 계층구조 및 개발 환경 등과 같은내용을 포함하는 딥러닝 모델을 문서화하여 적용하기 위한 구조적인 정보가 필요하다. 본 논문에서는 신뢰성있는 딥러닝 기반 데이터 분석 모델을 참조하기위한 딥러닝 기술 언어를 제안한다. 제안하는 기술 언어는 신뢰성 있는 딥러닝 모델을 개발하는데 필요한 학습데이터셋, 개발 환경 및 설정 등의 정보와더불어 딥러닝 모델의 계층구조를 표현할 수 있다. 제안하는 딥러닝 기술 언어를 이용하여 개발자는 지능형 교통 시스템에서 참조하는 분석 모델의정확도를 검증할 수 있다. 실험에서는 제안하는 언어의 유효성을 검증하기 위해, 번호판 인식 모델을 중심으로 딥러닝 기술 문서의 적용과정을 보인다With the recent advancements of deep learning, companies such as smart home, healthcare, and intelligent transportation systems areutilizing its functionality to provide high-quality services for vehicle detection, emergency situation detection, and controlling energyconsumption. To provide reliable services in such sensitive systems, deep learning models are required to have high accuracy. In orderto develop a deep learning model for analyzing previously mentioned services, developers should utilize the state of the art deep learningmodels that have already been verified for higher accuracy. The developers can verify the accuracy of the referenced model by validatingthe model on the dataset. For this validation, the developer needs structural information to document and apply deep learning models,including metadata such as learning dataset, network architecture, and development environments. In this paper, we propose a descriptionlanguage that represents the network architecture of the deep learning model along with its metadata that are necessary to develop adeep learning model. Through the proposed description language, developers can easily verify the accuracy of the referenced deep learningmodel. Our experiments demonstrate the application scenario of a deep learning description document that focuses on the license platerecognition for the detection of illegally parked vehicles.Trusted Deep Learning, Model Reference, Deep Learning Description Language, Traffic Situation Analysis Model"
782,딥러닝 분석을 이용한 중국 역내·외 위안화 변동성 예측,이우식 /Lee Woosik,2016,한국데이터정보과학회,"2008년 글로벌 금융위기 이후 중국은 위안화 국제화의 점진적 추진을 진행하면서 중국상하이 외환시장과 중국홍콩 외환시장에서 거래되는 통화인 역내위안화와 역외위안화를 형성시켰다. 본 연구는 위안화 국제화와 점진적인 중국 자본계정 개방에 따라 급변하는 외환시장상황의 변동성을 정확하게 파악하기 위해서 GARCH모형 (일반화된 자기회귀 조건부이분산성모형)에 다단계인공신경망을 결합한 MLP-GARCH 모형과 GARCH모형과 기계학습의 일종인 딥러닝 (deep learning)을 통합한 DL-GARCH을 가지고 위안화 변동성예측을 비교 실험과 분석을 하였다. 비교분석 결과 DLGARCH 모형은 MLP-GARCH보다 모형 위안화 역내·외 환율변동성 예측 면에서 더욱 더 개선된예측값을 제공하였다. 그래서 이분산시계열모형을 딥러닝과 결합한 DL-GARCH 모형은 시계열의환율 변동성 예측 문제에 딥러닝을 응용할 수 있음을 확인하였다. 향후 이분산시계열과 결합된 딥러닝 모형은 다른 금융시계열 데이터에 응용하여 그 일반화 가능성을 높일 수 있을 것이다.The People’s Republic of China has vigorously been pursuing the internationalization of the Chinese Yuan or Renminbi after the financial crisis of 2008. In this view, an abrupt increase of use of the Chinese Yuan in the onshore and offshore markets are important milestones to be one of important currencies. One of the most frequently used methods to forecast volatility is GARCH model. Since a prediction error of the GARCH model has been reported quite high, a lot of efforts have been made to improve forecasting capability of the GARCH model. In this paper, we have proposed MLPGARCH and a DL-GARCH by employing Artificial Neural Network to the GARCH. In an application to forecasting Chinese Yuan volatility, we have successfully shown their overall outperformance in forecasting over the GARCH.Deep learning, DL-GARCH, GARCH, The Chinese Yuan, volatility."
783,딥러닝 기법을 이용한 내일강수 예측,하지훈,2016,한국지능시스템학회,"정확한 강수예측을 위해서는 예측인자 선정과 예측방법에 대한 선택이 매우 중요하다. 최근에는 강수예측 방법으로기계학습 기법이 많이 사용되고 있으며, 그 중에서도 특히 인공신경망을 사용한 강수예측 방법은 좋은 성능을 보였다. 본논문에서는 딥러닝 기법 중 하나인 DBN(deep belief network)를 이용한 새로운 강수예측 방법을 제안한다. DBN는 비지도사전 학습을 통해 초기 가중치를 설정하여 기존 인공신경망의 문제점을 보완한다. 예측인자로는 기온, 전일-전주 강수일, 태양과 달 궤도 관련 자료를 선정하였다. 기온과 전일-전주 강수일은 서울에서의 1974년부터 2013년까지 총 40년간의AWS(automatic weather system) 관측 자료를 사용하였고, 태양과 달의 궤도 관련 자료는 서울을 중심으로 계산한 결과를사용하였다. 전체 기간에서 일부는 학습 자료로 사용하여 예측모델을 생성하였고, 나머지를 생성한 모델의 검증 자료로사용하였다. 모델 검증 결과로 나온 예측값들은 확률값을 가지며 임계치를 이용하여 강수유무를 판별하였다. 강수 정확도의척도로 양분예보기법 중 CSI(critical successive index)와 Bias(frequency bias)를 계산하였다. 이를 통해 DBN와 MLP(multilayer perceptron)의 성능을 비교한 결과 DBN의 강수 예측 정확도가 높았고, 수행속도 또한 2배 이상 빨랐다.For accurate precipitation forecasts the choice of weather faoctrs and prediction method is very important. Recently, machinele arning has been widely used for forecasting precipitation, and artificial neural network, one of machine learning techniques, showed good performance. In this paper, we suggest a new method for forecasting precipitation using DBN, one of deep learning techniques.
DBN has an advantage that initial weights are set by unsupervised learning, so this compensates for the defects of artificial neural networks. We used past precipitation, temperature, and the parameters of the sun and moon's motion as features for forecasting precipitation. The dataset consists of observation data which had been measured for 40 years from AWS in Seoul. Experiments were based on 8-fold cross validation. As a result of estimation, we got probabilities of test dataset, so threshold was used for the decision of precipitation. CSI and Bias were used for indicating the precision of precipitation. Our experimental results showed that DBN performed better than MLP.Deep learning, Deep belief network, Precipitation, Forecast"
784,딥러닝 신경망모형을 이용한 실시간 교통정보수집 연구,김대현 /KIM DAE HYON,2016,사단법인 인문사회과학기술융합학회,"비디오 기반의 실시간 교통정보수집 시스템은 자율주행을 포함한 지능형교통체계의 주요 서브시스템중 하나이며, 우수한 예측(추정)력 때문에 핵심 알고리즘으로 인공신경망 모형이 주로 사용되고 있다. 특히, 최근 과학 학술지 네이처에 구글 딥마인드의 알파고가 공개되면서 딥러닝과 인공지능에 대한 전 세계적인 관심이 집중되고 있다. 최근까지 가장 널리 사용된 인공신경망모형은 역전파 모형이었으나 최근 컴퓨터 하드웨어 및 알고리즘의 새로운 개발과 더불어 딥러닝 알고리즘을 이용한 딥인공신경망 모형에 대한 관심이 높아지고 있다. 그러나 아직 딥러닝 인공신경망에 대한 연구는 초기단계에 있는 실정이며 적용분야도 극히 한정되어 있다. 본 연구에서는 실시간 도로교통정보 수집을 위한 영상처리분야에서 딥러닝 알고리즘을 활용하고 실험결과를 통하여 향후 교통정보수집‧분석 분야에서의 딥러닝 인공신경망 모형의 적용가능성을 살펴보고자 한다. 특히, 본 연구에서 도출한 딥러닝 인공신경망모형의 실험결과를 기존 역전파 모형에 의한 실험결과와 비교하여 모형의 우수성을 비교‧평가하고자 한다.Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Normally, artificial neural networks have been used in real-time applications for collecting traffic information because of their remarkable performance in terms of predictive accuracy and real-time computing. Especially, the recent publication of a paper in the journal Nature and the success of Google DeepMind’s world-beating Go program AlphaGo caused significant increases of the public interest in artificial intelligence. Even though various types of learning models have been proposed, the Backpropagation has been the most popular neural network model. However, the Deep learning neural network model has attracted much interest recently. In this study, the Deep leaning neural network will be applied in image processing techniques for the real-time traffic information collection to investigate the performance of Deep learning algorithm in traffic engineering fields. In addition, the experimental results from Deep learing algorithm will be compared with the results from Backpropagation model to evaluate and propose better performing model.Intelligent Transportation Systems (ITS), artificial neural networks, Deep learning, traffic information, image processing"
785,딥러닝의 모형과 응용사례,안성만 /SungMahn Ahn,2016,한국지능정보시스템학회,"딥러닝은 인공신경망(neural network)이라는 인공지능분야의 모형이 발전된 형태로서, 계층구조로 이루어진 인공신경망의 내부계층(hidden layer)이 여러 단계로 이루어진 구조이다. 딥러닝에서의 주요 모형은 합성곱신경망(convolutional neural network), 순환신경망(recurrent neural network), 그리고 심층신뢰신경망(deep belief network)의 세가지라고 할 수있다. 그 중에서 현재 흥미로운 연구가 많이 발표되어서 관심이 집중되고 있는 모형은 지도학습(supervised learning)모형인 처음 두 개의 모형이다. 따라서 본 논문에서는 지도학습모형의 가중치를 최적화하는 기본적인 방법인 오류역전파 알고리즘을 살펴본 뒤에 합성곱신경망과 순환신경망의 구조와 응용사례 등을 살펴보고자 한다. 본문에서 다루지 않은 모형인 심층신뢰신경망은 아직까지는 합성곱신경망 이나 순환신경망보다는 상대적으로 주목을 덜 받고 있다. 그러나 심층신뢰신경망은 CNN이나 RNN과는 달리 비지도학습(unsupervised learning)모형이며, 사람이나 동물은 관찰을 통해서 스스로 학습한다는 점에서 궁극적으로는 비지도학습모형이 더 많이 연구되어야 할 주제가 될 것이다.Deep learning model is a kind of neural networks that allows multiple hidden layers. There are various deep learning architectures such as convolutional neural networks, deep belief networks and recurrent neural networks. Those have been applied to fields like computer vision, automatic speech recognition, natural language processing, audio recognition and bioinformatics where they have been shown to produce state-of-the-art results on various tasks. Among those architectures, convolutional neural networks and recurrent neural networks are classified as the supervised learning model. And in recent years, those supervised learning models have gained more popularity than unsupervised learning models such as deep belief networks, because supervised learning models have shown fashionable applications in such fields mentioned above.
Deep learning models can be trained with backpropagation algorithm. Backpropagation is an abbreviation for “backward propagation of errors” and a common method of training artificial neural networks used in conjunction with an optimization method such as gradient descent. The method calculates the gradient of an error function with respect to all the weights in the network. The gradient is fed to the optimization method which in turn uses it to update the weights, in an attempt to minimize the error function.
Convolutional neural networks use a special architecture which is particularly well-adapted to classify images. Using this architecture makes convolutional networks fast to train. This, in turn, helps us train deep, muti-layer networks, which are very good at classifying images. These days, deep convolutional networks are used in most neural networks for image recognition. Convolutional neural networks use three basic ideas: local receptive fields, shared weights, and pooling. By local receptive fields, we mean that each neuron in the first(or any) hidden layer will be connected to a small region of the input(or previous layer’s) neurons. Shared weights mean that we’re going to use the same weights and bias for each of the local receptive field. This means that all the neurons in the hidden layer detect exactly the same feature, just at different locations in the input image. In addition to the convolutional layers just described, convolutional neural networks also contain pooling layers. Pooling layers are usually used immediately after convolutional layers. What the pooling layers do is to simplify the information in the output from the convolutional layer.
Recent convolutional network architectures have 10 to 20 hidden layers and billions of connections between units. Training deep learning networks has taken weeks several years ago, but thanks to progress in GPU and algorithm enhancement, training time has reduced to several hours.
Neural networks with time-varying behavior are known as recurrent neural networks or RNNs. A recurrent neural network is a class of artificial neural network where connections between units form a directed cycle. This creates an internal state of the network which allows it to exhibit dynamic temporal behavior. Unlike feedforward neural networks, RNNs can use their internal memory to process arbitrary sequences of inputs. Early RNN models turned out to be very difficult to train, harder even than deep feedforward networks. The reason is the unstable gradient problem such as vanishing gradient and exploding gradient. The gradient can get smaller and smaller as it is propagated back through layers. This makes learning in early layers extremely slow. The problem actually gets worse in RNNs, since gradients aren’t just propagated backward through layers, they’re propagated backward through time. If the network runs for a long time, that can make the gradient extremely unstable and hard to learn from. It has been possible to incorporate an idea known as long short-term memory units (LSTMs) into RNNs. LSTMs make it much easier to get good results when training RNNs, and many recent papers make use of LSTMs or related ideas.Deep learning, Convolutional neural networks, Recurrent neural networks, Error backpropagation algorithm"
786,딥러닝분석과 기술적 분석 지표를 이용한 한국 코스피주가지수 방향성 예측,이우식 /Lee Woosik,2017,한국데이터정보과학회,"2016년 3월 구글 (Google)의 바둑인공지능 알파고 (AlphaGo)가 이세돌 9단과의 바둑대결에서승리한 이후 다양한 분야에서 인공지능 사용에 대한 관심이 높아지고 있는 가운데 금융투자 분야에서도 인공지능과 투자자문 전문가의 합성어인 로보어드바이저 (Robo-Advisor)에 대한 관심이 높아지고 있다. 인공지능 (artificial intelligence)기반의 의사결정은 비용 절감은 물론 효과적인 의사결정을가능하게 한다는 점에서 큰 장점이 있다. 본 연구에서는 기술적 분석 (technical analysis) 지표와 딥러닝 (deep learning) 모형을 결합하여 한국 코스피 지수를 예측하는 모형을 개발하고 제시한 모형들의 예측력을 비교, 분석한다. 분석 결과 기술적 분석 지표에 딥러닝 알고리즘을 결합한 모형이 주가지수 방향성 예측 문제에 응용될 수 있음을 확인하였다. 향후 본 연구에서 제안된 기술적 분석 지표와딥러닝모형을 결합한 기법은 로보어드바이저서비스에 응용할 수 있는 일반화 가능성을 보여준다Since Google’s AlphaGo defeated a world champion of Go players in 2016, there have been many interests in the deep learning. In the financial sector, a Robo-Advisor using deep learning gains a significant attention, which builds and manages portfolios of financial instruments for investors.In this paper, we have proposed the a deep learning algorithm geared toward identification and forecast of the KOSPI index direction,and we also have compared the accuracy of the prediction.In an application of forecasting the financial market index direction, we have shown that the Robo-Advisor using deep learning has a significant effect on finance industry. The Robo-Advisor collects a massive data such as earnings statements, news reports and regulatory filings, analyzes those and recommends investors how to view market trends and identify the best time to purchase financial assets. On the other hand, the Robo-Advisor allows businesses to learn more about their customers, develop better marketing strategies, increase sales and decrease costs.Artificial intelligence, deep learning, FinTech, Robo-Advisor, technical analysis."
787,딥러닝의 효율적인 학습을 위한 학습데이터 선별 기법에 관한 연구,주경돈 /Gyoung-don Joo,2017,한국정보과학회,"빅데이터의 증가와 병렬 하드웨어 처리, 학습 알고리즘의 개선은 딥러닝의 발전을 가져왔으며 인공지능이 적용 가능한 영역을 획기적으로 확장시키고 있다. 하지만 이러한 대규모의 학습 데이터에 기반한 딥러닝은 학습시간이 증가되고, 더 나아가 학습 데이터의 구축을 위한 레이블링(labeling) 비용이 데이터 수에 비례하여 증가하게 된다. 따라서 본 논문에서는, 딥러닝의 정확도를 최대한 유지하면서 필요한 학습 데이터의 수를 줄일수 있도록, 전체 데이터에서 제한된 수의 학습 데이터의 후보를 선별하는 기법에 대하여 연구한다. 특히 기존의 능동 학습에 기반한 기법들이 학습 데이터의 수가 매우 적어질 경우 딥러닝의 정확도가 급격히 떨어지는 문제를 해결할 수 있는 새로운 기법을 제안하였으며, 제안된 기법으로 선별된 학습 데이터는 그 수의 변화에 대해 딥러닝이 일관되게 경쟁력있는 정확도를 가짐을 다양한 실험을 통하여 검증하였다.The increase of big data, parallel hardware processing and enhanced learning algorithms have led to the development of deep learning and are dramatically expanding the domain of applications based on artificial intelligence. However, such large-scale training data extends the learning time, and furthermore, the labeling cost for building the training set increases in proportion to the volume of data. Therefore, in this paper, we study how to select a limited number of candidates for training set from given data so as to reduce the number of training set while maintaining the accuracy of deep learning. Especially, we found that, with the training set selected by the previous work, the accuracy of deep learning drastically decreases when the number of training set is very small and so proposed new methods to resolve this fall-off in accuracy. Through experiments, we showed that deep learning consistently has the competitive accuracy with the training set selected by the proposed method regardless of the size of training set.Deep Learning, Train Set Selection, Active Learning, Gamma distribution"
788,딥러닝을 이용한 범용적 스테그아날리시스,김현재 /Hyunjae Kim,2017,한국정보과학회,"스테그아날리시스(Steganalysis)란 이미지 등 일반적인 자료에 암호화된 정보를 은닉하는 스테가노그래피(Steganography)에 대한 검출 및 분석 방법으로, 기계학습 기반 방법론을 포함한다. 기존 기계학습 기반 스테그아날리시스는 영상(Image)의 특징(Feature) 추출 및 모델링에 기반하며, 최근 딥러닝(Deep Learning)의 적용으로 검출 정확도가 큰 폭으로 향상되었다. 하지만 현존하는 스테그아날리시스 모델은 단일 스테가노그래피 기법에 대해 국한되어 있어 학습에 사용되지 않은 스테고(Stego) 이미지의 경우 검출이 불가능한 결정적 한계를 가진다. 본 연구에서는 다양한 스테가노그래피 기법으로 생성된 스테고이미지에 딥러닝을 적용하여 스테그아날리시스를 학습하는 범용적 모델을 제안한다. 다양한 실험을 통해 제안 기법의 효용성 및 가능성을 확인하고, 범용적 스테그아날리시스 모델이 각각에 특화된 검출 기법과 유사한 정확도로 스테고 이미지를 검출할 수 있음을 보인다.Steganalysis is to detect information hidden by steganography inside general data such as images. There are stegoanalysis techniques that use machine learning (ML). Existing ML approaches to steganalysis are based on extracting features from stego images and modeling them.
Recently deep learning-based methodologies have shown significant improvements in detection accuracy. However, all the existing methods, including deep learning-based ones, have a critical limitation in that they can only detect stego images that are created by a specific steganography method. In this paper, we propose a generalized steganalysis method that can model multiple types of stego images using deep learning. Through various experiments, we confirm the effectiveness of our approach and envision directions for future research. In particular, we show that our method can detect each type of steganography with the same level of accuracy as that of a steganalysis method dedicated to that type of steganography, thereby demonstrating the general applicability of our approach to multiple types of stego images.steganalysis, steganography, deep learning, generalized steganalysis model"
789,"딥러닝 프레임워크의 비교: 티아노, 텐서플로, CNTK를 중심으로",정여진 /Yeojin Chung,2017,한국지능정보시스템학회,"딥러닝 프레임워크의 대표적인 기능으로는 ‘자동미분’과 ‘GPU의 활용’ 등을 들 수 있다. 본 논문은 파이썬의 라이브러리 형태로 사용 가능한 프레임워크 중에서 구글의 텐서플로와 마이크로소프트의 CNTK, 그리고 텐서 플로의 원조라고 할 수 있는 티아노를 비교하였다. 본문에서는 자동미분의 개념과 GPU의 활용형태를 간단히 설명하고, 그 다음에 logistic regression을 실행하는 예를 통하여 각 프레임워크의 문법을 알아본 뒤에, 마지막으로 대표적인 딥러닝 응용인 CNN의 예제를 실행시켜보고 코딩의 편의성과 실행속도 등을 확인해 보았다. 그 결과, 편의성의 관점에서 보면 티아노가 가장 코딩 하기가 어렵고, CNTK와 텐서플로는 많은 부분이 비슷하게 추상화 되어 있어서 코딩이 비슷하지만 가중치와 편향을 직접 정의하느냐의 여부에서 차이를 보였다. 그리고 각프레임워크의 실행속도에 대한 평가는 ‘큰 차이는 없다’는 것이다. 텐서플로는 티아노에 비하여 속도가 느리다는 평가가 있어왔는데, 본 연구의 실험에 의하면, 비록 CNN 모형에 국한되었지만, 텐서플로가 아주 조금이지만빠른 것으로 나타났다. CNTK의 경우에도, 비록 실험환경이 달랐지만, 실험환경의 차이에 의한 속도의 차이의 편차범위 이내에 있는 것으로 판단이 되었다. 본 연구에서는 세 종류의 딥러닝 프레임워크만을 살펴보았는데,위키피디아에 따르면 딥러닝 프레임워크의 종류는 12가지가 있으며, 각 프레임워크의 특징을 15가지 속성으로구분하여 차이를 특정하고 있다. 그 많은 속성 중에서 사용자의 입장에서 볼 때 중요한 속성은 어떤 언어(파이썬, C++, Java, 등)로 사용가능한지, 어떤 딥러닝 모형에 대한 라이브러리가 잘 구현되어 있는지 등일 것이다. 그리고 사용자가 대규모의 딥러닝 모형을 구축한다면, 다중 GPU 혹은 다중 서버를 지원하는지의 여부도 중요할 것이다. 또한 딥러닝 모형을 처음 학습하는 경우에는 사용설명서가 많은지 예제 프로그램이 많은지 여부도 중요한 기준이 될 것이다.The deep learning framework is software designed to help develop deep learning models. Some of its important functions include “automatic differentiation” and “utilization of GPU”. The list of popular deep learning framework includes Caffe (BVLC) and Theano (University of Montreal). And recently, Microsoft's deep learning framework, Microsoft Cognitive Toolkit, was released as open-source license, following Google’s Tensorflow a year earlier. The early deep learning frameworks have been developed mainly for research at universities. Beginning with the inception of Tensorflow, however, it seems that companies such as Microsoft and Facebook have started to join the competition of framework development. Given the trend, Google and other companies are expected to continue investing in the deep learning framework to bring forward the initiative in the artificial intelligence business. From this point of view, we think it is a good time to compare some of deep learning frameworks. So we compare three deep learning frameworks which can be used as a Python library. Those are Google's Tensorflow, Microsoft’s CNTK, and Theano which is sort of a predecessor of the preceding two.
The most common and important function of deep learning frameworks is the ability to perform automatic differentiation. Basically all the mathematical expressions of deep learning models can be represented as computational graphs, which consist of nodes and edges. Partial derivatives on each edge of a computational graph can then be obtained. With the partial derivatives, we can let software compute differentiation of any node with respect to any variable by utilizing chain rule of Calculus.
First of all, the convenience of coding is in the order of CNTK, Tensorflow, and Theano. The criterion is simply based on the lengths of the codes and the learning curve and the ease of coding are not the main concern. According to the criteria, Theano was the most difficult to implement with, and CNTK and Tensorflow were somewhat easier. With Tensorflow, we need to define weight variables and biases explicitly. The reason that CNTK and Tensorflow are easier to implement with is that those frameworks provide us with more abstraction than Theano. We, however, need to mention that low-level coding is not always bad. It gives us flexibility of coding. With the low-level coding such as in Theano, we can implement and test any new deep learning models or any new search methods that we can think of.
The assessment of the execution speed of each framework is that there is not meaningful difference. According to the experiment, execution speeds of Theano and Tensorflow are very similar, although the experiment was limited to a CNN model. In the case of CNTK, the experimental environment was not maintained as the same. The code written in CNTK has to be run in PC environment without GPU where codes execute as much as 50 times slower than with GPU. But we concluded that the difference of execution speed was within the range of variation caused by the different hardware setup.
In this study, we compared three types of deep learning framework: Theano, Tensorflow, and CNTK. According to Wikipedia, there are 12 available deep learning frameworks. And 15 different attributes differentiate each framework. Some of the important attributes would include interface language (Python, C ++, Java, etc.) and the availability of libraries on various deep learning models such as CNN, RNN, DBN, and etc. And if a user implements a large scale deep learning model, it will also be important to support multiple GPU or multiple servers. Also, if you are learning the deep learning model, it would also be important if there are enough examples and references.deep learning framework, Theano, TensorFlow, CNTK, computational graph, CIFAR-10"
790,딥러닝을 이용한 주변 무선단말 파악방안,이웅섭 /Lee， Woong-sup,2018,한국정보통신학회,"최근 단말-대-단말(Device-to-device, D2D) 통신기술이 차세대 무선통신시스템의 핵심기술로 큰 관심을 받고 있다. 이러한 단말간 통신에서는 자신의 주변에 어떠한 단말이 있는지 파악하는 주변단말 탐색(Neighbor discovery)이매우 중요하다. 본 논문에서는 최근 큰 관심을 받고 있는 딥러닝(Deep learning) 기술을 활용하여 단말간 통신에서 주변단말을 파악하는 방안에 대해서 제안한다. 제안 방안은 기존의 방안과 달리 무선채널의 공간적 연관성을 이용하여단말간의 신호 전송 없이 단말이 기지국으로 전송하는 상향링크 파일럿 신호를 기반으로 주변 단말을 찾고 따라서기존의 방식에 비해 신호전송 복잡도(signaling complexity)를 크게 줄일 수 있다. 또한 제안 방안에서는 떨어져 있는거리에 따라서 주변 단말을 분류 가능하여 기존 방안에 비해서 좀 더 세밀한 단말 탐색이 가능하다. 마지막으로 본 논문에서는 tensorflow를 이용한 컴퓨터 시뮬레이션을 통해 제안 방안의 성능을 검증하였다.Recently, the device-to-device (D2D) communication has been conceived as the key technology for the next-generationmobile communication systems. The neighbor discovery in which the nearby users are found, is essential for the properoperation of the D2D communication. In this paper, we propose new neighbor discovery scheme based on deep learningtechnology which has gained a lot of attention recently. In the proposed scheme, the neighboring users can be foundusing the uplink pilot transmission of users only, unlike conventional neighbor discovery schemes in which direct pilotcommunication among users is required, such that the signaling overhead can be greatly reduced in our proposed scheme.
Moreover, the neighbors with different proximity can also be classified accordingly which enables more accurate neighbordiscovery compared to the conventional schemes. The performance of our proposed scheme is verified through thetensorflow-based computer simulations.
키워드 : 딥러닝, 주변단말 탐색, 단말-대-단말 통신Deep learning, Neighbor discovery, Device-to-device communication, Wireless channel correlation"
791,신뢰성있는 딥러닝 기반 분석 모델을 참조하기 위한 딥러닝 기술 언어,문종혁 /Jong Hyeok Mun,2021,한국정보처리학회,"최근 딥러닝은 하드웨어 성능이 향상됨에 따라 자연어 처리, 영상 인식 등의 다양한 기술에 접목되어 활용되고 있다. 이러한 기술들을 활용해 지능형교통 시스템(ITS), 스마트홈, 헬스케어 등의 산업분야에서 데이터를 분석하여 고속도로 속도위반 차량 검출, 에너지 사용량 제어, 응급상황 등과 같은고품질의 서비스를 제공하며, 고품질의 서비스를 제공하기 위해서는 정확도가 향상된 딥러닝 모델이 적용되어야 한다. 이를 위해 서비스 환경의 데이터를분석하기 위한 딥러닝 모델을 개발할 때, 개발자는 신뢰성이 검증된 최신의 딥러닝 모델을 적용할 수 있어야 한다. 이는 개발자가 참조하는 딥러닝 모델에적용된 학습 데이터셋의 정확도를 측정하여 검증할 수 있다. 이러한 검증을 위해서 개발자는 학습 데이터셋, 딥러닝의 계층구조 및 개발 환경 등과 같은내용을 포함하는 딥러닝 모델을 문서화하여 적용하기 위한 구조적인 정보가 필요하다. 본 논문에서는 신뢰성있는 딥러닝 기반 데이터 분석 모델을 참조하기위한 딥러닝 기술 언어를 제안한다. 제안하는 기술 언어는 신뢰성 있는 딥러닝 모델을 개발하는데 필요한 학습데이터셋, 개발 환경 및 설정 등의 정보와더불어 딥러닝 모델의 계층구조를 표현할 수 있다. 제안하는 딥러닝 기술 언어를 이용하여 개발자는 지능형 교통 시스템에서 참조하는 분석 모델의정확도를 검증할 수 있다. 실험에서는 제안하는 언어의 유효성을 검증하기 위해, 번호판 인식 모델을 중심으로 딥러닝 기술 문서의 적용과정을 보인다With the recent advancements of deep learning, companies such as smart home, healthcare, and intelligent transportation systems areutilizing its functionality to provide high-quality services for vehicle detection, emergency situation detection, and controlling energyconsumption. To provide reliable services in such sensitive systems, deep learning models are required to have high accuracy. In orderto develop a deep learning model for analyzing previously mentioned services, developers should utilize the state of the art deep learningmodels that have already been verified for higher accuracy. The developers can verify the accuracy of the referenced model by validatingthe model on the dataset. For this validation, the developer needs structural information to document and apply deep learning models,including metadata such as learning dataset, network architecture, and development environments. In this paper, we propose a descriptionlanguage that represents the network architecture of the deep learning model along with its metadata that are necessary to develop adeep learning model. Through the proposed description language, developers can easily verify the accuracy of the referenced deep learningmodel. Our experiments demonstrate the application scenario of a deep learning description document that focuses on the license platerecognition for the detection of illegally parked vehicles.Trusted Deep Learning, Model Reference, Deep Learning Description Language, Traffic Situation Analysis Model"
792,딥러닝 분석을 이용한 중국 역내·외 위안화 변동성 예측,이우식 /Lee Woosik,2016,한국데이터정보과학회,"2008년 글로벌 금융위기 이후 중국은 위안화 국제화의 점진적 추진을 진행하면서 중국상하이 외환시장과 중국홍콩 외환시장에서 거래되는 통화인 역내위안화와 역외위안화를 형성시켰다. 본 연구는 위안화 국제화와 점진적인 중국 자본계정 개방에 따라 급변하는 외환시장상황의 변동성을 정확하게 파악하기 위해서 GARCH모형 (일반화된 자기회귀 조건부이분산성모형)에 다단계인공신경망을 결합한 MLP-GARCH 모형과 GARCH모형과 기계학습의 일종인 딥러닝 (deep learning)을 통합한 DL-GARCH을 가지고 위안화 변동성예측을 비교 실험과 분석을 하였다. 비교분석 결과 DLGARCH 모형은 MLP-GARCH보다 모형 위안화 역내·외 환율변동성 예측 면에서 더욱 더 개선된예측값을 제공하였다. 그래서 이분산시계열모형을 딥러닝과 결합한 DL-GARCH 모형은 시계열의환율 변동성 예측 문제에 딥러닝을 응용할 수 있음을 확인하였다. 향후 이분산시계열과 결합된 딥러닝 모형은 다른 금융시계열 데이터에 응용하여 그 일반화 가능성을 높일 수 있을 것이다.The People’s Republic of China has vigorously been pursuing the internationalization of the Chinese Yuan or Renminbi after the financial crisis of 2008. In this view, an abrupt increase of use of the Chinese Yuan in the onshore and offshore markets are important milestones to be one of important currencies. One of the most frequently used methods to forecast volatility is GARCH model. Since a prediction error of the GARCH model has been reported quite high, a lot of efforts have been made to improve forecasting capability of the GARCH model. In this paper, we have proposed MLPGARCH and a DL-GARCH by employing Artificial Neural Network to the GARCH. In an application to forecasting Chinese Yuan volatility, we have successfully shown their overall outperformance in forecasting over the GARCH.Deep learning, DL-GARCH, GARCH, The Chinese Yuan, volatility."
793,딥러닝 기법을 이용한 내일강수 예측,하지훈,2016,한국지능시스템학회,"정확한 강수예측을 위해서는 예측인자 선정과 예측방법에 대한 선택이 매우 중요하다. 최근에는 강수예측 방법으로기계학습 기법이 많이 사용되고 있으며, 그 중에서도 특히 인공신경망을 사용한 강수예측 방법은 좋은 성능을 보였다. 본논문에서는 딥러닝 기법 중 하나인 DBN(deep belief network)를 이용한 새로운 강수예측 방법을 제안한다. DBN는 비지도사전 학습을 통해 초기 가중치를 설정하여 기존 인공신경망의 문제점을 보완한다. 예측인자로는 기온, 전일-전주 강수일, 태양과 달 궤도 관련 자료를 선정하였다. 기온과 전일-전주 강수일은 서울에서의 1974년부터 2013년까지 총 40년간의AWS(automatic weather system) 관측 자료를 사용하였고, 태양과 달의 궤도 관련 자료는 서울을 중심으로 계산한 결과를사용하였다. 전체 기간에서 일부는 학습 자료로 사용하여 예측모델을 생성하였고, 나머지를 생성한 모델의 검증 자료로사용하였다. 모델 검증 결과로 나온 예측값들은 확률값을 가지며 임계치를 이용하여 강수유무를 판별하였다. 강수 정확도의척도로 양분예보기법 중 CSI(critical successive index)와 Bias(frequency bias)를 계산하였다. 이를 통해 DBN와 MLP(multilayer perceptron)의 성능을 비교한 결과 DBN의 강수 예측 정확도가 높았고, 수행속도 또한 2배 이상 빨랐다.For accurate precipitation forecasts the choice of weather faoctrs and prediction method is very important. Recently, machinele arning has been widely used for forecasting precipitation, and artificial neural network, one of machine learning techniques, showed good performance. In this paper, we suggest a new method for forecasting precipitation using DBN, one of deep learning techniques.
DBN has an advantage that initial weights are set by unsupervised learning, so this compensates for the defects of artificial neural networks. We used past precipitation, temperature, and the parameters of the sun and moon's motion as features for forecasting precipitation. The dataset consists of observation data which had been measured for 40 years from AWS in Seoul. Experiments were based on 8-fold cross validation. As a result of estimation, we got probabilities of test dataset, so threshold was used for the decision of precipitation. CSI and Bias were used for indicating the precision of precipitation. Our experimental results showed that DBN performed better than MLP.Deep learning, Deep belief network, Precipitation, Forecast"
794,딥러닝 신경망모형을 이용한 실시간 교통정보수집 연구,김대현 /KIM DAE HYON,2016,사단법인 인문사회과학기술융합학회,"비디오 기반의 실시간 교통정보수집 시스템은 자율주행을 포함한 지능형교통체계의 주요 서브시스템중 하나이며, 우수한 예측(추정)력 때문에 핵심 알고리즘으로 인공신경망 모형이 주로 사용되고 있다. 특히, 최근 과학 학술지 네이처에 구글 딥마인드의 알파고가 공개되면서 딥러닝과 인공지능에 대한 전 세계적인 관심이 집중되고 있다. 최근까지 가장 널리 사용된 인공신경망모형은 역전파 모형이었으나 최근 컴퓨터 하드웨어 및 알고리즘의 새로운 개발과 더불어 딥러닝 알고리즘을 이용한 딥인공신경망 모형에 대한 관심이 높아지고 있다. 그러나 아직 딥러닝 인공신경망에 대한 연구는 초기단계에 있는 실정이며 적용분야도 극히 한정되어 있다. 본 연구에서는 실시간 도로교통정보 수집을 위한 영상처리분야에서 딥러닝 알고리즘을 활용하고 실험결과를 통하여 향후 교통정보수집‧분석 분야에서의 딥러닝 인공신경망 모형의 적용가능성을 살펴보고자 한다. 특히, 본 연구에서 도출한 딥러닝 인공신경망모형의 실험결과를 기존 역전파 모형에 의한 실험결과와 비교하여 모형의 우수성을 비교‧평가하고자 한다.Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Normally, artificial neural networks have been used in real-time applications for collecting traffic information because of their remarkable performance in terms of predictive accuracy and real-time computing. Especially, the recent publication of a paper in the journal Nature and the success of Google DeepMind’s world-beating Go program AlphaGo caused significant increases of the public interest in artificial intelligence. Even though various types of learning models have been proposed, the Backpropagation has been the most popular neural network model. However, the Deep learning neural network model has attracted much interest recently. In this study, the Deep leaning neural network will be applied in image processing techniques for the real-time traffic information collection to investigate the performance of Deep learning algorithm in traffic engineering fields. In addition, the experimental results from Deep learing algorithm will be compared with the results from Backpropagation model to evaluate and propose better performing model.Intelligent Transportation Systems (ITS), artificial neural networks, Deep learning, traffic information, image processing"
795,딥러닝의 모형과 응용사례,안성만 /SungMahn Ahn,2016,한국지능정보시스템학회,"딥러닝은 인공신경망(neural network)이라는 인공지능분야의 모형이 발전된 형태로서, 계층구조로 이루어진 인공신경망의 내부계층(hidden layer)이 여러 단계로 이루어진 구조이다. 딥러닝에서의 주요 모형은 합성곱신경망(convolutional neural network), 순환신경망(recurrent neural network), 그리고 심층신뢰신경망(deep belief network)의 세가지라고 할 수있다. 그 중에서 현재 흥미로운 연구가 많이 발표되어서 관심이 집중되고 있는 모형은 지도학습(supervised learning)모형인 처음 두 개의 모형이다. 따라서 본 논문에서는 지도학습모형의 가중치를 최적화하는 기본적인 방법인 오류역전파 알고리즘을 살펴본 뒤에 합성곱신경망과 순환신경망의 구조와 응용사례 등을 살펴보고자 한다. 본문에서 다루지 않은 모형인 심층신뢰신경망은 아직까지는 합성곱신경망 이나 순환신경망보다는 상대적으로 주목을 덜 받고 있다. 그러나 심층신뢰신경망은 CNN이나 RNN과는 달리 비지도학습(unsupervised learning)모형이며, 사람이나 동물은 관찰을 통해서 스스로 학습한다는 점에서 궁극적으로는 비지도학습모형이 더 많이 연구되어야 할 주제가 될 것이다.Deep learning model is a kind of neural networks that allows multiple hidden layers. There are various deep learning architectures such as convolutional neural networks, deep belief networks and recurrent neural networks. Those have been applied to fields like computer vision, automatic speech recognition, natural language processing, audio recognition and bioinformatics where they have been shown to produce state-of-the-art results on various tasks. Among those architectures, convolutional neural networks and recurrent neural networks are classified as the supervised learning model. And in recent years, those supervised learning models have gained more popularity than unsupervised learning models such as deep belief networks, because supervised learning models have shown fashionable applications in such fields mentioned above.
Deep learning models can be trained with backpropagation algorithm. Backpropagation is an abbreviation for “backward propagation of errors” and a common method of training artificial neural networks used in conjunction with an optimization method such as gradient descent. The method calculates the gradient of an error function with respect to all the weights in the network. The gradient is fed to the optimization method which in turn uses it to update the weights, in an attempt to minimize the error function.
Convolutional neural networks use a special architecture which is particularly well-adapted to classify images. Using this architecture makes convolutional networks fast to train. This, in turn, helps us train deep, muti-layer networks, which are very good at classifying images. These days, deep convolutional networks are used in most neural networks for image recognition. Convolutional neural networks use three basic ideas: local receptive fields, shared weights, and pooling. By local receptive fields, we mean that each neuron in the first(or any) hidden layer will be connected to a small region of the input(or previous layer’s) neurons. Shared weights mean that we’re going to use the same weights and bias for each of the local receptive field. This means that all the neurons in the hidden layer detect exactly the same feature, just at different locations in the input image. In addition to the convolutional layers just described, convolutional neural networks also contain pooling layers. Pooling layers are usually used immediately after convolutional layers. What the pooling layers do is to simplify the information in the output from the convolutional layer.
Recent convolutional network architectures have 10 to 20 hidden layers and billions of connections between units. Training deep learning networks has taken weeks several years ago, but thanks to progress in GPU and algorithm enhancement, training time has reduced to several hours.
Neural networks with time-varying behavior are known as recurrent neural networks or RNNs. A recurrent neural network is a class of artificial neural network where connections between units form a directed cycle. This creates an internal state of the network which allows it to exhibit dynamic temporal behavior. Unlike feedforward neural networks, RNNs can use their internal memory to process arbitrary sequences of inputs. Early RNN models turned out to be very difficult to train, harder even than deep feedforward networks. The reason is the unstable gradient problem such as vanishing gradient and exploding gradient. The gradient can get smaller and smaller as it is propagated back through layers. This makes learning in early layers extremely slow. The problem actually gets worse in RNNs, since gradients aren’t just propagated backward through layers, they’re propagated backward through time. If the network runs for a long time, that can make the gradient extremely unstable and hard to learn from. It has been possible to incorporate an idea known as long short-term memory units (LSTMs) into RNNs. LSTMs make it much easier to get good results when training RNNs, and many recent papers make use of LSTMs or related ideas.Deep learning, Convolutional neural networks, Recurrent neural networks, Error backpropagation algorithm"
796,딥러닝분석과 기술적 분석 지표를 이용한 한국 코스피주가지수 방향성 예측,이우식 /Lee Woosik,2017,한국데이터정보과학회,"2016년 3월 구글 (Google)의 바둑인공지능 알파고 (AlphaGo)가 이세돌 9단과의 바둑대결에서승리한 이후 다양한 분야에서 인공지능 사용에 대한 관심이 높아지고 있는 가운데 금융투자 분야에서도 인공지능과 투자자문 전문가의 합성어인 로보어드바이저 (Robo-Advisor)에 대한 관심이 높아지고 있다. 인공지능 (artificial intelligence)기반의 의사결정은 비용 절감은 물론 효과적인 의사결정을가능하게 한다는 점에서 큰 장점이 있다. 본 연구에서는 기술적 분석 (technical analysis) 지표와 딥러닝 (deep learning) 모형을 결합하여 한국 코스피 지수를 예측하는 모형을 개발하고 제시한 모형들의 예측력을 비교, 분석한다. 분석 결과 기술적 분석 지표에 딥러닝 알고리즘을 결합한 모형이 주가지수 방향성 예측 문제에 응용될 수 있음을 확인하였다. 향후 본 연구에서 제안된 기술적 분석 지표와딥러닝모형을 결합한 기법은 로보어드바이저서비스에 응용할 수 있는 일반화 가능성을 보여준다Since Google’s AlphaGo defeated a world champion of Go players in 2016, there have been many interests in the deep learning. In the financial sector, a Robo-Advisor using deep learning gains a significant attention, which builds and manages portfolios of financial instruments for investors.In this paper, we have proposed the a deep learning algorithm geared toward identification and forecast of the KOSPI index direction,and we also have compared the accuracy of the prediction.In an application of forecasting the financial market index direction, we have shown that the Robo-Advisor using deep learning has a significant effect on finance industry. The Robo-Advisor collects a massive data such as earnings statements, news reports and regulatory filings, analyzes those and recommends investors how to view market trends and identify the best time to purchase financial assets. On the other hand, the Robo-Advisor allows businesses to learn more about their customers, develop better marketing strategies, increase sales and decrease costs.Artificial intelligence, deep learning, FinTech, Robo-Advisor, technical analysis."
797,딥러닝의 효율적인 학습을 위한 학습데이터 선별 기법에 관한 연구,주경돈 /Gyoung-don Joo,2017,한국정보과학회,"빅데이터의 증가와 병렬 하드웨어 처리, 학습 알고리즘의 개선은 딥러닝의 발전을 가져왔으며 인공지능이 적용 가능한 영역을 획기적으로 확장시키고 있다. 하지만 이러한 대규모의 학습 데이터에 기반한 딥러닝은 학습시간이 증가되고, 더 나아가 학습 데이터의 구축을 위한 레이블링(labeling) 비용이 데이터 수에 비례하여 증가하게 된다. 따라서 본 논문에서는, 딥러닝의 정확도를 최대한 유지하면서 필요한 학습 데이터의 수를 줄일수 있도록, 전체 데이터에서 제한된 수의 학습 데이터의 후보를 선별하는 기법에 대하여 연구한다. 특히 기존의 능동 학습에 기반한 기법들이 학습 데이터의 수가 매우 적어질 경우 딥러닝의 정확도가 급격히 떨어지는 문제를 해결할 수 있는 새로운 기법을 제안하였으며, 제안된 기법으로 선별된 학습 데이터는 그 수의 변화에 대해 딥러닝이 일관되게 경쟁력있는 정확도를 가짐을 다양한 실험을 통하여 검증하였다.The increase of big data, parallel hardware processing and enhanced learning algorithms have led to the development of deep learning and are dramatically expanding the domain of applications based on artificial intelligence. However, such large-scale training data extends the learning time, and furthermore, the labeling cost for building the training set increases in proportion to the volume of data. Therefore, in this paper, we study how to select a limited number of candidates for training set from given data so as to reduce the number of training set while maintaining the accuracy of deep learning. Especially, we found that, with the training set selected by the previous work, the accuracy of deep learning drastically decreases when the number of training set is very small and so proposed new methods to resolve this fall-off in accuracy. Through experiments, we showed that deep learning consistently has the competitive accuracy with the training set selected by the proposed method regardless of the size of training set.Deep Learning, Train Set Selection, Active Learning, Gamma distribution"
798,딥러닝을 이용한 범용적 스테그아날리시스,김현재 /Hyunjae Kim,2017,한국정보과학회,"스테그아날리시스(Steganalysis)란 이미지 등 일반적인 자료에 암호화된 정보를 은닉하는 스테가노그래피(Steganography)에 대한 검출 및 분석 방법으로, 기계학습 기반 방법론을 포함한다. 기존 기계학습 기반 스테그아날리시스는 영상(Image)의 특징(Feature) 추출 및 모델링에 기반하며, 최근 딥러닝(Deep Learning)의 적용으로 검출 정확도가 큰 폭으로 향상되었다. 하지만 현존하는 스테그아날리시스 모델은 단일 스테가노그래피 기법에 대해 국한되어 있어 학습에 사용되지 않은 스테고(Stego) 이미지의 경우 검출이 불가능한 결정적 한계를 가진다. 본 연구에서는 다양한 스테가노그래피 기법으로 생성된 스테고이미지에 딥러닝을 적용하여 스테그아날리시스를 학습하는 범용적 모델을 제안한다. 다양한 실험을 통해 제안 기법의 효용성 및 가능성을 확인하고, 범용적 스테그아날리시스 모델이 각각에 특화된 검출 기법과 유사한 정확도로 스테고 이미지를 검출할 수 있음을 보인다.Steganalysis is to detect information hidden by steganography inside general data such as images. There are stegoanalysis techniques that use machine learning (ML). Existing ML approaches to steganalysis are based on extracting features from stego images and modeling them.
Recently deep learning-based methodologies have shown significant improvements in detection accuracy. However, all the existing methods, including deep learning-based ones, have a critical limitation in that they can only detect stego images that are created by a specific steganography method. In this paper, we propose a generalized steganalysis method that can model multiple types of stego images using deep learning. Through various experiments, we confirm the effectiveness of our approach and envision directions for future research. In particular, we show that our method can detect each type of steganography with the same level of accuracy as that of a steganalysis method dedicated to that type of steganography, thereby demonstrating the general applicability of our approach to multiple types of stego images.steganalysis, steganography, deep learning, generalized steganalysis model"
799,"딥러닝 프레임워크의 비교: 티아노, 텐서플로, CNTK를 중심으로",정여진 /Yeojin Chung,2017,한국지능정보시스템학회,"딥러닝 프레임워크의 대표적인 기능으로는 ‘자동미분’과 ‘GPU의 활용’ 등을 들 수 있다. 본 논문은 파이썬의 라이브러리 형태로 사용 가능한 프레임워크 중에서 구글의 텐서플로와 마이크로소프트의 CNTK, 그리고 텐서 플로의 원조라고 할 수 있는 티아노를 비교하였다. 본문에서는 자동미분의 개념과 GPU의 활용형태를 간단히 설명하고, 그 다음에 logistic regression을 실행하는 예를 통하여 각 프레임워크의 문법을 알아본 뒤에, 마지막으로 대표적인 딥러닝 응용인 CNN의 예제를 실행시켜보고 코딩의 편의성과 실행속도 등을 확인해 보았다. 그 결과, 편의성의 관점에서 보면 티아노가 가장 코딩 하기가 어렵고, CNTK와 텐서플로는 많은 부분이 비슷하게 추상화 되어 있어서 코딩이 비슷하지만 가중치와 편향을 직접 정의하느냐의 여부에서 차이를 보였다. 그리고 각프레임워크의 실행속도에 대한 평가는 ‘큰 차이는 없다’는 것이다. 텐서플로는 티아노에 비하여 속도가 느리다는 평가가 있어왔는데, 본 연구의 실험에 의하면, 비록 CNN 모형에 국한되었지만, 텐서플로가 아주 조금이지만빠른 것으로 나타났다. CNTK의 경우에도, 비록 실험환경이 달랐지만, 실험환경의 차이에 의한 속도의 차이의 편차범위 이내에 있는 것으로 판단이 되었다. 본 연구에서는 세 종류의 딥러닝 프레임워크만을 살펴보았는데,위키피디아에 따르면 딥러닝 프레임워크의 종류는 12가지가 있으며, 각 프레임워크의 특징을 15가지 속성으로구분하여 차이를 특정하고 있다. 그 많은 속성 중에서 사용자의 입장에서 볼 때 중요한 속성은 어떤 언어(파이썬, C++, Java, 등)로 사용가능한지, 어떤 딥러닝 모형에 대한 라이브러리가 잘 구현되어 있는지 등일 것이다. 그리고 사용자가 대규모의 딥러닝 모형을 구축한다면, 다중 GPU 혹은 다중 서버를 지원하는지의 여부도 중요할 것이다. 또한 딥러닝 모형을 처음 학습하는 경우에는 사용설명서가 많은지 예제 프로그램이 많은지 여부도 중요한 기준이 될 것이다.The deep learning framework is software designed to help develop deep learning models. Some of its important functions include “automatic differentiation” and “utilization of GPU”. The list of popular deep learning framework includes Caffe (BVLC) and Theano (University of Montreal). And recently, Microsoft's deep learning framework, Microsoft Cognitive Toolkit, was released as open-source license, following Google’s Tensorflow a year earlier. The early deep learning frameworks have been developed mainly for research at universities. Beginning with the inception of Tensorflow, however, it seems that companies such as Microsoft and Facebook have started to join the competition of framework development. Given the trend, Google and other companies are expected to continue investing in the deep learning framework to bring forward the initiative in the artificial intelligence business. From this point of view, we think it is a good time to compare some of deep learning frameworks. So we compare three deep learning frameworks which can be used as a Python library. Those are Google's Tensorflow, Microsoft’s CNTK, and Theano which is sort of a predecessor of the preceding two.
The most common and important function of deep learning frameworks is the ability to perform automatic differentiation. Basically all the mathematical expressions of deep learning models can be represented as computational graphs, which consist of nodes and edges. Partial derivatives on each edge of a computational graph can then be obtained. With the partial derivatives, we can let software compute differentiation of any node with respect to any variable by utilizing chain rule of Calculus.
First of all, the convenience of coding is in the order of CNTK, Tensorflow, and Theano. The criterion is simply based on the lengths of the codes and the learning curve and the ease of coding are not the main concern. According to the criteria, Theano was the most difficult to implement with, and CNTK and Tensorflow were somewhat easier. With Tensorflow, we need to define weight variables and biases explicitly. The reason that CNTK and Tensorflow are easier to implement with is that those frameworks provide us with more abstraction than Theano. We, however, need to mention that low-level coding is not always bad. It gives us flexibility of coding. With the low-level coding such as in Theano, we can implement and test any new deep learning models or any new search methods that we can think of.
The assessment of the execution speed of each framework is that there is not meaningful difference. According to the experiment, execution speeds of Theano and Tensorflow are very similar, although the experiment was limited to a CNN model. In the case of CNTK, the experimental environment was not maintained as the same. The code written in CNTK has to be run in PC environment without GPU where codes execute as much as 50 times slower than with GPU. But we concluded that the difference of execution speed was within the range of variation caused by the different hardware setup.
In this study, we compared three types of deep learning framework: Theano, Tensorflow, and CNTK. According to Wikipedia, there are 12 available deep learning frameworks. And 15 different attributes differentiate each framework. Some of the important attributes would include interface language (Python, C ++, Java, etc.) and the availability of libraries on various deep learning models such as CNN, RNN, DBN, and etc. And if a user implements a large scale deep learning model, it will also be important to support multiple GPU or multiple servers. Also, if you are learning the deep learning model, it would also be important if there are enough examples and references.deep learning framework, Theano, TensorFlow, CNTK, computational graph, CIFAR-10"
800,딥러닝을 이용한 주변 무선단말 파악방안,이웅섭 /Lee， Woong-sup,2018,한국정보통신학회,"최근 단말-대-단말(Device-to-device, D2D) 통신기술이 차세대 무선통신시스템의 핵심기술로 큰 관심을 받고 있다. 이러한 단말간 통신에서는 자신의 주변에 어떠한 단말이 있는지 파악하는 주변단말 탐색(Neighbor discovery)이매우 중요하다. 본 논문에서는 최근 큰 관심을 받고 있는 딥러닝(Deep learning) 기술을 활용하여 단말간 통신에서 주변단말을 파악하는 방안에 대해서 제안한다. 제안 방안은 기존의 방안과 달리 무선채널의 공간적 연관성을 이용하여단말간의 신호 전송 없이 단말이 기지국으로 전송하는 상향링크 파일럿 신호를 기반으로 주변 단말을 찾고 따라서기존의 방식에 비해 신호전송 복잡도(signaling complexity)를 크게 줄일 수 있다. 또한 제안 방안에서는 떨어져 있는거리에 따라서 주변 단말을 분류 가능하여 기존 방안에 비해서 좀 더 세밀한 단말 탐색이 가능하다. 마지막으로 본 논문에서는 tensorflow를 이용한 컴퓨터 시뮬레이션을 통해 제안 방안의 성능을 검증하였다.Recently, the device-to-device (D2D) communication has been conceived as the key technology for the next-generationmobile communication systems. The neighbor discovery in which the nearby users are found, is essential for the properoperation of the D2D communication. In this paper, we propose new neighbor discovery scheme based on deep learningtechnology which has gained a lot of attention recently. In the proposed scheme, the neighboring users can be foundusing the uplink pilot transmission of users only, unlike conventional neighbor discovery schemes in which direct pilotcommunication among users is required, such that the signaling overhead can be greatly reduced in our proposed scheme.
Moreover, the neighbors with different proximity can also be classified accordingly which enables more accurate neighbordiscovery compared to the conventional schemes. The performance of our proposed scheme is verified through thetensorflow-based computer simulations.
키워드 : 딥러닝, 주변단말 탐색, 단말-대-단말 통신Deep learning, Neighbor discovery, Device-to-device communication, Wireless channel correlation"
801,신뢰성있는 딥러닝 기반 분석 모델을 참조하기 위한 딥러닝 기술 언어,문종혁 /Jong Hyeok Mun,2021,한국정보처리학회,"최근 딥러닝은 하드웨어 성능이 향상됨에 따라 자연어 처리, 영상 인식 등의 다양한 기술에 접목되어 활용되고 있다. 이러한 기술들을 활용해 지능형교통 시스템(ITS), 스마트홈, 헬스케어 등의 산업분야에서 데이터를 분석하여 고속도로 속도위반 차량 검출, 에너지 사용량 제어, 응급상황 등과 같은고품질의 서비스를 제공하며, 고품질의 서비스를 제공하기 위해서는 정확도가 향상된 딥러닝 모델이 적용되어야 한다. 이를 위해 서비스 환경의 데이터를분석하기 위한 딥러닝 모델을 개발할 때, 개발자는 신뢰성이 검증된 최신의 딥러닝 모델을 적용할 수 있어야 한다. 이는 개발자가 참조하는 딥러닝 모델에적용된 학습 데이터셋의 정확도를 측정하여 검증할 수 있다. 이러한 검증을 위해서 개발자는 학습 데이터셋, 딥러닝의 계층구조 및 개발 환경 등과 같은내용을 포함하는 딥러닝 모델을 문서화하여 적용하기 위한 구조적인 정보가 필요하다. 본 논문에서는 신뢰성있는 딥러닝 기반 데이터 분석 모델을 참조하기위한 딥러닝 기술 언어를 제안한다. 제안하는 기술 언어는 신뢰성 있는 딥러닝 모델을 개발하는데 필요한 학습데이터셋, 개발 환경 및 설정 등의 정보와더불어 딥러닝 모델의 계층구조를 표현할 수 있다. 제안하는 딥러닝 기술 언어를 이용하여 개발자는 지능형 교통 시스템에서 참조하는 분석 모델의정확도를 검증할 수 있다. 실험에서는 제안하는 언어의 유효성을 검증하기 위해, 번호판 인식 모델을 중심으로 딥러닝 기술 문서의 적용과정을 보인다With the recent advancements of deep learning, companies such as smart home, healthcare, and intelligent transportation systems areutilizing its functionality to provide high-quality services for vehicle detection, emergency situation detection, and controlling energyconsumption. To provide reliable services in such sensitive systems, deep learning models are required to have high accuracy. In orderto develop a deep learning model for analyzing previously mentioned services, developers should utilize the state of the art deep learningmodels that have already been verified for higher accuracy. The developers can verify the accuracy of the referenced model by validatingthe model on the dataset. For this validation, the developer needs structural information to document and apply deep learning models,including metadata such as learning dataset, network architecture, and development environments. In this paper, we propose a descriptionlanguage that represents the network architecture of the deep learning model along with its metadata that are necessary to develop adeep learning model. Through the proposed description language, developers can easily verify the accuracy of the referenced deep learningmodel. Our experiments demonstrate the application scenario of a deep learning description document that focuses on the license platerecognition for the detection of illegally parked vehicles.Trusted Deep Learning, Model Reference, Deep Learning Description Language, Traffic Situation Analysis Model"
802,딥러닝 분석을 이용한 중국 역내·외 위안화 변동성 예측,이우식 /Lee Woosik,2016,한국데이터정보과학회,"2008년 글로벌 금융위기 이후 중국은 위안화 국제화의 점진적 추진을 진행하면서 중국상하이 외환시장과 중국홍콩 외환시장에서 거래되는 통화인 역내위안화와 역외위안화를 형성시켰다. 본 연구는 위안화 국제화와 점진적인 중국 자본계정 개방에 따라 급변하는 외환시장상황의 변동성을 정확하게 파악하기 위해서 GARCH모형 (일반화된 자기회귀 조건부이분산성모형)에 다단계인공신경망을 결합한 MLP-GARCH 모형과 GARCH모형과 기계학습의 일종인 딥러닝 (deep learning)을 통합한 DL-GARCH을 가지고 위안화 변동성예측을 비교 실험과 분석을 하였다. 비교분석 결과 DLGARCH 모형은 MLP-GARCH보다 모형 위안화 역내·외 환율변동성 예측 면에서 더욱 더 개선된예측값을 제공하였다. 그래서 이분산시계열모형을 딥러닝과 결합한 DL-GARCH 모형은 시계열의환율 변동성 예측 문제에 딥러닝을 응용할 수 있음을 확인하였다. 향후 이분산시계열과 결합된 딥러닝 모형은 다른 금융시계열 데이터에 응용하여 그 일반화 가능성을 높일 수 있을 것이다.The People’s Republic of China has vigorously been pursuing the internationalization of the Chinese Yuan or Renminbi after the financial crisis of 2008. In this view, an abrupt increase of use of the Chinese Yuan in the onshore and offshore markets are important milestones to be one of important currencies. One of the most frequently used methods to forecast volatility is GARCH model. Since a prediction error of the GARCH model has been reported quite high, a lot of efforts have been made to improve forecasting capability of the GARCH model. In this paper, we have proposed MLPGARCH and a DL-GARCH by employing Artificial Neural Network to the GARCH. In an application to forecasting Chinese Yuan volatility, we have successfully shown their overall outperformance in forecasting over the GARCH.Deep learning, DL-GARCH, GARCH, The Chinese Yuan, volatility."
803,딥러닝 기법을 이용한 내일강수 예측,하지훈,2016,한국지능시스템학회,"정확한 강수예측을 위해서는 예측인자 선정과 예측방법에 대한 선택이 매우 중요하다. 최근에는 강수예측 방법으로기계학습 기법이 많이 사용되고 있으며, 그 중에서도 특히 인공신경망을 사용한 강수예측 방법은 좋은 성능을 보였다. 본논문에서는 딥러닝 기법 중 하나인 DBN(deep belief network)를 이용한 새로운 강수예측 방법을 제안한다. DBN는 비지도사전 학습을 통해 초기 가중치를 설정하여 기존 인공신경망의 문제점을 보완한다. 예측인자로는 기온, 전일-전주 강수일, 태양과 달 궤도 관련 자료를 선정하였다. 기온과 전일-전주 강수일은 서울에서의 1974년부터 2013년까지 총 40년간의AWS(automatic weather system) 관측 자료를 사용하였고, 태양과 달의 궤도 관련 자료는 서울을 중심으로 계산한 결과를사용하였다. 전체 기간에서 일부는 학습 자료로 사용하여 예측모델을 생성하였고, 나머지를 생성한 모델의 검증 자료로사용하였다. 모델 검증 결과로 나온 예측값들은 확률값을 가지며 임계치를 이용하여 강수유무를 판별하였다. 강수 정확도의척도로 양분예보기법 중 CSI(critical successive index)와 Bias(frequency bias)를 계산하였다. 이를 통해 DBN와 MLP(multilayer perceptron)의 성능을 비교한 결과 DBN의 강수 예측 정확도가 높았고, 수행속도 또한 2배 이상 빨랐다.For accurate precipitation forecasts the choice of weather faoctrs and prediction method is very important. Recently, machinele arning has been widely used for forecasting precipitation, and artificial neural network, one of machine learning techniques, showed good performance. In this paper, we suggest a new method for forecasting precipitation using DBN, one of deep learning techniques.
DBN has an advantage that initial weights are set by unsupervised learning, so this compensates for the defects of artificial neural networks. We used past precipitation, temperature, and the parameters of the sun and moon's motion as features for forecasting precipitation. The dataset consists of observation data which had been measured for 40 years from AWS in Seoul. Experiments were based on 8-fold cross validation. As a result of estimation, we got probabilities of test dataset, so threshold was used for the decision of precipitation. CSI and Bias were used for indicating the precision of precipitation. Our experimental results showed that DBN performed better than MLP.Deep learning, Deep belief network, Precipitation, Forecast"
804,딥러닝 신경망모형을 이용한 실시간 교통정보수집 연구,김대현 /KIM DAE HYON,2016,사단법인 인문사회과학기술융합학회,"비디오 기반의 실시간 교통정보수집 시스템은 자율주행을 포함한 지능형교통체계의 주요 서브시스템중 하나이며, 우수한 예측(추정)력 때문에 핵심 알고리즘으로 인공신경망 모형이 주로 사용되고 있다. 특히, 최근 과학 학술지 네이처에 구글 딥마인드의 알파고가 공개되면서 딥러닝과 인공지능에 대한 전 세계적인 관심이 집중되고 있다. 최근까지 가장 널리 사용된 인공신경망모형은 역전파 모형이었으나 최근 컴퓨터 하드웨어 및 알고리즘의 새로운 개발과 더불어 딥러닝 알고리즘을 이용한 딥인공신경망 모형에 대한 관심이 높아지고 있다. 그러나 아직 딥러닝 인공신경망에 대한 연구는 초기단계에 있는 실정이며 적용분야도 극히 한정되어 있다. 본 연구에서는 실시간 도로교통정보 수집을 위한 영상처리분야에서 딥러닝 알고리즘을 활용하고 실험결과를 통하여 향후 교통정보수집‧분석 분야에서의 딥러닝 인공신경망 모형의 적용가능성을 살펴보고자 한다. 특히, 본 연구에서 도출한 딥러닝 인공신경망모형의 실험결과를 기존 역전파 모형에 의한 실험결과와 비교하여 모형의 우수성을 비교‧평가하고자 한다.Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Normally, artificial neural networks have been used in real-time applications for collecting traffic information because of their remarkable performance in terms of predictive accuracy and real-time computing. Especially, the recent publication of a paper in the journal Nature and the success of Google DeepMind’s world-beating Go program AlphaGo caused significant increases of the public interest in artificial intelligence. Even though various types of learning models have been proposed, the Backpropagation has been the most popular neural network model. However, the Deep learning neural network model has attracted much interest recently. In this study, the Deep leaning neural network will be applied in image processing techniques for the real-time traffic information collection to investigate the performance of Deep learning algorithm in traffic engineering fields. In addition, the experimental results from Deep learing algorithm will be compared with the results from Backpropagation model to evaluate and propose better performing model.Intelligent Transportation Systems (ITS), artificial neural networks, Deep learning, traffic information, image processing"
805,딥러닝의 모형과 응용사례,안성만 /SungMahn Ahn,2016,한국지능정보시스템학회,"딥러닝은 인공신경망(neural network)이라는 인공지능분야의 모형이 발전된 형태로서, 계층구조로 이루어진 인공신경망의 내부계층(hidden layer)이 여러 단계로 이루어진 구조이다. 딥러닝에서의 주요 모형은 합성곱신경망(convolutional neural network), 순환신경망(recurrent neural network), 그리고 심층신뢰신경망(deep belief network)의 세가지라고 할 수있다. 그 중에서 현재 흥미로운 연구가 많이 발표되어서 관심이 집중되고 있는 모형은 지도학습(supervised learning)모형인 처음 두 개의 모형이다. 따라서 본 논문에서는 지도학습모형의 가중치를 최적화하는 기본적인 방법인 오류역전파 알고리즘을 살펴본 뒤에 합성곱신경망과 순환신경망의 구조와 응용사례 등을 살펴보고자 한다. 본문에서 다루지 않은 모형인 심층신뢰신경망은 아직까지는 합성곱신경망 이나 순환신경망보다는 상대적으로 주목을 덜 받고 있다. 그러나 심층신뢰신경망은 CNN이나 RNN과는 달리 비지도학습(unsupervised learning)모형이며, 사람이나 동물은 관찰을 통해서 스스로 학습한다는 점에서 궁극적으로는 비지도학습모형이 더 많이 연구되어야 할 주제가 될 것이다.Deep learning model is a kind of neural networks that allows multiple hidden layers. There are various deep learning architectures such as convolutional neural networks, deep belief networks and recurrent neural networks. Those have been applied to fields like computer vision, automatic speech recognition, natural language processing, audio recognition and bioinformatics where they have been shown to produce state-of-the-art results on various tasks. Among those architectures, convolutional neural networks and recurrent neural networks are classified as the supervised learning model. And in recent years, those supervised learning models have gained more popularity than unsupervised learning models such as deep belief networks, because supervised learning models have shown fashionable applications in such fields mentioned above.
Deep learning models can be trained with backpropagation algorithm. Backpropagation is an abbreviation for “backward propagation of errors” and a common method of training artificial neural networks used in conjunction with an optimization method such as gradient descent. The method calculates the gradient of an error function with respect to all the weights in the network. The gradient is fed to the optimization method which in turn uses it to update the weights, in an attempt to minimize the error function.
Convolutional neural networks use a special architecture which is particularly well-adapted to classify images. Using this architecture makes convolutional networks fast to train. This, in turn, helps us train deep, muti-layer networks, which are very good at classifying images. These days, deep convolutional networks are used in most neural networks for image recognition. Convolutional neural networks use three basic ideas: local receptive fields, shared weights, and pooling. By local receptive fields, we mean that each neuron in the first(or any) hidden layer will be connected to a small region of the input(or previous layer’s) neurons. Shared weights mean that we’re going to use the same weights and bias for each of the local receptive field. This means that all the neurons in the hidden layer detect exactly the same feature, just at different locations in the input image. In addition to the convolutional layers just described, convolutional neural networks also contain pooling layers. Pooling layers are usually used immediately after convolutional layers. What the pooling layers do is to simplify the information in the output from the convolutional layer.
Recent convolutional network architectures have 10 to 20 hidden layers and billions of connections between units. Training deep learning networks has taken weeks several years ago, but thanks to progress in GPU and algorithm enhancement, training time has reduced to several hours.
Neural networks with time-varying behavior are known as recurrent neural networks or RNNs. A recurrent neural network is a class of artificial neural network where connections between units form a directed cycle. This creates an internal state of the network which allows it to exhibit dynamic temporal behavior. Unlike feedforward neural networks, RNNs can use their internal memory to process arbitrary sequences of inputs. Early RNN models turned out to be very difficult to train, harder even than deep feedforward networks. The reason is the unstable gradient problem such as vanishing gradient and exploding gradient. The gradient can get smaller and smaller as it is propagated back through layers. This makes learning in early layers extremely slow. The problem actually gets worse in RNNs, since gradients aren’t just propagated backward through layers, they’re propagated backward through time. If the network runs for a long time, that can make the gradient extremely unstable and hard to learn from. It has been possible to incorporate an idea known as long short-term memory units (LSTMs) into RNNs. LSTMs make it much easier to get good results when training RNNs, and many recent papers make use of LSTMs or related ideas.Deep learning, Convolutional neural networks, Recurrent neural networks, Error backpropagation algorithm"
806,딥러닝분석과 기술적 분석 지표를 이용한 한국 코스피주가지수 방향성 예측,이우식 /Lee Woosik,2017,한국데이터정보과학회,"2016년 3월 구글 (Google)의 바둑인공지능 알파고 (AlphaGo)가 이세돌 9단과의 바둑대결에서승리한 이후 다양한 분야에서 인공지능 사용에 대한 관심이 높아지고 있는 가운데 금융투자 분야에서도 인공지능과 투자자문 전문가의 합성어인 로보어드바이저 (Robo-Advisor)에 대한 관심이 높아지고 있다. 인공지능 (artificial intelligence)기반의 의사결정은 비용 절감은 물론 효과적인 의사결정을가능하게 한다는 점에서 큰 장점이 있다. 본 연구에서는 기술적 분석 (technical analysis) 지표와 딥러닝 (deep learning) 모형을 결합하여 한국 코스피 지수를 예측하는 모형을 개발하고 제시한 모형들의 예측력을 비교, 분석한다. 분석 결과 기술적 분석 지표에 딥러닝 알고리즘을 결합한 모형이 주가지수 방향성 예측 문제에 응용될 수 있음을 확인하였다. 향후 본 연구에서 제안된 기술적 분석 지표와딥러닝모형을 결합한 기법은 로보어드바이저서비스에 응용할 수 있는 일반화 가능성을 보여준다Since Google’s AlphaGo defeated a world champion of Go players in 2016, there have been many interests in the deep learning. In the financial sector, a Robo-Advisor using deep learning gains a significant attention, which builds and manages portfolios of financial instruments for investors.In this paper, we have proposed the a deep learning algorithm geared toward identification and forecast of the KOSPI index direction,and we also have compared the accuracy of the prediction.In an application of forecasting the financial market index direction, we have shown that the Robo-Advisor using deep learning has a significant effect on finance industry. The Robo-Advisor collects a massive data such as earnings statements, news reports and regulatory filings, analyzes those and recommends investors how to view market trends and identify the best time to purchase financial assets. On the other hand, the Robo-Advisor allows businesses to learn more about their customers, develop better marketing strategies, increase sales and decrease costs.Artificial intelligence, deep learning, FinTech, Robo-Advisor, technical analysis."
807,딥러닝의 효율적인 학습을 위한 학습데이터 선별 기법에 관한 연구,주경돈 /Gyoung-don Joo,2017,한국정보과학회,"빅데이터의 증가와 병렬 하드웨어 처리, 학습 알고리즘의 개선은 딥러닝의 발전을 가져왔으며 인공지능이 적용 가능한 영역을 획기적으로 확장시키고 있다. 하지만 이러한 대규모의 학습 데이터에 기반한 딥러닝은 학습시간이 증가되고, 더 나아가 학습 데이터의 구축을 위한 레이블링(labeling) 비용이 데이터 수에 비례하여 증가하게 된다. 따라서 본 논문에서는, 딥러닝의 정확도를 최대한 유지하면서 필요한 학습 데이터의 수를 줄일수 있도록, 전체 데이터에서 제한된 수의 학습 데이터의 후보를 선별하는 기법에 대하여 연구한다. 특히 기존의 능동 학습에 기반한 기법들이 학습 데이터의 수가 매우 적어질 경우 딥러닝의 정확도가 급격히 떨어지는 문제를 해결할 수 있는 새로운 기법을 제안하였으며, 제안된 기법으로 선별된 학습 데이터는 그 수의 변화에 대해 딥러닝이 일관되게 경쟁력있는 정확도를 가짐을 다양한 실험을 통하여 검증하였다.The increase of big data, parallel hardware processing and enhanced learning algorithms have led to the development of deep learning and are dramatically expanding the domain of applications based on artificial intelligence. However, such large-scale training data extends the learning time, and furthermore, the labeling cost for building the training set increases in proportion to the volume of data. Therefore, in this paper, we study how to select a limited number of candidates for training set from given data so as to reduce the number of training set while maintaining the accuracy of deep learning. Especially, we found that, with the training set selected by the previous work, the accuracy of deep learning drastically decreases when the number of training set is very small and so proposed new methods to resolve this fall-off in accuracy. Through experiments, we showed that deep learning consistently has the competitive accuracy with the training set selected by the proposed method regardless of the size of training set.Deep Learning, Train Set Selection, Active Learning, Gamma distribution"
808,딥러닝을 이용한 범용적 스테그아날리시스,김현재 /Hyunjae Kim,2017,한국정보과학회,"스테그아날리시스(Steganalysis)란 이미지 등 일반적인 자료에 암호화된 정보를 은닉하는 스테가노그래피(Steganography)에 대한 검출 및 분석 방법으로, 기계학습 기반 방법론을 포함한다. 기존 기계학습 기반 스테그아날리시스는 영상(Image)의 특징(Feature) 추출 및 모델링에 기반하며, 최근 딥러닝(Deep Learning)의 적용으로 검출 정확도가 큰 폭으로 향상되었다. 하지만 현존하는 스테그아날리시스 모델은 단일 스테가노그래피 기법에 대해 국한되어 있어 학습에 사용되지 않은 스테고(Stego) 이미지의 경우 검출이 불가능한 결정적 한계를 가진다. 본 연구에서는 다양한 스테가노그래피 기법으로 생성된 스테고이미지에 딥러닝을 적용하여 스테그아날리시스를 학습하는 범용적 모델을 제안한다. 다양한 실험을 통해 제안 기법의 효용성 및 가능성을 확인하고, 범용적 스테그아날리시스 모델이 각각에 특화된 검출 기법과 유사한 정확도로 스테고 이미지를 검출할 수 있음을 보인다.Steganalysis is to detect information hidden by steganography inside general data such as images. There are stegoanalysis techniques that use machine learning (ML). Existing ML approaches to steganalysis are based on extracting features from stego images and modeling them.
Recently deep learning-based methodologies have shown significant improvements in detection accuracy. However, all the existing methods, including deep learning-based ones, have a critical limitation in that they can only detect stego images that are created by a specific steganography method. In this paper, we propose a generalized steganalysis method that can model multiple types of stego images using deep learning. Through various experiments, we confirm the effectiveness of our approach and envision directions for future research. In particular, we show that our method can detect each type of steganography with the same level of accuracy as that of a steganalysis method dedicated to that type of steganography, thereby demonstrating the general applicability of our approach to multiple types of stego images.steganalysis, steganography, deep learning, generalized steganalysis model"
809,"딥러닝 프레임워크의 비교: 티아노, 텐서플로, CNTK를 중심으로",정여진 /Yeojin Chung,2017,한국지능정보시스템학회,"딥러닝 프레임워크의 대표적인 기능으로는 ‘자동미분’과 ‘GPU의 활용’ 등을 들 수 있다. 본 논문은 파이썬의 라이브러리 형태로 사용 가능한 프레임워크 중에서 구글의 텐서플로와 마이크로소프트의 CNTK, 그리고 텐서 플로의 원조라고 할 수 있는 티아노를 비교하였다. 본문에서는 자동미분의 개념과 GPU의 활용형태를 간단히 설명하고, 그 다음에 logistic regression을 실행하는 예를 통하여 각 프레임워크의 문법을 알아본 뒤에, 마지막으로 대표적인 딥러닝 응용인 CNN의 예제를 실행시켜보고 코딩의 편의성과 실행속도 등을 확인해 보았다. 그 결과, 편의성의 관점에서 보면 티아노가 가장 코딩 하기가 어렵고, CNTK와 텐서플로는 많은 부분이 비슷하게 추상화 되어 있어서 코딩이 비슷하지만 가중치와 편향을 직접 정의하느냐의 여부에서 차이를 보였다. 그리고 각프레임워크의 실행속도에 대한 평가는 ‘큰 차이는 없다’는 것이다. 텐서플로는 티아노에 비하여 속도가 느리다는 평가가 있어왔는데, 본 연구의 실험에 의하면, 비록 CNN 모형에 국한되었지만, 텐서플로가 아주 조금이지만빠른 것으로 나타났다. CNTK의 경우에도, 비록 실험환경이 달랐지만, 실험환경의 차이에 의한 속도의 차이의 편차범위 이내에 있는 것으로 판단이 되었다. 본 연구에서는 세 종류의 딥러닝 프레임워크만을 살펴보았는데,위키피디아에 따르면 딥러닝 프레임워크의 종류는 12가지가 있으며, 각 프레임워크의 특징을 15가지 속성으로구분하여 차이를 특정하고 있다. 그 많은 속성 중에서 사용자의 입장에서 볼 때 중요한 속성은 어떤 언어(파이썬, C++, Java, 등)로 사용가능한지, 어떤 딥러닝 모형에 대한 라이브러리가 잘 구현되어 있는지 등일 것이다. 그리고 사용자가 대규모의 딥러닝 모형을 구축한다면, 다중 GPU 혹은 다중 서버를 지원하는지의 여부도 중요할 것이다. 또한 딥러닝 모형을 처음 학습하는 경우에는 사용설명서가 많은지 예제 프로그램이 많은지 여부도 중요한 기준이 될 것이다.The deep learning framework is software designed to help develop deep learning models. Some of its important functions include “automatic differentiation” and “utilization of GPU”. The list of popular deep learning framework includes Caffe (BVLC) and Theano (University of Montreal). And recently, Microsoft's deep learning framework, Microsoft Cognitive Toolkit, was released as open-source license, following Google’s Tensorflow a year earlier. The early deep learning frameworks have been developed mainly for research at universities. Beginning with the inception of Tensorflow, however, it seems that companies such as Microsoft and Facebook have started to join the competition of framework development. Given the trend, Google and other companies are expected to continue investing in the deep learning framework to bring forward the initiative in the artificial intelligence business. From this point of view, we think it is a good time to compare some of deep learning frameworks. So we compare three deep learning frameworks which can be used as a Python library. Those are Google's Tensorflow, Microsoft’s CNTK, and Theano which is sort of a predecessor of the preceding two.
The most common and important function of deep learning frameworks is the ability to perform automatic differentiation. Basically all the mathematical expressions of deep learning models can be represented as computational graphs, which consist of nodes and edges. Partial derivatives on each edge of a computational graph can then be obtained. With the partial derivatives, we can let software compute differentiation of any node with respect to any variable by utilizing chain rule of Calculus.
First of all, the convenience of coding is in the order of CNTK, Tensorflow, and Theano. The criterion is simply based on the lengths of the codes and the learning curve and the ease of coding are not the main concern. According to the criteria, Theano was the most difficult to implement with, and CNTK and Tensorflow were somewhat easier. With Tensorflow, we need to define weight variables and biases explicitly. The reason that CNTK and Tensorflow are easier to implement with is that those frameworks provide us with more abstraction than Theano. We, however, need to mention that low-level coding is not always bad. It gives us flexibility of coding. With the low-level coding such as in Theano, we can implement and test any new deep learning models or any new search methods that we can think of.
The assessment of the execution speed of each framework is that there is not meaningful difference. According to the experiment, execution speeds of Theano and Tensorflow are very similar, although the experiment was limited to a CNN model. In the case of CNTK, the experimental environment was not maintained as the same. The code written in CNTK has to be run in PC environment without GPU where codes execute as much as 50 times slower than with GPU. But we concluded that the difference of execution speed was within the range of variation caused by the different hardware setup.
In this study, we compared three types of deep learning framework: Theano, Tensorflow, and CNTK. According to Wikipedia, there are 12 available deep learning frameworks. And 15 different attributes differentiate each framework. Some of the important attributes would include interface language (Python, C ++, Java, etc.) and the availability of libraries on various deep learning models such as CNN, RNN, DBN, and etc. And if a user implements a large scale deep learning model, it will also be important to support multiple GPU or multiple servers. Also, if you are learning the deep learning model, it would also be important if there are enough examples and references.deep learning framework, Theano, TensorFlow, CNTK, computational graph, CIFAR-10"
810,딥러닝을 이용한 주변 무선단말 파악방안,이웅섭 /Lee， Woong-sup,2018,한국정보통신학회,"최근 단말-대-단말(Device-to-device, D2D) 통신기술이 차세대 무선통신시스템의 핵심기술로 큰 관심을 받고 있다. 이러한 단말간 통신에서는 자신의 주변에 어떠한 단말이 있는지 파악하는 주변단말 탐색(Neighbor discovery)이매우 중요하다. 본 논문에서는 최근 큰 관심을 받고 있는 딥러닝(Deep learning) 기술을 활용하여 단말간 통신에서 주변단말을 파악하는 방안에 대해서 제안한다. 제안 방안은 기존의 방안과 달리 무선채널의 공간적 연관성을 이용하여단말간의 신호 전송 없이 단말이 기지국으로 전송하는 상향링크 파일럿 신호를 기반으로 주변 단말을 찾고 따라서기존의 방식에 비해 신호전송 복잡도(signaling complexity)를 크게 줄일 수 있다. 또한 제안 방안에서는 떨어져 있는거리에 따라서 주변 단말을 분류 가능하여 기존 방안에 비해서 좀 더 세밀한 단말 탐색이 가능하다. 마지막으로 본 논문에서는 tensorflow를 이용한 컴퓨터 시뮬레이션을 통해 제안 방안의 성능을 검증하였다.Recently, the device-to-device (D2D) communication has been conceived as the key technology for the next-generationmobile communication systems. The neighbor discovery in which the nearby users are found, is essential for the properoperation of the D2D communication. In this paper, we propose new neighbor discovery scheme based on deep learningtechnology which has gained a lot of attention recently. In the proposed scheme, the neighboring users can be foundusing the uplink pilot transmission of users only, unlike conventional neighbor discovery schemes in which direct pilotcommunication among users is required, such that the signaling overhead can be greatly reduced in our proposed scheme.
Moreover, the neighbors with different proximity can also be classified accordingly which enables more accurate neighbordiscovery compared to the conventional schemes. The performance of our proposed scheme is verified through thetensorflow-based computer simulations.
키워드 : 딥러닝, 주변단말 탐색, 단말-대-단말 통신Deep learning, Neighbor discovery, Device-to-device communication, Wireless channel correlation"
811,신뢰성있는 딥러닝 기반 분석 모델을 참조하기 위한 딥러닝 기술 언어,문종혁 /Jong Hyeok Mun,2021,한국정보처리학회,"최근 딥러닝은 하드웨어 성능이 향상됨에 따라 자연어 처리, 영상 인식 등의 다양한 기술에 접목되어 활용되고 있다. 이러한 기술들을 활용해 지능형교통 시스템(ITS), 스마트홈, 헬스케어 등의 산업분야에서 데이터를 분석하여 고속도로 속도위반 차량 검출, 에너지 사용량 제어, 응급상황 등과 같은고품질의 서비스를 제공하며, 고품질의 서비스를 제공하기 위해서는 정확도가 향상된 딥러닝 모델이 적용되어야 한다. 이를 위해 서비스 환경의 데이터를분석하기 위한 딥러닝 모델을 개발할 때, 개발자는 신뢰성이 검증된 최신의 딥러닝 모델을 적용할 수 있어야 한다. 이는 개발자가 참조하는 딥러닝 모델에적용된 학습 데이터셋의 정확도를 측정하여 검증할 수 있다. 이러한 검증을 위해서 개발자는 학습 데이터셋, 딥러닝의 계층구조 및 개발 환경 등과 같은내용을 포함하는 딥러닝 모델을 문서화하여 적용하기 위한 구조적인 정보가 필요하다. 본 논문에서는 신뢰성있는 딥러닝 기반 데이터 분석 모델을 참조하기위한 딥러닝 기술 언어를 제안한다. 제안하는 기술 언어는 신뢰성 있는 딥러닝 모델을 개발하는데 필요한 학습데이터셋, 개발 환경 및 설정 등의 정보와더불어 딥러닝 모델의 계층구조를 표현할 수 있다. 제안하는 딥러닝 기술 언어를 이용하여 개발자는 지능형 교통 시스템에서 참조하는 분석 모델의정확도를 검증할 수 있다. 실험에서는 제안하는 언어의 유효성을 검증하기 위해, 번호판 인식 모델을 중심으로 딥러닝 기술 문서의 적용과정을 보인다With the recent advancements of deep learning, companies such as smart home, healthcare, and intelligent transportation systems areutilizing its functionality to provide high-quality services for vehicle detection, emergency situation detection, and controlling energyconsumption. To provide reliable services in such sensitive systems, deep learning models are required to have high accuracy. In orderto develop a deep learning model for analyzing previously mentioned services, developers should utilize the state of the art deep learningmodels that have already been verified for higher accuracy. The developers can verify the accuracy of the referenced model by validatingthe model on the dataset. For this validation, the developer needs structural information to document and apply deep learning models,including metadata such as learning dataset, network architecture, and development environments. In this paper, we propose a descriptionlanguage that represents the network architecture of the deep learning model along with its metadata that are necessary to develop adeep learning model. Through the proposed description language, developers can easily verify the accuracy of the referenced deep learningmodel. Our experiments demonstrate the application scenario of a deep learning description document that focuses on the license platerecognition for the detection of illegally parked vehicles.Trusted Deep Learning, Model Reference, Deep Learning Description Language, Traffic Situation Analysis Model"
812,딥러닝 분석을 이용한 중국 역내·외 위안화 변동성 예측,이우식 /Lee Woosik,2016,한국데이터정보과학회,"2008년 글로벌 금융위기 이후 중국은 위안화 국제화의 점진적 추진을 진행하면서 중국상하이 외환시장과 중국홍콩 외환시장에서 거래되는 통화인 역내위안화와 역외위안화를 형성시켰다. 본 연구는 위안화 국제화와 점진적인 중국 자본계정 개방에 따라 급변하는 외환시장상황의 변동성을 정확하게 파악하기 위해서 GARCH모형 (일반화된 자기회귀 조건부이분산성모형)에 다단계인공신경망을 결합한 MLP-GARCH 모형과 GARCH모형과 기계학습의 일종인 딥러닝 (deep learning)을 통합한 DL-GARCH을 가지고 위안화 변동성예측을 비교 실험과 분석을 하였다. 비교분석 결과 DLGARCH 모형은 MLP-GARCH보다 모형 위안화 역내·외 환율변동성 예측 면에서 더욱 더 개선된예측값을 제공하였다. 그래서 이분산시계열모형을 딥러닝과 결합한 DL-GARCH 모형은 시계열의환율 변동성 예측 문제에 딥러닝을 응용할 수 있음을 확인하였다. 향후 이분산시계열과 결합된 딥러닝 모형은 다른 금융시계열 데이터에 응용하여 그 일반화 가능성을 높일 수 있을 것이다.The People’s Republic of China has vigorously been pursuing the internationalization of the Chinese Yuan or Renminbi after the financial crisis of 2008. In this view, an abrupt increase of use of the Chinese Yuan in the onshore and offshore markets are important milestones to be one of important currencies. One of the most frequently used methods to forecast volatility is GARCH model. Since a prediction error of the GARCH model has been reported quite high, a lot of efforts have been made to improve forecasting capability of the GARCH model. In this paper, we have proposed MLPGARCH and a DL-GARCH by employing Artificial Neural Network to the GARCH. In an application to forecasting Chinese Yuan volatility, we have successfully shown their overall outperformance in forecasting over the GARCH.Deep learning, DL-GARCH, GARCH, The Chinese Yuan, volatility."
813,딥러닝 기법을 이용한 내일강수 예측,하지훈,2016,한국지능시스템학회,"정확한 강수예측을 위해서는 예측인자 선정과 예측방법에 대한 선택이 매우 중요하다. 최근에는 강수예측 방법으로기계학습 기법이 많이 사용되고 있으며, 그 중에서도 특히 인공신경망을 사용한 강수예측 방법은 좋은 성능을 보였다. 본논문에서는 딥러닝 기법 중 하나인 DBN(deep belief network)를 이용한 새로운 강수예측 방법을 제안한다. DBN는 비지도사전 학습을 통해 초기 가중치를 설정하여 기존 인공신경망의 문제점을 보완한다. 예측인자로는 기온, 전일-전주 강수일, 태양과 달 궤도 관련 자료를 선정하였다. 기온과 전일-전주 강수일은 서울에서의 1974년부터 2013년까지 총 40년간의AWS(automatic weather system) 관측 자료를 사용하였고, 태양과 달의 궤도 관련 자료는 서울을 중심으로 계산한 결과를사용하였다. 전체 기간에서 일부는 학습 자료로 사용하여 예측모델을 생성하였고, 나머지를 생성한 모델의 검증 자료로사용하였다. 모델 검증 결과로 나온 예측값들은 확률값을 가지며 임계치를 이용하여 강수유무를 판별하였다. 강수 정확도의척도로 양분예보기법 중 CSI(critical successive index)와 Bias(frequency bias)를 계산하였다. 이를 통해 DBN와 MLP(multilayer perceptron)의 성능을 비교한 결과 DBN의 강수 예측 정확도가 높았고, 수행속도 또한 2배 이상 빨랐다.For accurate precipitation forecasts the choice of weather faoctrs and prediction method is very important. Recently, machinele arning has been widely used for forecasting precipitation, and artificial neural network, one of machine learning techniques, showed good performance. In this paper, we suggest a new method for forecasting precipitation using DBN, one of deep learning techniques.
DBN has an advantage that initial weights are set by unsupervised learning, so this compensates for the defects of artificial neural networks. We used past precipitation, temperature, and the parameters of the sun and moon's motion as features for forecasting precipitation. The dataset consists of observation data which had been measured for 40 years from AWS in Seoul. Experiments were based on 8-fold cross validation. As a result of estimation, we got probabilities of test dataset, so threshold was used for the decision of precipitation. CSI and Bias were used for indicating the precision of precipitation. Our experimental results showed that DBN performed better than MLP.Deep learning, Deep belief network, Precipitation, Forecast"
814,딥러닝 신경망모형을 이용한 실시간 교통정보수집 연구,김대현 /KIM DAE HYON,2016,사단법인 인문사회과학기술융합학회,"비디오 기반의 실시간 교통정보수집 시스템은 자율주행을 포함한 지능형교통체계의 주요 서브시스템중 하나이며, 우수한 예측(추정)력 때문에 핵심 알고리즘으로 인공신경망 모형이 주로 사용되고 있다. 특히, 최근 과학 학술지 네이처에 구글 딥마인드의 알파고가 공개되면서 딥러닝과 인공지능에 대한 전 세계적인 관심이 집중되고 있다. 최근까지 가장 널리 사용된 인공신경망모형은 역전파 모형이었으나 최근 컴퓨터 하드웨어 및 알고리즘의 새로운 개발과 더불어 딥러닝 알고리즘을 이용한 딥인공신경망 모형에 대한 관심이 높아지고 있다. 그러나 아직 딥러닝 인공신경망에 대한 연구는 초기단계에 있는 실정이며 적용분야도 극히 한정되어 있다. 본 연구에서는 실시간 도로교통정보 수집을 위한 영상처리분야에서 딥러닝 알고리즘을 활용하고 실험결과를 통하여 향후 교통정보수집‧분석 분야에서의 딥러닝 인공신경망 모형의 적용가능성을 살펴보고자 한다. 특히, 본 연구에서 도출한 딥러닝 인공신경망모형의 실험결과를 기존 역전파 모형에 의한 실험결과와 비교하여 모형의 우수성을 비교‧평가하고자 한다.Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Normally, artificial neural networks have been used in real-time applications for collecting traffic information because of their remarkable performance in terms of predictive accuracy and real-time computing. Especially, the recent publication of a paper in the journal Nature and the success of Google DeepMind’s world-beating Go program AlphaGo caused significant increases of the public interest in artificial intelligence. Even though various types of learning models have been proposed, the Backpropagation has been the most popular neural network model. However, the Deep learning neural network model has attracted much interest recently. In this study, the Deep leaning neural network will be applied in image processing techniques for the real-time traffic information collection to investigate the performance of Deep learning algorithm in traffic engineering fields. In addition, the experimental results from Deep learing algorithm will be compared with the results from Backpropagation model to evaluate and propose better performing model.Intelligent Transportation Systems (ITS), artificial neural networks, Deep learning, traffic information, image processing"
815,딥러닝의 모형과 응용사례,안성만 /SungMahn Ahn,2016,한국지능정보시스템학회,"딥러닝은 인공신경망(neural network)이라는 인공지능분야의 모형이 발전된 형태로서, 계층구조로 이루어진 인공신경망의 내부계층(hidden layer)이 여러 단계로 이루어진 구조이다. 딥러닝에서의 주요 모형은 합성곱신경망(convolutional neural network), 순환신경망(recurrent neural network), 그리고 심층신뢰신경망(deep belief network)의 세가지라고 할 수있다. 그 중에서 현재 흥미로운 연구가 많이 발표되어서 관심이 집중되고 있는 모형은 지도학습(supervised learning)모형인 처음 두 개의 모형이다. 따라서 본 논문에서는 지도학습모형의 가중치를 최적화하는 기본적인 방법인 오류역전파 알고리즘을 살펴본 뒤에 합성곱신경망과 순환신경망의 구조와 응용사례 등을 살펴보고자 한다. 본문에서 다루지 않은 모형인 심층신뢰신경망은 아직까지는 합성곱신경망 이나 순환신경망보다는 상대적으로 주목을 덜 받고 있다. 그러나 심층신뢰신경망은 CNN이나 RNN과는 달리 비지도학습(unsupervised learning)모형이며, 사람이나 동물은 관찰을 통해서 스스로 학습한다는 점에서 궁극적으로는 비지도학습모형이 더 많이 연구되어야 할 주제가 될 것이다.Deep learning model is a kind of neural networks that allows multiple hidden layers. There are various deep learning architectures such as convolutional neural networks, deep belief networks and recurrent neural networks. Those have been applied to fields like computer vision, automatic speech recognition, natural language processing, audio recognition and bioinformatics where they have been shown to produce state-of-the-art results on various tasks. Among those architectures, convolutional neural networks and recurrent neural networks are classified as the supervised learning model. And in recent years, those supervised learning models have gained more popularity than unsupervised learning models such as deep belief networks, because supervised learning models have shown fashionable applications in such fields mentioned above.
Deep learning models can be trained with backpropagation algorithm. Backpropagation is an abbreviation for “backward propagation of errors” and a common method of training artificial neural networks used in conjunction with an optimization method such as gradient descent. The method calculates the gradient of an error function with respect to all the weights in the network. The gradient is fed to the optimization method which in turn uses it to update the weights, in an attempt to minimize the error function.
Convolutional neural networks use a special architecture which is particularly well-adapted to classify images. Using this architecture makes convolutional networks fast to train. This, in turn, helps us train deep, muti-layer networks, which are very good at classifying images. These days, deep convolutional networks are used in most neural networks for image recognition. Convolutional neural networks use three basic ideas: local receptive fields, shared weights, and pooling. By local receptive fields, we mean that each neuron in the first(or any) hidden layer will be connected to a small region of the input(or previous layer’s) neurons. Shared weights mean that we’re going to use the same weights and bias for each of the local receptive field. This means that all the neurons in the hidden layer detect exactly the same feature, just at different locations in the input image. In addition to the convolutional layers just described, convolutional neural networks also contain pooling layers. Pooling layers are usually used immediately after convolutional layers. What the pooling layers do is to simplify the information in the output from the convolutional layer.
Recent convolutional network architectures have 10 to 20 hidden layers and billions of connections between units. Training deep learning networks has taken weeks several years ago, but thanks to progress in GPU and algorithm enhancement, training time has reduced to several hours.
Neural networks with time-varying behavior are known as recurrent neural networks or RNNs. A recurrent neural network is a class of artificial neural network where connections between units form a directed cycle. This creates an internal state of the network which allows it to exhibit dynamic temporal behavior. Unlike feedforward neural networks, RNNs can use their internal memory to process arbitrary sequences of inputs. Early RNN models turned out to be very difficult to train, harder even than deep feedforward networks. The reason is the unstable gradient problem such as vanishing gradient and exploding gradient. The gradient can get smaller and smaller as it is propagated back through layers. This makes learning in early layers extremely slow. The problem actually gets worse in RNNs, since gradients aren’t just propagated backward through layers, they’re propagated backward through time. If the network runs for a long time, that can make the gradient extremely unstable and hard to learn from. It has been possible to incorporate an idea known as long short-term memory units (LSTMs) into RNNs. LSTMs make it much easier to get good results when training RNNs, and many recent papers make use of LSTMs or related ideas.Deep learning, Convolutional neural networks, Recurrent neural networks, Error backpropagation algorithm"
816,딥러닝분석과 기술적 분석 지표를 이용한 한국 코스피주가지수 방향성 예측,이우식 /Lee Woosik,2017,한국데이터정보과학회,"2016년 3월 구글 (Google)의 바둑인공지능 알파고 (AlphaGo)가 이세돌 9단과의 바둑대결에서승리한 이후 다양한 분야에서 인공지능 사용에 대한 관심이 높아지고 있는 가운데 금융투자 분야에서도 인공지능과 투자자문 전문가의 합성어인 로보어드바이저 (Robo-Advisor)에 대한 관심이 높아지고 있다. 인공지능 (artificial intelligence)기반의 의사결정은 비용 절감은 물론 효과적인 의사결정을가능하게 한다는 점에서 큰 장점이 있다. 본 연구에서는 기술적 분석 (technical analysis) 지표와 딥러닝 (deep learning) 모형을 결합하여 한국 코스피 지수를 예측하는 모형을 개발하고 제시한 모형들의 예측력을 비교, 분석한다. 분석 결과 기술적 분석 지표에 딥러닝 알고리즘을 결합한 모형이 주가지수 방향성 예측 문제에 응용될 수 있음을 확인하였다. 향후 본 연구에서 제안된 기술적 분석 지표와딥러닝모형을 결합한 기법은 로보어드바이저서비스에 응용할 수 있는 일반화 가능성을 보여준다Since Google’s AlphaGo defeated a world champion of Go players in 2016, there have been many interests in the deep learning. In the financial sector, a Robo-Advisor using deep learning gains a significant attention, which builds and manages portfolios of financial instruments for investors.In this paper, we have proposed the a deep learning algorithm geared toward identification and forecast of the KOSPI index direction,and we also have compared the accuracy of the prediction.In an application of forecasting the financial market index direction, we have shown that the Robo-Advisor using deep learning has a significant effect on finance industry. The Robo-Advisor collects a massive data such as earnings statements, news reports and regulatory filings, analyzes those and recommends investors how to view market trends and identify the best time to purchase financial assets. On the other hand, the Robo-Advisor allows businesses to learn more about their customers, develop better marketing strategies, increase sales and decrease costs.Artificial intelligence, deep learning, FinTech, Robo-Advisor, technical analysis."
817,딥러닝의 효율적인 학습을 위한 학습데이터 선별 기법에 관한 연구,주경돈 /Gyoung-don Joo,2017,한국정보과학회,"빅데이터의 증가와 병렬 하드웨어 처리, 학습 알고리즘의 개선은 딥러닝의 발전을 가져왔으며 인공지능이 적용 가능한 영역을 획기적으로 확장시키고 있다. 하지만 이러한 대규모의 학습 데이터에 기반한 딥러닝은 학습시간이 증가되고, 더 나아가 학습 데이터의 구축을 위한 레이블링(labeling) 비용이 데이터 수에 비례하여 증가하게 된다. 따라서 본 논문에서는, 딥러닝의 정확도를 최대한 유지하면서 필요한 학습 데이터의 수를 줄일수 있도록, 전체 데이터에서 제한된 수의 학습 데이터의 후보를 선별하는 기법에 대하여 연구한다. 특히 기존의 능동 학습에 기반한 기법들이 학습 데이터의 수가 매우 적어질 경우 딥러닝의 정확도가 급격히 떨어지는 문제를 해결할 수 있는 새로운 기법을 제안하였으며, 제안된 기법으로 선별된 학습 데이터는 그 수의 변화에 대해 딥러닝이 일관되게 경쟁력있는 정확도를 가짐을 다양한 실험을 통하여 검증하였다.The increase of big data, parallel hardware processing and enhanced learning algorithms have led to the development of deep learning and are dramatically expanding the domain of applications based on artificial intelligence. However, such large-scale training data extends the learning time, and furthermore, the labeling cost for building the training set increases in proportion to the volume of data. Therefore, in this paper, we study how to select a limited number of candidates for training set from given data so as to reduce the number of training set while maintaining the accuracy of deep learning. Especially, we found that, with the training set selected by the previous work, the accuracy of deep learning drastically decreases when the number of training set is very small and so proposed new methods to resolve this fall-off in accuracy. Through experiments, we showed that deep learning consistently has the competitive accuracy with the training set selected by the proposed method regardless of the size of training set.Deep Learning, Train Set Selection, Active Learning, Gamma distribution"
818,딥러닝을 이용한 범용적 스테그아날리시스,김현재 /Hyunjae Kim,2017,한국정보과학회,"스테그아날리시스(Steganalysis)란 이미지 등 일반적인 자료에 암호화된 정보를 은닉하는 스테가노그래피(Steganography)에 대한 검출 및 분석 방법으로, 기계학습 기반 방법론을 포함한다. 기존 기계학습 기반 스테그아날리시스는 영상(Image)의 특징(Feature) 추출 및 모델링에 기반하며, 최근 딥러닝(Deep Learning)의 적용으로 검출 정확도가 큰 폭으로 향상되었다. 하지만 현존하는 스테그아날리시스 모델은 단일 스테가노그래피 기법에 대해 국한되어 있어 학습에 사용되지 않은 스테고(Stego) 이미지의 경우 검출이 불가능한 결정적 한계를 가진다. 본 연구에서는 다양한 스테가노그래피 기법으로 생성된 스테고이미지에 딥러닝을 적용하여 스테그아날리시스를 학습하는 범용적 모델을 제안한다. 다양한 실험을 통해 제안 기법의 효용성 및 가능성을 확인하고, 범용적 스테그아날리시스 모델이 각각에 특화된 검출 기법과 유사한 정확도로 스테고 이미지를 검출할 수 있음을 보인다.Steganalysis is to detect information hidden by steganography inside general data such as images. There are stegoanalysis techniques that use machine learning (ML). Existing ML approaches to steganalysis are based on extracting features from stego images and modeling them.
Recently deep learning-based methodologies have shown significant improvements in detection accuracy. However, all the existing methods, including deep learning-based ones, have a critical limitation in that they can only detect stego images that are created by a specific steganography method. In this paper, we propose a generalized steganalysis method that can model multiple types of stego images using deep learning. Through various experiments, we confirm the effectiveness of our approach and envision directions for future research. In particular, we show that our method can detect each type of steganography with the same level of accuracy as that of a steganalysis method dedicated to that type of steganography, thereby demonstrating the general applicability of our approach to multiple types of stego images.steganalysis, steganography, deep learning, generalized steganalysis model"
819,"딥러닝 프레임워크의 비교: 티아노, 텐서플로, CNTK를 중심으로",정여진 /Yeojin Chung,2017,한국지능정보시스템학회,"딥러닝 프레임워크의 대표적인 기능으로는 ‘자동미분’과 ‘GPU의 활용’ 등을 들 수 있다. 본 논문은 파이썬의 라이브러리 형태로 사용 가능한 프레임워크 중에서 구글의 텐서플로와 마이크로소프트의 CNTK, 그리고 텐서 플로의 원조라고 할 수 있는 티아노를 비교하였다. 본문에서는 자동미분의 개념과 GPU의 활용형태를 간단히 설명하고, 그 다음에 logistic regression을 실행하는 예를 통하여 각 프레임워크의 문법을 알아본 뒤에, 마지막으로 대표적인 딥러닝 응용인 CNN의 예제를 실행시켜보고 코딩의 편의성과 실행속도 등을 확인해 보았다. 그 결과, 편의성의 관점에서 보면 티아노가 가장 코딩 하기가 어렵고, CNTK와 텐서플로는 많은 부분이 비슷하게 추상화 되어 있어서 코딩이 비슷하지만 가중치와 편향을 직접 정의하느냐의 여부에서 차이를 보였다. 그리고 각프레임워크의 실행속도에 대한 평가는 ‘큰 차이는 없다’는 것이다. 텐서플로는 티아노에 비하여 속도가 느리다는 평가가 있어왔는데, 본 연구의 실험에 의하면, 비록 CNN 모형에 국한되었지만, 텐서플로가 아주 조금이지만빠른 것으로 나타났다. CNTK의 경우에도, 비록 실험환경이 달랐지만, 실험환경의 차이에 의한 속도의 차이의 편차범위 이내에 있는 것으로 판단이 되었다. 본 연구에서는 세 종류의 딥러닝 프레임워크만을 살펴보았는데,위키피디아에 따르면 딥러닝 프레임워크의 종류는 12가지가 있으며, 각 프레임워크의 특징을 15가지 속성으로구분하여 차이를 특정하고 있다. 그 많은 속성 중에서 사용자의 입장에서 볼 때 중요한 속성은 어떤 언어(파이썬, C++, Java, 등)로 사용가능한지, 어떤 딥러닝 모형에 대한 라이브러리가 잘 구현되어 있는지 등일 것이다. 그리고 사용자가 대규모의 딥러닝 모형을 구축한다면, 다중 GPU 혹은 다중 서버를 지원하는지의 여부도 중요할 것이다. 또한 딥러닝 모형을 처음 학습하는 경우에는 사용설명서가 많은지 예제 프로그램이 많은지 여부도 중요한 기준이 될 것이다.The deep learning framework is software designed to help develop deep learning models. Some of its important functions include “automatic differentiation” and “utilization of GPU”. The list of popular deep learning framework includes Caffe (BVLC) and Theano (University of Montreal). And recently, Microsoft's deep learning framework, Microsoft Cognitive Toolkit, was released as open-source license, following Google’s Tensorflow a year earlier. The early deep learning frameworks have been developed mainly for research at universities. Beginning with the inception of Tensorflow, however, it seems that companies such as Microsoft and Facebook have started to join the competition of framework development. Given the trend, Google and other companies are expected to continue investing in the deep learning framework to bring forward the initiative in the artificial intelligence business. From this point of view, we think it is a good time to compare some of deep learning frameworks. So we compare three deep learning frameworks which can be used as a Python library. Those are Google's Tensorflow, Microsoft’s CNTK, and Theano which is sort of a predecessor of the preceding two.
The most common and important function of deep learning frameworks is the ability to perform automatic differentiation. Basically all the mathematical expressions of deep learning models can be represented as computational graphs, which consist of nodes and edges. Partial derivatives on each edge of a computational graph can then be obtained. With the partial derivatives, we can let software compute differentiation of any node with respect to any variable by utilizing chain rule of Calculus.
First of all, the convenience of coding is in the order of CNTK, Tensorflow, and Theano. The criterion is simply based on the lengths of the codes and the learning curve and the ease of coding are not the main concern. According to the criteria, Theano was the most difficult to implement with, and CNTK and Tensorflow were somewhat easier. With Tensorflow, we need to define weight variables and biases explicitly. The reason that CNTK and Tensorflow are easier to implement with is that those frameworks provide us with more abstraction than Theano. We, however, need to mention that low-level coding is not always bad. It gives us flexibility of coding. With the low-level coding such as in Theano, we can implement and test any new deep learning models or any new search methods that we can think of.
The assessment of the execution speed of each framework is that there is not meaningful difference. According to the experiment, execution speeds of Theano and Tensorflow are very similar, although the experiment was limited to a CNN model. In the case of CNTK, the experimental environment was not maintained as the same. The code written in CNTK has to be run in PC environment without GPU where codes execute as much as 50 times slower than with GPU. But we concluded that the difference of execution speed was within the range of variation caused by the different hardware setup.
In this study, we compared three types of deep learning framework: Theano, Tensorflow, and CNTK. According to Wikipedia, there are 12 available deep learning frameworks. And 15 different attributes differentiate each framework. Some of the important attributes would include interface language (Python, C ++, Java, etc.) and the availability of libraries on various deep learning models such as CNN, RNN, DBN, and etc. And if a user implements a large scale deep learning model, it will also be important to support multiple GPU or multiple servers. Also, if you are learning the deep learning model, it would also be important if there are enough examples and references.deep learning framework, Theano, TensorFlow, CNTK, computational graph, CIFAR-10"
820,딥러닝을 이용한 주변 무선단말 파악방안,이웅섭 /Lee， Woong-sup,2018,한국정보통신학회,"최근 단말-대-단말(Device-to-device, D2D) 통신기술이 차세대 무선통신시스템의 핵심기술로 큰 관심을 받고 있다. 이러한 단말간 통신에서는 자신의 주변에 어떠한 단말이 있는지 파악하는 주변단말 탐색(Neighbor discovery)이매우 중요하다. 본 논문에서는 최근 큰 관심을 받고 있는 딥러닝(Deep learning) 기술을 활용하여 단말간 통신에서 주변단말을 파악하는 방안에 대해서 제안한다. 제안 방안은 기존의 방안과 달리 무선채널의 공간적 연관성을 이용하여단말간의 신호 전송 없이 단말이 기지국으로 전송하는 상향링크 파일럿 신호를 기반으로 주변 단말을 찾고 따라서기존의 방식에 비해 신호전송 복잡도(signaling complexity)를 크게 줄일 수 있다. 또한 제안 방안에서는 떨어져 있는거리에 따라서 주변 단말을 분류 가능하여 기존 방안에 비해서 좀 더 세밀한 단말 탐색이 가능하다. 마지막으로 본 논문에서는 tensorflow를 이용한 컴퓨터 시뮬레이션을 통해 제안 방안의 성능을 검증하였다.Recently, the device-to-device (D2D) communication has been conceived as the key technology for the next-generationmobile communication systems. The neighbor discovery in which the nearby users are found, is essential for the properoperation of the D2D communication. In this paper, we propose new neighbor discovery scheme based on deep learningtechnology which has gained a lot of attention recently. In the proposed scheme, the neighboring users can be foundusing the uplink pilot transmission of users only, unlike conventional neighbor discovery schemes in which direct pilotcommunication among users is required, such that the signaling overhead can be greatly reduced in our proposed scheme.
Moreover, the neighbors with different proximity can also be classified accordingly which enables more accurate neighbordiscovery compared to the conventional schemes. The performance of our proposed scheme is verified through thetensorflow-based computer simulations.
키워드 : 딥러닝, 주변단말 탐색, 단말-대-단말 통신Deep learning, Neighbor discovery, Device-to-device communication, Wireless channel correlation"
821,신뢰성있는 딥러닝 기반 분석 모델을 참조하기 위한 딥러닝 기술 언어,문종혁 /Jong Hyeok Mun,2021,한국정보처리학회,"최근 딥러닝은 하드웨어 성능이 향상됨에 따라 자연어 처리, 영상 인식 등의 다양한 기술에 접목되어 활용되고 있다. 이러한 기술들을 활용해 지능형교통 시스템(ITS), 스마트홈, 헬스케어 등의 산업분야에서 데이터를 분석하여 고속도로 속도위반 차량 검출, 에너지 사용량 제어, 응급상황 등과 같은고품질의 서비스를 제공하며, 고품질의 서비스를 제공하기 위해서는 정확도가 향상된 딥러닝 모델이 적용되어야 한다. 이를 위해 서비스 환경의 데이터를분석하기 위한 딥러닝 모델을 개발할 때, 개발자는 신뢰성이 검증된 최신의 딥러닝 모델을 적용할 수 있어야 한다. 이는 개발자가 참조하는 딥러닝 모델에적용된 학습 데이터셋의 정확도를 측정하여 검증할 수 있다. 이러한 검증을 위해서 개발자는 학습 데이터셋, 딥러닝의 계층구조 및 개발 환경 등과 같은내용을 포함하는 딥러닝 모델을 문서화하여 적용하기 위한 구조적인 정보가 필요하다. 본 논문에서는 신뢰성있는 딥러닝 기반 데이터 분석 모델을 참조하기위한 딥러닝 기술 언어를 제안한다. 제안하는 기술 언어는 신뢰성 있는 딥러닝 모델을 개발하는데 필요한 학습데이터셋, 개발 환경 및 설정 등의 정보와더불어 딥러닝 모델의 계층구조를 표현할 수 있다. 제안하는 딥러닝 기술 언어를 이용하여 개발자는 지능형 교통 시스템에서 참조하는 분석 모델의정확도를 검증할 수 있다. 실험에서는 제안하는 언어의 유효성을 검증하기 위해, 번호판 인식 모델을 중심으로 딥러닝 기술 문서의 적용과정을 보인다With the recent advancements of deep learning, companies such as smart home, healthcare, and intelligent transportation systems areutilizing its functionality to provide high-quality services for vehicle detection, emergency situation detection, and controlling energyconsumption. To provide reliable services in such sensitive systems, deep learning models are required to have high accuracy. In orderto develop a deep learning model for analyzing previously mentioned services, developers should utilize the state of the art deep learningmodels that have already been verified for higher accuracy. The developers can verify the accuracy of the referenced model by validatingthe model on the dataset. For this validation, the developer needs structural information to document and apply deep learning models,including metadata such as learning dataset, network architecture, and development environments. In this paper, we propose a descriptionlanguage that represents the network architecture of the deep learning model along with its metadata that are necessary to develop adeep learning model. Through the proposed description language, developers can easily verify the accuracy of the referenced deep learningmodel. Our experiments demonstrate the application scenario of a deep learning description document that focuses on the license platerecognition for the detection of illegally parked vehicles.Trusted Deep Learning, Model Reference, Deep Learning Description Language, Traffic Situation Analysis Model"
822,딥러닝 분석을 이용한 중국 역내·외 위안화 변동성 예측,이우식 /Lee Woosik,2016,한국데이터정보과학회,"2008년 글로벌 금융위기 이후 중국은 위안화 국제화의 점진적 추진을 진행하면서 중국상하이 외환시장과 중국홍콩 외환시장에서 거래되는 통화인 역내위안화와 역외위안화를 형성시켰다. 본 연구는 위안화 국제화와 점진적인 중국 자본계정 개방에 따라 급변하는 외환시장상황의 변동성을 정확하게 파악하기 위해서 GARCH모형 (일반화된 자기회귀 조건부이분산성모형)에 다단계인공신경망을 결합한 MLP-GARCH 모형과 GARCH모형과 기계학습의 일종인 딥러닝 (deep learning)을 통합한 DL-GARCH을 가지고 위안화 변동성예측을 비교 실험과 분석을 하였다. 비교분석 결과 DLGARCH 모형은 MLP-GARCH보다 모형 위안화 역내·외 환율변동성 예측 면에서 더욱 더 개선된예측값을 제공하였다. 그래서 이분산시계열모형을 딥러닝과 결합한 DL-GARCH 모형은 시계열의환율 변동성 예측 문제에 딥러닝을 응용할 수 있음을 확인하였다. 향후 이분산시계열과 결합된 딥러닝 모형은 다른 금융시계열 데이터에 응용하여 그 일반화 가능성을 높일 수 있을 것이다.The People’s Republic of China has vigorously been pursuing the internationalization of the Chinese Yuan or Renminbi after the financial crisis of 2008. In this view, an abrupt increase of use of the Chinese Yuan in the onshore and offshore markets are important milestones to be one of important currencies. One of the most frequently used methods to forecast volatility is GARCH model. Since a prediction error of the GARCH model has been reported quite high, a lot of efforts have been made to improve forecasting capability of the GARCH model. In this paper, we have proposed MLPGARCH and a DL-GARCH by employing Artificial Neural Network to the GARCH. In an application to forecasting Chinese Yuan volatility, we have successfully shown their overall outperformance in forecasting over the GARCH.Deep learning, DL-GARCH, GARCH, The Chinese Yuan, volatility."
823,딥러닝 기법을 이용한 내일강수 예측,하지훈,2016,한국지능시스템학회,"정확한 강수예측을 위해서는 예측인자 선정과 예측방법에 대한 선택이 매우 중요하다. 최근에는 강수예측 방법으로기계학습 기법이 많이 사용되고 있으며, 그 중에서도 특히 인공신경망을 사용한 강수예측 방법은 좋은 성능을 보였다. 본논문에서는 딥러닝 기법 중 하나인 DBN(deep belief network)를 이용한 새로운 강수예측 방법을 제안한다. DBN는 비지도사전 학습을 통해 초기 가중치를 설정하여 기존 인공신경망의 문제점을 보완한다. 예측인자로는 기온, 전일-전주 강수일, 태양과 달 궤도 관련 자료를 선정하였다. 기온과 전일-전주 강수일은 서울에서의 1974년부터 2013년까지 총 40년간의AWS(automatic weather system) 관측 자료를 사용하였고, 태양과 달의 궤도 관련 자료는 서울을 중심으로 계산한 결과를사용하였다. 전체 기간에서 일부는 학습 자료로 사용하여 예측모델을 생성하였고, 나머지를 생성한 모델의 검증 자료로사용하였다. 모델 검증 결과로 나온 예측값들은 확률값을 가지며 임계치를 이용하여 강수유무를 판별하였다. 강수 정확도의척도로 양분예보기법 중 CSI(critical successive index)와 Bias(frequency bias)를 계산하였다. 이를 통해 DBN와 MLP(multilayer perceptron)의 성능을 비교한 결과 DBN의 강수 예측 정확도가 높았고, 수행속도 또한 2배 이상 빨랐다.For accurate precipitation forecasts the choice of weather faoctrs and prediction method is very important. Recently, machinele arning has been widely used for forecasting precipitation, and artificial neural network, one of machine learning techniques, showed good performance. In this paper, we suggest a new method for forecasting precipitation using DBN, one of deep learning techniques.
DBN has an advantage that initial weights are set by unsupervised learning, so this compensates for the defects of artificial neural networks. We used past precipitation, temperature, and the parameters of the sun and moon's motion as features for forecasting precipitation. The dataset consists of observation data which had been measured for 40 years from AWS in Seoul. Experiments were based on 8-fold cross validation. As a result of estimation, we got probabilities of test dataset, so threshold was used for the decision of precipitation. CSI and Bias were used for indicating the precision of precipitation. Our experimental results showed that DBN performed better than MLP.Deep learning, Deep belief network, Precipitation, Forecast"
824,딥러닝 신경망모형을 이용한 실시간 교통정보수집 연구,김대현 /KIM DAE HYON,2016,사단법인 인문사회과학기술융합학회,"비디오 기반의 실시간 교통정보수집 시스템은 자율주행을 포함한 지능형교통체계의 주요 서브시스템중 하나이며, 우수한 예측(추정)력 때문에 핵심 알고리즘으로 인공신경망 모형이 주로 사용되고 있다. 특히, 최근 과학 학술지 네이처에 구글 딥마인드의 알파고가 공개되면서 딥러닝과 인공지능에 대한 전 세계적인 관심이 집중되고 있다. 최근까지 가장 널리 사용된 인공신경망모형은 역전파 모형이었으나 최근 컴퓨터 하드웨어 및 알고리즘의 새로운 개발과 더불어 딥러닝 알고리즘을 이용한 딥인공신경망 모형에 대한 관심이 높아지고 있다. 그러나 아직 딥러닝 인공신경망에 대한 연구는 초기단계에 있는 실정이며 적용분야도 극히 한정되어 있다. 본 연구에서는 실시간 도로교통정보 수집을 위한 영상처리분야에서 딥러닝 알고리즘을 활용하고 실험결과를 통하여 향후 교통정보수집‧분석 분야에서의 딥러닝 인공신경망 모형의 적용가능성을 살펴보고자 한다. 특히, 본 연구에서 도출한 딥러닝 인공신경망모형의 실험결과를 기존 역전파 모형에 의한 실험결과와 비교하여 모형의 우수성을 비교‧평가하고자 한다.Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Normally, artificial neural networks have been used in real-time applications for collecting traffic information because of their remarkable performance in terms of predictive accuracy and real-time computing. Especially, the recent publication of a paper in the journal Nature and the success of Google DeepMind’s world-beating Go program AlphaGo caused significant increases of the public interest in artificial intelligence. Even though various types of learning models have been proposed, the Backpropagation has been the most popular neural network model. However, the Deep learning neural network model has attracted much interest recently. In this study, the Deep leaning neural network will be applied in image processing techniques for the real-time traffic information collection to investigate the performance of Deep learning algorithm in traffic engineering fields. In addition, the experimental results from Deep learing algorithm will be compared with the results from Backpropagation model to evaluate and propose better performing model.Intelligent Transportation Systems (ITS), artificial neural networks, Deep learning, traffic information, image processing"
825,딥러닝의 모형과 응용사례,안성만 /SungMahn Ahn,2016,한국지능정보시스템학회,"딥러닝은 인공신경망(neural network)이라는 인공지능분야의 모형이 발전된 형태로서, 계층구조로 이루어진 인공신경망의 내부계층(hidden layer)이 여러 단계로 이루어진 구조이다. 딥러닝에서의 주요 모형은 합성곱신경망(convolutional neural network), 순환신경망(recurrent neural network), 그리고 심층신뢰신경망(deep belief network)의 세가지라고 할 수있다. 그 중에서 현재 흥미로운 연구가 많이 발표되어서 관심이 집중되고 있는 모형은 지도학습(supervised learning)모형인 처음 두 개의 모형이다. 따라서 본 논문에서는 지도학습모형의 가중치를 최적화하는 기본적인 방법인 오류역전파 알고리즘을 살펴본 뒤에 합성곱신경망과 순환신경망의 구조와 응용사례 등을 살펴보고자 한다. 본문에서 다루지 않은 모형인 심층신뢰신경망은 아직까지는 합성곱신경망 이나 순환신경망보다는 상대적으로 주목을 덜 받고 있다. 그러나 심층신뢰신경망은 CNN이나 RNN과는 달리 비지도학습(unsupervised learning)모형이며, 사람이나 동물은 관찰을 통해서 스스로 학습한다는 점에서 궁극적으로는 비지도학습모형이 더 많이 연구되어야 할 주제가 될 것이다.Deep learning model is a kind of neural networks that allows multiple hidden layers. There are various deep learning architectures such as convolutional neural networks, deep belief networks and recurrent neural networks. Those have been applied to fields like computer vision, automatic speech recognition, natural language processing, audio recognition and bioinformatics where they have been shown to produce state-of-the-art results on various tasks. Among those architectures, convolutional neural networks and recurrent neural networks are classified as the supervised learning model. And in recent years, those supervised learning models have gained more popularity than unsupervised learning models such as deep belief networks, because supervised learning models have shown fashionable applications in such fields mentioned above.
Deep learning models can be trained with backpropagation algorithm. Backpropagation is an abbreviation for “backward propagation of errors” and a common method of training artificial neural networks used in conjunction with an optimization method such as gradient descent. The method calculates the gradient of an error function with respect to all the weights in the network. The gradient is fed to the optimization method which in turn uses it to update the weights, in an attempt to minimize the error function.
Convolutional neural networks use a special architecture which is particularly well-adapted to classify images. Using this architecture makes convolutional networks fast to train. This, in turn, helps us train deep, muti-layer networks, which are very good at classifying images. These days, deep convolutional networks are used in most neural networks for image recognition. Convolutional neural networks use three basic ideas: local receptive fields, shared weights, and pooling. By local receptive fields, we mean that each neuron in the first(or any) hidden layer will be connected to a small region of the input(or previous layer’s) neurons. Shared weights mean that we’re going to use the same weights and bias for each of the local receptive field. This means that all the neurons in the hidden layer detect exactly the same feature, just at different locations in the input image. In addition to the convolutional layers just described, convolutional neural networks also contain pooling layers. Pooling layers are usually used immediately after convolutional layers. What the pooling layers do is to simplify the information in the output from the convolutional layer.
Recent convolutional network architectures have 10 to 20 hidden layers and billions of connections between units. Training deep learning networks has taken weeks several years ago, but thanks to progress in GPU and algorithm enhancement, training time has reduced to several hours.
Neural networks with time-varying behavior are known as recurrent neural networks or RNNs. A recurrent neural network is a class of artificial neural network where connections between units form a directed cycle. This creates an internal state of the network which allows it to exhibit dynamic temporal behavior. Unlike feedforward neural networks, RNNs can use their internal memory to process arbitrary sequences of inputs. Early RNN models turned out to be very difficult to train, harder even than deep feedforward networks. The reason is the unstable gradient problem such as vanishing gradient and exploding gradient. The gradient can get smaller and smaller as it is propagated back through layers. This makes learning in early layers extremely slow. The problem actually gets worse in RNNs, since gradients aren’t just propagated backward through layers, they’re propagated backward through time. If the network runs for a long time, that can make the gradient extremely unstable and hard to learn from. It has been possible to incorporate an idea known as long short-term memory units (LSTMs) into RNNs. LSTMs make it much easier to get good results when training RNNs, and many recent papers make use of LSTMs or related ideas.Deep learning, Convolutional neural networks, Recurrent neural networks, Error backpropagation algorithm"
826,딥러닝분석과 기술적 분석 지표를 이용한 한국 코스피주가지수 방향성 예측,이우식 /Lee Woosik,2017,한국데이터정보과학회,"2016년 3월 구글 (Google)의 바둑인공지능 알파고 (AlphaGo)가 이세돌 9단과의 바둑대결에서승리한 이후 다양한 분야에서 인공지능 사용에 대한 관심이 높아지고 있는 가운데 금융투자 분야에서도 인공지능과 투자자문 전문가의 합성어인 로보어드바이저 (Robo-Advisor)에 대한 관심이 높아지고 있다. 인공지능 (artificial intelligence)기반의 의사결정은 비용 절감은 물론 효과적인 의사결정을가능하게 한다는 점에서 큰 장점이 있다. 본 연구에서는 기술적 분석 (technical analysis) 지표와 딥러닝 (deep learning) 모형을 결합하여 한국 코스피 지수를 예측하는 모형을 개발하고 제시한 모형들의 예측력을 비교, 분석한다. 분석 결과 기술적 분석 지표에 딥러닝 알고리즘을 결합한 모형이 주가지수 방향성 예측 문제에 응용될 수 있음을 확인하였다. 향후 본 연구에서 제안된 기술적 분석 지표와딥러닝모형을 결합한 기법은 로보어드바이저서비스에 응용할 수 있는 일반화 가능성을 보여준다Since Google’s AlphaGo defeated a world champion of Go players in 2016, there have been many interests in the deep learning. In the financial sector, a Robo-Advisor using deep learning gains a significant attention, which builds and manages portfolios of financial instruments for investors.In this paper, we have proposed the a deep learning algorithm geared toward identification and forecast of the KOSPI index direction,and we also have compared the accuracy of the prediction.In an application of forecasting the financial market index direction, we have shown that the Robo-Advisor using deep learning has a significant effect on finance industry. The Robo-Advisor collects a massive data such as earnings statements, news reports and regulatory filings, analyzes those and recommends investors how to view market trends and identify the best time to purchase financial assets. On the other hand, the Robo-Advisor allows businesses to learn more about their customers, develop better marketing strategies, increase sales and decrease costs.Artificial intelligence, deep learning, FinTech, Robo-Advisor, technical analysis."
827,딥러닝의 효율적인 학습을 위한 학습데이터 선별 기법에 관한 연구,주경돈 /Gyoung-don Joo,2017,한국정보과학회,"빅데이터의 증가와 병렬 하드웨어 처리, 학습 알고리즘의 개선은 딥러닝의 발전을 가져왔으며 인공지능이 적용 가능한 영역을 획기적으로 확장시키고 있다. 하지만 이러한 대규모의 학습 데이터에 기반한 딥러닝은 학습시간이 증가되고, 더 나아가 학습 데이터의 구축을 위한 레이블링(labeling) 비용이 데이터 수에 비례하여 증가하게 된다. 따라서 본 논문에서는, 딥러닝의 정확도를 최대한 유지하면서 필요한 학습 데이터의 수를 줄일수 있도록, 전체 데이터에서 제한된 수의 학습 데이터의 후보를 선별하는 기법에 대하여 연구한다. 특히 기존의 능동 학습에 기반한 기법들이 학습 데이터의 수가 매우 적어질 경우 딥러닝의 정확도가 급격히 떨어지는 문제를 해결할 수 있는 새로운 기법을 제안하였으며, 제안된 기법으로 선별된 학습 데이터는 그 수의 변화에 대해 딥러닝이 일관되게 경쟁력있는 정확도를 가짐을 다양한 실험을 통하여 검증하였다.The increase of big data, parallel hardware processing and enhanced learning algorithms have led to the development of deep learning and are dramatically expanding the domain of applications based on artificial intelligence. However, such large-scale training data extends the learning time, and furthermore, the labeling cost for building the training set increases in proportion to the volume of data. Therefore, in this paper, we study how to select a limited number of candidates for training set from given data so as to reduce the number of training set while maintaining the accuracy of deep learning. Especially, we found that, with the training set selected by the previous work, the accuracy of deep learning drastically decreases when the number of training set is very small and so proposed new methods to resolve this fall-off in accuracy. Through experiments, we showed that deep learning consistently has the competitive accuracy with the training set selected by the proposed method regardless of the size of training set.Deep Learning, Train Set Selection, Active Learning, Gamma distribution"
828,딥러닝을 이용한 범용적 스테그아날리시스,김현재 /Hyunjae Kim,2017,한국정보과학회,"스테그아날리시스(Steganalysis)란 이미지 등 일반적인 자료에 암호화된 정보를 은닉하는 스테가노그래피(Steganography)에 대한 검출 및 분석 방법으로, 기계학습 기반 방법론을 포함한다. 기존 기계학습 기반 스테그아날리시스는 영상(Image)의 특징(Feature) 추출 및 모델링에 기반하며, 최근 딥러닝(Deep Learning)의 적용으로 검출 정확도가 큰 폭으로 향상되었다. 하지만 현존하는 스테그아날리시스 모델은 단일 스테가노그래피 기법에 대해 국한되어 있어 학습에 사용되지 않은 스테고(Stego) 이미지의 경우 검출이 불가능한 결정적 한계를 가진다. 본 연구에서는 다양한 스테가노그래피 기법으로 생성된 스테고이미지에 딥러닝을 적용하여 스테그아날리시스를 학습하는 범용적 모델을 제안한다. 다양한 실험을 통해 제안 기법의 효용성 및 가능성을 확인하고, 범용적 스테그아날리시스 모델이 각각에 특화된 검출 기법과 유사한 정확도로 스테고 이미지를 검출할 수 있음을 보인다.Steganalysis is to detect information hidden by steganography inside general data such as images. There are stegoanalysis techniques that use machine learning (ML). Existing ML approaches to steganalysis are based on extracting features from stego images and modeling them.
Recently deep learning-based methodologies have shown significant improvements in detection accuracy. However, all the existing methods, including deep learning-based ones, have a critical limitation in that they can only detect stego images that are created by a specific steganography method. In this paper, we propose a generalized steganalysis method that can model multiple types of stego images using deep learning. Through various experiments, we confirm the effectiveness of our approach and envision directions for future research. In particular, we show that our method can detect each type of steganography with the same level of accuracy as that of a steganalysis method dedicated to that type of steganography, thereby demonstrating the general applicability of our approach to multiple types of stego images.steganalysis, steganography, deep learning, generalized steganalysis model"
829,"딥러닝 프레임워크의 비교: 티아노, 텐서플로, CNTK를 중심으로",정여진 /Yeojin Chung,2017,한국지능정보시스템학회,"딥러닝 프레임워크의 대표적인 기능으로는 ‘자동미분’과 ‘GPU의 활용’ 등을 들 수 있다. 본 논문은 파이썬의 라이브러리 형태로 사용 가능한 프레임워크 중에서 구글의 텐서플로와 마이크로소프트의 CNTK, 그리고 텐서 플로의 원조라고 할 수 있는 티아노를 비교하였다. 본문에서는 자동미분의 개념과 GPU의 활용형태를 간단히 설명하고, 그 다음에 logistic regression을 실행하는 예를 통하여 각 프레임워크의 문법을 알아본 뒤에, 마지막으로 대표적인 딥러닝 응용인 CNN의 예제를 실행시켜보고 코딩의 편의성과 실행속도 등을 확인해 보았다. 그 결과, 편의성의 관점에서 보면 티아노가 가장 코딩 하기가 어렵고, CNTK와 텐서플로는 많은 부분이 비슷하게 추상화 되어 있어서 코딩이 비슷하지만 가중치와 편향을 직접 정의하느냐의 여부에서 차이를 보였다. 그리고 각프레임워크의 실행속도에 대한 평가는 ‘큰 차이는 없다’는 것이다. 텐서플로는 티아노에 비하여 속도가 느리다는 평가가 있어왔는데, 본 연구의 실험에 의하면, 비록 CNN 모형에 국한되었지만, 텐서플로가 아주 조금이지만빠른 것으로 나타났다. CNTK의 경우에도, 비록 실험환경이 달랐지만, 실험환경의 차이에 의한 속도의 차이의 편차범위 이내에 있는 것으로 판단이 되었다. 본 연구에서는 세 종류의 딥러닝 프레임워크만을 살펴보았는데,위키피디아에 따르면 딥러닝 프레임워크의 종류는 12가지가 있으며, 각 프레임워크의 특징을 15가지 속성으로구분하여 차이를 특정하고 있다. 그 많은 속성 중에서 사용자의 입장에서 볼 때 중요한 속성은 어떤 언어(파이썬, C++, Java, 등)로 사용가능한지, 어떤 딥러닝 모형에 대한 라이브러리가 잘 구현되어 있는지 등일 것이다. 그리고 사용자가 대규모의 딥러닝 모형을 구축한다면, 다중 GPU 혹은 다중 서버를 지원하는지의 여부도 중요할 것이다. 또한 딥러닝 모형을 처음 학습하는 경우에는 사용설명서가 많은지 예제 프로그램이 많은지 여부도 중요한 기준이 될 것이다.The deep learning framework is software designed to help develop deep learning models. Some of its important functions include “automatic differentiation” and “utilization of GPU”. The list of popular deep learning framework includes Caffe (BVLC) and Theano (University of Montreal). And recently, Microsoft's deep learning framework, Microsoft Cognitive Toolkit, was released as open-source license, following Google’s Tensorflow a year earlier. The early deep learning frameworks have been developed mainly for research at universities. Beginning with the inception of Tensorflow, however, it seems that companies such as Microsoft and Facebook have started to join the competition of framework development. Given the trend, Google and other companies are expected to continue investing in the deep learning framework to bring forward the initiative in the artificial intelligence business. From this point of view, we think it is a good time to compare some of deep learning frameworks. So we compare three deep learning frameworks which can be used as a Python library. Those are Google's Tensorflow, Microsoft’s CNTK, and Theano which is sort of a predecessor of the preceding two.
The most common and important function of deep learning frameworks is the ability to perform automatic differentiation. Basically all the mathematical expressions of deep learning models can be represented as computational graphs, which consist of nodes and edges. Partial derivatives on each edge of a computational graph can then be obtained. With the partial derivatives, we can let software compute differentiation of any node with respect to any variable by utilizing chain rule of Calculus.
First of all, the convenience of coding is in the order of CNTK, Tensorflow, and Theano. The criterion is simply based on the lengths of the codes and the learning curve and the ease of coding are not the main concern. According to the criteria, Theano was the most difficult to implement with, and CNTK and Tensorflow were somewhat easier. With Tensorflow, we need to define weight variables and biases explicitly. The reason that CNTK and Tensorflow are easier to implement with is that those frameworks provide us with more abstraction than Theano. We, however, need to mention that low-level coding is not always bad. It gives us flexibility of coding. With the low-level coding such as in Theano, we can implement and test any new deep learning models or any new search methods that we can think of.
The assessment of the execution speed of each framework is that there is not meaningful difference. According to the experiment, execution speeds of Theano and Tensorflow are very similar, although the experiment was limited to a CNN model. In the case of CNTK, the experimental environment was not maintained as the same. The code written in CNTK has to be run in PC environment without GPU where codes execute as much as 50 times slower than with GPU. But we concluded that the difference of execution speed was within the range of variation caused by the different hardware setup.
In this study, we compared three types of deep learning framework: Theano, Tensorflow, and CNTK. According to Wikipedia, there are 12 available deep learning frameworks. And 15 different attributes differentiate each framework. Some of the important attributes would include interface language (Python, C ++, Java, etc.) and the availability of libraries on various deep learning models such as CNN, RNN, DBN, and etc. And if a user implements a large scale deep learning model, it will also be important to support multiple GPU or multiple servers. Also, if you are learning the deep learning model, it would also be important if there are enough examples and references.deep learning framework, Theano, TensorFlow, CNTK, computational graph, CIFAR-10"
830,딥러닝을 이용한 주변 무선단말 파악방안,이웅섭 /Lee， Woong-sup,2018,한국정보통신학회,"최근 단말-대-단말(Device-to-device, D2D) 통신기술이 차세대 무선통신시스템의 핵심기술로 큰 관심을 받고 있다. 이러한 단말간 통신에서는 자신의 주변에 어떠한 단말이 있는지 파악하는 주변단말 탐색(Neighbor discovery)이매우 중요하다. 본 논문에서는 최근 큰 관심을 받고 있는 딥러닝(Deep learning) 기술을 활용하여 단말간 통신에서 주변단말을 파악하는 방안에 대해서 제안한다. 제안 방안은 기존의 방안과 달리 무선채널의 공간적 연관성을 이용하여단말간의 신호 전송 없이 단말이 기지국으로 전송하는 상향링크 파일럿 신호를 기반으로 주변 단말을 찾고 따라서기존의 방식에 비해 신호전송 복잡도(signaling complexity)를 크게 줄일 수 있다. 또한 제안 방안에서는 떨어져 있는거리에 따라서 주변 단말을 분류 가능하여 기존 방안에 비해서 좀 더 세밀한 단말 탐색이 가능하다. 마지막으로 본 논문에서는 tensorflow를 이용한 컴퓨터 시뮬레이션을 통해 제안 방안의 성능을 검증하였다.Recently, the device-to-device (D2D) communication has been conceived as the key technology for the next-generationmobile communication systems. The neighbor discovery in which the nearby users are found, is essential for the properoperation of the D2D communication. In this paper, we propose new neighbor discovery scheme based on deep learningtechnology which has gained a lot of attention recently. In the proposed scheme, the neighboring users can be foundusing the uplink pilot transmission of users only, unlike conventional neighbor discovery schemes in which direct pilotcommunication among users is required, such that the signaling overhead can be greatly reduced in our proposed scheme.
Moreover, the neighbors with different proximity can also be classified accordingly which enables more accurate neighbordiscovery compared to the conventional schemes. The performance of our proposed scheme is verified through thetensorflow-based computer simulations.
키워드 : 딥러닝, 주변단말 탐색, 단말-대-단말 통신Deep learning, Neighbor discovery, Device-to-device communication, Wireless channel correlation"
831,신뢰성있는 딥러닝 기반 분석 모델을 참조하기 위한 딥러닝 기술 언어,문종혁 /Jong Hyeok Mun,2021,한국정보처리학회,"최근 딥러닝은 하드웨어 성능이 향상됨에 따라 자연어 처리, 영상 인식 등의 다양한 기술에 접목되어 활용되고 있다. 이러한 기술들을 활용해 지능형교통 시스템(ITS), 스마트홈, 헬스케어 등의 산업분야에서 데이터를 분석하여 고속도로 속도위반 차량 검출, 에너지 사용량 제어, 응급상황 등과 같은고품질의 서비스를 제공하며, 고품질의 서비스를 제공하기 위해서는 정확도가 향상된 딥러닝 모델이 적용되어야 한다. 이를 위해 서비스 환경의 데이터를분석하기 위한 딥러닝 모델을 개발할 때, 개발자는 신뢰성이 검증된 최신의 딥러닝 모델을 적용할 수 있어야 한다. 이는 개발자가 참조하는 딥러닝 모델에적용된 학습 데이터셋의 정확도를 측정하여 검증할 수 있다. 이러한 검증을 위해서 개발자는 학습 데이터셋, 딥러닝의 계층구조 및 개발 환경 등과 같은내용을 포함하는 딥러닝 모델을 문서화하여 적용하기 위한 구조적인 정보가 필요하다. 본 논문에서는 신뢰성있는 딥러닝 기반 데이터 분석 모델을 참조하기위한 딥러닝 기술 언어를 제안한다. 제안하는 기술 언어는 신뢰성 있는 딥러닝 모델을 개발하는데 필요한 학습데이터셋, 개발 환경 및 설정 등의 정보와더불어 딥러닝 모델의 계층구조를 표현할 수 있다. 제안하는 딥러닝 기술 언어를 이용하여 개발자는 지능형 교통 시스템에서 참조하는 분석 모델의정확도를 검증할 수 있다. 실험에서는 제안하는 언어의 유효성을 검증하기 위해, 번호판 인식 모델을 중심으로 딥러닝 기술 문서의 적용과정을 보인다With the recent advancements of deep learning, companies such as smart home, healthcare, and intelligent transportation systems areutilizing its functionality to provide high-quality services for vehicle detection, emergency situation detection, and controlling energyconsumption. To provide reliable services in such sensitive systems, deep learning models are required to have high accuracy. In orderto develop a deep learning model for analyzing previously mentioned services, developers should utilize the state of the art deep learningmodels that have already been verified for higher accuracy. The developers can verify the accuracy of the referenced model by validatingthe model on the dataset. For this validation, the developer needs structural information to document and apply deep learning models,including metadata such as learning dataset, network architecture, and development environments. In this paper, we propose a descriptionlanguage that represents the network architecture of the deep learning model along with its metadata that are necessary to develop adeep learning model. Through the proposed description language, developers can easily verify the accuracy of the referenced deep learningmodel. Our experiments demonstrate the application scenario of a deep learning description document that focuses on the license platerecognition for the detection of illegally parked vehicles.Trusted Deep Learning, Model Reference, Deep Learning Description Language, Traffic Situation Analysis Model"
832,딥러닝 분석을 이용한 중국 역내·외 위안화 변동성 예측,이우식 /Lee Woosik,2016,한국데이터정보과학회,"2008년 글로벌 금융위기 이후 중국은 위안화 국제화의 점진적 추진을 진행하면서 중국상하이 외환시장과 중국홍콩 외환시장에서 거래되는 통화인 역내위안화와 역외위안화를 형성시켰다. 본 연구는 위안화 국제화와 점진적인 중국 자본계정 개방에 따라 급변하는 외환시장상황의 변동성을 정확하게 파악하기 위해서 GARCH모형 (일반화된 자기회귀 조건부이분산성모형)에 다단계인공신경망을 결합한 MLP-GARCH 모형과 GARCH모형과 기계학습의 일종인 딥러닝 (deep learning)을 통합한 DL-GARCH을 가지고 위안화 변동성예측을 비교 실험과 분석을 하였다. 비교분석 결과 DLGARCH 모형은 MLP-GARCH보다 모형 위안화 역내·외 환율변동성 예측 면에서 더욱 더 개선된예측값을 제공하였다. 그래서 이분산시계열모형을 딥러닝과 결합한 DL-GARCH 모형은 시계열의환율 변동성 예측 문제에 딥러닝을 응용할 수 있음을 확인하였다. 향후 이분산시계열과 결합된 딥러닝 모형은 다른 금융시계열 데이터에 응용하여 그 일반화 가능성을 높일 수 있을 것이다.The People’s Republic of China has vigorously been pursuing the internationalization of the Chinese Yuan or Renminbi after the financial crisis of 2008. In this view, an abrupt increase of use of the Chinese Yuan in the onshore and offshore markets are important milestones to be one of important currencies. One of the most frequently used methods to forecast volatility is GARCH model. Since a prediction error of the GARCH model has been reported quite high, a lot of efforts have been made to improve forecasting capability of the GARCH model. In this paper, we have proposed MLPGARCH and a DL-GARCH by employing Artificial Neural Network to the GARCH. In an application to forecasting Chinese Yuan volatility, we have successfully shown their overall outperformance in forecasting over the GARCH.Deep learning, DL-GARCH, GARCH, The Chinese Yuan, volatility."
833,딥러닝 기법을 이용한 내일강수 예측,하지훈,2016,한국지능시스템학회,"정확한 강수예측을 위해서는 예측인자 선정과 예측방법에 대한 선택이 매우 중요하다. 최근에는 강수예측 방법으로기계학습 기법이 많이 사용되고 있으며, 그 중에서도 특히 인공신경망을 사용한 강수예측 방법은 좋은 성능을 보였다. 본논문에서는 딥러닝 기법 중 하나인 DBN(deep belief network)를 이용한 새로운 강수예측 방법을 제안한다. DBN는 비지도사전 학습을 통해 초기 가중치를 설정하여 기존 인공신경망의 문제점을 보완한다. 예측인자로는 기온, 전일-전주 강수일, 태양과 달 궤도 관련 자료를 선정하였다. 기온과 전일-전주 강수일은 서울에서의 1974년부터 2013년까지 총 40년간의AWS(automatic weather system) 관측 자료를 사용하였고, 태양과 달의 궤도 관련 자료는 서울을 중심으로 계산한 결과를사용하였다. 전체 기간에서 일부는 학습 자료로 사용하여 예측모델을 생성하였고, 나머지를 생성한 모델의 검증 자료로사용하였다. 모델 검증 결과로 나온 예측값들은 확률값을 가지며 임계치를 이용하여 강수유무를 판별하였다. 강수 정확도의척도로 양분예보기법 중 CSI(critical successive index)와 Bias(frequency bias)를 계산하였다. 이를 통해 DBN와 MLP(multilayer perceptron)의 성능을 비교한 결과 DBN의 강수 예측 정확도가 높았고, 수행속도 또한 2배 이상 빨랐다.For accurate precipitation forecasts the choice of weather faoctrs and prediction method is very important. Recently, machinele arning has been widely used for forecasting precipitation, and artificial neural network, one of machine learning techniques, showed good performance. In this paper, we suggest a new method for forecasting precipitation using DBN, one of deep learning techniques.
DBN has an advantage that initial weights are set by unsupervised learning, so this compensates for the defects of artificial neural networks. We used past precipitation, temperature, and the parameters of the sun and moon's motion as features for forecasting precipitation. The dataset consists of observation data which had been measured for 40 years from AWS in Seoul. Experiments were based on 8-fold cross validation. As a result of estimation, we got probabilities of test dataset, so threshold was used for the decision of precipitation. CSI and Bias were used for indicating the precision of precipitation. Our experimental results showed that DBN performed better than MLP.Deep learning, Deep belief network, Precipitation, Forecast"
834,딥러닝 신경망모형을 이용한 실시간 교통정보수집 연구,김대현 /KIM DAE HYON,2016,사단법인 인문사회과학기술융합학회,"비디오 기반의 실시간 교통정보수집 시스템은 자율주행을 포함한 지능형교통체계의 주요 서브시스템중 하나이며, 우수한 예측(추정)력 때문에 핵심 알고리즘으로 인공신경망 모형이 주로 사용되고 있다. 특히, 최근 과학 학술지 네이처에 구글 딥마인드의 알파고가 공개되면서 딥러닝과 인공지능에 대한 전 세계적인 관심이 집중되고 있다. 최근까지 가장 널리 사용된 인공신경망모형은 역전파 모형이었으나 최근 컴퓨터 하드웨어 및 알고리즘의 새로운 개발과 더불어 딥러닝 알고리즘을 이용한 딥인공신경망 모형에 대한 관심이 높아지고 있다. 그러나 아직 딥러닝 인공신경망에 대한 연구는 초기단계에 있는 실정이며 적용분야도 극히 한정되어 있다. 본 연구에서는 실시간 도로교통정보 수집을 위한 영상처리분야에서 딥러닝 알고리즘을 활용하고 실험결과를 통하여 향후 교통정보수집‧분석 분야에서의 딥러닝 인공신경망 모형의 적용가능성을 살펴보고자 한다. 특히, 본 연구에서 도출한 딥러닝 인공신경망모형의 실험결과를 기존 역전파 모형에 의한 실험결과와 비교하여 모형의 우수성을 비교‧평가하고자 한다.Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Normally, artificial neural networks have been used in real-time applications for collecting traffic information because of their remarkable performance in terms of predictive accuracy and real-time computing. Especially, the recent publication of a paper in the journal Nature and the success of Google DeepMind’s world-beating Go program AlphaGo caused significant increases of the public interest in artificial intelligence. Even though various types of learning models have been proposed, the Backpropagation has been the most popular neural network model. However, the Deep learning neural network model has attracted much interest recently. In this study, the Deep leaning neural network will be applied in image processing techniques for the real-time traffic information collection to investigate the performance of Deep learning algorithm in traffic engineering fields. In addition, the experimental results from Deep learing algorithm will be compared with the results from Backpropagation model to evaluate and propose better performing model.Intelligent Transportation Systems (ITS), artificial neural networks, Deep learning, traffic information, image processing"
835,딥러닝의 모형과 응용사례,안성만 /SungMahn Ahn,2016,한국지능정보시스템학회,"딥러닝은 인공신경망(neural network)이라는 인공지능분야의 모형이 발전된 형태로서, 계층구조로 이루어진 인공신경망의 내부계층(hidden layer)이 여러 단계로 이루어진 구조이다. 딥러닝에서의 주요 모형은 합성곱신경망(convolutional neural network), 순환신경망(recurrent neural network), 그리고 심층신뢰신경망(deep belief network)의 세가지라고 할 수있다. 그 중에서 현재 흥미로운 연구가 많이 발표되어서 관심이 집중되고 있는 모형은 지도학습(supervised learning)모형인 처음 두 개의 모형이다. 따라서 본 논문에서는 지도학습모형의 가중치를 최적화하는 기본적인 방법인 오류역전파 알고리즘을 살펴본 뒤에 합성곱신경망과 순환신경망의 구조와 응용사례 등을 살펴보고자 한다. 본문에서 다루지 않은 모형인 심층신뢰신경망은 아직까지는 합성곱신경망 이나 순환신경망보다는 상대적으로 주목을 덜 받고 있다. 그러나 심층신뢰신경망은 CNN이나 RNN과는 달리 비지도학습(unsupervised learning)모형이며, 사람이나 동물은 관찰을 통해서 스스로 학습한다는 점에서 궁극적으로는 비지도학습모형이 더 많이 연구되어야 할 주제가 될 것이다.Deep learning model is a kind of neural networks that allows multiple hidden layers. There are various deep learning architectures such as convolutional neural networks, deep belief networks and recurrent neural networks. Those have been applied to fields like computer vision, automatic speech recognition, natural language processing, audio recognition and bioinformatics where they have been shown to produce state-of-the-art results on various tasks. Among those architectures, convolutional neural networks and recurrent neural networks are classified as the supervised learning model. And in recent years, those supervised learning models have gained more popularity than unsupervised learning models such as deep belief networks, because supervised learning models have shown fashionable applications in such fields mentioned above.
Deep learning models can be trained with backpropagation algorithm. Backpropagation is an abbreviation for “backward propagation of errors” and a common method of training artificial neural networks used in conjunction with an optimization method such as gradient descent. The method calculates the gradient of an error function with respect to all the weights in the network. The gradient is fed to the optimization method which in turn uses it to update the weights, in an attempt to minimize the error function.
Convolutional neural networks use a special architecture which is particularly well-adapted to classify images. Using this architecture makes convolutional networks fast to train. This, in turn, helps us train deep, muti-layer networks, which are very good at classifying images. These days, deep convolutional networks are used in most neural networks for image recognition. Convolutional neural networks use three basic ideas: local receptive fields, shared weights, and pooling. By local receptive fields, we mean that each neuron in the first(or any) hidden layer will be connected to a small region of the input(or previous layer’s) neurons. Shared weights mean that we’re going to use the same weights and bias for each of the local receptive field. This means that all the neurons in the hidden layer detect exactly the same feature, just at different locations in the input image. In addition to the convolutional layers just described, convolutional neural networks also contain pooling layers. Pooling layers are usually used immediately after convolutional layers. What the pooling layers do is to simplify the information in the output from the convolutional layer.
Recent convolutional network architectures have 10 to 20 hidden layers and billions of connections between units. Training deep learning networks has taken weeks several years ago, but thanks to progress in GPU and algorithm enhancement, training time has reduced to several hours.
Neural networks with time-varying behavior are known as recurrent neural networks or RNNs. A recurrent neural network is a class of artificial neural network where connections between units form a directed cycle. This creates an internal state of the network which allows it to exhibit dynamic temporal behavior. Unlike feedforward neural networks, RNNs can use their internal memory to process arbitrary sequences of inputs. Early RNN models turned out to be very difficult to train, harder even than deep feedforward networks. The reason is the unstable gradient problem such as vanishing gradient and exploding gradient. The gradient can get smaller and smaller as it is propagated back through layers. This makes learning in early layers extremely slow. The problem actually gets worse in RNNs, since gradients aren’t just propagated backward through layers, they’re propagated backward through time. If the network runs for a long time, that can make the gradient extremely unstable and hard to learn from. It has been possible to incorporate an idea known as long short-term memory units (LSTMs) into RNNs. LSTMs make it much easier to get good results when training RNNs, and many recent papers make use of LSTMs or related ideas.Deep learning, Convolutional neural networks, Recurrent neural networks, Error backpropagation algorithm"
836,딥러닝분석과 기술적 분석 지표를 이용한 한국 코스피주가지수 방향성 예측,이우식 /Lee Woosik,2017,한국데이터정보과학회,"2016년 3월 구글 (Google)의 바둑인공지능 알파고 (AlphaGo)가 이세돌 9단과의 바둑대결에서승리한 이후 다양한 분야에서 인공지능 사용에 대한 관심이 높아지고 있는 가운데 금융투자 분야에서도 인공지능과 투자자문 전문가의 합성어인 로보어드바이저 (Robo-Advisor)에 대한 관심이 높아지고 있다. 인공지능 (artificial intelligence)기반의 의사결정은 비용 절감은 물론 효과적인 의사결정을가능하게 한다는 점에서 큰 장점이 있다. 본 연구에서는 기술적 분석 (technical analysis) 지표와 딥러닝 (deep learning) 모형을 결합하여 한국 코스피 지수를 예측하는 모형을 개발하고 제시한 모형들의 예측력을 비교, 분석한다. 분석 결과 기술적 분석 지표에 딥러닝 알고리즘을 결합한 모형이 주가지수 방향성 예측 문제에 응용될 수 있음을 확인하였다. 향후 본 연구에서 제안된 기술적 분석 지표와딥러닝모형을 결합한 기법은 로보어드바이저서비스에 응용할 수 있는 일반화 가능성을 보여준다Since Google’s AlphaGo defeated a world champion of Go players in 2016, there have been many interests in the deep learning. In the financial sector, a Robo-Advisor using deep learning gains a significant attention, which builds and manages portfolios of financial instruments for investors.In this paper, we have proposed the a deep learning algorithm geared toward identification and forecast of the KOSPI index direction,and we also have compared the accuracy of the prediction.In an application of forecasting the financial market index direction, we have shown that the Robo-Advisor using deep learning has a significant effect on finance industry. The Robo-Advisor collects a massive data such as earnings statements, news reports and regulatory filings, analyzes those and recommends investors how to view market trends and identify the best time to purchase financial assets. On the other hand, the Robo-Advisor allows businesses to learn more about their customers, develop better marketing strategies, increase sales and decrease costs.Artificial intelligence, deep learning, FinTech, Robo-Advisor, technical analysis."
837,딥러닝의 효율적인 학습을 위한 학습데이터 선별 기법에 관한 연구,주경돈 /Gyoung-don Joo,2017,한국정보과학회,"빅데이터의 증가와 병렬 하드웨어 처리, 학습 알고리즘의 개선은 딥러닝의 발전을 가져왔으며 인공지능이 적용 가능한 영역을 획기적으로 확장시키고 있다. 하지만 이러한 대규모의 학습 데이터에 기반한 딥러닝은 학습시간이 증가되고, 더 나아가 학습 데이터의 구축을 위한 레이블링(labeling) 비용이 데이터 수에 비례하여 증가하게 된다. 따라서 본 논문에서는, 딥러닝의 정확도를 최대한 유지하면서 필요한 학습 데이터의 수를 줄일수 있도록, 전체 데이터에서 제한된 수의 학습 데이터의 후보를 선별하는 기법에 대하여 연구한다. 특히 기존의 능동 학습에 기반한 기법들이 학습 데이터의 수가 매우 적어질 경우 딥러닝의 정확도가 급격히 떨어지는 문제를 해결할 수 있는 새로운 기법을 제안하였으며, 제안된 기법으로 선별된 학습 데이터는 그 수의 변화에 대해 딥러닝이 일관되게 경쟁력있는 정확도를 가짐을 다양한 실험을 통하여 검증하였다.The increase of big data, parallel hardware processing and enhanced learning algorithms have led to the development of deep learning and are dramatically expanding the domain of applications based on artificial intelligence. However, such large-scale training data extends the learning time, and furthermore, the labeling cost for building the training set increases in proportion to the volume of data. Therefore, in this paper, we study how to select a limited number of candidates for training set from given data so as to reduce the number of training set while maintaining the accuracy of deep learning. Especially, we found that, with the training set selected by the previous work, the accuracy of deep learning drastically decreases when the number of training set is very small and so proposed new methods to resolve this fall-off in accuracy. Through experiments, we showed that deep learning consistently has the competitive accuracy with the training set selected by the proposed method regardless of the size of training set.Deep Learning, Train Set Selection, Active Learning, Gamma distribution"
838,딥러닝을 이용한 범용적 스테그아날리시스,김현재 /Hyunjae Kim,2017,한국정보과학회,"스테그아날리시스(Steganalysis)란 이미지 등 일반적인 자료에 암호화된 정보를 은닉하는 스테가노그래피(Steganography)에 대한 검출 및 분석 방법으로, 기계학습 기반 방법론을 포함한다. 기존 기계학습 기반 스테그아날리시스는 영상(Image)의 특징(Feature) 추출 및 모델링에 기반하며, 최근 딥러닝(Deep Learning)의 적용으로 검출 정확도가 큰 폭으로 향상되었다. 하지만 현존하는 스테그아날리시스 모델은 단일 스테가노그래피 기법에 대해 국한되어 있어 학습에 사용되지 않은 스테고(Stego) 이미지의 경우 검출이 불가능한 결정적 한계를 가진다. 본 연구에서는 다양한 스테가노그래피 기법으로 생성된 스테고이미지에 딥러닝을 적용하여 스테그아날리시스를 학습하는 범용적 모델을 제안한다. 다양한 실험을 통해 제안 기법의 효용성 및 가능성을 확인하고, 범용적 스테그아날리시스 모델이 각각에 특화된 검출 기법과 유사한 정확도로 스테고 이미지를 검출할 수 있음을 보인다.Steganalysis is to detect information hidden by steganography inside general data such as images. There are stegoanalysis techniques that use machine learning (ML). Existing ML approaches to steganalysis are based on extracting features from stego images and modeling them.
Recently deep learning-based methodologies have shown significant improvements in detection accuracy. However, all the existing methods, including deep learning-based ones, have a critical limitation in that they can only detect stego images that are created by a specific steganography method. In this paper, we propose a generalized steganalysis method that can model multiple types of stego images using deep learning. Through various experiments, we confirm the effectiveness of our approach and envision directions for future research. In particular, we show that our method can detect each type of steganography with the same level of accuracy as that of a steganalysis method dedicated to that type of steganography, thereby demonstrating the general applicability of our approach to multiple types of stego images.steganalysis, steganography, deep learning, generalized steganalysis model"
839,"딥러닝 프레임워크의 비교: 티아노, 텐서플로, CNTK를 중심으로",정여진 /Yeojin Chung,2017,한국지능정보시스템학회,"딥러닝 프레임워크의 대표적인 기능으로는 ‘자동미분’과 ‘GPU의 활용’ 등을 들 수 있다. 본 논문은 파이썬의 라이브러리 형태로 사용 가능한 프레임워크 중에서 구글의 텐서플로와 마이크로소프트의 CNTK, 그리고 텐서 플로의 원조라고 할 수 있는 티아노를 비교하였다. 본문에서는 자동미분의 개념과 GPU의 활용형태를 간단히 설명하고, 그 다음에 logistic regression을 실행하는 예를 통하여 각 프레임워크의 문법을 알아본 뒤에, 마지막으로 대표적인 딥러닝 응용인 CNN의 예제를 실행시켜보고 코딩의 편의성과 실행속도 등을 확인해 보았다. 그 결과, 편의성의 관점에서 보면 티아노가 가장 코딩 하기가 어렵고, CNTK와 텐서플로는 많은 부분이 비슷하게 추상화 되어 있어서 코딩이 비슷하지만 가중치와 편향을 직접 정의하느냐의 여부에서 차이를 보였다. 그리고 각프레임워크의 실행속도에 대한 평가는 ‘큰 차이는 없다’는 것이다. 텐서플로는 티아노에 비하여 속도가 느리다는 평가가 있어왔는데, 본 연구의 실험에 의하면, 비록 CNN 모형에 국한되었지만, 텐서플로가 아주 조금이지만빠른 것으로 나타났다. CNTK의 경우에도, 비록 실험환경이 달랐지만, 실험환경의 차이에 의한 속도의 차이의 편차범위 이내에 있는 것으로 판단이 되었다. 본 연구에서는 세 종류의 딥러닝 프레임워크만을 살펴보았는데,위키피디아에 따르면 딥러닝 프레임워크의 종류는 12가지가 있으며, 각 프레임워크의 특징을 15가지 속성으로구분하여 차이를 특정하고 있다. 그 많은 속성 중에서 사용자의 입장에서 볼 때 중요한 속성은 어떤 언어(파이썬, C++, Java, 등)로 사용가능한지, 어떤 딥러닝 모형에 대한 라이브러리가 잘 구현되어 있는지 등일 것이다. 그리고 사용자가 대규모의 딥러닝 모형을 구축한다면, 다중 GPU 혹은 다중 서버를 지원하는지의 여부도 중요할 것이다. 또한 딥러닝 모형을 처음 학습하는 경우에는 사용설명서가 많은지 예제 프로그램이 많은지 여부도 중요한 기준이 될 것이다.The deep learning framework is software designed to help develop deep learning models. Some of its important functions include “automatic differentiation” and “utilization of GPU”. The list of popular deep learning framework includes Caffe (BVLC) and Theano (University of Montreal). And recently, Microsoft's deep learning framework, Microsoft Cognitive Toolkit, was released as open-source license, following Google’s Tensorflow a year earlier. The early deep learning frameworks have been developed mainly for research at universities. Beginning with the inception of Tensorflow, however, it seems that companies such as Microsoft and Facebook have started to join the competition of framework development. Given the trend, Google and other companies are expected to continue investing in the deep learning framework to bring forward the initiative in the artificial intelligence business. From this point of view, we think it is a good time to compare some of deep learning frameworks. So we compare three deep learning frameworks which can be used as a Python library. Those are Google's Tensorflow, Microsoft’s CNTK, and Theano which is sort of a predecessor of the preceding two.
The most common and important function of deep learning frameworks is the ability to perform automatic differentiation. Basically all the mathematical expressions of deep learning models can be represented as computational graphs, which consist of nodes and edges. Partial derivatives on each edge of a computational graph can then be obtained. With the partial derivatives, we can let software compute differentiation of any node with respect to any variable by utilizing chain rule of Calculus.
First of all, the convenience of coding is in the order of CNTK, Tensorflow, and Theano. The criterion is simply based on the lengths of the codes and the learning curve and the ease of coding are not the main concern. According to the criteria, Theano was the most difficult to implement with, and CNTK and Tensorflow were somewhat easier. With Tensorflow, we need to define weight variables and biases explicitly. The reason that CNTK and Tensorflow are easier to implement with is that those frameworks provide us with more abstraction than Theano. We, however, need to mention that low-level coding is not always bad. It gives us flexibility of coding. With the low-level coding such as in Theano, we can implement and test any new deep learning models or any new search methods that we can think of.
The assessment of the execution speed of each framework is that there is not meaningful difference. According to the experiment, execution speeds of Theano and Tensorflow are very similar, although the experiment was limited to a CNN model. In the case of CNTK, the experimental environment was not maintained as the same. The code written in CNTK has to be run in PC environment without GPU where codes execute as much as 50 times slower than with GPU. But we concluded that the difference of execution speed was within the range of variation caused by the different hardware setup.
In this study, we compared three types of deep learning framework: Theano, Tensorflow, and CNTK. According to Wikipedia, there are 12 available deep learning frameworks. And 15 different attributes differentiate each framework. Some of the important attributes would include interface language (Python, C ++, Java, etc.) and the availability of libraries on various deep learning models such as CNN, RNN, DBN, and etc. And if a user implements a large scale deep learning model, it will also be important to support multiple GPU or multiple servers. Also, if you are learning the deep learning model, it would also be important if there are enough examples and references.deep learning framework, Theano, TensorFlow, CNTK, computational graph, CIFAR-10"
840,딥러닝을 이용한 주변 무선단말 파악방안,이웅섭 /Lee， Woong-sup,2018,한국정보통신학회,"최근 단말-대-단말(Device-to-device, D2D) 통신기술이 차세대 무선통신시스템의 핵심기술로 큰 관심을 받고 있다. 이러한 단말간 통신에서는 자신의 주변에 어떠한 단말이 있는지 파악하는 주변단말 탐색(Neighbor discovery)이매우 중요하다. 본 논문에서는 최근 큰 관심을 받고 있는 딥러닝(Deep learning) 기술을 활용하여 단말간 통신에서 주변단말을 파악하는 방안에 대해서 제안한다. 제안 방안은 기존의 방안과 달리 무선채널의 공간적 연관성을 이용하여단말간의 신호 전송 없이 단말이 기지국으로 전송하는 상향링크 파일럿 신호를 기반으로 주변 단말을 찾고 따라서기존의 방식에 비해 신호전송 복잡도(signaling complexity)를 크게 줄일 수 있다. 또한 제안 방안에서는 떨어져 있는거리에 따라서 주변 단말을 분류 가능하여 기존 방안에 비해서 좀 더 세밀한 단말 탐색이 가능하다. 마지막으로 본 논문에서는 tensorflow를 이용한 컴퓨터 시뮬레이션을 통해 제안 방안의 성능을 검증하였다.Recently, the device-to-device (D2D) communication has been conceived as the key technology for the next-generationmobile communication systems. The neighbor discovery in which the nearby users are found, is essential for the properoperation of the D2D communication. In this paper, we propose new neighbor discovery scheme based on deep learningtechnology which has gained a lot of attention recently. In the proposed scheme, the neighboring users can be foundusing the uplink pilot transmission of users only, unlike conventional neighbor discovery schemes in which direct pilotcommunication among users is required, such that the signaling overhead can be greatly reduced in our proposed scheme.
Moreover, the neighbors with different proximity can also be classified accordingly which enables more accurate neighbordiscovery compared to the conventional schemes. The performance of our proposed scheme is verified through thetensorflow-based computer simulations.
키워드 : 딥러닝, 주변단말 탐색, 단말-대-단말 통신Deep learning, Neighbor discovery, Device-to-device communication, Wireless channel correlation"
841,신뢰성있는 딥러닝 기반 분석 모델을 참조하기 위한 딥러닝 기술 언어,문종혁 /Jong Hyeok Mun,2021,한국정보처리학회,"최근 딥러닝은 하드웨어 성능이 향상됨에 따라 자연어 처리, 영상 인식 등의 다양한 기술에 접목되어 활용되고 있다. 이러한 기술들을 활용해 지능형교통 시스템(ITS), 스마트홈, 헬스케어 등의 산업분야에서 데이터를 분석하여 고속도로 속도위반 차량 검출, 에너지 사용량 제어, 응급상황 등과 같은고품질의 서비스를 제공하며, 고품질의 서비스를 제공하기 위해서는 정확도가 향상된 딥러닝 모델이 적용되어야 한다. 이를 위해 서비스 환경의 데이터를분석하기 위한 딥러닝 모델을 개발할 때, 개발자는 신뢰성이 검증된 최신의 딥러닝 모델을 적용할 수 있어야 한다. 이는 개발자가 참조하는 딥러닝 모델에적용된 학습 데이터셋의 정확도를 측정하여 검증할 수 있다. 이러한 검증을 위해서 개발자는 학습 데이터셋, 딥러닝의 계층구조 및 개발 환경 등과 같은내용을 포함하는 딥러닝 모델을 문서화하여 적용하기 위한 구조적인 정보가 필요하다. 본 논문에서는 신뢰성있는 딥러닝 기반 데이터 분석 모델을 참조하기위한 딥러닝 기술 언어를 제안한다. 제안하는 기술 언어는 신뢰성 있는 딥러닝 모델을 개발하는데 필요한 학습데이터셋, 개발 환경 및 설정 등의 정보와더불어 딥러닝 모델의 계층구조를 표현할 수 있다. 제안하는 딥러닝 기술 언어를 이용하여 개발자는 지능형 교통 시스템에서 참조하는 분석 모델의정확도를 검증할 수 있다. 실험에서는 제안하는 언어의 유효성을 검증하기 위해, 번호판 인식 모델을 중심으로 딥러닝 기술 문서의 적용과정을 보인다With the recent advancements of deep learning, companies such as smart home, healthcare, and intelligent transportation systems areutilizing its functionality to provide high-quality services for vehicle detection, emergency situation detection, and controlling energyconsumption. To provide reliable services in such sensitive systems, deep learning models are required to have high accuracy. In orderto develop a deep learning model for analyzing previously mentioned services, developers should utilize the state of the art deep learningmodels that have already been verified for higher accuracy. The developers can verify the accuracy of the referenced model by validatingthe model on the dataset. For this validation, the developer needs structural information to document and apply deep learning models,including metadata such as learning dataset, network architecture, and development environments. In this paper, we propose a descriptionlanguage that represents the network architecture of the deep learning model along with its metadata that are necessary to develop adeep learning model. Through the proposed description language, developers can easily verify the accuracy of the referenced deep learningmodel. Our experiments demonstrate the application scenario of a deep learning description document that focuses on the license platerecognition for the detection of illegally parked vehicles.Trusted Deep Learning, Model Reference, Deep Learning Description Language, Traffic Situation Analysis Model"
842,딥러닝 분석을 이용한 중국 역내·외 위안화 변동성 예측,이우식 /Lee Woosik,2016,한국데이터정보과학회,"2008년 글로벌 금융위기 이후 중국은 위안화 국제화의 점진적 추진을 진행하면서 중국상하이 외환시장과 중국홍콩 외환시장에서 거래되는 통화인 역내위안화와 역외위안화를 형성시켰다. 본 연구는 위안화 국제화와 점진적인 중국 자본계정 개방에 따라 급변하는 외환시장상황의 변동성을 정확하게 파악하기 위해서 GARCH모형 (일반화된 자기회귀 조건부이분산성모형)에 다단계인공신경망을 결합한 MLP-GARCH 모형과 GARCH모형과 기계학습의 일종인 딥러닝 (deep learning)을 통합한 DL-GARCH을 가지고 위안화 변동성예측을 비교 실험과 분석을 하였다. 비교분석 결과 DLGARCH 모형은 MLP-GARCH보다 모형 위안화 역내·외 환율변동성 예측 면에서 더욱 더 개선된예측값을 제공하였다. 그래서 이분산시계열모형을 딥러닝과 결합한 DL-GARCH 모형은 시계열의환율 변동성 예측 문제에 딥러닝을 응용할 수 있음을 확인하였다. 향후 이분산시계열과 결합된 딥러닝 모형은 다른 금융시계열 데이터에 응용하여 그 일반화 가능성을 높일 수 있을 것이다.The People’s Republic of China has vigorously been pursuing the internationalization of the Chinese Yuan or Renminbi after the financial crisis of 2008. In this view, an abrupt increase of use of the Chinese Yuan in the onshore and offshore markets are important milestones to be one of important currencies. One of the most frequently used methods to forecast volatility is GARCH model. Since a prediction error of the GARCH model has been reported quite high, a lot of efforts have been made to improve forecasting capability of the GARCH model. In this paper, we have proposed MLPGARCH and a DL-GARCH by employing Artificial Neural Network to the GARCH. In an application to forecasting Chinese Yuan volatility, we have successfully shown their overall outperformance in forecasting over the GARCH.Deep learning, DL-GARCH, GARCH, The Chinese Yuan, volatility."
843,딥러닝 기법을 이용한 내일강수 예측,하지훈,2016,한국지능시스템학회,"정확한 강수예측을 위해서는 예측인자 선정과 예측방법에 대한 선택이 매우 중요하다. 최근에는 강수예측 방법으로기계학습 기법이 많이 사용되고 있으며, 그 중에서도 특히 인공신경망을 사용한 강수예측 방법은 좋은 성능을 보였다. 본논문에서는 딥러닝 기법 중 하나인 DBN(deep belief network)를 이용한 새로운 강수예측 방법을 제안한다. DBN는 비지도사전 학습을 통해 초기 가중치를 설정하여 기존 인공신경망의 문제점을 보완한다. 예측인자로는 기온, 전일-전주 강수일, 태양과 달 궤도 관련 자료를 선정하였다. 기온과 전일-전주 강수일은 서울에서의 1974년부터 2013년까지 총 40년간의AWS(automatic weather system) 관측 자료를 사용하였고, 태양과 달의 궤도 관련 자료는 서울을 중심으로 계산한 결과를사용하였다. 전체 기간에서 일부는 학습 자료로 사용하여 예측모델을 생성하였고, 나머지를 생성한 모델의 검증 자료로사용하였다. 모델 검증 결과로 나온 예측값들은 확률값을 가지며 임계치를 이용하여 강수유무를 판별하였다. 강수 정확도의척도로 양분예보기법 중 CSI(critical successive index)와 Bias(frequency bias)를 계산하였다. 이를 통해 DBN와 MLP(multilayer perceptron)의 성능을 비교한 결과 DBN의 강수 예측 정확도가 높았고, 수행속도 또한 2배 이상 빨랐다.For accurate precipitation forecasts the choice of weather faoctrs and prediction method is very important. Recently, machinele arning has been widely used for forecasting precipitation, and artificial neural network, one of machine learning techniques, showed good performance. In this paper, we suggest a new method for forecasting precipitation using DBN, one of deep learning techniques.
DBN has an advantage that initial weights are set by unsupervised learning, so this compensates for the defects of artificial neural networks. We used past precipitation, temperature, and the parameters of the sun and moon's motion as features for forecasting precipitation. The dataset consists of observation data which had been measured for 40 years from AWS in Seoul. Experiments were based on 8-fold cross validation. As a result of estimation, we got probabilities of test dataset, so threshold was used for the decision of precipitation. CSI and Bias were used for indicating the precision of precipitation. Our experimental results showed that DBN performed better than MLP.Deep learning, Deep belief network, Precipitation, Forecast"
844,딥러닝 신경망모형을 이용한 실시간 교통정보수집 연구,김대현 /KIM DAE HYON,2016,사단법인 인문사회과학기술융합학회,"비디오 기반의 실시간 교통정보수집 시스템은 자율주행을 포함한 지능형교통체계의 주요 서브시스템중 하나이며, 우수한 예측(추정)력 때문에 핵심 알고리즘으로 인공신경망 모형이 주로 사용되고 있다. 특히, 최근 과학 학술지 네이처에 구글 딥마인드의 알파고가 공개되면서 딥러닝과 인공지능에 대한 전 세계적인 관심이 집중되고 있다. 최근까지 가장 널리 사용된 인공신경망모형은 역전파 모형이었으나 최근 컴퓨터 하드웨어 및 알고리즘의 새로운 개발과 더불어 딥러닝 알고리즘을 이용한 딥인공신경망 모형에 대한 관심이 높아지고 있다. 그러나 아직 딥러닝 인공신경망에 대한 연구는 초기단계에 있는 실정이며 적용분야도 극히 한정되어 있다. 본 연구에서는 실시간 도로교통정보 수집을 위한 영상처리분야에서 딥러닝 알고리즘을 활용하고 실험결과를 통하여 향후 교통정보수집‧분석 분야에서의 딥러닝 인공신경망 모형의 적용가능성을 살펴보고자 한다. 특히, 본 연구에서 도출한 딥러닝 인공신경망모형의 실험결과를 기존 역전파 모형에 의한 실험결과와 비교하여 모형의 우수성을 비교‧평가하고자 한다.Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Normally, artificial neural networks have been used in real-time applications for collecting traffic information because of their remarkable performance in terms of predictive accuracy and real-time computing. Especially, the recent publication of a paper in the journal Nature and the success of Google DeepMind’s world-beating Go program AlphaGo caused significant increases of the public interest in artificial intelligence. Even though various types of learning models have been proposed, the Backpropagation has been the most popular neural network model. However, the Deep learning neural network model has attracted much interest recently. In this study, the Deep leaning neural network will be applied in image processing techniques for the real-time traffic information collection to investigate the performance of Deep learning algorithm in traffic engineering fields. In addition, the experimental results from Deep learing algorithm will be compared with the results from Backpropagation model to evaluate and propose better performing model.Intelligent Transportation Systems (ITS), artificial neural networks, Deep learning, traffic information, image processing"
845,딥러닝의 모형과 응용사례,안성만 /SungMahn Ahn,2016,한국지능정보시스템학회,"딥러닝은 인공신경망(neural network)이라는 인공지능분야의 모형이 발전된 형태로서, 계층구조로 이루어진 인공신경망의 내부계층(hidden layer)이 여러 단계로 이루어진 구조이다. 딥러닝에서의 주요 모형은 합성곱신경망(convolutional neural network), 순환신경망(recurrent neural network), 그리고 심층신뢰신경망(deep belief network)의 세가지라고 할 수있다. 그 중에서 현재 흥미로운 연구가 많이 발표되어서 관심이 집중되고 있는 모형은 지도학습(supervised learning)모형인 처음 두 개의 모형이다. 따라서 본 논문에서는 지도학습모형의 가중치를 최적화하는 기본적인 방법인 오류역전파 알고리즘을 살펴본 뒤에 합성곱신경망과 순환신경망의 구조와 응용사례 등을 살펴보고자 한다. 본문에서 다루지 않은 모형인 심층신뢰신경망은 아직까지는 합성곱신경망 이나 순환신경망보다는 상대적으로 주목을 덜 받고 있다. 그러나 심층신뢰신경망은 CNN이나 RNN과는 달리 비지도학습(unsupervised learning)모형이며, 사람이나 동물은 관찰을 통해서 스스로 학습한다는 점에서 궁극적으로는 비지도학습모형이 더 많이 연구되어야 할 주제가 될 것이다.Deep learning model is a kind of neural networks that allows multiple hidden layers. There are various deep learning architectures such as convolutional neural networks, deep belief networks and recurrent neural networks. Those have been applied to fields like computer vision, automatic speech recognition, natural language processing, audio recognition and bioinformatics where they have been shown to produce state-of-the-art results on various tasks. Among those architectures, convolutional neural networks and recurrent neural networks are classified as the supervised learning model. And in recent years, those supervised learning models have gained more popularity than unsupervised learning models such as deep belief networks, because supervised learning models have shown fashionable applications in such fields mentioned above.
Deep learning models can be trained with backpropagation algorithm. Backpropagation is an abbreviation for “backward propagation of errors” and a common method of training artificial neural networks used in conjunction with an optimization method such as gradient descent. The method calculates the gradient of an error function with respect to all the weights in the network. The gradient is fed to the optimization method which in turn uses it to update the weights, in an attempt to minimize the error function.
Convolutional neural networks use a special architecture which is particularly well-adapted to classify images. Using this architecture makes convolutional networks fast to train. This, in turn, helps us train deep, muti-layer networks, which are very good at classifying images. These days, deep convolutional networks are used in most neural networks for image recognition. Convolutional neural networks use three basic ideas: local receptive fields, shared weights, and pooling. By local receptive fields, we mean that each neuron in the first(or any) hidden layer will be connected to a small region of the input(or previous layer’s) neurons. Shared weights mean that we’re going to use the same weights and bias for each of the local receptive field. This means that all the neurons in the hidden layer detect exactly the same feature, just at different locations in the input image. In addition to the convolutional layers just described, convolutional neural networks also contain pooling layers. Pooling layers are usually used immediately after convolutional layers. What the pooling layers do is to simplify the information in the output from the convolutional layer.
Recent convolutional network architectures have 10 to 20 hidden layers and billions of connections between units. Training deep learning networks has taken weeks several years ago, but thanks to progress in GPU and algorithm enhancement, training time has reduced to several hours.
Neural networks with time-varying behavior are known as recurrent neural networks or RNNs. A recurrent neural network is a class of artificial neural network where connections between units form a directed cycle. This creates an internal state of the network which allows it to exhibit dynamic temporal behavior. Unlike feedforward neural networks, RNNs can use their internal memory to process arbitrary sequences of inputs. Early RNN models turned out to be very difficult to train, harder even than deep feedforward networks. The reason is the unstable gradient problem such as vanishing gradient and exploding gradient. The gradient can get smaller and smaller as it is propagated back through layers. This makes learning in early layers extremely slow. The problem actually gets worse in RNNs, since gradients aren’t just propagated backward through layers, they’re propagated backward through time. If the network runs for a long time, that can make the gradient extremely unstable and hard to learn from. It has been possible to incorporate an idea known as long short-term memory units (LSTMs) into RNNs. LSTMs make it much easier to get good results when training RNNs, and many recent papers make use of LSTMs or related ideas.Deep learning, Convolutional neural networks, Recurrent neural networks, Error backpropagation algorithm"
846,딥러닝분석과 기술적 분석 지표를 이용한 한국 코스피주가지수 방향성 예측,이우식 /Lee Woosik,2017,한국데이터정보과학회,"2016년 3월 구글 (Google)의 바둑인공지능 알파고 (AlphaGo)가 이세돌 9단과의 바둑대결에서승리한 이후 다양한 분야에서 인공지능 사용에 대한 관심이 높아지고 있는 가운데 금융투자 분야에서도 인공지능과 투자자문 전문가의 합성어인 로보어드바이저 (Robo-Advisor)에 대한 관심이 높아지고 있다. 인공지능 (artificial intelligence)기반의 의사결정은 비용 절감은 물론 효과적인 의사결정을가능하게 한다는 점에서 큰 장점이 있다. 본 연구에서는 기술적 분석 (technical analysis) 지표와 딥러닝 (deep learning) 모형을 결합하여 한국 코스피 지수를 예측하는 모형을 개발하고 제시한 모형들의 예측력을 비교, 분석한다. 분석 결과 기술적 분석 지표에 딥러닝 알고리즘을 결합한 모형이 주가지수 방향성 예측 문제에 응용될 수 있음을 확인하였다. 향후 본 연구에서 제안된 기술적 분석 지표와딥러닝모형을 결합한 기법은 로보어드바이저서비스에 응용할 수 있는 일반화 가능성을 보여준다Since Google’s AlphaGo defeated a world champion of Go players in 2016, there have been many interests in the deep learning. In the financial sector, a Robo-Advisor using deep learning gains a significant attention, which builds and manages portfolios of financial instruments for investors.In this paper, we have proposed the a deep learning algorithm geared toward identification and forecast of the KOSPI index direction,and we also have compared the accuracy of the prediction.In an application of forecasting the financial market index direction, we have shown that the Robo-Advisor using deep learning has a significant effect on finance industry. The Robo-Advisor collects a massive data such as earnings statements, news reports and regulatory filings, analyzes those and recommends investors how to view market trends and identify the best time to purchase financial assets. On the other hand, the Robo-Advisor allows businesses to learn more about their customers, develop better marketing strategies, increase sales and decrease costs.Artificial intelligence, deep learning, FinTech, Robo-Advisor, technical analysis."
847,딥러닝의 효율적인 학습을 위한 학습데이터 선별 기법에 관한 연구,주경돈 /Gyoung-don Joo,2017,한국정보과학회,"빅데이터의 증가와 병렬 하드웨어 처리, 학습 알고리즘의 개선은 딥러닝의 발전을 가져왔으며 인공지능이 적용 가능한 영역을 획기적으로 확장시키고 있다. 하지만 이러한 대규모의 학습 데이터에 기반한 딥러닝은 학습시간이 증가되고, 더 나아가 학습 데이터의 구축을 위한 레이블링(labeling) 비용이 데이터 수에 비례하여 증가하게 된다. 따라서 본 논문에서는, 딥러닝의 정확도를 최대한 유지하면서 필요한 학습 데이터의 수를 줄일수 있도록, 전체 데이터에서 제한된 수의 학습 데이터의 후보를 선별하는 기법에 대하여 연구한다. 특히 기존의 능동 학습에 기반한 기법들이 학습 데이터의 수가 매우 적어질 경우 딥러닝의 정확도가 급격히 떨어지는 문제를 해결할 수 있는 새로운 기법을 제안하였으며, 제안된 기법으로 선별된 학습 데이터는 그 수의 변화에 대해 딥러닝이 일관되게 경쟁력있는 정확도를 가짐을 다양한 실험을 통하여 검증하였다.The increase of big data, parallel hardware processing and enhanced learning algorithms have led to the development of deep learning and are dramatically expanding the domain of applications based on artificial intelligence. However, such large-scale training data extends the learning time, and furthermore, the labeling cost for building the training set increases in proportion to the volume of data. Therefore, in this paper, we study how to select a limited number of candidates for training set from given data so as to reduce the number of training set while maintaining the accuracy of deep learning. Especially, we found that, with the training set selected by the previous work, the accuracy of deep learning drastically decreases when the number of training set is very small and so proposed new methods to resolve this fall-off in accuracy. Through experiments, we showed that deep learning consistently has the competitive accuracy with the training set selected by the proposed method regardless of the size of training set.Deep Learning, Train Set Selection, Active Learning, Gamma distribution"
848,딥러닝을 이용한 범용적 스테그아날리시스,김현재 /Hyunjae Kim,2017,한국정보과학회,"스테그아날리시스(Steganalysis)란 이미지 등 일반적인 자료에 암호화된 정보를 은닉하는 스테가노그래피(Steganography)에 대한 검출 및 분석 방법으로, 기계학습 기반 방법론을 포함한다. 기존 기계학습 기반 스테그아날리시스는 영상(Image)의 특징(Feature) 추출 및 모델링에 기반하며, 최근 딥러닝(Deep Learning)의 적용으로 검출 정확도가 큰 폭으로 향상되었다. 하지만 현존하는 스테그아날리시스 모델은 단일 스테가노그래피 기법에 대해 국한되어 있어 학습에 사용되지 않은 스테고(Stego) 이미지의 경우 검출이 불가능한 결정적 한계를 가진다. 본 연구에서는 다양한 스테가노그래피 기법으로 생성된 스테고이미지에 딥러닝을 적용하여 스테그아날리시스를 학습하는 범용적 모델을 제안한다. 다양한 실험을 통해 제안 기법의 효용성 및 가능성을 확인하고, 범용적 스테그아날리시스 모델이 각각에 특화된 검출 기법과 유사한 정확도로 스테고 이미지를 검출할 수 있음을 보인다.Steganalysis is to detect information hidden by steganography inside general data such as images. There are stegoanalysis techniques that use machine learning (ML). Existing ML approaches to steganalysis are based on extracting features from stego images and modeling them.
Recently deep learning-based methodologies have shown significant improvements in detection accuracy. However, all the existing methods, including deep learning-based ones, have a critical limitation in that they can only detect stego images that are created by a specific steganography method. In this paper, we propose a generalized steganalysis method that can model multiple types of stego images using deep learning. Through various experiments, we confirm the effectiveness of our approach and envision directions for future research. In particular, we show that our method can detect each type of steganography with the same level of accuracy as that of a steganalysis method dedicated to that type of steganography, thereby demonstrating the general applicability of our approach to multiple types of stego images.steganalysis, steganography, deep learning, generalized steganalysis model"
849,"딥러닝 프레임워크의 비교: 티아노, 텐서플로, CNTK를 중심으로",정여진 /Yeojin Chung,2017,한국지능정보시스템학회,"딥러닝 프레임워크의 대표적인 기능으로는 ‘자동미분’과 ‘GPU의 활용’ 등을 들 수 있다. 본 논문은 파이썬의 라이브러리 형태로 사용 가능한 프레임워크 중에서 구글의 텐서플로와 마이크로소프트의 CNTK, 그리고 텐서 플로의 원조라고 할 수 있는 티아노를 비교하였다. 본문에서는 자동미분의 개념과 GPU의 활용형태를 간단히 설명하고, 그 다음에 logistic regression을 실행하는 예를 통하여 각 프레임워크의 문법을 알아본 뒤에, 마지막으로 대표적인 딥러닝 응용인 CNN의 예제를 실행시켜보고 코딩의 편의성과 실행속도 등을 확인해 보았다. 그 결과, 편의성의 관점에서 보면 티아노가 가장 코딩 하기가 어렵고, CNTK와 텐서플로는 많은 부분이 비슷하게 추상화 되어 있어서 코딩이 비슷하지만 가중치와 편향을 직접 정의하느냐의 여부에서 차이를 보였다. 그리고 각프레임워크의 실행속도에 대한 평가는 ‘큰 차이는 없다’는 것이다. 텐서플로는 티아노에 비하여 속도가 느리다는 평가가 있어왔는데, 본 연구의 실험에 의하면, 비록 CNN 모형에 국한되었지만, 텐서플로가 아주 조금이지만빠른 것으로 나타났다. CNTK의 경우에도, 비록 실험환경이 달랐지만, 실험환경의 차이에 의한 속도의 차이의 편차범위 이내에 있는 것으로 판단이 되었다. 본 연구에서는 세 종류의 딥러닝 프레임워크만을 살펴보았는데,위키피디아에 따르면 딥러닝 프레임워크의 종류는 12가지가 있으며, 각 프레임워크의 특징을 15가지 속성으로구분하여 차이를 특정하고 있다. 그 많은 속성 중에서 사용자의 입장에서 볼 때 중요한 속성은 어떤 언어(파이썬, C++, Java, 등)로 사용가능한지, 어떤 딥러닝 모형에 대한 라이브러리가 잘 구현되어 있는지 등일 것이다. 그리고 사용자가 대규모의 딥러닝 모형을 구축한다면, 다중 GPU 혹은 다중 서버를 지원하는지의 여부도 중요할 것이다. 또한 딥러닝 모형을 처음 학습하는 경우에는 사용설명서가 많은지 예제 프로그램이 많은지 여부도 중요한 기준이 될 것이다.The deep learning framework is software designed to help develop deep learning models. Some of its important functions include “automatic differentiation” and “utilization of GPU”. The list of popular deep learning framework includes Caffe (BVLC) and Theano (University of Montreal). And recently, Microsoft's deep learning framework, Microsoft Cognitive Toolkit, was released as open-source license, following Google’s Tensorflow a year earlier. The early deep learning frameworks have been developed mainly for research at universities. Beginning with the inception of Tensorflow, however, it seems that companies such as Microsoft and Facebook have started to join the competition of framework development. Given the trend, Google and other companies are expected to continue investing in the deep learning framework to bring forward the initiative in the artificial intelligence business. From this point of view, we think it is a good time to compare some of deep learning frameworks. So we compare three deep learning frameworks which can be used as a Python library. Those are Google's Tensorflow, Microsoft’s CNTK, and Theano which is sort of a predecessor of the preceding two.
The most common and important function of deep learning frameworks is the ability to perform automatic differentiation. Basically all the mathematical expressions of deep learning models can be represented as computational graphs, which consist of nodes and edges. Partial derivatives on each edge of a computational graph can then be obtained. With the partial derivatives, we can let software compute differentiation of any node with respect to any variable by utilizing chain rule of Calculus.
First of all, the convenience of coding is in the order of CNTK, Tensorflow, and Theano. The criterion is simply based on the lengths of the codes and the learning curve and the ease of coding are not the main concern. According to the criteria, Theano was the most difficult to implement with, and CNTK and Tensorflow were somewhat easier. With Tensorflow, we need to define weight variables and biases explicitly. The reason that CNTK and Tensorflow are easier to implement with is that those frameworks provide us with more abstraction than Theano. We, however, need to mention that low-level coding is not always bad. It gives us flexibility of coding. With the low-level coding such as in Theano, we can implement and test any new deep learning models or any new search methods that we can think of.
The assessment of the execution speed of each framework is that there is not meaningful difference. According to the experiment, execution speeds of Theano and Tensorflow are very similar, although the experiment was limited to a CNN model. In the case of CNTK, the experimental environment was not maintained as the same. The code written in CNTK has to be run in PC environment without GPU where codes execute as much as 50 times slower than with GPU. But we concluded that the difference of execution speed was within the range of variation caused by the different hardware setup.
In this study, we compared three types of deep learning framework: Theano, Tensorflow, and CNTK. According to Wikipedia, there are 12 available deep learning frameworks. And 15 different attributes differentiate each framework. Some of the important attributes would include interface language (Python, C ++, Java, etc.) and the availability of libraries on various deep learning models such as CNN, RNN, DBN, and etc. And if a user implements a large scale deep learning model, it will also be important to support multiple GPU or multiple servers. Also, if you are learning the deep learning model, it would also be important if there are enough examples and references.deep learning framework, Theano, TensorFlow, CNTK, computational graph, CIFAR-10"
850,딥러닝을 이용한 주변 무선단말 파악방안,이웅섭 /Lee， Woong-sup,2018,한국정보통신학회,"최근 단말-대-단말(Device-to-device, D2D) 통신기술이 차세대 무선통신시스템의 핵심기술로 큰 관심을 받고 있다. 이러한 단말간 통신에서는 자신의 주변에 어떠한 단말이 있는지 파악하는 주변단말 탐색(Neighbor discovery)이매우 중요하다. 본 논문에서는 최근 큰 관심을 받고 있는 딥러닝(Deep learning) 기술을 활용하여 단말간 통신에서 주변단말을 파악하는 방안에 대해서 제안한다. 제안 방안은 기존의 방안과 달리 무선채널의 공간적 연관성을 이용하여단말간의 신호 전송 없이 단말이 기지국으로 전송하는 상향링크 파일럿 신호를 기반으로 주변 단말을 찾고 따라서기존의 방식에 비해 신호전송 복잡도(signaling complexity)를 크게 줄일 수 있다. 또한 제안 방안에서는 떨어져 있는거리에 따라서 주변 단말을 분류 가능하여 기존 방안에 비해서 좀 더 세밀한 단말 탐색이 가능하다. 마지막으로 본 논문에서는 tensorflow를 이용한 컴퓨터 시뮬레이션을 통해 제안 방안의 성능을 검증하였다.Recently, the device-to-device (D2D) communication has been conceived as the key technology for the next-generationmobile communication systems. The neighbor discovery in which the nearby users are found, is essential for the properoperation of the D2D communication. In this paper, we propose new neighbor discovery scheme based on deep learningtechnology which has gained a lot of attention recently. In the proposed scheme, the neighboring users can be foundusing the uplink pilot transmission of users only, unlike conventional neighbor discovery schemes in which direct pilotcommunication among users is required, such that the signaling overhead can be greatly reduced in our proposed scheme.
Moreover, the neighbors with different proximity can also be classified accordingly which enables more accurate neighbordiscovery compared to the conventional schemes. The performance of our proposed scheme is verified through thetensorflow-based computer simulations.
키워드 : 딥러닝, 주변단말 탐색, 단말-대-단말 통신Deep learning, Neighbor discovery, Device-to-device communication, Wireless channel correlation"
851,신뢰성있는 딥러닝 기반 분석 모델을 참조하기 위한 딥러닝 기술 언어,문종혁 /Jong Hyeok Mun,2021,한국정보처리학회,"최근 딥러닝은 하드웨어 성능이 향상됨에 따라 자연어 처리, 영상 인식 등의 다양한 기술에 접목되어 활용되고 있다. 이러한 기술들을 활용해 지능형교통 시스템(ITS), 스마트홈, 헬스케어 등의 산업분야에서 데이터를 분석하여 고속도로 속도위반 차량 검출, 에너지 사용량 제어, 응급상황 등과 같은고품질의 서비스를 제공하며, 고품질의 서비스를 제공하기 위해서는 정확도가 향상된 딥러닝 모델이 적용되어야 한다. 이를 위해 서비스 환경의 데이터를분석하기 위한 딥러닝 모델을 개발할 때, 개발자는 신뢰성이 검증된 최신의 딥러닝 모델을 적용할 수 있어야 한다. 이는 개발자가 참조하는 딥러닝 모델에적용된 학습 데이터셋의 정확도를 측정하여 검증할 수 있다. 이러한 검증을 위해서 개발자는 학습 데이터셋, 딥러닝의 계층구조 및 개발 환경 등과 같은내용을 포함하는 딥러닝 모델을 문서화하여 적용하기 위한 구조적인 정보가 필요하다. 본 논문에서는 신뢰성있는 딥러닝 기반 데이터 분석 모델을 참조하기위한 딥러닝 기술 언어를 제안한다. 제안하는 기술 언어는 신뢰성 있는 딥러닝 모델을 개발하는데 필요한 학습데이터셋, 개발 환경 및 설정 등의 정보와더불어 딥러닝 모델의 계층구조를 표현할 수 있다. 제안하는 딥러닝 기술 언어를 이용하여 개발자는 지능형 교통 시스템에서 참조하는 분석 모델의정확도를 검증할 수 있다. 실험에서는 제안하는 언어의 유효성을 검증하기 위해, 번호판 인식 모델을 중심으로 딥러닝 기술 문서의 적용과정을 보인다With the recent advancements of deep learning, companies such as smart home, healthcare, and intelligent transportation systems areutilizing its functionality to provide high-quality services for vehicle detection, emergency situation detection, and controlling energyconsumption. To provide reliable services in such sensitive systems, deep learning models are required to have high accuracy. In orderto develop a deep learning model for analyzing previously mentioned services, developers should utilize the state of the art deep learningmodels that have already been verified for higher accuracy. The developers can verify the accuracy of the referenced model by validatingthe model on the dataset. For this validation, the developer needs structural information to document and apply deep learning models,including metadata such as learning dataset, network architecture, and development environments. In this paper, we propose a descriptionlanguage that represents the network architecture of the deep learning model along with its metadata that are necessary to develop adeep learning model. Through the proposed description language, developers can easily verify the accuracy of the referenced deep learningmodel. Our experiments demonstrate the application scenario of a deep learning description document that focuses on the license platerecognition for the detection of illegally parked vehicles.Trusted Deep Learning, Model Reference, Deep Learning Description Language, Traffic Situation Analysis Model"
852,딥러닝 분석을 이용한 중국 역내·외 위안화 변동성 예측,이우식 /Lee Woosik,2016,한국데이터정보과학회,"2008년 글로벌 금융위기 이후 중국은 위안화 국제화의 점진적 추진을 진행하면서 중국상하이 외환시장과 중국홍콩 외환시장에서 거래되는 통화인 역내위안화와 역외위안화를 형성시켰다. 본 연구는 위안화 국제화와 점진적인 중국 자본계정 개방에 따라 급변하는 외환시장상황의 변동성을 정확하게 파악하기 위해서 GARCH모형 (일반화된 자기회귀 조건부이분산성모형)에 다단계인공신경망을 결합한 MLP-GARCH 모형과 GARCH모형과 기계학습의 일종인 딥러닝 (deep learning)을 통합한 DL-GARCH을 가지고 위안화 변동성예측을 비교 실험과 분석을 하였다. 비교분석 결과 DLGARCH 모형은 MLP-GARCH보다 모형 위안화 역내·외 환율변동성 예측 면에서 더욱 더 개선된예측값을 제공하였다. 그래서 이분산시계열모형을 딥러닝과 결합한 DL-GARCH 모형은 시계열의환율 변동성 예측 문제에 딥러닝을 응용할 수 있음을 확인하였다. 향후 이분산시계열과 결합된 딥러닝 모형은 다른 금융시계열 데이터에 응용하여 그 일반화 가능성을 높일 수 있을 것이다.The People’s Republic of China has vigorously been pursuing the internationalization of the Chinese Yuan or Renminbi after the financial crisis of 2008. In this view, an abrupt increase of use of the Chinese Yuan in the onshore and offshore markets are important milestones to be one of important currencies. One of the most frequently used methods to forecast volatility is GARCH model. Since a prediction error of the GARCH model has been reported quite high, a lot of efforts have been made to improve forecasting capability of the GARCH model. In this paper, we have proposed MLPGARCH and a DL-GARCH by employing Artificial Neural Network to the GARCH. In an application to forecasting Chinese Yuan volatility, we have successfully shown their overall outperformance in forecasting over the GARCH.Deep learning, DL-GARCH, GARCH, The Chinese Yuan, volatility."
853,딥러닝 기법을 이용한 내일강수 예측,하지훈,2016,한국지능시스템학회,"정확한 강수예측을 위해서는 예측인자 선정과 예측방법에 대한 선택이 매우 중요하다. 최근에는 강수예측 방법으로기계학습 기법이 많이 사용되고 있으며, 그 중에서도 특히 인공신경망을 사용한 강수예측 방법은 좋은 성능을 보였다. 본논문에서는 딥러닝 기법 중 하나인 DBN(deep belief network)를 이용한 새로운 강수예측 방법을 제안한다. DBN는 비지도사전 학습을 통해 초기 가중치를 설정하여 기존 인공신경망의 문제점을 보완한다. 예측인자로는 기온, 전일-전주 강수일, 태양과 달 궤도 관련 자료를 선정하였다. 기온과 전일-전주 강수일은 서울에서의 1974년부터 2013년까지 총 40년간의AWS(automatic weather system) 관측 자료를 사용하였고, 태양과 달의 궤도 관련 자료는 서울을 중심으로 계산한 결과를사용하였다. 전체 기간에서 일부는 학습 자료로 사용하여 예측모델을 생성하였고, 나머지를 생성한 모델의 검증 자료로사용하였다. 모델 검증 결과로 나온 예측값들은 확률값을 가지며 임계치를 이용하여 강수유무를 판별하였다. 강수 정확도의척도로 양분예보기법 중 CSI(critical successive index)와 Bias(frequency bias)를 계산하였다. 이를 통해 DBN와 MLP(multilayer perceptron)의 성능을 비교한 결과 DBN의 강수 예측 정확도가 높았고, 수행속도 또한 2배 이상 빨랐다.For accurate precipitation forecasts the choice of weather faoctrs and prediction method is very important. Recently, machinele arning has been widely used for forecasting precipitation, and artificial neural network, one of machine learning techniques, showed good performance. In this paper, we suggest a new method for forecasting precipitation using DBN, one of deep learning techniques.
DBN has an advantage that initial weights are set by unsupervised learning, so this compensates for the defects of artificial neural networks. We used past precipitation, temperature, and the parameters of the sun and moon's motion as features for forecasting precipitation. The dataset consists of observation data which had been measured for 40 years from AWS in Seoul. Experiments were based on 8-fold cross validation. As a result of estimation, we got probabilities of test dataset, so threshold was used for the decision of precipitation. CSI and Bias were used for indicating the precision of precipitation. Our experimental results showed that DBN performed better than MLP.Deep learning, Deep belief network, Precipitation, Forecast"
854,딥러닝 신경망모형을 이용한 실시간 교통정보수집 연구,김대현 /KIM DAE HYON,2016,사단법인 인문사회과학기술융합학회,"비디오 기반의 실시간 교통정보수집 시스템은 자율주행을 포함한 지능형교통체계의 주요 서브시스템중 하나이며, 우수한 예측(추정)력 때문에 핵심 알고리즘으로 인공신경망 모형이 주로 사용되고 있다. 특히, 최근 과학 학술지 네이처에 구글 딥마인드의 알파고가 공개되면서 딥러닝과 인공지능에 대한 전 세계적인 관심이 집중되고 있다. 최근까지 가장 널리 사용된 인공신경망모형은 역전파 모형이었으나 최근 컴퓨터 하드웨어 및 알고리즘의 새로운 개발과 더불어 딥러닝 알고리즘을 이용한 딥인공신경망 모형에 대한 관심이 높아지고 있다. 그러나 아직 딥러닝 인공신경망에 대한 연구는 초기단계에 있는 실정이며 적용분야도 극히 한정되어 있다. 본 연구에서는 실시간 도로교통정보 수집을 위한 영상처리분야에서 딥러닝 알고리즘을 활용하고 실험결과를 통하여 향후 교통정보수집‧분석 분야에서의 딥러닝 인공신경망 모형의 적용가능성을 살펴보고자 한다. 특히, 본 연구에서 도출한 딥러닝 인공신경망모형의 실험결과를 기존 역전파 모형에 의한 실험결과와 비교하여 모형의 우수성을 비교‧평가하고자 한다.Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Normally, artificial neural networks have been used in real-time applications for collecting traffic information because of their remarkable performance in terms of predictive accuracy and real-time computing. Especially, the recent publication of a paper in the journal Nature and the success of Google DeepMind’s world-beating Go program AlphaGo caused significant increases of the public interest in artificial intelligence. Even though various types of learning models have been proposed, the Backpropagation has been the most popular neural network model. However, the Deep learning neural network model has attracted much interest recently. In this study, the Deep leaning neural network will be applied in image processing techniques for the real-time traffic information collection to investigate the performance of Deep learning algorithm in traffic engineering fields. In addition, the experimental results from Deep learing algorithm will be compared with the results from Backpropagation model to evaluate and propose better performing model.Intelligent Transportation Systems (ITS), artificial neural networks, Deep learning, traffic information, image processing"
855,딥러닝의 모형과 응용사례,안성만 /SungMahn Ahn,2016,한국지능정보시스템학회,"딥러닝은 인공신경망(neural network)이라는 인공지능분야의 모형이 발전된 형태로서, 계층구조로 이루어진 인공신경망의 내부계층(hidden layer)이 여러 단계로 이루어진 구조이다. 딥러닝에서의 주요 모형은 합성곱신경망(convolutional neural network), 순환신경망(recurrent neural network), 그리고 심층신뢰신경망(deep belief network)의 세가지라고 할 수있다. 그 중에서 현재 흥미로운 연구가 많이 발표되어서 관심이 집중되고 있는 모형은 지도학습(supervised learning)모형인 처음 두 개의 모형이다. 따라서 본 논문에서는 지도학습모형의 가중치를 최적화하는 기본적인 방법인 오류역전파 알고리즘을 살펴본 뒤에 합성곱신경망과 순환신경망의 구조와 응용사례 등을 살펴보고자 한다. 본문에서 다루지 않은 모형인 심층신뢰신경망은 아직까지는 합성곱신경망 이나 순환신경망보다는 상대적으로 주목을 덜 받고 있다. 그러나 심층신뢰신경망은 CNN이나 RNN과는 달리 비지도학습(unsupervised learning)모형이며, 사람이나 동물은 관찰을 통해서 스스로 학습한다는 점에서 궁극적으로는 비지도학습모형이 더 많이 연구되어야 할 주제가 될 것이다.Deep learning model is a kind of neural networks that allows multiple hidden layers. There are various deep learning architectures such as convolutional neural networks, deep belief networks and recurrent neural networks. Those have been applied to fields like computer vision, automatic speech recognition, natural language processing, audio recognition and bioinformatics where they have been shown to produce state-of-the-art results on various tasks. Among those architectures, convolutional neural networks and recurrent neural networks are classified as the supervised learning model. And in recent years, those supervised learning models have gained more popularity than unsupervised learning models such as deep belief networks, because supervised learning models have shown fashionable applications in such fields mentioned above.
Deep learning models can be trained with backpropagation algorithm. Backpropagation is an abbreviation for “backward propagation of errors” and a common method of training artificial neural networks used in conjunction with an optimization method such as gradient descent. The method calculates the gradient of an error function with respect to all the weights in the network. The gradient is fed to the optimization method which in turn uses it to update the weights, in an attempt to minimize the error function.
Convolutional neural networks use a special architecture which is particularly well-adapted to classify images. Using this architecture makes convolutional networks fast to train. This, in turn, helps us train deep, muti-layer networks, which are very good at classifying images. These days, deep convolutional networks are used in most neural networks for image recognition. Convolutional neural networks use three basic ideas: local receptive fields, shared weights, and pooling. By local receptive fields, we mean that each neuron in the first(or any) hidden layer will be connected to a small region of the input(or previous layer’s) neurons. Shared weights mean that we’re going to use the same weights and bias for each of the local receptive field. This means that all the neurons in the hidden layer detect exactly the same feature, just at different locations in the input image. In addition to the convolutional layers just described, convolutional neural networks also contain pooling layers. Pooling layers are usually used immediately after convolutional layers. What the pooling layers do is to simplify the information in the output from the convolutional layer.
Recent convolutional network architectures have 10 to 20 hidden layers and billions of connections between units. Training deep learning networks has taken weeks several years ago, but thanks to progress in GPU and algorithm enhancement, training time has reduced to several hours.
Neural networks with time-varying behavior are known as recurrent neural networks or RNNs. A recurrent neural network is a class of artificial neural network where connections between units form a directed cycle. This creates an internal state of the network which allows it to exhibit dynamic temporal behavior. Unlike feedforward neural networks, RNNs can use their internal memory to process arbitrary sequences of inputs. Early RNN models turned out to be very difficult to train, harder even than deep feedforward networks. The reason is the unstable gradient problem such as vanishing gradient and exploding gradient. The gradient can get smaller and smaller as it is propagated back through layers. This makes learning in early layers extremely slow. The problem actually gets worse in RNNs, since gradients aren’t just propagated backward through layers, they’re propagated backward through time. If the network runs for a long time, that can make the gradient extremely unstable and hard to learn from. It has been possible to incorporate an idea known as long short-term memory units (LSTMs) into RNNs. LSTMs make it much easier to get good results when training RNNs, and many recent papers make use of LSTMs or related ideas.Deep learning, Convolutional neural networks, Recurrent neural networks, Error backpropagation algorithm"
856,딥러닝분석과 기술적 분석 지표를 이용한 한국 코스피주가지수 방향성 예측,이우식 /Lee Woosik,2017,한국데이터정보과학회,"2016년 3월 구글 (Google)의 바둑인공지능 알파고 (AlphaGo)가 이세돌 9단과의 바둑대결에서승리한 이후 다양한 분야에서 인공지능 사용에 대한 관심이 높아지고 있는 가운데 금융투자 분야에서도 인공지능과 투자자문 전문가의 합성어인 로보어드바이저 (Robo-Advisor)에 대한 관심이 높아지고 있다. 인공지능 (artificial intelligence)기반의 의사결정은 비용 절감은 물론 효과적인 의사결정을가능하게 한다는 점에서 큰 장점이 있다. 본 연구에서는 기술적 분석 (technical analysis) 지표와 딥러닝 (deep learning) 모형을 결합하여 한국 코스피 지수를 예측하는 모형을 개발하고 제시한 모형들의 예측력을 비교, 분석한다. 분석 결과 기술적 분석 지표에 딥러닝 알고리즘을 결합한 모형이 주가지수 방향성 예측 문제에 응용될 수 있음을 확인하였다. 향후 본 연구에서 제안된 기술적 분석 지표와딥러닝모형을 결합한 기법은 로보어드바이저서비스에 응용할 수 있는 일반화 가능성을 보여준다Since Google’s AlphaGo defeated a world champion of Go players in 2016, there have been many interests in the deep learning. In the financial sector, a Robo-Advisor using deep learning gains a significant attention, which builds and manages portfolios of financial instruments for investors.In this paper, we have proposed the a deep learning algorithm geared toward identification and forecast of the KOSPI index direction,and we also have compared the accuracy of the prediction.In an application of forecasting the financial market index direction, we have shown that the Robo-Advisor using deep learning has a significant effect on finance industry. The Robo-Advisor collects a massive data such as earnings statements, news reports and regulatory filings, analyzes those and recommends investors how to view market trends and identify the best time to purchase financial assets. On the other hand, the Robo-Advisor allows businesses to learn more about their customers, develop better marketing strategies, increase sales and decrease costs.Artificial intelligence, deep learning, FinTech, Robo-Advisor, technical analysis."
857,딥러닝의 효율적인 학습을 위한 학습데이터 선별 기법에 관한 연구,주경돈 /Gyoung-don Joo,2017,한국정보과학회,"빅데이터의 증가와 병렬 하드웨어 처리, 학습 알고리즘의 개선은 딥러닝의 발전을 가져왔으며 인공지능이 적용 가능한 영역을 획기적으로 확장시키고 있다. 하지만 이러한 대규모의 학습 데이터에 기반한 딥러닝은 학습시간이 증가되고, 더 나아가 학습 데이터의 구축을 위한 레이블링(labeling) 비용이 데이터 수에 비례하여 증가하게 된다. 따라서 본 논문에서는, 딥러닝의 정확도를 최대한 유지하면서 필요한 학습 데이터의 수를 줄일수 있도록, 전체 데이터에서 제한된 수의 학습 데이터의 후보를 선별하는 기법에 대하여 연구한다. 특히 기존의 능동 학습에 기반한 기법들이 학습 데이터의 수가 매우 적어질 경우 딥러닝의 정확도가 급격히 떨어지는 문제를 해결할 수 있는 새로운 기법을 제안하였으며, 제안된 기법으로 선별된 학습 데이터는 그 수의 변화에 대해 딥러닝이 일관되게 경쟁력있는 정확도를 가짐을 다양한 실험을 통하여 검증하였다.The increase of big data, parallel hardware processing and enhanced learning algorithms have led to the development of deep learning and are dramatically expanding the domain of applications based on artificial intelligence. However, such large-scale training data extends the learning time, and furthermore, the labeling cost for building the training set increases in proportion to the volume of data. Therefore, in this paper, we study how to select a limited number of candidates for training set from given data so as to reduce the number of training set while maintaining the accuracy of deep learning. Especially, we found that, with the training set selected by the previous work, the accuracy of deep learning drastically decreases when the number of training set is very small and so proposed new methods to resolve this fall-off in accuracy. Through experiments, we showed that deep learning consistently has the competitive accuracy with the training set selected by the proposed method regardless of the size of training set.Deep Learning, Train Set Selection, Active Learning, Gamma distribution"
858,딥러닝을 이용한 범용적 스테그아날리시스,김현재 /Hyunjae Kim,2017,한국정보과학회,"스테그아날리시스(Steganalysis)란 이미지 등 일반적인 자료에 암호화된 정보를 은닉하는 스테가노그래피(Steganography)에 대한 검출 및 분석 방법으로, 기계학습 기반 방법론을 포함한다. 기존 기계학습 기반 스테그아날리시스는 영상(Image)의 특징(Feature) 추출 및 모델링에 기반하며, 최근 딥러닝(Deep Learning)의 적용으로 검출 정확도가 큰 폭으로 향상되었다. 하지만 현존하는 스테그아날리시스 모델은 단일 스테가노그래피 기법에 대해 국한되어 있어 학습에 사용되지 않은 스테고(Stego) 이미지의 경우 검출이 불가능한 결정적 한계를 가진다. 본 연구에서는 다양한 스테가노그래피 기법으로 생성된 스테고이미지에 딥러닝을 적용하여 스테그아날리시스를 학습하는 범용적 모델을 제안한다. 다양한 실험을 통해 제안 기법의 효용성 및 가능성을 확인하고, 범용적 스테그아날리시스 모델이 각각에 특화된 검출 기법과 유사한 정확도로 스테고 이미지를 검출할 수 있음을 보인다.Steganalysis is to detect information hidden by steganography inside general data such as images. There are stegoanalysis techniques that use machine learning (ML). Existing ML approaches to steganalysis are based on extracting features from stego images and modeling them.
Recently deep learning-based methodologies have shown significant improvements in detection accuracy. However, all the existing methods, including deep learning-based ones, have a critical limitation in that they can only detect stego images that are created by a specific steganography method. In this paper, we propose a generalized steganalysis method that can model multiple types of stego images using deep learning. Through various experiments, we confirm the effectiveness of our approach and envision directions for future research. In particular, we show that our method can detect each type of steganography with the same level of accuracy as that of a steganalysis method dedicated to that type of steganography, thereby demonstrating the general applicability of our approach to multiple types of stego images.steganalysis, steganography, deep learning, generalized steganalysis model"
859,"딥러닝 프레임워크의 비교: 티아노, 텐서플로, CNTK를 중심으로",정여진 /Yeojin Chung,2017,한국지능정보시스템학회,"딥러닝 프레임워크의 대표적인 기능으로는 ‘자동미분’과 ‘GPU의 활용’ 등을 들 수 있다. 본 논문은 파이썬의 라이브러리 형태로 사용 가능한 프레임워크 중에서 구글의 텐서플로와 마이크로소프트의 CNTK, 그리고 텐서 플로의 원조라고 할 수 있는 티아노를 비교하였다. 본문에서는 자동미분의 개념과 GPU의 활용형태를 간단히 설명하고, 그 다음에 logistic regression을 실행하는 예를 통하여 각 프레임워크의 문법을 알아본 뒤에, 마지막으로 대표적인 딥러닝 응용인 CNN의 예제를 실행시켜보고 코딩의 편의성과 실행속도 등을 확인해 보았다. 그 결과, 편의성의 관점에서 보면 티아노가 가장 코딩 하기가 어렵고, CNTK와 텐서플로는 많은 부분이 비슷하게 추상화 되어 있어서 코딩이 비슷하지만 가중치와 편향을 직접 정의하느냐의 여부에서 차이를 보였다. 그리고 각프레임워크의 실행속도에 대한 평가는 ‘큰 차이는 없다’는 것이다. 텐서플로는 티아노에 비하여 속도가 느리다는 평가가 있어왔는데, 본 연구의 실험에 의하면, 비록 CNN 모형에 국한되었지만, 텐서플로가 아주 조금이지만빠른 것으로 나타났다. CNTK의 경우에도, 비록 실험환경이 달랐지만, 실험환경의 차이에 의한 속도의 차이의 편차범위 이내에 있는 것으로 판단이 되었다. 본 연구에서는 세 종류의 딥러닝 프레임워크만을 살펴보았는데,위키피디아에 따르면 딥러닝 프레임워크의 종류는 12가지가 있으며, 각 프레임워크의 특징을 15가지 속성으로구분하여 차이를 특정하고 있다. 그 많은 속성 중에서 사용자의 입장에서 볼 때 중요한 속성은 어떤 언어(파이썬, C++, Java, 등)로 사용가능한지, 어떤 딥러닝 모형에 대한 라이브러리가 잘 구현되어 있는지 등일 것이다. 그리고 사용자가 대규모의 딥러닝 모형을 구축한다면, 다중 GPU 혹은 다중 서버를 지원하는지의 여부도 중요할 것이다. 또한 딥러닝 모형을 처음 학습하는 경우에는 사용설명서가 많은지 예제 프로그램이 많은지 여부도 중요한 기준이 될 것이다.The deep learning framework is software designed to help develop deep learning models. Some of its important functions include “automatic differentiation” and “utilization of GPU”. The list of popular deep learning framework includes Caffe (BVLC) and Theano (University of Montreal). And recently, Microsoft's deep learning framework, Microsoft Cognitive Toolkit, was released as open-source license, following Google’s Tensorflow a year earlier. The early deep learning frameworks have been developed mainly for research at universities. Beginning with the inception of Tensorflow, however, it seems that companies such as Microsoft and Facebook have started to join the competition of framework development. Given the trend, Google and other companies are expected to continue investing in the deep learning framework to bring forward the initiative in the artificial intelligence business. From this point of view, we think it is a good time to compare some of deep learning frameworks. So we compare three deep learning frameworks which can be used as a Python library. Those are Google's Tensorflow, Microsoft’s CNTK, and Theano which is sort of a predecessor of the preceding two.
The most common and important function of deep learning frameworks is the ability to perform automatic differentiation. Basically all the mathematical expressions of deep learning models can be represented as computational graphs, which consist of nodes and edges. Partial derivatives on each edge of a computational graph can then be obtained. With the partial derivatives, we can let software compute differentiation of any node with respect to any variable by utilizing chain rule of Calculus.
First of all, the convenience of coding is in the order of CNTK, Tensorflow, and Theano. The criterion is simply based on the lengths of the codes and the learning curve and the ease of coding are not the main concern. According to the criteria, Theano was the most difficult to implement with, and CNTK and Tensorflow were somewhat easier. With Tensorflow, we need to define weight variables and biases explicitly. The reason that CNTK and Tensorflow are easier to implement with is that those frameworks provide us with more abstraction than Theano. We, however, need to mention that low-level coding is not always bad. It gives us flexibility of coding. With the low-level coding such as in Theano, we can implement and test any new deep learning models or any new search methods that we can think of.
The assessment of the execution speed of each framework is that there is not meaningful difference. According to the experiment, execution speeds of Theano and Tensorflow are very similar, although the experiment was limited to a CNN model. In the case of CNTK, the experimental environment was not maintained as the same. The code written in CNTK has to be run in PC environment without GPU where codes execute as much as 50 times slower than with GPU. But we concluded that the difference of execution speed was within the range of variation caused by the different hardware setup.
In this study, we compared three types of deep learning framework: Theano, Tensorflow, and CNTK. According to Wikipedia, there are 12 available deep learning frameworks. And 15 different attributes differentiate each framework. Some of the important attributes would include interface language (Python, C ++, Java, etc.) and the availability of libraries on various deep learning models such as CNN, RNN, DBN, and etc. And if a user implements a large scale deep learning model, it will also be important to support multiple GPU or multiple servers. Also, if you are learning the deep learning model, it would also be important if there are enough examples and references.deep learning framework, Theano, TensorFlow, CNTK, computational graph, CIFAR-10"
860,딥러닝을 이용한 주변 무선단말 파악방안,이웅섭 /Lee， Woong-sup,2018,한국정보통신학회,"최근 단말-대-단말(Device-to-device, D2D) 통신기술이 차세대 무선통신시스템의 핵심기술로 큰 관심을 받고 있다. 이러한 단말간 통신에서는 자신의 주변에 어떠한 단말이 있는지 파악하는 주변단말 탐색(Neighbor discovery)이매우 중요하다. 본 논문에서는 최근 큰 관심을 받고 있는 딥러닝(Deep learning) 기술을 활용하여 단말간 통신에서 주변단말을 파악하는 방안에 대해서 제안한다. 제안 방안은 기존의 방안과 달리 무선채널의 공간적 연관성을 이용하여단말간의 신호 전송 없이 단말이 기지국으로 전송하는 상향링크 파일럿 신호를 기반으로 주변 단말을 찾고 따라서기존의 방식에 비해 신호전송 복잡도(signaling complexity)를 크게 줄일 수 있다. 또한 제안 방안에서는 떨어져 있는거리에 따라서 주변 단말을 분류 가능하여 기존 방안에 비해서 좀 더 세밀한 단말 탐색이 가능하다. 마지막으로 본 논문에서는 tensorflow를 이용한 컴퓨터 시뮬레이션을 통해 제안 방안의 성능을 검증하였다.Recently, the device-to-device (D2D) communication has been conceived as the key technology for the next-generationmobile communication systems. The neighbor discovery in which the nearby users are found, is essential for the properoperation of the D2D communication. In this paper, we propose new neighbor discovery scheme based on deep learningtechnology which has gained a lot of attention recently. In the proposed scheme, the neighboring users can be foundusing the uplink pilot transmission of users only, unlike conventional neighbor discovery schemes in which direct pilotcommunication among users is required, such that the signaling overhead can be greatly reduced in our proposed scheme.
Moreover, the neighbors with different proximity can also be classified accordingly which enables more accurate neighbordiscovery compared to the conventional schemes. The performance of our proposed scheme is verified through thetensorflow-based computer simulations.
키워드 : 딥러닝, 주변단말 탐색, 단말-대-단말 통신Deep learning, Neighbor discovery, Device-to-device communication, Wireless channel correlation"
861,신뢰성있는 딥러닝 기반 분석 모델을 참조하기 위한 딥러닝 기술 언어,문종혁 /Jong Hyeok Mun,2021,한국정보처리학회,"최근 딥러닝은 하드웨어 성능이 향상됨에 따라 자연어 처리, 영상 인식 등의 다양한 기술에 접목되어 활용되고 있다. 이러한 기술들을 활용해 지능형교통 시스템(ITS), 스마트홈, 헬스케어 등의 산업분야에서 데이터를 분석하여 고속도로 속도위반 차량 검출, 에너지 사용량 제어, 응급상황 등과 같은고품질의 서비스를 제공하며, 고품질의 서비스를 제공하기 위해서는 정확도가 향상된 딥러닝 모델이 적용되어야 한다. 이를 위해 서비스 환경의 데이터를분석하기 위한 딥러닝 모델을 개발할 때, 개발자는 신뢰성이 검증된 최신의 딥러닝 모델을 적용할 수 있어야 한다. 이는 개발자가 참조하는 딥러닝 모델에적용된 학습 데이터셋의 정확도를 측정하여 검증할 수 있다. 이러한 검증을 위해서 개발자는 학습 데이터셋, 딥러닝의 계층구조 및 개발 환경 등과 같은내용을 포함하는 딥러닝 모델을 문서화하여 적용하기 위한 구조적인 정보가 필요하다. 본 논문에서는 신뢰성있는 딥러닝 기반 데이터 분석 모델을 참조하기위한 딥러닝 기술 언어를 제안한다. 제안하는 기술 언어는 신뢰성 있는 딥러닝 모델을 개발하는데 필요한 학습데이터셋, 개발 환경 및 설정 등의 정보와더불어 딥러닝 모델의 계층구조를 표현할 수 있다. 제안하는 딥러닝 기술 언어를 이용하여 개발자는 지능형 교통 시스템에서 참조하는 분석 모델의정확도를 검증할 수 있다. 실험에서는 제안하는 언어의 유효성을 검증하기 위해, 번호판 인식 모델을 중심으로 딥러닝 기술 문서의 적용과정을 보인다With the recent advancements of deep learning, companies such as smart home, healthcare, and intelligent transportation systems areutilizing its functionality to provide high-quality services for vehicle detection, emergency situation detection, and controlling energyconsumption. To provide reliable services in such sensitive systems, deep learning models are required to have high accuracy. In orderto develop a deep learning model for analyzing previously mentioned services, developers should utilize the state of the art deep learningmodels that have already been verified for higher accuracy. The developers can verify the accuracy of the referenced model by validatingthe model on the dataset. For this validation, the developer needs structural information to document and apply deep learning models,including metadata such as learning dataset, network architecture, and development environments. In this paper, we propose a descriptionlanguage that represents the network architecture of the deep learning model along with its metadata that are necessary to develop adeep learning model. Through the proposed description language, developers can easily verify the accuracy of the referenced deep learningmodel. Our experiments demonstrate the application scenario of a deep learning description document that focuses on the license platerecognition for the detection of illegally parked vehicles.Trusted Deep Learning, Model Reference, Deep Learning Description Language, Traffic Situation Analysis Model"
862,딥러닝 분석을 이용한 중국 역내·외 위안화 변동성 예측,이우식 /Lee Woosik,2016,한국데이터정보과학회,"2008년 글로벌 금융위기 이후 중국은 위안화 국제화의 점진적 추진을 진행하면서 중국상하이 외환시장과 중국홍콩 외환시장에서 거래되는 통화인 역내위안화와 역외위안화를 형성시켰다. 본 연구는 위안화 국제화와 점진적인 중국 자본계정 개방에 따라 급변하는 외환시장상황의 변동성을 정확하게 파악하기 위해서 GARCH모형 (일반화된 자기회귀 조건부이분산성모형)에 다단계인공신경망을 결합한 MLP-GARCH 모형과 GARCH모형과 기계학습의 일종인 딥러닝 (deep learning)을 통합한 DL-GARCH을 가지고 위안화 변동성예측을 비교 실험과 분석을 하였다. 비교분석 결과 DLGARCH 모형은 MLP-GARCH보다 모형 위안화 역내·외 환율변동성 예측 면에서 더욱 더 개선된예측값을 제공하였다. 그래서 이분산시계열모형을 딥러닝과 결합한 DL-GARCH 모형은 시계열의환율 변동성 예측 문제에 딥러닝을 응용할 수 있음을 확인하였다. 향후 이분산시계열과 결합된 딥러닝 모형은 다른 금융시계열 데이터에 응용하여 그 일반화 가능성을 높일 수 있을 것이다.The People’s Republic of China has vigorously been pursuing the internationalization of the Chinese Yuan or Renminbi after the financial crisis of 2008. In this view, an abrupt increase of use of the Chinese Yuan in the onshore and offshore markets are important milestones to be one of important currencies. One of the most frequently used methods to forecast volatility is GARCH model. Since a prediction error of the GARCH model has been reported quite high, a lot of efforts have been made to improve forecasting capability of the GARCH model. In this paper, we have proposed MLPGARCH and a DL-GARCH by employing Artificial Neural Network to the GARCH. In an application to forecasting Chinese Yuan volatility, we have successfully shown their overall outperformance in forecasting over the GARCH.Deep learning, DL-GARCH, GARCH, The Chinese Yuan, volatility."
863,딥러닝 기법을 이용한 내일강수 예측,하지훈,2016,한국지능시스템학회,"정확한 강수예측을 위해서는 예측인자 선정과 예측방법에 대한 선택이 매우 중요하다. 최근에는 강수예측 방법으로기계학습 기법이 많이 사용되고 있으며, 그 중에서도 특히 인공신경망을 사용한 강수예측 방법은 좋은 성능을 보였다. 본논문에서는 딥러닝 기법 중 하나인 DBN(deep belief network)를 이용한 새로운 강수예측 방법을 제안한다. DBN는 비지도사전 학습을 통해 초기 가중치를 설정하여 기존 인공신경망의 문제점을 보완한다. 예측인자로는 기온, 전일-전주 강수일, 태양과 달 궤도 관련 자료를 선정하였다. 기온과 전일-전주 강수일은 서울에서의 1974년부터 2013년까지 총 40년간의AWS(automatic weather system) 관측 자료를 사용하였고, 태양과 달의 궤도 관련 자료는 서울을 중심으로 계산한 결과를사용하였다. 전체 기간에서 일부는 학습 자료로 사용하여 예측모델을 생성하였고, 나머지를 생성한 모델의 검증 자료로사용하였다. 모델 검증 결과로 나온 예측값들은 확률값을 가지며 임계치를 이용하여 강수유무를 판별하였다. 강수 정확도의척도로 양분예보기법 중 CSI(critical successive index)와 Bias(frequency bias)를 계산하였다. 이를 통해 DBN와 MLP(multilayer perceptron)의 성능을 비교한 결과 DBN의 강수 예측 정확도가 높았고, 수행속도 또한 2배 이상 빨랐다.For accurate precipitation forecasts the choice of weather faoctrs and prediction method is very important. Recently, machinele arning has been widely used for forecasting precipitation, and artificial neural network, one of machine learning techniques, showed good performance. In this paper, we suggest a new method for forecasting precipitation using DBN, one of deep learning techniques.
DBN has an advantage that initial weights are set by unsupervised learning, so this compensates for the defects of artificial neural networks. We used past precipitation, temperature, and the parameters of the sun and moon's motion as features for forecasting precipitation. The dataset consists of observation data which had been measured for 40 years from AWS in Seoul. Experiments were based on 8-fold cross validation. As a result of estimation, we got probabilities of test dataset, so threshold was used for the decision of precipitation. CSI and Bias were used for indicating the precision of precipitation. Our experimental results showed that DBN performed better than MLP.Deep learning, Deep belief network, Precipitation, Forecast"
864,딥러닝 신경망모형을 이용한 실시간 교통정보수집 연구,김대현 /KIM DAE HYON,2016,사단법인 인문사회과학기술융합학회,"비디오 기반의 실시간 교통정보수집 시스템은 자율주행을 포함한 지능형교통체계의 주요 서브시스템중 하나이며, 우수한 예측(추정)력 때문에 핵심 알고리즘으로 인공신경망 모형이 주로 사용되고 있다. 특히, 최근 과학 학술지 네이처에 구글 딥마인드의 알파고가 공개되면서 딥러닝과 인공지능에 대한 전 세계적인 관심이 집중되고 있다. 최근까지 가장 널리 사용된 인공신경망모형은 역전파 모형이었으나 최근 컴퓨터 하드웨어 및 알고리즘의 새로운 개발과 더불어 딥러닝 알고리즘을 이용한 딥인공신경망 모형에 대한 관심이 높아지고 있다. 그러나 아직 딥러닝 인공신경망에 대한 연구는 초기단계에 있는 실정이며 적용분야도 극히 한정되어 있다. 본 연구에서는 실시간 도로교통정보 수집을 위한 영상처리분야에서 딥러닝 알고리즘을 활용하고 실험결과를 통하여 향후 교통정보수집‧분석 분야에서의 딥러닝 인공신경망 모형의 적용가능성을 살펴보고자 한다. 특히, 본 연구에서 도출한 딥러닝 인공신경망모형의 실험결과를 기존 역전파 모형에 의한 실험결과와 비교하여 모형의 우수성을 비교‧평가하고자 한다.Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Normally, artificial neural networks have been used in real-time applications for collecting traffic information because of their remarkable performance in terms of predictive accuracy and real-time computing. Especially, the recent publication of a paper in the journal Nature and the success of Google DeepMind’s world-beating Go program AlphaGo caused significant increases of the public interest in artificial intelligence. Even though various types of learning models have been proposed, the Backpropagation has been the most popular neural network model. However, the Deep learning neural network model has attracted much interest recently. In this study, the Deep leaning neural network will be applied in image processing techniques for the real-time traffic information collection to investigate the performance of Deep learning algorithm in traffic engineering fields. In addition, the experimental results from Deep learing algorithm will be compared with the results from Backpropagation model to evaluate and propose better performing model.Intelligent Transportation Systems (ITS), artificial neural networks, Deep learning, traffic information, image processing"
865,딥러닝의 모형과 응용사례,안성만 /SungMahn Ahn,2016,한국지능정보시스템학회,"딥러닝은 인공신경망(neural network)이라는 인공지능분야의 모형이 발전된 형태로서, 계층구조로 이루어진 인공신경망의 내부계층(hidden layer)이 여러 단계로 이루어진 구조이다. 딥러닝에서의 주요 모형은 합성곱신경망(convolutional neural network), 순환신경망(recurrent neural network), 그리고 심층신뢰신경망(deep belief network)의 세가지라고 할 수있다. 그 중에서 현재 흥미로운 연구가 많이 발표되어서 관심이 집중되고 있는 모형은 지도학습(supervised learning)모형인 처음 두 개의 모형이다. 따라서 본 논문에서는 지도학습모형의 가중치를 최적화하는 기본적인 방법인 오류역전파 알고리즘을 살펴본 뒤에 합성곱신경망과 순환신경망의 구조와 응용사례 등을 살펴보고자 한다. 본문에서 다루지 않은 모형인 심층신뢰신경망은 아직까지는 합성곱신경망 이나 순환신경망보다는 상대적으로 주목을 덜 받고 있다. 그러나 심층신뢰신경망은 CNN이나 RNN과는 달리 비지도학습(unsupervised learning)모형이며, 사람이나 동물은 관찰을 통해서 스스로 학습한다는 점에서 궁극적으로는 비지도학습모형이 더 많이 연구되어야 할 주제가 될 것이다.Deep learning model is a kind of neural networks that allows multiple hidden layers. There are various deep learning architectures such as convolutional neural networks, deep belief networks and recurrent neural networks. Those have been applied to fields like computer vision, automatic speech recognition, natural language processing, audio recognition and bioinformatics where they have been shown to produce state-of-the-art results on various tasks. Among those architectures, convolutional neural networks and recurrent neural networks are classified as the supervised learning model. And in recent years, those supervised learning models have gained more popularity than unsupervised learning models such as deep belief networks, because supervised learning models have shown fashionable applications in such fields mentioned above.
Deep learning models can be trained with backpropagation algorithm. Backpropagation is an abbreviation for “backward propagation of errors” and a common method of training artificial neural networks used in conjunction with an optimization method such as gradient descent. The method calculates the gradient of an error function with respect to all the weights in the network. The gradient is fed to the optimization method which in turn uses it to update the weights, in an attempt to minimize the error function.
Convolutional neural networks use a special architecture which is particularly well-adapted to classify images. Using this architecture makes convolutional networks fast to train. This, in turn, helps us train deep, muti-layer networks, which are very good at classifying images. These days, deep convolutional networks are used in most neural networks for image recognition. Convolutional neural networks use three basic ideas: local receptive fields, shared weights, and pooling. By local receptive fields, we mean that each neuron in the first(or any) hidden layer will be connected to a small region of the input(or previous layer’s) neurons. Shared weights mean that we’re going to use the same weights and bias for each of the local receptive field. This means that all the neurons in the hidden layer detect exactly the same feature, just at different locations in the input image. In addition to the convolutional layers just described, convolutional neural networks also contain pooling layers. Pooling layers are usually used immediately after convolutional layers. What the pooling layers do is to simplify the information in the output from the convolutional layer.
Recent convolutional network architectures have 10 to 20 hidden layers and billions of connections between units. Training deep learning networks has taken weeks several years ago, but thanks to progress in GPU and algorithm enhancement, training time has reduced to several hours.
Neural networks with time-varying behavior are known as recurrent neural networks or RNNs. A recurrent neural network is a class of artificial neural network where connections between units form a directed cycle. This creates an internal state of the network which allows it to exhibit dynamic temporal behavior. Unlike feedforward neural networks, RNNs can use their internal memory to process arbitrary sequences of inputs. Early RNN models turned out to be very difficult to train, harder even than deep feedforward networks. The reason is the unstable gradient problem such as vanishing gradient and exploding gradient. The gradient can get smaller and smaller as it is propagated back through layers. This makes learning in early layers extremely slow. The problem actually gets worse in RNNs, since gradients aren’t just propagated backward through layers, they’re propagated backward through time. If the network runs for a long time, that can make the gradient extremely unstable and hard to learn from. It has been possible to incorporate an idea known as long short-term memory units (LSTMs) into RNNs. LSTMs make it much easier to get good results when training RNNs, and many recent papers make use of LSTMs or related ideas.Deep learning, Convolutional neural networks, Recurrent neural networks, Error backpropagation algorithm"
866,딥러닝분석과 기술적 분석 지표를 이용한 한국 코스피주가지수 방향성 예측,이우식 /Lee Woosik,2017,한국데이터정보과학회,"2016년 3월 구글 (Google)의 바둑인공지능 알파고 (AlphaGo)가 이세돌 9단과의 바둑대결에서승리한 이후 다양한 분야에서 인공지능 사용에 대한 관심이 높아지고 있는 가운데 금융투자 분야에서도 인공지능과 투자자문 전문가의 합성어인 로보어드바이저 (Robo-Advisor)에 대한 관심이 높아지고 있다. 인공지능 (artificial intelligence)기반의 의사결정은 비용 절감은 물론 효과적인 의사결정을가능하게 한다는 점에서 큰 장점이 있다. 본 연구에서는 기술적 분석 (technical analysis) 지표와 딥러닝 (deep learning) 모형을 결합하여 한국 코스피 지수를 예측하는 모형을 개발하고 제시한 모형들의 예측력을 비교, 분석한다. 분석 결과 기술적 분석 지표에 딥러닝 알고리즘을 결합한 모형이 주가지수 방향성 예측 문제에 응용될 수 있음을 확인하였다. 향후 본 연구에서 제안된 기술적 분석 지표와딥러닝모형을 결합한 기법은 로보어드바이저서비스에 응용할 수 있는 일반화 가능성을 보여준다Since Google’s AlphaGo defeated a world champion of Go players in 2016, there have been many interests in the deep learning. In the financial sector, a Robo-Advisor using deep learning gains a significant attention, which builds and manages portfolios of financial instruments for investors.In this paper, we have proposed the a deep learning algorithm geared toward identification and forecast of the KOSPI index direction,and we also have compared the accuracy of the prediction.In an application of forecasting the financial market index direction, we have shown that the Robo-Advisor using deep learning has a significant effect on finance industry. The Robo-Advisor collects a massive data such as earnings statements, news reports and regulatory filings, analyzes those and recommends investors how to view market trends and identify the best time to purchase financial assets. On the other hand, the Robo-Advisor allows businesses to learn more about their customers, develop better marketing strategies, increase sales and decrease costs.Artificial intelligence, deep learning, FinTech, Robo-Advisor, technical analysis."
867,딥러닝의 효율적인 학습을 위한 학습데이터 선별 기법에 관한 연구,주경돈 /Gyoung-don Joo,2017,한국정보과학회,"빅데이터의 증가와 병렬 하드웨어 처리, 학습 알고리즘의 개선은 딥러닝의 발전을 가져왔으며 인공지능이 적용 가능한 영역을 획기적으로 확장시키고 있다. 하지만 이러한 대규모의 학습 데이터에 기반한 딥러닝은 학습시간이 증가되고, 더 나아가 학습 데이터의 구축을 위한 레이블링(labeling) 비용이 데이터 수에 비례하여 증가하게 된다. 따라서 본 논문에서는, 딥러닝의 정확도를 최대한 유지하면서 필요한 학습 데이터의 수를 줄일수 있도록, 전체 데이터에서 제한된 수의 학습 데이터의 후보를 선별하는 기법에 대하여 연구한다. 특히 기존의 능동 학습에 기반한 기법들이 학습 데이터의 수가 매우 적어질 경우 딥러닝의 정확도가 급격히 떨어지는 문제를 해결할 수 있는 새로운 기법을 제안하였으며, 제안된 기법으로 선별된 학습 데이터는 그 수의 변화에 대해 딥러닝이 일관되게 경쟁력있는 정확도를 가짐을 다양한 실험을 통하여 검증하였다.The increase of big data, parallel hardware processing and enhanced learning algorithms have led to the development of deep learning and are dramatically expanding the domain of applications based on artificial intelligence. However, such large-scale training data extends the learning time, and furthermore, the labeling cost for building the training set increases in proportion to the volume of data. Therefore, in this paper, we study how to select a limited number of candidates for training set from given data so as to reduce the number of training set while maintaining the accuracy of deep learning. Especially, we found that, with the training set selected by the previous work, the accuracy of deep learning drastically decreases when the number of training set is very small and so proposed new methods to resolve this fall-off in accuracy. Through experiments, we showed that deep learning consistently has the competitive accuracy with the training set selected by the proposed method regardless of the size of training set.Deep Learning, Train Set Selection, Active Learning, Gamma distribution"
868,딥러닝을 이용한 범용적 스테그아날리시스,김현재 /Hyunjae Kim,2017,한국정보과학회,"스테그아날리시스(Steganalysis)란 이미지 등 일반적인 자료에 암호화된 정보를 은닉하는 스테가노그래피(Steganography)에 대한 검출 및 분석 방법으로, 기계학습 기반 방법론을 포함한다. 기존 기계학습 기반 스테그아날리시스는 영상(Image)의 특징(Feature) 추출 및 모델링에 기반하며, 최근 딥러닝(Deep Learning)의 적용으로 검출 정확도가 큰 폭으로 향상되었다. 하지만 현존하는 스테그아날리시스 모델은 단일 스테가노그래피 기법에 대해 국한되어 있어 학습에 사용되지 않은 스테고(Stego) 이미지의 경우 검출이 불가능한 결정적 한계를 가진다. 본 연구에서는 다양한 스테가노그래피 기법으로 생성된 스테고이미지에 딥러닝을 적용하여 스테그아날리시스를 학습하는 범용적 모델을 제안한다. 다양한 실험을 통해 제안 기법의 효용성 및 가능성을 확인하고, 범용적 스테그아날리시스 모델이 각각에 특화된 검출 기법과 유사한 정확도로 스테고 이미지를 검출할 수 있음을 보인다.Steganalysis is to detect information hidden by steganography inside general data such as images. There are stegoanalysis techniques that use machine learning (ML). Existing ML approaches to steganalysis are based on extracting features from stego images and modeling them.
Recently deep learning-based methodologies have shown significant improvements in detection accuracy. However, all the existing methods, including deep learning-based ones, have a critical limitation in that they can only detect stego images that are created by a specific steganography method. In this paper, we propose a generalized steganalysis method that can model multiple types of stego images using deep learning. Through various experiments, we confirm the effectiveness of our approach and envision directions for future research. In particular, we show that our method can detect each type of steganography with the same level of accuracy as that of a steganalysis method dedicated to that type of steganography, thereby demonstrating the general applicability of our approach to multiple types of stego images.steganalysis, steganography, deep learning, generalized steganalysis model"
869,"딥러닝 프레임워크의 비교: 티아노, 텐서플로, CNTK를 중심으로",정여진 /Yeojin Chung,2017,한국지능정보시스템학회,"딥러닝 프레임워크의 대표적인 기능으로는 ‘자동미분’과 ‘GPU의 활용’ 등을 들 수 있다. 본 논문은 파이썬의 라이브러리 형태로 사용 가능한 프레임워크 중에서 구글의 텐서플로와 마이크로소프트의 CNTK, 그리고 텐서 플로의 원조라고 할 수 있는 티아노를 비교하였다. 본문에서는 자동미분의 개념과 GPU의 활용형태를 간단히 설명하고, 그 다음에 logistic regression을 실행하는 예를 통하여 각 프레임워크의 문법을 알아본 뒤에, 마지막으로 대표적인 딥러닝 응용인 CNN의 예제를 실행시켜보고 코딩의 편의성과 실행속도 등을 확인해 보았다. 그 결과, 편의성의 관점에서 보면 티아노가 가장 코딩 하기가 어렵고, CNTK와 텐서플로는 많은 부분이 비슷하게 추상화 되어 있어서 코딩이 비슷하지만 가중치와 편향을 직접 정의하느냐의 여부에서 차이를 보였다. 그리고 각프레임워크의 실행속도에 대한 평가는 ‘큰 차이는 없다’는 것이다. 텐서플로는 티아노에 비하여 속도가 느리다는 평가가 있어왔는데, 본 연구의 실험에 의하면, 비록 CNN 모형에 국한되었지만, 텐서플로가 아주 조금이지만빠른 것으로 나타났다. CNTK의 경우에도, 비록 실험환경이 달랐지만, 실험환경의 차이에 의한 속도의 차이의 편차범위 이내에 있는 것으로 판단이 되었다. 본 연구에서는 세 종류의 딥러닝 프레임워크만을 살펴보았는데,위키피디아에 따르면 딥러닝 프레임워크의 종류는 12가지가 있으며, 각 프레임워크의 특징을 15가지 속성으로구분하여 차이를 특정하고 있다. 그 많은 속성 중에서 사용자의 입장에서 볼 때 중요한 속성은 어떤 언어(파이썬, C++, Java, 등)로 사용가능한지, 어떤 딥러닝 모형에 대한 라이브러리가 잘 구현되어 있는지 등일 것이다. 그리고 사용자가 대규모의 딥러닝 모형을 구축한다면, 다중 GPU 혹은 다중 서버를 지원하는지의 여부도 중요할 것이다. 또한 딥러닝 모형을 처음 학습하는 경우에는 사용설명서가 많은지 예제 프로그램이 많은지 여부도 중요한 기준이 될 것이다.The deep learning framework is software designed to help develop deep learning models. Some of its important functions include “automatic differentiation” and “utilization of GPU”. The list of popular deep learning framework includes Caffe (BVLC) and Theano (University of Montreal). And recently, Microsoft's deep learning framework, Microsoft Cognitive Toolkit, was released as open-source license, following Google’s Tensorflow a year earlier. The early deep learning frameworks have been developed mainly for research at universities. Beginning with the inception of Tensorflow, however, it seems that companies such as Microsoft and Facebook have started to join the competition of framework development. Given the trend, Google and other companies are expected to continue investing in the deep learning framework to bring forward the initiative in the artificial intelligence business. From this point of view, we think it is a good time to compare some of deep learning frameworks. So we compare three deep learning frameworks which can be used as a Python library. Those are Google's Tensorflow, Microsoft’s CNTK, and Theano which is sort of a predecessor of the preceding two.
The most common and important function of deep learning frameworks is the ability to perform automatic differentiation. Basically all the mathematical expressions of deep learning models can be represented as computational graphs, which consist of nodes and edges. Partial derivatives on each edge of a computational graph can then be obtained. With the partial derivatives, we can let software compute differentiation of any node with respect to any variable by utilizing chain rule of Calculus.
First of all, the convenience of coding is in the order of CNTK, Tensorflow, and Theano. The criterion is simply based on the lengths of the codes and the learning curve and the ease of coding are not the main concern. According to the criteria, Theano was the most difficult to implement with, and CNTK and Tensorflow were somewhat easier. With Tensorflow, we need to define weight variables and biases explicitly. The reason that CNTK and Tensorflow are easier to implement with is that those frameworks provide us with more abstraction than Theano. We, however, need to mention that low-level coding is not always bad. It gives us flexibility of coding. With the low-level coding such as in Theano, we can implement and test any new deep learning models or any new search methods that we can think of.
The assessment of the execution speed of each framework is that there is not meaningful difference. According to the experiment, execution speeds of Theano and Tensorflow are very similar, although the experiment was limited to a CNN model. In the case of CNTK, the experimental environment was not maintained as the same. The code written in CNTK has to be run in PC environment without GPU where codes execute as much as 50 times slower than with GPU. But we concluded that the difference of execution speed was within the range of variation caused by the different hardware setup.
In this study, we compared three types of deep learning framework: Theano, Tensorflow, and CNTK. According to Wikipedia, there are 12 available deep learning frameworks. And 15 different attributes differentiate each framework. Some of the important attributes would include interface language (Python, C ++, Java, etc.) and the availability of libraries on various deep learning models such as CNN, RNN, DBN, and etc. And if a user implements a large scale deep learning model, it will also be important to support multiple GPU or multiple servers. Also, if you are learning the deep learning model, it would also be important if there are enough examples and references.deep learning framework, Theano, TensorFlow, CNTK, computational graph, CIFAR-10"
870,딥러닝을 이용한 주변 무선단말 파악방안,이웅섭 /Lee， Woong-sup,2018,한국정보통신학회,"최근 단말-대-단말(Device-to-device, D2D) 통신기술이 차세대 무선통신시스템의 핵심기술로 큰 관심을 받고 있다. 이러한 단말간 통신에서는 자신의 주변에 어떠한 단말이 있는지 파악하는 주변단말 탐색(Neighbor discovery)이매우 중요하다. 본 논문에서는 최근 큰 관심을 받고 있는 딥러닝(Deep learning) 기술을 활용하여 단말간 통신에서 주변단말을 파악하는 방안에 대해서 제안한다. 제안 방안은 기존의 방안과 달리 무선채널의 공간적 연관성을 이용하여단말간의 신호 전송 없이 단말이 기지국으로 전송하는 상향링크 파일럿 신호를 기반으로 주변 단말을 찾고 따라서기존의 방식에 비해 신호전송 복잡도(signaling complexity)를 크게 줄일 수 있다. 또한 제안 방안에서는 떨어져 있는거리에 따라서 주변 단말을 분류 가능하여 기존 방안에 비해서 좀 더 세밀한 단말 탐색이 가능하다. 마지막으로 본 논문에서는 tensorflow를 이용한 컴퓨터 시뮬레이션을 통해 제안 방안의 성능을 검증하였다.Recently, the device-to-device (D2D) communication has been conceived as the key technology for the next-generationmobile communication systems. The neighbor discovery in which the nearby users are found, is essential for the properoperation of the D2D communication. In this paper, we propose new neighbor discovery scheme based on deep learningtechnology which has gained a lot of attention recently. In the proposed scheme, the neighboring users can be foundusing the uplink pilot transmission of users only, unlike conventional neighbor discovery schemes in which direct pilotcommunication among users is required, such that the signaling overhead can be greatly reduced in our proposed scheme.
Moreover, the neighbors with different proximity can also be classified accordingly which enables more accurate neighbordiscovery compared to the conventional schemes. The performance of our proposed scheme is verified through thetensorflow-based computer simulations.
키워드 : 딥러닝, 주변단말 탐색, 단말-대-단말 통신Deep learning, Neighbor discovery, Device-to-device communication, Wireless channel correlation"
871,신뢰성있는 딥러닝 기반 분석 모델을 참조하기 위한 딥러닝 기술 언어,문종혁 /Jong Hyeok Mun,2021,한국정보처리학회,"최근 딥러닝은 하드웨어 성능이 향상됨에 따라 자연어 처리, 영상 인식 등의 다양한 기술에 접목되어 활용되고 있다. 이러한 기술들을 활용해 지능형교통 시스템(ITS), 스마트홈, 헬스케어 등의 산업분야에서 데이터를 분석하여 고속도로 속도위반 차량 검출, 에너지 사용량 제어, 응급상황 등과 같은고품질의 서비스를 제공하며, 고품질의 서비스를 제공하기 위해서는 정확도가 향상된 딥러닝 모델이 적용되어야 한다. 이를 위해 서비스 환경의 데이터를분석하기 위한 딥러닝 모델을 개발할 때, 개발자는 신뢰성이 검증된 최신의 딥러닝 모델을 적용할 수 있어야 한다. 이는 개발자가 참조하는 딥러닝 모델에적용된 학습 데이터셋의 정확도를 측정하여 검증할 수 있다. 이러한 검증을 위해서 개발자는 학습 데이터셋, 딥러닝의 계층구조 및 개발 환경 등과 같은내용을 포함하는 딥러닝 모델을 문서화하여 적용하기 위한 구조적인 정보가 필요하다. 본 논문에서는 신뢰성있는 딥러닝 기반 데이터 분석 모델을 참조하기위한 딥러닝 기술 언어를 제안한다. 제안하는 기술 언어는 신뢰성 있는 딥러닝 모델을 개발하는데 필요한 학습데이터셋, 개발 환경 및 설정 등의 정보와더불어 딥러닝 모델의 계층구조를 표현할 수 있다. 제안하는 딥러닝 기술 언어를 이용하여 개발자는 지능형 교통 시스템에서 참조하는 분석 모델의정확도를 검증할 수 있다. 실험에서는 제안하는 언어의 유효성을 검증하기 위해, 번호판 인식 모델을 중심으로 딥러닝 기술 문서의 적용과정을 보인다With the recent advancements of deep learning, companies such as smart home, healthcare, and intelligent transportation systems areutilizing its functionality to provide high-quality services for vehicle detection, emergency situation detection, and controlling energyconsumption. To provide reliable services in such sensitive systems, deep learning models are required to have high accuracy. In orderto develop a deep learning model for analyzing previously mentioned services, developers should utilize the state of the art deep learningmodels that have already been verified for higher accuracy. The developers can verify the accuracy of the referenced model by validatingthe model on the dataset. For this validation, the developer needs structural information to document and apply deep learning models,including metadata such as learning dataset, network architecture, and development environments. In this paper, we propose a descriptionlanguage that represents the network architecture of the deep learning model along with its metadata that are necessary to develop adeep learning model. Through the proposed description language, developers can easily verify the accuracy of the referenced deep learningmodel. Our experiments demonstrate the application scenario of a deep learning description document that focuses on the license platerecognition for the detection of illegally parked vehicles.Trusted Deep Learning, Model Reference, Deep Learning Description Language, Traffic Situation Analysis Model"
872,딥러닝 분석을 이용한 중국 역내·외 위안화 변동성 예측,이우식 /Lee Woosik,2016,한국데이터정보과학회,"2008년 글로벌 금융위기 이후 중국은 위안화 국제화의 점진적 추진을 진행하면서 중국상하이 외환시장과 중국홍콩 외환시장에서 거래되는 통화인 역내위안화와 역외위안화를 형성시켰다. 본 연구는 위안화 국제화와 점진적인 중국 자본계정 개방에 따라 급변하는 외환시장상황의 변동성을 정확하게 파악하기 위해서 GARCH모형 (일반화된 자기회귀 조건부이분산성모형)에 다단계인공신경망을 결합한 MLP-GARCH 모형과 GARCH모형과 기계학습의 일종인 딥러닝 (deep learning)을 통합한 DL-GARCH을 가지고 위안화 변동성예측을 비교 실험과 분석을 하였다. 비교분석 결과 DLGARCH 모형은 MLP-GARCH보다 모형 위안화 역내·외 환율변동성 예측 면에서 더욱 더 개선된예측값을 제공하였다. 그래서 이분산시계열모형을 딥러닝과 결합한 DL-GARCH 모형은 시계열의환율 변동성 예측 문제에 딥러닝을 응용할 수 있음을 확인하였다. 향후 이분산시계열과 결합된 딥러닝 모형은 다른 금융시계열 데이터에 응용하여 그 일반화 가능성을 높일 수 있을 것이다.The People’s Republic of China has vigorously been pursuing the internationalization of the Chinese Yuan or Renminbi after the financial crisis of 2008. In this view, an abrupt increase of use of the Chinese Yuan in the onshore and offshore markets are important milestones to be one of important currencies. One of the most frequently used methods to forecast volatility is GARCH model. Since a prediction error of the GARCH model has been reported quite high, a lot of efforts have been made to improve forecasting capability of the GARCH model. In this paper, we have proposed MLPGARCH and a DL-GARCH by employing Artificial Neural Network to the GARCH. In an application to forecasting Chinese Yuan volatility, we have successfully shown their overall outperformance in forecasting over the GARCH.Deep learning, DL-GARCH, GARCH, The Chinese Yuan, volatility."
873,딥러닝 기법을 이용한 내일강수 예측,하지훈,2016,한국지능시스템학회,"정확한 강수예측을 위해서는 예측인자 선정과 예측방법에 대한 선택이 매우 중요하다. 최근에는 강수예측 방법으로기계학습 기법이 많이 사용되고 있으며, 그 중에서도 특히 인공신경망을 사용한 강수예측 방법은 좋은 성능을 보였다. 본논문에서는 딥러닝 기법 중 하나인 DBN(deep belief network)를 이용한 새로운 강수예측 방법을 제안한다. DBN는 비지도사전 학습을 통해 초기 가중치를 설정하여 기존 인공신경망의 문제점을 보완한다. 예측인자로는 기온, 전일-전주 강수일, 태양과 달 궤도 관련 자료를 선정하였다. 기온과 전일-전주 강수일은 서울에서의 1974년부터 2013년까지 총 40년간의AWS(automatic weather system) 관측 자료를 사용하였고, 태양과 달의 궤도 관련 자료는 서울을 중심으로 계산한 결과를사용하였다. 전체 기간에서 일부는 학습 자료로 사용하여 예측모델을 생성하였고, 나머지를 생성한 모델의 검증 자료로사용하였다. 모델 검증 결과로 나온 예측값들은 확률값을 가지며 임계치를 이용하여 강수유무를 판별하였다. 강수 정확도의척도로 양분예보기법 중 CSI(critical successive index)와 Bias(frequency bias)를 계산하였다. 이를 통해 DBN와 MLP(multilayer perceptron)의 성능을 비교한 결과 DBN의 강수 예측 정확도가 높았고, 수행속도 또한 2배 이상 빨랐다.For accurate precipitation forecasts the choice of weather faoctrs and prediction method is very important. Recently, machinele arning has been widely used for forecasting precipitation, and artificial neural network, one of machine learning techniques, showed good performance. In this paper, we suggest a new method for forecasting precipitation using DBN, one of deep learning techniques.
DBN has an advantage that initial weights are set by unsupervised learning, so this compensates for the defects of artificial neural networks. We used past precipitation, temperature, and the parameters of the sun and moon's motion as features for forecasting precipitation. The dataset consists of observation data which had been measured for 40 years from AWS in Seoul. Experiments were based on 8-fold cross validation. As a result of estimation, we got probabilities of test dataset, so threshold was used for the decision of precipitation. CSI and Bias were used for indicating the precision of precipitation. Our experimental results showed that DBN performed better than MLP.Deep learning, Deep belief network, Precipitation, Forecast"
874,딥러닝 신경망모형을 이용한 실시간 교통정보수집 연구,김대현 /KIM DAE HYON,2016,사단법인 인문사회과학기술융합학회,"비디오 기반의 실시간 교통정보수집 시스템은 자율주행을 포함한 지능형교통체계의 주요 서브시스템중 하나이며, 우수한 예측(추정)력 때문에 핵심 알고리즘으로 인공신경망 모형이 주로 사용되고 있다. 특히, 최근 과학 학술지 네이처에 구글 딥마인드의 알파고가 공개되면서 딥러닝과 인공지능에 대한 전 세계적인 관심이 집중되고 있다. 최근까지 가장 널리 사용된 인공신경망모형은 역전파 모형이었으나 최근 컴퓨터 하드웨어 및 알고리즘의 새로운 개발과 더불어 딥러닝 알고리즘을 이용한 딥인공신경망 모형에 대한 관심이 높아지고 있다. 그러나 아직 딥러닝 인공신경망에 대한 연구는 초기단계에 있는 실정이며 적용분야도 극히 한정되어 있다. 본 연구에서는 실시간 도로교통정보 수집을 위한 영상처리분야에서 딥러닝 알고리즘을 활용하고 실험결과를 통하여 향후 교통정보수집‧분석 분야에서의 딥러닝 인공신경망 모형의 적용가능성을 살펴보고자 한다. 특히, 본 연구에서 도출한 딥러닝 인공신경망모형의 실험결과를 기존 역전파 모형에 의한 실험결과와 비교하여 모형의 우수성을 비교‧평가하고자 한다.Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Normally, artificial neural networks have been used in real-time applications for collecting traffic information because of their remarkable performance in terms of predictive accuracy and real-time computing. Especially, the recent publication of a paper in the journal Nature and the success of Google DeepMind’s world-beating Go program AlphaGo caused significant increases of the public interest in artificial intelligence. Even though various types of learning models have been proposed, the Backpropagation has been the most popular neural network model. However, the Deep learning neural network model has attracted much interest recently. In this study, the Deep leaning neural network will be applied in image processing techniques for the real-time traffic information collection to investigate the performance of Deep learning algorithm in traffic engineering fields. In addition, the experimental results from Deep learing algorithm will be compared with the results from Backpropagation model to evaluate and propose better performing model.Intelligent Transportation Systems (ITS), artificial neural networks, Deep learning, traffic information, image processing"
875,딥러닝의 모형과 응용사례,안성만 /SungMahn Ahn,2016,한국지능정보시스템학회,"딥러닝은 인공신경망(neural network)이라는 인공지능분야의 모형이 발전된 형태로서, 계층구조로 이루어진 인공신경망의 내부계층(hidden layer)이 여러 단계로 이루어진 구조이다. 딥러닝에서의 주요 모형은 합성곱신경망(convolutional neural network), 순환신경망(recurrent neural network), 그리고 심층신뢰신경망(deep belief network)의 세가지라고 할 수있다. 그 중에서 현재 흥미로운 연구가 많이 발표되어서 관심이 집중되고 있는 모형은 지도학습(supervised learning)모형인 처음 두 개의 모형이다. 따라서 본 논문에서는 지도학습모형의 가중치를 최적화하는 기본적인 방법인 오류역전파 알고리즘을 살펴본 뒤에 합성곱신경망과 순환신경망의 구조와 응용사례 등을 살펴보고자 한다. 본문에서 다루지 않은 모형인 심층신뢰신경망은 아직까지는 합성곱신경망 이나 순환신경망보다는 상대적으로 주목을 덜 받고 있다. 그러나 심층신뢰신경망은 CNN이나 RNN과는 달리 비지도학습(unsupervised learning)모형이며, 사람이나 동물은 관찰을 통해서 스스로 학습한다는 점에서 궁극적으로는 비지도학습모형이 더 많이 연구되어야 할 주제가 될 것이다.Deep learning model is a kind of neural networks that allows multiple hidden layers. There are various deep learning architectures such as convolutional neural networks, deep belief networks and recurrent neural networks. Those have been applied to fields like computer vision, automatic speech recognition, natural language processing, audio recognition and bioinformatics where they have been shown to produce state-of-the-art results on various tasks. Among those architectures, convolutional neural networks and recurrent neural networks are classified as the supervised learning model. And in recent years, those supervised learning models have gained more popularity than unsupervised learning models such as deep belief networks, because supervised learning models have shown fashionable applications in such fields mentioned above.
Deep learning models can be trained with backpropagation algorithm. Backpropagation is an abbreviation for “backward propagation of errors” and a common method of training artificial neural networks used in conjunction with an optimization method such as gradient descent. The method calculates the gradient of an error function with respect to all the weights in the network. The gradient is fed to the optimization method which in turn uses it to update the weights, in an attempt to minimize the error function.
Convolutional neural networks use a special architecture which is particularly well-adapted to classify images. Using this architecture makes convolutional networks fast to train. This, in turn, helps us train deep, muti-layer networks, which are very good at classifying images. These days, deep convolutional networks are used in most neural networks for image recognition. Convolutional neural networks use three basic ideas: local receptive fields, shared weights, and pooling. By local receptive fields, we mean that each neuron in the first(or any) hidden layer will be connected to a small region of the input(or previous layer’s) neurons. Shared weights mean that we’re going to use the same weights and bias for each of the local receptive field. This means that all the neurons in the hidden layer detect exactly the same feature, just at different locations in the input image. In addition to the convolutional layers just described, convolutional neural networks also contain pooling layers. Pooling layers are usually used immediately after convolutional layers. What the pooling layers do is to simplify the information in the output from the convolutional layer.
Recent convolutional network architectures have 10 to 20 hidden layers and billions of connections between units. Training deep learning networks has taken weeks several years ago, but thanks to progress in GPU and algorithm enhancement, training time has reduced to several hours.
Neural networks with time-varying behavior are known as recurrent neural networks or RNNs. A recurrent neural network is a class of artificial neural network where connections between units form a directed cycle. This creates an internal state of the network which allows it to exhibit dynamic temporal behavior. Unlike feedforward neural networks, RNNs can use their internal memory to process arbitrary sequences of inputs. Early RNN models turned out to be very difficult to train, harder even than deep feedforward networks. The reason is the unstable gradient problem such as vanishing gradient and exploding gradient. The gradient can get smaller and smaller as it is propagated back through layers. This makes learning in early layers extremely slow. The problem actually gets worse in RNNs, since gradients aren’t just propagated backward through layers, they’re propagated backward through time. If the network runs for a long time, that can make the gradient extremely unstable and hard to learn from. It has been possible to incorporate an idea known as long short-term memory units (LSTMs) into RNNs. LSTMs make it much easier to get good results when training RNNs, and many recent papers make use of LSTMs or related ideas.Deep learning, Convolutional neural networks, Recurrent neural networks, Error backpropagation algorithm"
876,딥러닝분석과 기술적 분석 지표를 이용한 한국 코스피주가지수 방향성 예측,이우식 /Lee Woosik,2017,한국데이터정보과학회,"2016년 3월 구글 (Google)의 바둑인공지능 알파고 (AlphaGo)가 이세돌 9단과의 바둑대결에서승리한 이후 다양한 분야에서 인공지능 사용에 대한 관심이 높아지고 있는 가운데 금융투자 분야에서도 인공지능과 투자자문 전문가의 합성어인 로보어드바이저 (Robo-Advisor)에 대한 관심이 높아지고 있다. 인공지능 (artificial intelligence)기반의 의사결정은 비용 절감은 물론 효과적인 의사결정을가능하게 한다는 점에서 큰 장점이 있다. 본 연구에서는 기술적 분석 (technical analysis) 지표와 딥러닝 (deep learning) 모형을 결합하여 한국 코스피 지수를 예측하는 모형을 개발하고 제시한 모형들의 예측력을 비교, 분석한다. 분석 결과 기술적 분석 지표에 딥러닝 알고리즘을 결합한 모형이 주가지수 방향성 예측 문제에 응용될 수 있음을 확인하였다. 향후 본 연구에서 제안된 기술적 분석 지표와딥러닝모형을 결합한 기법은 로보어드바이저서비스에 응용할 수 있는 일반화 가능성을 보여준다Since Google’s AlphaGo defeated a world champion of Go players in 2016, there have been many interests in the deep learning. In the financial sector, a Robo-Advisor using deep learning gains a significant attention, which builds and manages portfolios of financial instruments for investors.In this paper, we have proposed the a deep learning algorithm geared toward identification and forecast of the KOSPI index direction,and we also have compared the accuracy of the prediction.In an application of forecasting the financial market index direction, we have shown that the Robo-Advisor using deep learning has a significant effect on finance industry. The Robo-Advisor collects a massive data such as earnings statements, news reports and regulatory filings, analyzes those and recommends investors how to view market trends and identify the best time to purchase financial assets. On the other hand, the Robo-Advisor allows businesses to learn more about their customers, develop better marketing strategies, increase sales and decrease costs.Artificial intelligence, deep learning, FinTech, Robo-Advisor, technical analysis."
877,딥러닝의 효율적인 학습을 위한 학습데이터 선별 기법에 관한 연구,주경돈 /Gyoung-don Joo,2017,한국정보과학회,"빅데이터의 증가와 병렬 하드웨어 처리, 학습 알고리즘의 개선은 딥러닝의 발전을 가져왔으며 인공지능이 적용 가능한 영역을 획기적으로 확장시키고 있다. 하지만 이러한 대규모의 학습 데이터에 기반한 딥러닝은 학습시간이 증가되고, 더 나아가 학습 데이터의 구축을 위한 레이블링(labeling) 비용이 데이터 수에 비례하여 증가하게 된다. 따라서 본 논문에서는, 딥러닝의 정확도를 최대한 유지하면서 필요한 학습 데이터의 수를 줄일수 있도록, 전체 데이터에서 제한된 수의 학습 데이터의 후보를 선별하는 기법에 대하여 연구한다. 특히 기존의 능동 학습에 기반한 기법들이 학습 데이터의 수가 매우 적어질 경우 딥러닝의 정확도가 급격히 떨어지는 문제를 해결할 수 있는 새로운 기법을 제안하였으며, 제안된 기법으로 선별된 학습 데이터는 그 수의 변화에 대해 딥러닝이 일관되게 경쟁력있는 정확도를 가짐을 다양한 실험을 통하여 검증하였다.The increase of big data, parallel hardware processing and enhanced learning algorithms have led to the development of deep learning and are dramatically expanding the domain of applications based on artificial intelligence. However, such large-scale training data extends the learning time, and furthermore, the labeling cost for building the training set increases in proportion to the volume of data. Therefore, in this paper, we study how to select a limited number of candidates for training set from given data so as to reduce the number of training set while maintaining the accuracy of deep learning. Especially, we found that, with the training set selected by the previous work, the accuracy of deep learning drastically decreases when the number of training set is very small and so proposed new methods to resolve this fall-off in accuracy. Through experiments, we showed that deep learning consistently has the competitive accuracy with the training set selected by the proposed method regardless of the size of training set.Deep Learning, Train Set Selection, Active Learning, Gamma distribution"
878,딥러닝을 이용한 범용적 스테그아날리시스,김현재 /Hyunjae Kim,2017,한국정보과학회,"스테그아날리시스(Steganalysis)란 이미지 등 일반적인 자료에 암호화된 정보를 은닉하는 스테가노그래피(Steganography)에 대한 검출 및 분석 방법으로, 기계학습 기반 방법론을 포함한다. 기존 기계학습 기반 스테그아날리시스는 영상(Image)의 특징(Feature) 추출 및 모델링에 기반하며, 최근 딥러닝(Deep Learning)의 적용으로 검출 정확도가 큰 폭으로 향상되었다. 하지만 현존하는 스테그아날리시스 모델은 단일 스테가노그래피 기법에 대해 국한되어 있어 학습에 사용되지 않은 스테고(Stego) 이미지의 경우 검출이 불가능한 결정적 한계를 가진다. 본 연구에서는 다양한 스테가노그래피 기법으로 생성된 스테고이미지에 딥러닝을 적용하여 스테그아날리시스를 학습하는 범용적 모델을 제안한다. 다양한 실험을 통해 제안 기법의 효용성 및 가능성을 확인하고, 범용적 스테그아날리시스 모델이 각각에 특화된 검출 기법과 유사한 정확도로 스테고 이미지를 검출할 수 있음을 보인다.Steganalysis is to detect information hidden by steganography inside general data such as images. There are stegoanalysis techniques that use machine learning (ML). Existing ML approaches to steganalysis are based on extracting features from stego images and modeling them.
Recently deep learning-based methodologies have shown significant improvements in detection accuracy. However, all the existing methods, including deep learning-based ones, have a critical limitation in that they can only detect stego images that are created by a specific steganography method. In this paper, we propose a generalized steganalysis method that can model multiple types of stego images using deep learning. Through various experiments, we confirm the effectiveness of our approach and envision directions for future research. In particular, we show that our method can detect each type of steganography with the same level of accuracy as that of a steganalysis method dedicated to that type of steganography, thereby demonstrating the general applicability of our approach to multiple types of stego images.steganalysis, steganography, deep learning, generalized steganalysis model"
879,"딥러닝 프레임워크의 비교: 티아노, 텐서플로, CNTK를 중심으로",정여진 /Yeojin Chung,2017,한국지능정보시스템학회,"딥러닝 프레임워크의 대표적인 기능으로는 ‘자동미분’과 ‘GPU의 활용’ 등을 들 수 있다. 본 논문은 파이썬의 라이브러리 형태로 사용 가능한 프레임워크 중에서 구글의 텐서플로와 마이크로소프트의 CNTK, 그리고 텐서 플로의 원조라고 할 수 있는 티아노를 비교하였다. 본문에서는 자동미분의 개념과 GPU의 활용형태를 간단히 설명하고, 그 다음에 logistic regression을 실행하는 예를 통하여 각 프레임워크의 문법을 알아본 뒤에, 마지막으로 대표적인 딥러닝 응용인 CNN의 예제를 실행시켜보고 코딩의 편의성과 실행속도 등을 확인해 보았다. 그 결과, 편의성의 관점에서 보면 티아노가 가장 코딩 하기가 어렵고, CNTK와 텐서플로는 많은 부분이 비슷하게 추상화 되어 있어서 코딩이 비슷하지만 가중치와 편향을 직접 정의하느냐의 여부에서 차이를 보였다. 그리고 각프레임워크의 실행속도에 대한 평가는 ‘큰 차이는 없다’는 것이다. 텐서플로는 티아노에 비하여 속도가 느리다는 평가가 있어왔는데, 본 연구의 실험에 의하면, 비록 CNN 모형에 국한되었지만, 텐서플로가 아주 조금이지만빠른 것으로 나타났다. CNTK의 경우에도, 비록 실험환경이 달랐지만, 실험환경의 차이에 의한 속도의 차이의 편차범위 이내에 있는 것으로 판단이 되었다. 본 연구에서는 세 종류의 딥러닝 프레임워크만을 살펴보았는데,위키피디아에 따르면 딥러닝 프레임워크의 종류는 12가지가 있으며, 각 프레임워크의 특징을 15가지 속성으로구분하여 차이를 특정하고 있다. 그 많은 속성 중에서 사용자의 입장에서 볼 때 중요한 속성은 어떤 언어(파이썬, C++, Java, 등)로 사용가능한지, 어떤 딥러닝 모형에 대한 라이브러리가 잘 구현되어 있는지 등일 것이다. 그리고 사용자가 대규모의 딥러닝 모형을 구축한다면, 다중 GPU 혹은 다중 서버를 지원하는지의 여부도 중요할 것이다. 또한 딥러닝 모형을 처음 학습하는 경우에는 사용설명서가 많은지 예제 프로그램이 많은지 여부도 중요한 기준이 될 것이다.The deep learning framework is software designed to help develop deep learning models. Some of its important functions include “automatic differentiation” and “utilization of GPU”. The list of popular deep learning framework includes Caffe (BVLC) and Theano (University of Montreal). And recently, Microsoft's deep learning framework, Microsoft Cognitive Toolkit, was released as open-source license, following Google’s Tensorflow a year earlier. The early deep learning frameworks have been developed mainly for research at universities. Beginning with the inception of Tensorflow, however, it seems that companies such as Microsoft and Facebook have started to join the competition of framework development. Given the trend, Google and other companies are expected to continue investing in the deep learning framework to bring forward the initiative in the artificial intelligence business. From this point of view, we think it is a good time to compare some of deep learning frameworks. So we compare three deep learning frameworks which can be used as a Python library. Those are Google's Tensorflow, Microsoft’s CNTK, and Theano which is sort of a predecessor of the preceding two.
The most common and important function of deep learning frameworks is the ability to perform automatic differentiation. Basically all the mathematical expressions of deep learning models can be represented as computational graphs, which consist of nodes and edges. Partial derivatives on each edge of a computational graph can then be obtained. With the partial derivatives, we can let software compute differentiation of any node with respect to any variable by utilizing chain rule of Calculus.
First of all, the convenience of coding is in the order of CNTK, Tensorflow, and Theano. The criterion is simply based on the lengths of the codes and the learning curve and the ease of coding are not the main concern. According to the criteria, Theano was the most difficult to implement with, and CNTK and Tensorflow were somewhat easier. With Tensorflow, we need to define weight variables and biases explicitly. The reason that CNTK and Tensorflow are easier to implement with is that those frameworks provide us with more abstraction than Theano. We, however, need to mention that low-level coding is not always bad. It gives us flexibility of coding. With the low-level coding such as in Theano, we can implement and test any new deep learning models or any new search methods that we can think of.
The assessment of the execution speed of each framework is that there is not meaningful difference. According to the experiment, execution speeds of Theano and Tensorflow are very similar, although the experiment was limited to a CNN model. In the case of CNTK, the experimental environment was not maintained as the same. The code written in CNTK has to be run in PC environment without GPU where codes execute as much as 50 times slower than with GPU. But we concluded that the difference of execution speed was within the range of variation caused by the different hardware setup.
In this study, we compared three types of deep learning framework: Theano, Tensorflow, and CNTK. According to Wikipedia, there are 12 available deep learning frameworks. And 15 different attributes differentiate each framework. Some of the important attributes would include interface language (Python, C ++, Java, etc.) and the availability of libraries on various deep learning models such as CNN, RNN, DBN, and etc. And if a user implements a large scale deep learning model, it will also be important to support multiple GPU or multiple servers. Also, if you are learning the deep learning model, it would also be important if there are enough examples and references.deep learning framework, Theano, TensorFlow, CNTK, computational graph, CIFAR-10"
880,딥러닝을 이용한 주변 무선단말 파악방안,이웅섭 /Lee， Woong-sup,2018,한국정보통신학회,"최근 단말-대-단말(Device-to-device, D2D) 통신기술이 차세대 무선통신시스템의 핵심기술로 큰 관심을 받고 있다. 이러한 단말간 통신에서는 자신의 주변에 어떠한 단말이 있는지 파악하는 주변단말 탐색(Neighbor discovery)이매우 중요하다. 본 논문에서는 최근 큰 관심을 받고 있는 딥러닝(Deep learning) 기술을 활용하여 단말간 통신에서 주변단말을 파악하는 방안에 대해서 제안한다. 제안 방안은 기존의 방안과 달리 무선채널의 공간적 연관성을 이용하여단말간의 신호 전송 없이 단말이 기지국으로 전송하는 상향링크 파일럿 신호를 기반으로 주변 단말을 찾고 따라서기존의 방식에 비해 신호전송 복잡도(signaling complexity)를 크게 줄일 수 있다. 또한 제안 방안에서는 떨어져 있는거리에 따라서 주변 단말을 분류 가능하여 기존 방안에 비해서 좀 더 세밀한 단말 탐색이 가능하다. 마지막으로 본 논문에서는 tensorflow를 이용한 컴퓨터 시뮬레이션을 통해 제안 방안의 성능을 검증하였다.Recently, the device-to-device (D2D) communication has been conceived as the key technology for the next-generationmobile communication systems. The neighbor discovery in which the nearby users are found, is essential for the properoperation of the D2D communication. In this paper, we propose new neighbor discovery scheme based on deep learningtechnology which has gained a lot of attention recently. In the proposed scheme, the neighboring users can be foundusing the uplink pilot transmission of users only, unlike conventional neighbor discovery schemes in which direct pilotcommunication among users is required, such that the signaling overhead can be greatly reduced in our proposed scheme.
Moreover, the neighbors with different proximity can also be classified accordingly which enables more accurate neighbordiscovery compared to the conventional schemes. The performance of our proposed scheme is verified through thetensorflow-based computer simulations.
키워드 : 딥러닝, 주변단말 탐색, 단말-대-단말 통신Deep learning, Neighbor discovery, Device-to-device communication, Wireless channel correlation"
881,신뢰성있는 딥러닝 기반 분석 모델을 참조하기 위한 딥러닝 기술 언어,문종혁 /Jong Hyeok Mun,2021,한국정보처리학회,"최근 딥러닝은 하드웨어 성능이 향상됨에 따라 자연어 처리, 영상 인식 등의 다양한 기술에 접목되어 활용되고 있다. 이러한 기술들을 활용해 지능형교통 시스템(ITS), 스마트홈, 헬스케어 등의 산업분야에서 데이터를 분석하여 고속도로 속도위반 차량 검출, 에너지 사용량 제어, 응급상황 등과 같은고품질의 서비스를 제공하며, 고품질의 서비스를 제공하기 위해서는 정확도가 향상된 딥러닝 모델이 적용되어야 한다. 이를 위해 서비스 환경의 데이터를분석하기 위한 딥러닝 모델을 개발할 때, 개발자는 신뢰성이 검증된 최신의 딥러닝 모델을 적용할 수 있어야 한다. 이는 개발자가 참조하는 딥러닝 모델에적용된 학습 데이터셋의 정확도를 측정하여 검증할 수 있다. 이러한 검증을 위해서 개발자는 학습 데이터셋, 딥러닝의 계층구조 및 개발 환경 등과 같은내용을 포함하는 딥러닝 모델을 문서화하여 적용하기 위한 구조적인 정보가 필요하다. 본 논문에서는 신뢰성있는 딥러닝 기반 데이터 분석 모델을 참조하기위한 딥러닝 기술 언어를 제안한다. 제안하는 기술 언어는 신뢰성 있는 딥러닝 모델을 개발하는데 필요한 학습데이터셋, 개발 환경 및 설정 등의 정보와더불어 딥러닝 모델의 계층구조를 표현할 수 있다. 제안하는 딥러닝 기술 언어를 이용하여 개발자는 지능형 교통 시스템에서 참조하는 분석 모델의정확도를 검증할 수 있다. 실험에서는 제안하는 언어의 유효성을 검증하기 위해, 번호판 인식 모델을 중심으로 딥러닝 기술 문서의 적용과정을 보인다With the recent advancements of deep learning, companies such as smart home, healthcare, and intelligent transportation systems areutilizing its functionality to provide high-quality services for vehicle detection, emergency situation detection, and controlling energyconsumption. To provide reliable services in such sensitive systems, deep learning models are required to have high accuracy. In orderto develop a deep learning model for analyzing previously mentioned services, developers should utilize the state of the art deep learningmodels that have already been verified for higher accuracy. The developers can verify the accuracy of the referenced model by validatingthe model on the dataset. For this validation, the developer needs structural information to document and apply deep learning models,including metadata such as learning dataset, network architecture, and development environments. In this paper, we propose a descriptionlanguage that represents the network architecture of the deep learning model along with its metadata that are necessary to develop adeep learning model. Through the proposed description language, developers can easily verify the accuracy of the referenced deep learningmodel. Our experiments demonstrate the application scenario of a deep learning description document that focuses on the license platerecognition for the detection of illegally parked vehicles.Trusted Deep Learning, Model Reference, Deep Learning Description Language, Traffic Situation Analysis Model"
882,딥러닝 분석을 이용한 중국 역내·외 위안화 변동성 예측,이우식 /Lee Woosik,2016,한국데이터정보과학회,"2008년 글로벌 금융위기 이후 중국은 위안화 국제화의 점진적 추진을 진행하면서 중국상하이 외환시장과 중국홍콩 외환시장에서 거래되는 통화인 역내위안화와 역외위안화를 형성시켰다. 본 연구는 위안화 국제화와 점진적인 중국 자본계정 개방에 따라 급변하는 외환시장상황의 변동성을 정확하게 파악하기 위해서 GARCH모형 (일반화된 자기회귀 조건부이분산성모형)에 다단계인공신경망을 결합한 MLP-GARCH 모형과 GARCH모형과 기계학습의 일종인 딥러닝 (deep learning)을 통합한 DL-GARCH을 가지고 위안화 변동성예측을 비교 실험과 분석을 하였다. 비교분석 결과 DLGARCH 모형은 MLP-GARCH보다 모형 위안화 역내·외 환율변동성 예측 면에서 더욱 더 개선된예측값을 제공하였다. 그래서 이분산시계열모형을 딥러닝과 결합한 DL-GARCH 모형은 시계열의환율 변동성 예측 문제에 딥러닝을 응용할 수 있음을 확인하였다. 향후 이분산시계열과 결합된 딥러닝 모형은 다른 금융시계열 데이터에 응용하여 그 일반화 가능성을 높일 수 있을 것이다.The People’s Republic of China has vigorously been pursuing the internationalization of the Chinese Yuan or Renminbi after the financial crisis of 2008. In this view, an abrupt increase of use of the Chinese Yuan in the onshore and offshore markets are important milestones to be one of important currencies. One of the most frequently used methods to forecast volatility is GARCH model. Since a prediction error of the GARCH model has been reported quite high, a lot of efforts have been made to improve forecasting capability of the GARCH model. In this paper, we have proposed MLPGARCH and a DL-GARCH by employing Artificial Neural Network to the GARCH. In an application to forecasting Chinese Yuan volatility, we have successfully shown their overall outperformance in forecasting over the GARCH.Deep learning, DL-GARCH, GARCH, The Chinese Yuan, volatility."
883,딥러닝 기법을 이용한 내일강수 예측,하지훈,2016,한국지능시스템학회,"정확한 강수예측을 위해서는 예측인자 선정과 예측방법에 대한 선택이 매우 중요하다. 최근에는 강수예측 방법으로기계학습 기법이 많이 사용되고 있으며, 그 중에서도 특히 인공신경망을 사용한 강수예측 방법은 좋은 성능을 보였다. 본논문에서는 딥러닝 기법 중 하나인 DBN(deep belief network)를 이용한 새로운 강수예측 방법을 제안한다. DBN는 비지도사전 학습을 통해 초기 가중치를 설정하여 기존 인공신경망의 문제점을 보완한다. 예측인자로는 기온, 전일-전주 강수일, 태양과 달 궤도 관련 자료를 선정하였다. 기온과 전일-전주 강수일은 서울에서의 1974년부터 2013년까지 총 40년간의AWS(automatic weather system) 관측 자료를 사용하였고, 태양과 달의 궤도 관련 자료는 서울을 중심으로 계산한 결과를사용하였다. 전체 기간에서 일부는 학습 자료로 사용하여 예측모델을 생성하였고, 나머지를 생성한 모델의 검증 자료로사용하였다. 모델 검증 결과로 나온 예측값들은 확률값을 가지며 임계치를 이용하여 강수유무를 판별하였다. 강수 정확도의척도로 양분예보기법 중 CSI(critical successive index)와 Bias(frequency bias)를 계산하였다. 이를 통해 DBN와 MLP(multilayer perceptron)의 성능을 비교한 결과 DBN의 강수 예측 정확도가 높았고, 수행속도 또한 2배 이상 빨랐다.For accurate precipitation forecasts the choice of weather faoctrs and prediction method is very important. Recently, machinele arning has been widely used for forecasting precipitation, and artificial neural network, one of machine learning techniques, showed good performance. In this paper, we suggest a new method for forecasting precipitation using DBN, one of deep learning techniques.
DBN has an advantage that initial weights are set by unsupervised learning, so this compensates for the defects of artificial neural networks. We used past precipitation, temperature, and the parameters of the sun and moon's motion as features for forecasting precipitation. The dataset consists of observation data which had been measured for 40 years from AWS in Seoul. Experiments were based on 8-fold cross validation. As a result of estimation, we got probabilities of test dataset, so threshold was used for the decision of precipitation. CSI and Bias were used for indicating the precision of precipitation. Our experimental results showed that DBN performed better than MLP.Deep learning, Deep belief network, Precipitation, Forecast"
884,딥러닝 신경망모형을 이용한 실시간 교통정보수집 연구,김대현 /KIM DAE HYON,2016,사단법인 인문사회과학기술융합학회,"비디오 기반의 실시간 교통정보수집 시스템은 자율주행을 포함한 지능형교통체계의 주요 서브시스템중 하나이며, 우수한 예측(추정)력 때문에 핵심 알고리즘으로 인공신경망 모형이 주로 사용되고 있다. 특히, 최근 과학 학술지 네이처에 구글 딥마인드의 알파고가 공개되면서 딥러닝과 인공지능에 대한 전 세계적인 관심이 집중되고 있다. 최근까지 가장 널리 사용된 인공신경망모형은 역전파 모형이었으나 최근 컴퓨터 하드웨어 및 알고리즘의 새로운 개발과 더불어 딥러닝 알고리즘을 이용한 딥인공신경망 모형에 대한 관심이 높아지고 있다. 그러나 아직 딥러닝 인공신경망에 대한 연구는 초기단계에 있는 실정이며 적용분야도 극히 한정되어 있다. 본 연구에서는 실시간 도로교통정보 수집을 위한 영상처리분야에서 딥러닝 알고리즘을 활용하고 실험결과를 통하여 향후 교통정보수집‧분석 분야에서의 딥러닝 인공신경망 모형의 적용가능성을 살펴보고자 한다. 특히, 본 연구에서 도출한 딥러닝 인공신경망모형의 실험결과를 기존 역전파 모형에 의한 실험결과와 비교하여 모형의 우수성을 비교‧평가하고자 한다.Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Normally, artificial neural networks have been used in real-time applications for collecting traffic information because of their remarkable performance in terms of predictive accuracy and real-time computing. Especially, the recent publication of a paper in the journal Nature and the success of Google DeepMind’s world-beating Go program AlphaGo caused significant increases of the public interest in artificial intelligence. Even though various types of learning models have been proposed, the Backpropagation has been the most popular neural network model. However, the Deep learning neural network model has attracted much interest recently. In this study, the Deep leaning neural network will be applied in image processing techniques for the real-time traffic information collection to investigate the performance of Deep learning algorithm in traffic engineering fields. In addition, the experimental results from Deep learing algorithm will be compared with the results from Backpropagation model to evaluate and propose better performing model.Intelligent Transportation Systems (ITS), artificial neural networks, Deep learning, traffic information, image processing"
885,딥러닝의 모형과 응용사례,안성만 /SungMahn Ahn,2016,한국지능정보시스템학회,"딥러닝은 인공신경망(neural network)이라는 인공지능분야의 모형이 발전된 형태로서, 계층구조로 이루어진 인공신경망의 내부계층(hidden layer)이 여러 단계로 이루어진 구조이다. 딥러닝에서의 주요 모형은 합성곱신경망(convolutional neural network), 순환신경망(recurrent neural network), 그리고 심층신뢰신경망(deep belief network)의 세가지라고 할 수있다. 그 중에서 현재 흥미로운 연구가 많이 발표되어서 관심이 집중되고 있는 모형은 지도학습(supervised learning)모형인 처음 두 개의 모형이다. 따라서 본 논문에서는 지도학습모형의 가중치를 최적화하는 기본적인 방법인 오류역전파 알고리즘을 살펴본 뒤에 합성곱신경망과 순환신경망의 구조와 응용사례 등을 살펴보고자 한다. 본문에서 다루지 않은 모형인 심층신뢰신경망은 아직까지는 합성곱신경망 이나 순환신경망보다는 상대적으로 주목을 덜 받고 있다. 그러나 심층신뢰신경망은 CNN이나 RNN과는 달리 비지도학습(unsupervised learning)모형이며, 사람이나 동물은 관찰을 통해서 스스로 학습한다는 점에서 궁극적으로는 비지도학습모형이 더 많이 연구되어야 할 주제가 될 것이다.Deep learning model is a kind of neural networks that allows multiple hidden layers. There are various deep learning architectures such as convolutional neural networks, deep belief networks and recurrent neural networks. Those have been applied to fields like computer vision, automatic speech recognition, natural language processing, audio recognition and bioinformatics where they have been shown to produce state-of-the-art results on various tasks. Among those architectures, convolutional neural networks and recurrent neural networks are classified as the supervised learning model. And in recent years, those supervised learning models have gained more popularity than unsupervised learning models such as deep belief networks, because supervised learning models have shown fashionable applications in such fields mentioned above.
Deep learning models can be trained with backpropagation algorithm. Backpropagation is an abbreviation for “backward propagation of errors” and a common method of training artificial neural networks used in conjunction with an optimization method such as gradient descent. The method calculates the gradient of an error function with respect to all the weights in the network. The gradient is fed to the optimization method which in turn uses it to update the weights, in an attempt to minimize the error function.
Convolutional neural networks use a special architecture which is particularly well-adapted to classify images. Using this architecture makes convolutional networks fast to train. This, in turn, helps us train deep, muti-layer networks, which are very good at classifying images. These days, deep convolutional networks are used in most neural networks for image recognition. Convolutional neural networks use three basic ideas: local receptive fields, shared weights, and pooling. By local receptive fields, we mean that each neuron in the first(or any) hidden layer will be connected to a small region of the input(or previous layer’s) neurons. Shared weights mean that we’re going to use the same weights and bias for each of the local receptive field. This means that all the neurons in the hidden layer detect exactly the same feature, just at different locations in the input image. In addition to the convolutional layers just described, convolutional neural networks also contain pooling layers. Pooling layers are usually used immediately after convolutional layers. What the pooling layers do is to simplify the information in the output from the convolutional layer.
Recent convolutional network architectures have 10 to 20 hidden layers and billions of connections between units. Training deep learning networks has taken weeks several years ago, but thanks to progress in GPU and algorithm enhancement, training time has reduced to several hours.
Neural networks with time-varying behavior are known as recurrent neural networks or RNNs. A recurrent neural network is a class of artificial neural network where connections between units form a directed cycle. This creates an internal state of the network which allows it to exhibit dynamic temporal behavior. Unlike feedforward neural networks, RNNs can use their internal memory to process arbitrary sequences of inputs. Early RNN models turned out to be very difficult to train, harder even than deep feedforward networks. The reason is the unstable gradient problem such as vanishing gradient and exploding gradient. The gradient can get smaller and smaller as it is propagated back through layers. This makes learning in early layers extremely slow. The problem actually gets worse in RNNs, since gradients aren’t just propagated backward through layers, they’re propagated backward through time. If the network runs for a long time, that can make the gradient extremely unstable and hard to learn from. It has been possible to incorporate an idea known as long short-term memory units (LSTMs) into RNNs. LSTMs make it much easier to get good results when training RNNs, and many recent papers make use of LSTMs or related ideas.Deep learning, Convolutional neural networks, Recurrent neural networks, Error backpropagation algorithm"
886,딥러닝분석과 기술적 분석 지표를 이용한 한국 코스피주가지수 방향성 예측,이우식 /Lee Woosik,2017,한국데이터정보과학회,"2016년 3월 구글 (Google)의 바둑인공지능 알파고 (AlphaGo)가 이세돌 9단과의 바둑대결에서승리한 이후 다양한 분야에서 인공지능 사용에 대한 관심이 높아지고 있는 가운데 금융투자 분야에서도 인공지능과 투자자문 전문가의 합성어인 로보어드바이저 (Robo-Advisor)에 대한 관심이 높아지고 있다. 인공지능 (artificial intelligence)기반의 의사결정은 비용 절감은 물론 효과적인 의사결정을가능하게 한다는 점에서 큰 장점이 있다. 본 연구에서는 기술적 분석 (technical analysis) 지표와 딥러닝 (deep learning) 모형을 결합하여 한국 코스피 지수를 예측하는 모형을 개발하고 제시한 모형들의 예측력을 비교, 분석한다. 분석 결과 기술적 분석 지표에 딥러닝 알고리즘을 결합한 모형이 주가지수 방향성 예측 문제에 응용될 수 있음을 확인하였다. 향후 본 연구에서 제안된 기술적 분석 지표와딥러닝모형을 결합한 기법은 로보어드바이저서비스에 응용할 수 있는 일반화 가능성을 보여준다Since Google’s AlphaGo defeated a world champion of Go players in 2016, there have been many interests in the deep learning. In the financial sector, a Robo-Advisor using deep learning gains a significant attention, which builds and manages portfolios of financial instruments for investors.In this paper, we have proposed the a deep learning algorithm geared toward identification and forecast of the KOSPI index direction,and we also have compared the accuracy of the prediction.In an application of forecasting the financial market index direction, we have shown that the Robo-Advisor using deep learning has a significant effect on finance industry. The Robo-Advisor collects a massive data such as earnings statements, news reports and regulatory filings, analyzes those and recommends investors how to view market trends and identify the best time to purchase financial assets. On the other hand, the Robo-Advisor allows businesses to learn more about their customers, develop better marketing strategies, increase sales and decrease costs.Artificial intelligence, deep learning, FinTech, Robo-Advisor, technical analysis."
887,딥러닝의 효율적인 학습을 위한 학습데이터 선별 기법에 관한 연구,주경돈 /Gyoung-don Joo,2017,한국정보과학회,"빅데이터의 증가와 병렬 하드웨어 처리, 학습 알고리즘의 개선은 딥러닝의 발전을 가져왔으며 인공지능이 적용 가능한 영역을 획기적으로 확장시키고 있다. 하지만 이러한 대규모의 학습 데이터에 기반한 딥러닝은 학습시간이 증가되고, 더 나아가 학습 데이터의 구축을 위한 레이블링(labeling) 비용이 데이터 수에 비례하여 증가하게 된다. 따라서 본 논문에서는, 딥러닝의 정확도를 최대한 유지하면서 필요한 학습 데이터의 수를 줄일수 있도록, 전체 데이터에서 제한된 수의 학습 데이터의 후보를 선별하는 기법에 대하여 연구한다. 특히 기존의 능동 학습에 기반한 기법들이 학습 데이터의 수가 매우 적어질 경우 딥러닝의 정확도가 급격히 떨어지는 문제를 해결할 수 있는 새로운 기법을 제안하였으며, 제안된 기법으로 선별된 학습 데이터는 그 수의 변화에 대해 딥러닝이 일관되게 경쟁력있는 정확도를 가짐을 다양한 실험을 통하여 검증하였다.The increase of big data, parallel hardware processing and enhanced learning algorithms have led to the development of deep learning and are dramatically expanding the domain of applications based on artificial intelligence. However, such large-scale training data extends the learning time, and furthermore, the labeling cost for building the training set increases in proportion to the volume of data. Therefore, in this paper, we study how to select a limited number of candidates for training set from given data so as to reduce the number of training set while maintaining the accuracy of deep learning. Especially, we found that, with the training set selected by the previous work, the accuracy of deep learning drastically decreases when the number of training set is very small and so proposed new methods to resolve this fall-off in accuracy. Through experiments, we showed that deep learning consistently has the competitive accuracy with the training set selected by the proposed method regardless of the size of training set.Deep Learning, Train Set Selection, Active Learning, Gamma distribution"
888,딥러닝을 이용한 범용적 스테그아날리시스,김현재 /Hyunjae Kim,2017,한국정보과학회,"스테그아날리시스(Steganalysis)란 이미지 등 일반적인 자료에 암호화된 정보를 은닉하는 스테가노그래피(Steganography)에 대한 검출 및 분석 방법으로, 기계학습 기반 방법론을 포함한다. 기존 기계학습 기반 스테그아날리시스는 영상(Image)의 특징(Feature) 추출 및 모델링에 기반하며, 최근 딥러닝(Deep Learning)의 적용으로 검출 정확도가 큰 폭으로 향상되었다. 하지만 현존하는 스테그아날리시스 모델은 단일 스테가노그래피 기법에 대해 국한되어 있어 학습에 사용되지 않은 스테고(Stego) 이미지의 경우 검출이 불가능한 결정적 한계를 가진다. 본 연구에서는 다양한 스테가노그래피 기법으로 생성된 스테고이미지에 딥러닝을 적용하여 스테그아날리시스를 학습하는 범용적 모델을 제안한다. 다양한 실험을 통해 제안 기법의 효용성 및 가능성을 확인하고, 범용적 스테그아날리시스 모델이 각각에 특화된 검출 기법과 유사한 정확도로 스테고 이미지를 검출할 수 있음을 보인다.Steganalysis is to detect information hidden by steganography inside general data such as images. There are stegoanalysis techniques that use machine learning (ML). Existing ML approaches to steganalysis are based on extracting features from stego images and modeling them.
Recently deep learning-based methodologies have shown significant improvements in detection accuracy. However, all the existing methods, including deep learning-based ones, have a critical limitation in that they can only detect stego images that are created by a specific steganography method. In this paper, we propose a generalized steganalysis method that can model multiple types of stego images using deep learning. Through various experiments, we confirm the effectiveness of our approach and envision directions for future research. In particular, we show that our method can detect each type of steganography with the same level of accuracy as that of a steganalysis method dedicated to that type of steganography, thereby demonstrating the general applicability of our approach to multiple types of stego images.steganalysis, steganography, deep learning, generalized steganalysis model"
889,"딥러닝 프레임워크의 비교: 티아노, 텐서플로, CNTK를 중심으로",정여진 /Yeojin Chung,2017,한국지능정보시스템학회,"딥러닝 프레임워크의 대표적인 기능으로는 ‘자동미분’과 ‘GPU의 활용’ 등을 들 수 있다. 본 논문은 파이썬의 라이브러리 형태로 사용 가능한 프레임워크 중에서 구글의 텐서플로와 마이크로소프트의 CNTK, 그리고 텐서 플로의 원조라고 할 수 있는 티아노를 비교하였다. 본문에서는 자동미분의 개념과 GPU의 활용형태를 간단히 설명하고, 그 다음에 logistic regression을 실행하는 예를 통하여 각 프레임워크의 문법을 알아본 뒤에, 마지막으로 대표적인 딥러닝 응용인 CNN의 예제를 실행시켜보고 코딩의 편의성과 실행속도 등을 확인해 보았다. 그 결과, 편의성의 관점에서 보면 티아노가 가장 코딩 하기가 어렵고, CNTK와 텐서플로는 많은 부분이 비슷하게 추상화 되어 있어서 코딩이 비슷하지만 가중치와 편향을 직접 정의하느냐의 여부에서 차이를 보였다. 그리고 각프레임워크의 실행속도에 대한 평가는 ‘큰 차이는 없다’는 것이다. 텐서플로는 티아노에 비하여 속도가 느리다는 평가가 있어왔는데, 본 연구의 실험에 의하면, 비록 CNN 모형에 국한되었지만, 텐서플로가 아주 조금이지만빠른 것으로 나타났다. CNTK의 경우에도, 비록 실험환경이 달랐지만, 실험환경의 차이에 의한 속도의 차이의 편차범위 이내에 있는 것으로 판단이 되었다. 본 연구에서는 세 종류의 딥러닝 프레임워크만을 살펴보았는데,위키피디아에 따르면 딥러닝 프레임워크의 종류는 12가지가 있으며, 각 프레임워크의 특징을 15가지 속성으로구분하여 차이를 특정하고 있다. 그 많은 속성 중에서 사용자의 입장에서 볼 때 중요한 속성은 어떤 언어(파이썬, C++, Java, 등)로 사용가능한지, 어떤 딥러닝 모형에 대한 라이브러리가 잘 구현되어 있는지 등일 것이다. 그리고 사용자가 대규모의 딥러닝 모형을 구축한다면, 다중 GPU 혹은 다중 서버를 지원하는지의 여부도 중요할 것이다. 또한 딥러닝 모형을 처음 학습하는 경우에는 사용설명서가 많은지 예제 프로그램이 많은지 여부도 중요한 기준이 될 것이다.The deep learning framework is software designed to help develop deep learning models. Some of its important functions include “automatic differentiation” and “utilization of GPU”. The list of popular deep learning framework includes Caffe (BVLC) and Theano (University of Montreal). And recently, Microsoft's deep learning framework, Microsoft Cognitive Toolkit, was released as open-source license, following Google’s Tensorflow a year earlier. The early deep learning frameworks have been developed mainly for research at universities. Beginning with the inception of Tensorflow, however, it seems that companies such as Microsoft and Facebook have started to join the competition of framework development. Given the trend, Google and other companies are expected to continue investing in the deep learning framework to bring forward the initiative in the artificial intelligence business. From this point of view, we think it is a good time to compare some of deep learning frameworks. So we compare three deep learning frameworks which can be used as a Python library. Those are Google's Tensorflow, Microsoft’s CNTK, and Theano which is sort of a predecessor of the preceding two.
The most common and important function of deep learning frameworks is the ability to perform automatic differentiation. Basically all the mathematical expressions of deep learning models can be represented as computational graphs, which consist of nodes and edges. Partial derivatives on each edge of a computational graph can then be obtained. With the partial derivatives, we can let software compute differentiation of any node with respect to any variable by utilizing chain rule of Calculus.
First of all, the convenience of coding is in the order of CNTK, Tensorflow, and Theano. The criterion is simply based on the lengths of the codes and the learning curve and the ease of coding are not the main concern. According to the criteria, Theano was the most difficult to implement with, and CNTK and Tensorflow were somewhat easier. With Tensorflow, we need to define weight variables and biases explicitly. The reason that CNTK and Tensorflow are easier to implement with is that those frameworks provide us with more abstraction than Theano. We, however, need to mention that low-level coding is not always bad. It gives us flexibility of coding. With the low-level coding such as in Theano, we can implement and test any new deep learning models or any new search methods that we can think of.
The assessment of the execution speed of each framework is that there is not meaningful difference. According to the experiment, execution speeds of Theano and Tensorflow are very similar, although the experiment was limited to a CNN model. In the case of CNTK, the experimental environment was not maintained as the same. The code written in CNTK has to be run in PC environment without GPU where codes execute as much as 50 times slower than with GPU. But we concluded that the difference of execution speed was within the range of variation caused by the different hardware setup.
In this study, we compared three types of deep learning framework: Theano, Tensorflow, and CNTK. According to Wikipedia, there are 12 available deep learning frameworks. And 15 different attributes differentiate each framework. Some of the important attributes would include interface language (Python, C ++, Java, etc.) and the availability of libraries on various deep learning models such as CNN, RNN, DBN, and etc. And if a user implements a large scale deep learning model, it will also be important to support multiple GPU or multiple servers. Also, if you are learning the deep learning model, it would also be important if there are enough examples and references.deep learning framework, Theano, TensorFlow, CNTK, computational graph, CIFAR-10"
890,딥러닝을 이용한 주변 무선단말 파악방안,이웅섭 /Lee， Woong-sup,2018,한국정보통신학회,"최근 단말-대-단말(Device-to-device, D2D) 통신기술이 차세대 무선통신시스템의 핵심기술로 큰 관심을 받고 있다. 이러한 단말간 통신에서는 자신의 주변에 어떠한 단말이 있는지 파악하는 주변단말 탐색(Neighbor discovery)이매우 중요하다. 본 논문에서는 최근 큰 관심을 받고 있는 딥러닝(Deep learning) 기술을 활용하여 단말간 통신에서 주변단말을 파악하는 방안에 대해서 제안한다. 제안 방안은 기존의 방안과 달리 무선채널의 공간적 연관성을 이용하여단말간의 신호 전송 없이 단말이 기지국으로 전송하는 상향링크 파일럿 신호를 기반으로 주변 단말을 찾고 따라서기존의 방식에 비해 신호전송 복잡도(signaling complexity)를 크게 줄일 수 있다. 또한 제안 방안에서는 떨어져 있는거리에 따라서 주변 단말을 분류 가능하여 기존 방안에 비해서 좀 더 세밀한 단말 탐색이 가능하다. 마지막으로 본 논문에서는 tensorflow를 이용한 컴퓨터 시뮬레이션을 통해 제안 방안의 성능을 검증하였다.Recently, the device-to-device (D2D) communication has been conceived as the key technology for the next-generationmobile communication systems. The neighbor discovery in which the nearby users are found, is essential for the properoperation of the D2D communication. In this paper, we propose new neighbor discovery scheme based on deep learningtechnology which has gained a lot of attention recently. In the proposed scheme, the neighboring users can be foundusing the uplink pilot transmission of users only, unlike conventional neighbor discovery schemes in which direct pilotcommunication among users is required, such that the signaling overhead can be greatly reduced in our proposed scheme.
Moreover, the neighbors with different proximity can also be classified accordingly which enables more accurate neighbordiscovery compared to the conventional schemes. The performance of our proposed scheme is verified through thetensorflow-based computer simulations.
키워드 : 딥러닝, 주변단말 탐색, 단말-대-단말 통신Deep learning, Neighbor discovery, Device-to-device communication, Wireless channel correlation"
891,신뢰성있는 딥러닝 기반 분석 모델을 참조하기 위한 딥러닝 기술 언어,문종혁 /Jong Hyeok Mun,2021,한국정보처리학회,"최근 딥러닝은 하드웨어 성능이 향상됨에 따라 자연어 처리, 영상 인식 등의 다양한 기술에 접목되어 활용되고 있다. 이러한 기술들을 활용해 지능형교통 시스템(ITS), 스마트홈, 헬스케어 등의 산업분야에서 데이터를 분석하여 고속도로 속도위반 차량 검출, 에너지 사용량 제어, 응급상황 등과 같은고품질의 서비스를 제공하며, 고품질의 서비스를 제공하기 위해서는 정확도가 향상된 딥러닝 모델이 적용되어야 한다. 이를 위해 서비스 환경의 데이터를분석하기 위한 딥러닝 모델을 개발할 때, 개발자는 신뢰성이 검증된 최신의 딥러닝 모델을 적용할 수 있어야 한다. 이는 개발자가 참조하는 딥러닝 모델에적용된 학습 데이터셋의 정확도를 측정하여 검증할 수 있다. 이러한 검증을 위해서 개발자는 학습 데이터셋, 딥러닝의 계층구조 및 개발 환경 등과 같은내용을 포함하는 딥러닝 모델을 문서화하여 적용하기 위한 구조적인 정보가 필요하다. 본 논문에서는 신뢰성있는 딥러닝 기반 데이터 분석 모델을 참조하기위한 딥러닝 기술 언어를 제안한다. 제안하는 기술 언어는 신뢰성 있는 딥러닝 모델을 개발하는데 필요한 학습데이터셋, 개발 환경 및 설정 등의 정보와더불어 딥러닝 모델의 계층구조를 표현할 수 있다. 제안하는 딥러닝 기술 언어를 이용하여 개발자는 지능형 교통 시스템에서 참조하는 분석 모델의정확도를 검증할 수 있다. 실험에서는 제안하는 언어의 유효성을 검증하기 위해, 번호판 인식 모델을 중심으로 딥러닝 기술 문서의 적용과정을 보인다With the recent advancements of deep learning, companies such as smart home, healthcare, and intelligent transportation systems areutilizing its functionality to provide high-quality services for vehicle detection, emergency situation detection, and controlling energyconsumption. To provide reliable services in such sensitive systems, deep learning models are required to have high accuracy. In orderto develop a deep learning model for analyzing previously mentioned services, developers should utilize the state of the art deep learningmodels that have already been verified for higher accuracy. The developers can verify the accuracy of the referenced model by validatingthe model on the dataset. For this validation, the developer needs structural information to document and apply deep learning models,including metadata such as learning dataset, network architecture, and development environments. In this paper, we propose a descriptionlanguage that represents the network architecture of the deep learning model along with its metadata that are necessary to develop adeep learning model. Through the proposed description language, developers can easily verify the accuracy of the referenced deep learningmodel. Our experiments demonstrate the application scenario of a deep learning description document that focuses on the license platerecognition for the detection of illegally parked vehicles.Trusted Deep Learning, Model Reference, Deep Learning Description Language, Traffic Situation Analysis Model"
892,딥러닝 분석을 이용한 중국 역내·외 위안화 변동성 예측,이우식 /Lee Woosik,2016,한국데이터정보과학회,"2008년 글로벌 금융위기 이후 중국은 위안화 국제화의 점진적 추진을 진행하면서 중국상하이 외환시장과 중국홍콩 외환시장에서 거래되는 통화인 역내위안화와 역외위안화를 형성시켰다. 본 연구는 위안화 국제화와 점진적인 중국 자본계정 개방에 따라 급변하는 외환시장상황의 변동성을 정확하게 파악하기 위해서 GARCH모형 (일반화된 자기회귀 조건부이분산성모형)에 다단계인공신경망을 결합한 MLP-GARCH 모형과 GARCH모형과 기계학습의 일종인 딥러닝 (deep learning)을 통합한 DL-GARCH을 가지고 위안화 변동성예측을 비교 실험과 분석을 하였다. 비교분석 결과 DLGARCH 모형은 MLP-GARCH보다 모형 위안화 역내·외 환율변동성 예측 면에서 더욱 더 개선된예측값을 제공하였다. 그래서 이분산시계열모형을 딥러닝과 결합한 DL-GARCH 모형은 시계열의환율 변동성 예측 문제에 딥러닝을 응용할 수 있음을 확인하였다. 향후 이분산시계열과 결합된 딥러닝 모형은 다른 금융시계열 데이터에 응용하여 그 일반화 가능성을 높일 수 있을 것이다.The People’s Republic of China has vigorously been pursuing the internationalization of the Chinese Yuan or Renminbi after the financial crisis of 2008. In this view, an abrupt increase of use of the Chinese Yuan in the onshore and offshore markets are important milestones to be one of important currencies. One of the most frequently used methods to forecast volatility is GARCH model. Since a prediction error of the GARCH model has been reported quite high, a lot of efforts have been made to improve forecasting capability of the GARCH model. In this paper, we have proposed MLPGARCH and a DL-GARCH by employing Artificial Neural Network to the GARCH. In an application to forecasting Chinese Yuan volatility, we have successfully shown their overall outperformance in forecasting over the GARCH.Deep learning, DL-GARCH, GARCH, The Chinese Yuan, volatility."
893,딥러닝 기법을 이용한 내일강수 예측,하지훈,2016,한국지능시스템학회,"정확한 강수예측을 위해서는 예측인자 선정과 예측방법에 대한 선택이 매우 중요하다. 최근에는 강수예측 방법으로기계학습 기법이 많이 사용되고 있으며, 그 중에서도 특히 인공신경망을 사용한 강수예측 방법은 좋은 성능을 보였다. 본논문에서는 딥러닝 기법 중 하나인 DBN(deep belief network)를 이용한 새로운 강수예측 방법을 제안한다. DBN는 비지도사전 학습을 통해 초기 가중치를 설정하여 기존 인공신경망의 문제점을 보완한다. 예측인자로는 기온, 전일-전주 강수일, 태양과 달 궤도 관련 자료를 선정하였다. 기온과 전일-전주 강수일은 서울에서의 1974년부터 2013년까지 총 40년간의AWS(automatic weather system) 관측 자료를 사용하였고, 태양과 달의 궤도 관련 자료는 서울을 중심으로 계산한 결과를사용하였다. 전체 기간에서 일부는 학습 자료로 사용하여 예측모델을 생성하였고, 나머지를 생성한 모델의 검증 자료로사용하였다. 모델 검증 결과로 나온 예측값들은 확률값을 가지며 임계치를 이용하여 강수유무를 판별하였다. 강수 정확도의척도로 양분예보기법 중 CSI(critical successive index)와 Bias(frequency bias)를 계산하였다. 이를 통해 DBN와 MLP(multilayer perceptron)의 성능을 비교한 결과 DBN의 강수 예측 정확도가 높았고, 수행속도 또한 2배 이상 빨랐다.For accurate precipitation forecasts the choice of weather faoctrs and prediction method is very important. Recently, machinele arning has been widely used for forecasting precipitation, and artificial neural network, one of machine learning techniques, showed good performance. In this paper, we suggest a new method for forecasting precipitation using DBN, one of deep learning techniques.
DBN has an advantage that initial weights are set by unsupervised learning, so this compensates for the defects of artificial neural networks. We used past precipitation, temperature, and the parameters of the sun and moon's motion as features for forecasting precipitation. The dataset consists of observation data which had been measured for 40 years from AWS in Seoul. Experiments were based on 8-fold cross validation. As a result of estimation, we got probabilities of test dataset, so threshold was used for the decision of precipitation. CSI and Bias were used for indicating the precision of precipitation. Our experimental results showed that DBN performed better than MLP.Deep learning, Deep belief network, Precipitation, Forecast"
894,딥러닝 신경망모형을 이용한 실시간 교통정보수집 연구,김대현 /KIM DAE HYON,2016,사단법인 인문사회과학기술융합학회,"비디오 기반의 실시간 교통정보수집 시스템은 자율주행을 포함한 지능형교통체계의 주요 서브시스템중 하나이며, 우수한 예측(추정)력 때문에 핵심 알고리즘으로 인공신경망 모형이 주로 사용되고 있다. 특히, 최근 과학 학술지 네이처에 구글 딥마인드의 알파고가 공개되면서 딥러닝과 인공지능에 대한 전 세계적인 관심이 집중되고 있다. 최근까지 가장 널리 사용된 인공신경망모형은 역전파 모형이었으나 최근 컴퓨터 하드웨어 및 알고리즘의 새로운 개발과 더불어 딥러닝 알고리즘을 이용한 딥인공신경망 모형에 대한 관심이 높아지고 있다. 그러나 아직 딥러닝 인공신경망에 대한 연구는 초기단계에 있는 실정이며 적용분야도 극히 한정되어 있다. 본 연구에서는 실시간 도로교통정보 수집을 위한 영상처리분야에서 딥러닝 알고리즘을 활용하고 실험결과를 통하여 향후 교통정보수집‧분석 분야에서의 딥러닝 인공신경망 모형의 적용가능성을 살펴보고자 한다. 특히, 본 연구에서 도출한 딥러닝 인공신경망모형의 실험결과를 기존 역전파 모형에 의한 실험결과와 비교하여 모형의 우수성을 비교‧평가하고자 한다.Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Normally, artificial neural networks have been used in real-time applications for collecting traffic information because of their remarkable performance in terms of predictive accuracy and real-time computing. Especially, the recent publication of a paper in the journal Nature and the success of Google DeepMind’s world-beating Go program AlphaGo caused significant increases of the public interest in artificial intelligence. Even though various types of learning models have been proposed, the Backpropagation has been the most popular neural network model. However, the Deep learning neural network model has attracted much interest recently. In this study, the Deep leaning neural network will be applied in image processing techniques for the real-time traffic information collection to investigate the performance of Deep learning algorithm in traffic engineering fields. In addition, the experimental results from Deep learing algorithm will be compared with the results from Backpropagation model to evaluate and propose better performing model.Intelligent Transportation Systems (ITS), artificial neural networks, Deep learning, traffic information, image processing"
895,딥러닝의 모형과 응용사례,안성만 /SungMahn Ahn,2016,한국지능정보시스템학회,"딥러닝은 인공신경망(neural network)이라는 인공지능분야의 모형이 발전된 형태로서, 계층구조로 이루어진 인공신경망의 내부계층(hidden layer)이 여러 단계로 이루어진 구조이다. 딥러닝에서의 주요 모형은 합성곱신경망(convolutional neural network), 순환신경망(recurrent neural network), 그리고 심층신뢰신경망(deep belief network)의 세가지라고 할 수있다. 그 중에서 현재 흥미로운 연구가 많이 발표되어서 관심이 집중되고 있는 모형은 지도학습(supervised learning)모형인 처음 두 개의 모형이다. 따라서 본 논문에서는 지도학습모형의 가중치를 최적화하는 기본적인 방법인 오류역전파 알고리즘을 살펴본 뒤에 합성곱신경망과 순환신경망의 구조와 응용사례 등을 살펴보고자 한다. 본문에서 다루지 않은 모형인 심층신뢰신경망은 아직까지는 합성곱신경망 이나 순환신경망보다는 상대적으로 주목을 덜 받고 있다. 그러나 심층신뢰신경망은 CNN이나 RNN과는 달리 비지도학습(unsupervised learning)모형이며, 사람이나 동물은 관찰을 통해서 스스로 학습한다는 점에서 궁극적으로는 비지도학습모형이 더 많이 연구되어야 할 주제가 될 것이다.Deep learning model is a kind of neural networks that allows multiple hidden layers. There are various deep learning architectures such as convolutional neural networks, deep belief networks and recurrent neural networks. Those have been applied to fields like computer vision, automatic speech recognition, natural language processing, audio recognition and bioinformatics where they have been shown to produce state-of-the-art results on various tasks. Among those architectures, convolutional neural networks and recurrent neural networks are classified as the supervised learning model. And in recent years, those supervised learning models have gained more popularity than unsupervised learning models such as deep belief networks, because supervised learning models have shown fashionable applications in such fields mentioned above.
Deep learning models can be trained with backpropagation algorithm. Backpropagation is an abbreviation for “backward propagation of errors” and a common method of training artificial neural networks used in conjunction with an optimization method such as gradient descent. The method calculates the gradient of an error function with respect to all the weights in the network. The gradient is fed to the optimization method which in turn uses it to update the weights, in an attempt to minimize the error function.
Convolutional neural networks use a special architecture which is particularly well-adapted to classify images. Using this architecture makes convolutional networks fast to train. This, in turn, helps us train deep, muti-layer networks, which are very good at classifying images. These days, deep convolutional networks are used in most neural networks for image recognition. Convolutional neural networks use three basic ideas: local receptive fields, shared weights, and pooling. By local receptive fields, we mean that each neuron in the first(or any) hidden layer will be connected to a small region of the input(or previous layer’s) neurons. Shared weights mean that we’re going to use the same weights and bias for each of the local receptive field. This means that all the neurons in the hidden layer detect exactly the same feature, just at different locations in the input image. In addition to the convolutional layers just described, convolutional neural networks also contain pooling layers. Pooling layers are usually used immediately after convolutional layers. What the pooling layers do is to simplify the information in the output from the convolutional layer.
Recent convolutional network architectures have 10 to 20 hidden layers and billions of connections between units. Training deep learning networks has taken weeks several years ago, but thanks to progress in GPU and algorithm enhancement, training time has reduced to several hours.
Neural networks with time-varying behavior are known as recurrent neural networks or RNNs. A recurrent neural network is a class of artificial neural network where connections between units form a directed cycle. This creates an internal state of the network which allows it to exhibit dynamic temporal behavior. Unlike feedforward neural networks, RNNs can use their internal memory to process arbitrary sequences of inputs. Early RNN models turned out to be very difficult to train, harder even than deep feedforward networks. The reason is the unstable gradient problem such as vanishing gradient and exploding gradient. The gradient can get smaller and smaller as it is propagated back through layers. This makes learning in early layers extremely slow. The problem actually gets worse in RNNs, since gradients aren’t just propagated backward through layers, they’re propagated backward through time. If the network runs for a long time, that can make the gradient extremely unstable and hard to learn from. It has been possible to incorporate an idea known as long short-term memory units (LSTMs) into RNNs. LSTMs make it much easier to get good results when training RNNs, and many recent papers make use of LSTMs or related ideas.Deep learning, Convolutional neural networks, Recurrent neural networks, Error backpropagation algorithm"
896,딥러닝분석과 기술적 분석 지표를 이용한 한국 코스피주가지수 방향성 예측,이우식 /Lee Woosik,2017,한국데이터정보과학회,"2016년 3월 구글 (Google)의 바둑인공지능 알파고 (AlphaGo)가 이세돌 9단과의 바둑대결에서승리한 이후 다양한 분야에서 인공지능 사용에 대한 관심이 높아지고 있는 가운데 금융투자 분야에서도 인공지능과 투자자문 전문가의 합성어인 로보어드바이저 (Robo-Advisor)에 대한 관심이 높아지고 있다. 인공지능 (artificial intelligence)기반의 의사결정은 비용 절감은 물론 효과적인 의사결정을가능하게 한다는 점에서 큰 장점이 있다. 본 연구에서는 기술적 분석 (technical analysis) 지표와 딥러닝 (deep learning) 모형을 결합하여 한국 코스피 지수를 예측하는 모형을 개발하고 제시한 모형들의 예측력을 비교, 분석한다. 분석 결과 기술적 분석 지표에 딥러닝 알고리즘을 결합한 모형이 주가지수 방향성 예측 문제에 응용될 수 있음을 확인하였다. 향후 본 연구에서 제안된 기술적 분석 지표와딥러닝모형을 결합한 기법은 로보어드바이저서비스에 응용할 수 있는 일반화 가능성을 보여준다Since Google’s AlphaGo defeated a world champion of Go players in 2016, there have been many interests in the deep learning. In the financial sector, a Robo-Advisor using deep learning gains a significant attention, which builds and manages portfolios of financial instruments for investors.In this paper, we have proposed the a deep learning algorithm geared toward identification and forecast of the KOSPI index direction,and we also have compared the accuracy of the prediction.In an application of forecasting the financial market index direction, we have shown that the Robo-Advisor using deep learning has a significant effect on finance industry. The Robo-Advisor collects a massive data such as earnings statements, news reports and regulatory filings, analyzes those and recommends investors how to view market trends and identify the best time to purchase financial assets. On the other hand, the Robo-Advisor allows businesses to learn more about their customers, develop better marketing strategies, increase sales and decrease costs.Artificial intelligence, deep learning, FinTech, Robo-Advisor, technical analysis."
897,딥러닝의 효율적인 학습을 위한 학습데이터 선별 기법에 관한 연구,주경돈 /Gyoung-don Joo,2017,한국정보과학회,"빅데이터의 증가와 병렬 하드웨어 처리, 학습 알고리즘의 개선은 딥러닝의 발전을 가져왔으며 인공지능이 적용 가능한 영역을 획기적으로 확장시키고 있다. 하지만 이러한 대규모의 학습 데이터에 기반한 딥러닝은 학습시간이 증가되고, 더 나아가 학습 데이터의 구축을 위한 레이블링(labeling) 비용이 데이터 수에 비례하여 증가하게 된다. 따라서 본 논문에서는, 딥러닝의 정확도를 최대한 유지하면서 필요한 학습 데이터의 수를 줄일수 있도록, 전체 데이터에서 제한된 수의 학습 데이터의 후보를 선별하는 기법에 대하여 연구한다. 특히 기존의 능동 학습에 기반한 기법들이 학습 데이터의 수가 매우 적어질 경우 딥러닝의 정확도가 급격히 떨어지는 문제를 해결할 수 있는 새로운 기법을 제안하였으며, 제안된 기법으로 선별된 학습 데이터는 그 수의 변화에 대해 딥러닝이 일관되게 경쟁력있는 정확도를 가짐을 다양한 실험을 통하여 검증하였다.The increase of big data, parallel hardware processing and enhanced learning algorithms have led to the development of deep learning and are dramatically expanding the domain of applications based on artificial intelligence. However, such large-scale training data extends the learning time, and furthermore, the labeling cost for building the training set increases in proportion to the volume of data. Therefore, in this paper, we study how to select a limited number of candidates for training set from given data so as to reduce the number of training set while maintaining the accuracy of deep learning. Especially, we found that, with the training set selected by the previous work, the accuracy of deep learning drastically decreases when the number of training set is very small and so proposed new methods to resolve this fall-off in accuracy. Through experiments, we showed that deep learning consistently has the competitive accuracy with the training set selected by the proposed method regardless of the size of training set.Deep Learning, Train Set Selection, Active Learning, Gamma distribution"
898,딥러닝을 이용한 범용적 스테그아날리시스,김현재 /Hyunjae Kim,2017,한국정보과학회,"스테그아날리시스(Steganalysis)란 이미지 등 일반적인 자료에 암호화된 정보를 은닉하는 스테가노그래피(Steganography)에 대한 검출 및 분석 방법으로, 기계학습 기반 방법론을 포함한다. 기존 기계학습 기반 스테그아날리시스는 영상(Image)의 특징(Feature) 추출 및 모델링에 기반하며, 최근 딥러닝(Deep Learning)의 적용으로 검출 정확도가 큰 폭으로 향상되었다. 하지만 현존하는 스테그아날리시스 모델은 단일 스테가노그래피 기법에 대해 국한되어 있어 학습에 사용되지 않은 스테고(Stego) 이미지의 경우 검출이 불가능한 결정적 한계를 가진다. 본 연구에서는 다양한 스테가노그래피 기법으로 생성된 스테고이미지에 딥러닝을 적용하여 스테그아날리시스를 학습하는 범용적 모델을 제안한다. 다양한 실험을 통해 제안 기법의 효용성 및 가능성을 확인하고, 범용적 스테그아날리시스 모델이 각각에 특화된 검출 기법과 유사한 정확도로 스테고 이미지를 검출할 수 있음을 보인다.Steganalysis is to detect information hidden by steganography inside general data such as images. There are stegoanalysis techniques that use machine learning (ML). Existing ML approaches to steganalysis are based on extracting features from stego images and modeling them.
Recently deep learning-based methodologies have shown significant improvements in detection accuracy. However, all the existing methods, including deep learning-based ones, have a critical limitation in that they can only detect stego images that are created by a specific steganography method. In this paper, we propose a generalized steganalysis method that can model multiple types of stego images using deep learning. Through various experiments, we confirm the effectiveness of our approach and envision directions for future research. In particular, we show that our method can detect each type of steganography with the same level of accuracy as that of a steganalysis method dedicated to that type of steganography, thereby demonstrating the general applicability of our approach to multiple types of stego images.steganalysis, steganography, deep learning, generalized steganalysis model"
899,"딥러닝 프레임워크의 비교: 티아노, 텐서플로, CNTK를 중심으로",정여진 /Yeojin Chung,2017,한국지능정보시스템학회,"딥러닝 프레임워크의 대표적인 기능으로는 ‘자동미분’과 ‘GPU의 활용’ 등을 들 수 있다. 본 논문은 파이썬의 라이브러리 형태로 사용 가능한 프레임워크 중에서 구글의 텐서플로와 마이크로소프트의 CNTK, 그리고 텐서 플로의 원조라고 할 수 있는 티아노를 비교하였다. 본문에서는 자동미분의 개념과 GPU의 활용형태를 간단히 설명하고, 그 다음에 logistic regression을 실행하는 예를 통하여 각 프레임워크의 문법을 알아본 뒤에, 마지막으로 대표적인 딥러닝 응용인 CNN의 예제를 실행시켜보고 코딩의 편의성과 실행속도 등을 확인해 보았다. 그 결과, 편의성의 관점에서 보면 티아노가 가장 코딩 하기가 어렵고, CNTK와 텐서플로는 많은 부분이 비슷하게 추상화 되어 있어서 코딩이 비슷하지만 가중치와 편향을 직접 정의하느냐의 여부에서 차이를 보였다. 그리고 각프레임워크의 실행속도에 대한 평가는 ‘큰 차이는 없다’는 것이다. 텐서플로는 티아노에 비하여 속도가 느리다는 평가가 있어왔는데, 본 연구의 실험에 의하면, 비록 CNN 모형에 국한되었지만, 텐서플로가 아주 조금이지만빠른 것으로 나타났다. CNTK의 경우에도, 비록 실험환경이 달랐지만, 실험환경의 차이에 의한 속도의 차이의 편차범위 이내에 있는 것으로 판단이 되었다. 본 연구에서는 세 종류의 딥러닝 프레임워크만을 살펴보았는데,위키피디아에 따르면 딥러닝 프레임워크의 종류는 12가지가 있으며, 각 프레임워크의 특징을 15가지 속성으로구분하여 차이를 특정하고 있다. 그 많은 속성 중에서 사용자의 입장에서 볼 때 중요한 속성은 어떤 언어(파이썬, C++, Java, 등)로 사용가능한지, 어떤 딥러닝 모형에 대한 라이브러리가 잘 구현되어 있는지 등일 것이다. 그리고 사용자가 대규모의 딥러닝 모형을 구축한다면, 다중 GPU 혹은 다중 서버를 지원하는지의 여부도 중요할 것이다. 또한 딥러닝 모형을 처음 학습하는 경우에는 사용설명서가 많은지 예제 프로그램이 많은지 여부도 중요한 기준이 될 것이다.The deep learning framework is software designed to help develop deep learning models. Some of its important functions include “automatic differentiation” and “utilization of GPU”. The list of popular deep learning framework includes Caffe (BVLC) and Theano (University of Montreal). And recently, Microsoft's deep learning framework, Microsoft Cognitive Toolkit, was released as open-source license, following Google’s Tensorflow a year earlier. The early deep learning frameworks have been developed mainly for research at universities. Beginning with the inception of Tensorflow, however, it seems that companies such as Microsoft and Facebook have started to join the competition of framework development. Given the trend, Google and other companies are expected to continue investing in the deep learning framework to bring forward the initiative in the artificial intelligence business. From this point of view, we think it is a good time to compare some of deep learning frameworks. So we compare three deep learning frameworks which can be used as a Python library. Those are Google's Tensorflow, Microsoft’s CNTK, and Theano which is sort of a predecessor of the preceding two.
The most common and important function of deep learning frameworks is the ability to perform automatic differentiation. Basically all the mathematical expressions of deep learning models can be represented as computational graphs, which consist of nodes and edges. Partial derivatives on each edge of a computational graph can then be obtained. With the partial derivatives, we can let software compute differentiation of any node with respect to any variable by utilizing chain rule of Calculus.
First of all, the convenience of coding is in the order of CNTK, Tensorflow, and Theano. The criterion is simply based on the lengths of the codes and the learning curve and the ease of coding are not the main concern. According to the criteria, Theano was the most difficult to implement with, and CNTK and Tensorflow were somewhat easier. With Tensorflow, we need to define weight variables and biases explicitly. The reason that CNTK and Tensorflow are easier to implement with is that those frameworks provide us with more abstraction than Theano. We, however, need to mention that low-level coding is not always bad. It gives us flexibility of coding. With the low-level coding such as in Theano, we can implement and test any new deep learning models or any new search methods that we can think of.
The assessment of the execution speed of each framework is that there is not meaningful difference. According to the experiment, execution speeds of Theano and Tensorflow are very similar, although the experiment was limited to a CNN model. In the case of CNTK, the experimental environment was not maintained as the same. The code written in CNTK has to be run in PC environment without GPU where codes execute as much as 50 times slower than with GPU. But we concluded that the difference of execution speed was within the range of variation caused by the different hardware setup.
In this study, we compared three types of deep learning framework: Theano, Tensorflow, and CNTK. According to Wikipedia, there are 12 available deep learning frameworks. And 15 different attributes differentiate each framework. Some of the important attributes would include interface language (Python, C ++, Java, etc.) and the availability of libraries on various deep learning models such as CNN, RNN, DBN, and etc. And if a user implements a large scale deep learning model, it will also be important to support multiple GPU or multiple servers. Also, if you are learning the deep learning model, it would also be important if there are enough examples and references.deep learning framework, Theano, TensorFlow, CNTK, computational graph, CIFAR-10"
900,딥러닝을 이용한 주변 무선단말 파악방안,이웅섭 /Lee， Woong-sup,2018,한국정보통신학회,"최근 단말-대-단말(Device-to-device, D2D) 통신기술이 차세대 무선통신시스템의 핵심기술로 큰 관심을 받고 있다. 이러한 단말간 통신에서는 자신의 주변에 어떠한 단말이 있는지 파악하는 주변단말 탐색(Neighbor discovery)이매우 중요하다. 본 논문에서는 최근 큰 관심을 받고 있는 딥러닝(Deep learning) 기술을 활용하여 단말간 통신에서 주변단말을 파악하는 방안에 대해서 제안한다. 제안 방안은 기존의 방안과 달리 무선채널의 공간적 연관성을 이용하여단말간의 신호 전송 없이 단말이 기지국으로 전송하는 상향링크 파일럿 신호를 기반으로 주변 단말을 찾고 따라서기존의 방식에 비해 신호전송 복잡도(signaling complexity)를 크게 줄일 수 있다. 또한 제안 방안에서는 떨어져 있는거리에 따라서 주변 단말을 분류 가능하여 기존 방안에 비해서 좀 더 세밀한 단말 탐색이 가능하다. 마지막으로 본 논문에서는 tensorflow를 이용한 컴퓨터 시뮬레이션을 통해 제안 방안의 성능을 검증하였다.Recently, the device-to-device (D2D) communication has been conceived as the key technology for the next-generationmobile communication systems. The neighbor discovery in which the nearby users are found, is essential for the properoperation of the D2D communication. In this paper, we propose new neighbor discovery scheme based on deep learningtechnology which has gained a lot of attention recently. In the proposed scheme, the neighboring users can be foundusing the uplink pilot transmission of users only, unlike conventional neighbor discovery schemes in which direct pilotcommunication among users is required, such that the signaling overhead can be greatly reduced in our proposed scheme.
Moreover, the neighbors with different proximity can also be classified accordingly which enables more accurate neighbordiscovery compared to the conventional schemes. The performance of our proposed scheme is verified through thetensorflow-based computer simulations.
키워드 : 딥러닝, 주변단말 탐색, 단말-대-단말 통신Deep learning, Neighbor discovery, Device-to-device communication, Wireless channel correlation"
901,신뢰성있는 딥러닝 기반 분석 모델을 참조하기 위한 딥러닝 기술 언어,문종혁 /Jong Hyeok Mun,2021,한국정보처리학회,"최근 딥러닝은 하드웨어 성능이 향상됨에 따라 자연어 처리, 영상 인식 등의 다양한 기술에 접목되어 활용되고 있다. 이러한 기술들을 활용해 지능형교통 시스템(ITS), 스마트홈, 헬스케어 등의 산업분야에서 데이터를 분석하여 고속도로 속도위반 차량 검출, 에너지 사용량 제어, 응급상황 등과 같은고품질의 서비스를 제공하며, 고품질의 서비스를 제공하기 위해서는 정확도가 향상된 딥러닝 모델이 적용되어야 한다. 이를 위해 서비스 환경의 데이터를분석하기 위한 딥러닝 모델을 개발할 때, 개발자는 신뢰성이 검증된 최신의 딥러닝 모델을 적용할 수 있어야 한다. 이는 개발자가 참조하는 딥러닝 모델에적용된 학습 데이터셋의 정확도를 측정하여 검증할 수 있다. 이러한 검증을 위해서 개발자는 학습 데이터셋, 딥러닝의 계층구조 및 개발 환경 등과 같은내용을 포함하는 딥러닝 모델을 문서화하여 적용하기 위한 구조적인 정보가 필요하다. 본 논문에서는 신뢰성있는 딥러닝 기반 데이터 분석 모델을 참조하기위한 딥러닝 기술 언어를 제안한다. 제안하는 기술 언어는 신뢰성 있는 딥러닝 모델을 개발하는데 필요한 학습데이터셋, 개발 환경 및 설정 등의 정보와더불어 딥러닝 모델의 계층구조를 표현할 수 있다. 제안하는 딥러닝 기술 언어를 이용하여 개발자는 지능형 교통 시스템에서 참조하는 분석 모델의정확도를 검증할 수 있다. 실험에서는 제안하는 언어의 유효성을 검증하기 위해, 번호판 인식 모델을 중심으로 딥러닝 기술 문서의 적용과정을 보인다With the recent advancements of deep learning, companies such as smart home, healthcare, and intelligent transportation systems areutilizing its functionality to provide high-quality services for vehicle detection, emergency situation detection, and controlling energyconsumption. To provide reliable services in such sensitive systems, deep learning models are required to have high accuracy. In orderto develop a deep learning model for analyzing previously mentioned services, developers should utilize the state of the art deep learningmodels that have already been verified for higher accuracy. The developers can verify the accuracy of the referenced model by validatingthe model on the dataset. For this validation, the developer needs structural information to document and apply deep learning models,including metadata such as learning dataset, network architecture, and development environments. In this paper, we propose a descriptionlanguage that represents the network architecture of the deep learning model along with its metadata that are necessary to develop adeep learning model. Through the proposed description language, developers can easily verify the accuracy of the referenced deep learningmodel. Our experiments demonstrate the application scenario of a deep learning description document that focuses on the license platerecognition for the detection of illegally parked vehicles.Trusted Deep Learning, Model Reference, Deep Learning Description Language, Traffic Situation Analysis Model"
902,딥러닝 분석을 이용한 중국 역내·외 위안화 변동성 예측,이우식 /Lee Woosik,2016,한국데이터정보과학회,"2008년 글로벌 금융위기 이후 중국은 위안화 국제화의 점진적 추진을 진행하면서 중국상하이 외환시장과 중국홍콩 외환시장에서 거래되는 통화인 역내위안화와 역외위안화를 형성시켰다. 본 연구는 위안화 국제화와 점진적인 중국 자본계정 개방에 따라 급변하는 외환시장상황의 변동성을 정확하게 파악하기 위해서 GARCH모형 (일반화된 자기회귀 조건부이분산성모형)에 다단계인공신경망을 결합한 MLP-GARCH 모형과 GARCH모형과 기계학습의 일종인 딥러닝 (deep learning)을 통합한 DL-GARCH을 가지고 위안화 변동성예측을 비교 실험과 분석을 하였다. 비교분석 결과 DLGARCH 모형은 MLP-GARCH보다 모형 위안화 역내·외 환율변동성 예측 면에서 더욱 더 개선된예측값을 제공하였다. 그래서 이분산시계열모형을 딥러닝과 결합한 DL-GARCH 모형은 시계열의환율 변동성 예측 문제에 딥러닝을 응용할 수 있음을 확인하였다. 향후 이분산시계열과 결합된 딥러닝 모형은 다른 금융시계열 데이터에 응용하여 그 일반화 가능성을 높일 수 있을 것이다.The People’s Republic of China has vigorously been pursuing the internationalization of the Chinese Yuan or Renminbi after the financial crisis of 2008. In this view, an abrupt increase of use of the Chinese Yuan in the onshore and offshore markets are important milestones to be one of important currencies. One of the most frequently used methods to forecast volatility is GARCH model. Since a prediction error of the GARCH model has been reported quite high, a lot of efforts have been made to improve forecasting capability of the GARCH model. In this paper, we have proposed MLPGARCH and a DL-GARCH by employing Artificial Neural Network to the GARCH. In an application to forecasting Chinese Yuan volatility, we have successfully shown their overall outperformance in forecasting over the GARCH.Deep learning, DL-GARCH, GARCH, The Chinese Yuan, volatility."
903,딥러닝 기법을 이용한 내일강수 예측,하지훈,2016,한국지능시스템학회,"정확한 강수예측을 위해서는 예측인자 선정과 예측방법에 대한 선택이 매우 중요하다. 최근에는 강수예측 방법으로기계학습 기법이 많이 사용되고 있으며, 그 중에서도 특히 인공신경망을 사용한 강수예측 방법은 좋은 성능을 보였다. 본논문에서는 딥러닝 기법 중 하나인 DBN(deep belief network)를 이용한 새로운 강수예측 방법을 제안한다. DBN는 비지도사전 학습을 통해 초기 가중치를 설정하여 기존 인공신경망의 문제점을 보완한다. 예측인자로는 기온, 전일-전주 강수일, 태양과 달 궤도 관련 자료를 선정하였다. 기온과 전일-전주 강수일은 서울에서의 1974년부터 2013년까지 총 40년간의AWS(automatic weather system) 관측 자료를 사용하였고, 태양과 달의 궤도 관련 자료는 서울을 중심으로 계산한 결과를사용하였다. 전체 기간에서 일부는 학습 자료로 사용하여 예측모델을 생성하였고, 나머지를 생성한 모델의 검증 자료로사용하였다. 모델 검증 결과로 나온 예측값들은 확률값을 가지며 임계치를 이용하여 강수유무를 판별하였다. 강수 정확도의척도로 양분예보기법 중 CSI(critical successive index)와 Bias(frequency bias)를 계산하였다. 이를 통해 DBN와 MLP(multilayer perceptron)의 성능을 비교한 결과 DBN의 강수 예측 정확도가 높았고, 수행속도 또한 2배 이상 빨랐다.For accurate precipitation forecasts the choice of weather faoctrs and prediction method is very important. Recently, machinele arning has been widely used for forecasting precipitation, and artificial neural network, one of machine learning techniques, showed good performance. In this paper, we suggest a new method for forecasting precipitation using DBN, one of deep learning techniques.
DBN has an advantage that initial weights are set by unsupervised learning, so this compensates for the defects of artificial neural networks. We used past precipitation, temperature, and the parameters of the sun and moon's motion as features for forecasting precipitation. The dataset consists of observation data which had been measured for 40 years from AWS in Seoul. Experiments were based on 8-fold cross validation. As a result of estimation, we got probabilities of test dataset, so threshold was used for the decision of precipitation. CSI and Bias were used for indicating the precision of precipitation. Our experimental results showed that DBN performed better than MLP.Deep learning, Deep belief network, Precipitation, Forecast"
904,딥러닝 신경망모형을 이용한 실시간 교통정보수집 연구,김대현 /KIM DAE HYON,2016,사단법인 인문사회과학기술융합학회,"비디오 기반의 실시간 교통정보수집 시스템은 자율주행을 포함한 지능형교통체계의 주요 서브시스템중 하나이며, 우수한 예측(추정)력 때문에 핵심 알고리즘으로 인공신경망 모형이 주로 사용되고 있다. 특히, 최근 과학 학술지 네이처에 구글 딥마인드의 알파고가 공개되면서 딥러닝과 인공지능에 대한 전 세계적인 관심이 집중되고 있다. 최근까지 가장 널리 사용된 인공신경망모형은 역전파 모형이었으나 최근 컴퓨터 하드웨어 및 알고리즘의 새로운 개발과 더불어 딥러닝 알고리즘을 이용한 딥인공신경망 모형에 대한 관심이 높아지고 있다. 그러나 아직 딥러닝 인공신경망에 대한 연구는 초기단계에 있는 실정이며 적용분야도 극히 한정되어 있다. 본 연구에서는 실시간 도로교통정보 수집을 위한 영상처리분야에서 딥러닝 알고리즘을 활용하고 실험결과를 통하여 향후 교통정보수집‧분석 분야에서의 딥러닝 인공신경망 모형의 적용가능성을 살펴보고자 한다. 특히, 본 연구에서 도출한 딥러닝 인공신경망모형의 실험결과를 기존 역전파 모형에 의한 실험결과와 비교하여 모형의 우수성을 비교‧평가하고자 한다.Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Normally, artificial neural networks have been used in real-time applications for collecting traffic information because of their remarkable performance in terms of predictive accuracy and real-time computing. Especially, the recent publication of a paper in the journal Nature and the success of Google DeepMind’s world-beating Go program AlphaGo caused significant increases of the public interest in artificial intelligence. Even though various types of learning models have been proposed, the Backpropagation has been the most popular neural network model. However, the Deep learning neural network model has attracted much interest recently. In this study, the Deep leaning neural network will be applied in image processing techniques for the real-time traffic information collection to investigate the performance of Deep learning algorithm in traffic engineering fields. In addition, the experimental results from Deep learing algorithm will be compared with the results from Backpropagation model to evaluate and propose better performing model.Intelligent Transportation Systems (ITS), artificial neural networks, Deep learning, traffic information, image processing"
905,딥러닝의 모형과 응용사례,안성만 /SungMahn Ahn,2016,한국지능정보시스템학회,"딥러닝은 인공신경망(neural network)이라는 인공지능분야의 모형이 발전된 형태로서, 계층구조로 이루어진 인공신경망의 내부계층(hidden layer)이 여러 단계로 이루어진 구조이다. 딥러닝에서의 주요 모형은 합성곱신경망(convolutional neural network), 순환신경망(recurrent neural network), 그리고 심층신뢰신경망(deep belief network)의 세가지라고 할 수있다. 그 중에서 현재 흥미로운 연구가 많이 발표되어서 관심이 집중되고 있는 모형은 지도학습(supervised learning)모형인 처음 두 개의 모형이다. 따라서 본 논문에서는 지도학습모형의 가중치를 최적화하는 기본적인 방법인 오류역전파 알고리즘을 살펴본 뒤에 합성곱신경망과 순환신경망의 구조와 응용사례 등을 살펴보고자 한다. 본문에서 다루지 않은 모형인 심층신뢰신경망은 아직까지는 합성곱신경망 이나 순환신경망보다는 상대적으로 주목을 덜 받고 있다. 그러나 심층신뢰신경망은 CNN이나 RNN과는 달리 비지도학습(unsupervised learning)모형이며, 사람이나 동물은 관찰을 통해서 스스로 학습한다는 점에서 궁극적으로는 비지도학습모형이 더 많이 연구되어야 할 주제가 될 것이다.Deep learning model is a kind of neural networks that allows multiple hidden layers. There are various deep learning architectures such as convolutional neural networks, deep belief networks and recurrent neural networks. Those have been applied to fields like computer vision, automatic speech recognition, natural language processing, audio recognition and bioinformatics where they have been shown to produce state-of-the-art results on various tasks. Among those architectures, convolutional neural networks and recurrent neural networks are classified as the supervised learning model. And in recent years, those supervised learning models have gained more popularity than unsupervised learning models such as deep belief networks, because supervised learning models have shown fashionable applications in such fields mentioned above.
Deep learning models can be trained with backpropagation algorithm. Backpropagation is an abbreviation for “backward propagation of errors” and a common method of training artificial neural networks used in conjunction with an optimization method such as gradient descent. The method calculates the gradient of an error function with respect to all the weights in the network. The gradient is fed to the optimization method which in turn uses it to update the weights, in an attempt to minimize the error function.
Convolutional neural networks use a special architecture which is particularly well-adapted to classify images. Using this architecture makes convolutional networks fast to train. This, in turn, helps us train deep, muti-layer networks, which are very good at classifying images. These days, deep convolutional networks are used in most neural networks for image recognition. Convolutional neural networks use three basic ideas: local receptive fields, shared weights, and pooling. By local receptive fields, we mean that each neuron in the first(or any) hidden layer will be connected to a small region of the input(or previous layer’s) neurons. Shared weights mean that we’re going to use the same weights and bias for each of the local receptive field. This means that all the neurons in the hidden layer detect exactly the same feature, just at different locations in the input image. In addition to the convolutional layers just described, convolutional neural networks also contain pooling layers. Pooling layers are usually used immediately after convolutional layers. What the pooling layers do is to simplify the information in the output from the convolutional layer.
Recent convolutional network architectures have 10 to 20 hidden layers and billions of connections between units. Training deep learning networks has taken weeks several years ago, but thanks to progress in GPU and algorithm enhancement, training time has reduced to several hours.
Neural networks with time-varying behavior are known as recurrent neural networks or RNNs. A recurrent neural network is a class of artificial neural network where connections between units form a directed cycle. This creates an internal state of the network which allows it to exhibit dynamic temporal behavior. Unlike feedforward neural networks, RNNs can use their internal memory to process arbitrary sequences of inputs. Early RNN models turned out to be very difficult to train, harder even than deep feedforward networks. The reason is the unstable gradient problem such as vanishing gradient and exploding gradient. The gradient can get smaller and smaller as it is propagated back through layers. This makes learning in early layers extremely slow. The problem actually gets worse in RNNs, since gradients aren’t just propagated backward through layers, they’re propagated backward through time. If the network runs for a long time, that can make the gradient extremely unstable and hard to learn from. It has been possible to incorporate an idea known as long short-term memory units (LSTMs) into RNNs. LSTMs make it much easier to get good results when training RNNs, and many recent papers make use of LSTMs or related ideas.Deep learning, Convolutional neural networks, Recurrent neural networks, Error backpropagation algorithm"
906,딥러닝분석과 기술적 분석 지표를 이용한 한국 코스피주가지수 방향성 예측,이우식 /Lee Woosik,2017,한국데이터정보과학회,"2016년 3월 구글 (Google)의 바둑인공지능 알파고 (AlphaGo)가 이세돌 9단과의 바둑대결에서승리한 이후 다양한 분야에서 인공지능 사용에 대한 관심이 높아지고 있는 가운데 금융투자 분야에서도 인공지능과 투자자문 전문가의 합성어인 로보어드바이저 (Robo-Advisor)에 대한 관심이 높아지고 있다. 인공지능 (artificial intelligence)기반의 의사결정은 비용 절감은 물론 효과적인 의사결정을가능하게 한다는 점에서 큰 장점이 있다. 본 연구에서는 기술적 분석 (technical analysis) 지표와 딥러닝 (deep learning) 모형을 결합하여 한국 코스피 지수를 예측하는 모형을 개발하고 제시한 모형들의 예측력을 비교, 분석한다. 분석 결과 기술적 분석 지표에 딥러닝 알고리즘을 결합한 모형이 주가지수 방향성 예측 문제에 응용될 수 있음을 확인하였다. 향후 본 연구에서 제안된 기술적 분석 지표와딥러닝모형을 결합한 기법은 로보어드바이저서비스에 응용할 수 있는 일반화 가능성을 보여준다Since Google’s AlphaGo defeated a world champion of Go players in 2016, there have been many interests in the deep learning. In the financial sector, a Robo-Advisor using deep learning gains a significant attention, which builds and manages portfolios of financial instruments for investors.In this paper, we have proposed the a deep learning algorithm geared toward identification and forecast of the KOSPI index direction,and we also have compared the accuracy of the prediction.In an application of forecasting the financial market index direction, we have shown that the Robo-Advisor using deep learning has a significant effect on finance industry. The Robo-Advisor collects a massive data such as earnings statements, news reports and regulatory filings, analyzes those and recommends investors how to view market trends and identify the best time to purchase financial assets. On the other hand, the Robo-Advisor allows businesses to learn more about their customers, develop better marketing strategies, increase sales and decrease costs.Artificial intelligence, deep learning, FinTech, Robo-Advisor, technical analysis."
907,딥러닝의 효율적인 학습을 위한 학습데이터 선별 기법에 관한 연구,주경돈 /Gyoung-don Joo,2017,한국정보과학회,"빅데이터의 증가와 병렬 하드웨어 처리, 학습 알고리즘의 개선은 딥러닝의 발전을 가져왔으며 인공지능이 적용 가능한 영역을 획기적으로 확장시키고 있다. 하지만 이러한 대규모의 학습 데이터에 기반한 딥러닝은 학습시간이 증가되고, 더 나아가 학습 데이터의 구축을 위한 레이블링(labeling) 비용이 데이터 수에 비례하여 증가하게 된다. 따라서 본 논문에서는, 딥러닝의 정확도를 최대한 유지하면서 필요한 학습 데이터의 수를 줄일수 있도록, 전체 데이터에서 제한된 수의 학습 데이터의 후보를 선별하는 기법에 대하여 연구한다. 특히 기존의 능동 학습에 기반한 기법들이 학습 데이터의 수가 매우 적어질 경우 딥러닝의 정확도가 급격히 떨어지는 문제를 해결할 수 있는 새로운 기법을 제안하였으며, 제안된 기법으로 선별된 학습 데이터는 그 수의 변화에 대해 딥러닝이 일관되게 경쟁력있는 정확도를 가짐을 다양한 실험을 통하여 검증하였다.The increase of big data, parallel hardware processing and enhanced learning algorithms have led to the development of deep learning and are dramatically expanding the domain of applications based on artificial intelligence. However, such large-scale training data extends the learning time, and furthermore, the labeling cost for building the training set increases in proportion to the volume of data. Therefore, in this paper, we study how to select a limited number of candidates for training set from given data so as to reduce the number of training set while maintaining the accuracy of deep learning. Especially, we found that, with the training set selected by the previous work, the accuracy of deep learning drastically decreases when the number of training set is very small and so proposed new methods to resolve this fall-off in accuracy. Through experiments, we showed that deep learning consistently has the competitive accuracy with the training set selected by the proposed method regardless of the size of training set.Deep Learning, Train Set Selection, Active Learning, Gamma distribution"
908,딥러닝을 이용한 범용적 스테그아날리시스,김현재 /Hyunjae Kim,2017,한국정보과학회,"스테그아날리시스(Steganalysis)란 이미지 등 일반적인 자료에 암호화된 정보를 은닉하는 스테가노그래피(Steganography)에 대한 검출 및 분석 방법으로, 기계학습 기반 방법론을 포함한다. 기존 기계학습 기반 스테그아날리시스는 영상(Image)의 특징(Feature) 추출 및 모델링에 기반하며, 최근 딥러닝(Deep Learning)의 적용으로 검출 정확도가 큰 폭으로 향상되었다. 하지만 현존하는 스테그아날리시스 모델은 단일 스테가노그래피 기법에 대해 국한되어 있어 학습에 사용되지 않은 스테고(Stego) 이미지의 경우 검출이 불가능한 결정적 한계를 가진다. 본 연구에서는 다양한 스테가노그래피 기법으로 생성된 스테고이미지에 딥러닝을 적용하여 스테그아날리시스를 학습하는 범용적 모델을 제안한다. 다양한 실험을 통해 제안 기법의 효용성 및 가능성을 확인하고, 범용적 스테그아날리시스 모델이 각각에 특화된 검출 기법과 유사한 정확도로 스테고 이미지를 검출할 수 있음을 보인다.Steganalysis is to detect information hidden by steganography inside general data such as images. There are stegoanalysis techniques that use machine learning (ML). Existing ML approaches to steganalysis are based on extracting features from stego images and modeling them.
Recently deep learning-based methodologies have shown significant improvements in detection accuracy. However, all the existing methods, including deep learning-based ones, have a critical limitation in that they can only detect stego images that are created by a specific steganography method. In this paper, we propose a generalized steganalysis method that can model multiple types of stego images using deep learning. Through various experiments, we confirm the effectiveness of our approach and envision directions for future research. In particular, we show that our method can detect each type of steganography with the same level of accuracy as that of a steganalysis method dedicated to that type of steganography, thereby demonstrating the general applicability of our approach to multiple types of stego images.steganalysis, steganography, deep learning, generalized steganalysis model"
909,"딥러닝 프레임워크의 비교: 티아노, 텐서플로, CNTK를 중심으로",정여진 /Yeojin Chung,2017,한국지능정보시스템학회,"딥러닝 프레임워크의 대표적인 기능으로는 ‘자동미분’과 ‘GPU의 활용’ 등을 들 수 있다. 본 논문은 파이썬의 라이브러리 형태로 사용 가능한 프레임워크 중에서 구글의 텐서플로와 마이크로소프트의 CNTK, 그리고 텐서 플로의 원조라고 할 수 있는 티아노를 비교하였다. 본문에서는 자동미분의 개념과 GPU의 활용형태를 간단히 설명하고, 그 다음에 logistic regression을 실행하는 예를 통하여 각 프레임워크의 문법을 알아본 뒤에, 마지막으로 대표적인 딥러닝 응용인 CNN의 예제를 실행시켜보고 코딩의 편의성과 실행속도 등을 확인해 보았다. 그 결과, 편의성의 관점에서 보면 티아노가 가장 코딩 하기가 어렵고, CNTK와 텐서플로는 많은 부분이 비슷하게 추상화 되어 있어서 코딩이 비슷하지만 가중치와 편향을 직접 정의하느냐의 여부에서 차이를 보였다. 그리고 각프레임워크의 실행속도에 대한 평가는 ‘큰 차이는 없다’는 것이다. 텐서플로는 티아노에 비하여 속도가 느리다는 평가가 있어왔는데, 본 연구의 실험에 의하면, 비록 CNN 모형에 국한되었지만, 텐서플로가 아주 조금이지만빠른 것으로 나타났다. CNTK의 경우에도, 비록 실험환경이 달랐지만, 실험환경의 차이에 의한 속도의 차이의 편차범위 이내에 있는 것으로 판단이 되었다. 본 연구에서는 세 종류의 딥러닝 프레임워크만을 살펴보았는데,위키피디아에 따르면 딥러닝 프레임워크의 종류는 12가지가 있으며, 각 프레임워크의 특징을 15가지 속성으로구분하여 차이를 특정하고 있다. 그 많은 속성 중에서 사용자의 입장에서 볼 때 중요한 속성은 어떤 언어(파이썬, C++, Java, 등)로 사용가능한지, 어떤 딥러닝 모형에 대한 라이브러리가 잘 구현되어 있는지 등일 것이다. 그리고 사용자가 대규모의 딥러닝 모형을 구축한다면, 다중 GPU 혹은 다중 서버를 지원하는지의 여부도 중요할 것이다. 또한 딥러닝 모형을 처음 학습하는 경우에는 사용설명서가 많은지 예제 프로그램이 많은지 여부도 중요한 기준이 될 것이다.The deep learning framework is software designed to help develop deep learning models. Some of its important functions include “automatic differentiation” and “utilization of GPU”. The list of popular deep learning framework includes Caffe (BVLC) and Theano (University of Montreal). And recently, Microsoft's deep learning framework, Microsoft Cognitive Toolkit, was released as open-source license, following Google’s Tensorflow a year earlier. The early deep learning frameworks have been developed mainly for research at universities. Beginning with the inception of Tensorflow, however, it seems that companies such as Microsoft and Facebook have started to join the competition of framework development. Given the trend, Google and other companies are expected to continue investing in the deep learning framework to bring forward the initiative in the artificial intelligence business. From this point of view, we think it is a good time to compare some of deep learning frameworks. So we compare three deep learning frameworks which can be used as a Python library. Those are Google's Tensorflow, Microsoft’s CNTK, and Theano which is sort of a predecessor of the preceding two.
The most common and important function of deep learning frameworks is the ability to perform automatic differentiation. Basically all the mathematical expressions of deep learning models can be represented as computational graphs, which consist of nodes and edges. Partial derivatives on each edge of a computational graph can then be obtained. With the partial derivatives, we can let software compute differentiation of any node with respect to any variable by utilizing chain rule of Calculus.
First of all, the convenience of coding is in the order of CNTK, Tensorflow, and Theano. The criterion is simply based on the lengths of the codes and the learning curve and the ease of coding are not the main concern. According to the criteria, Theano was the most difficult to implement with, and CNTK and Tensorflow were somewhat easier. With Tensorflow, we need to define weight variables and biases explicitly. The reason that CNTK and Tensorflow are easier to implement with is that those frameworks provide us with more abstraction than Theano. We, however, need to mention that low-level coding is not always bad. It gives us flexibility of coding. With the low-level coding such as in Theano, we can implement and test any new deep learning models or any new search methods that we can think of.
The assessment of the execution speed of each framework is that there is not meaningful difference. According to the experiment, execution speeds of Theano and Tensorflow are very similar, although the experiment was limited to a CNN model. In the case of CNTK, the experimental environment was not maintained as the same. The code written in CNTK has to be run in PC environment without GPU where codes execute as much as 50 times slower than with GPU. But we concluded that the difference of execution speed was within the range of variation caused by the different hardware setup.
In this study, we compared three types of deep learning framework: Theano, Tensorflow, and CNTK. According to Wikipedia, there are 12 available deep learning frameworks. And 15 different attributes differentiate each framework. Some of the important attributes would include interface language (Python, C ++, Java, etc.) and the availability of libraries on various deep learning models such as CNN, RNN, DBN, and etc. And if a user implements a large scale deep learning model, it will also be important to support multiple GPU or multiple servers. Also, if you are learning the deep learning model, it would also be important if there are enough examples and references.deep learning framework, Theano, TensorFlow, CNTK, computational graph, CIFAR-10"
910,딥러닝을 이용한 주변 무선단말 파악방안,이웅섭 /Lee， Woong-sup,2018,한국정보통신학회,"최근 단말-대-단말(Device-to-device, D2D) 통신기술이 차세대 무선통신시스템의 핵심기술로 큰 관심을 받고 있다. 이러한 단말간 통신에서는 자신의 주변에 어떠한 단말이 있는지 파악하는 주변단말 탐색(Neighbor discovery)이매우 중요하다. 본 논문에서는 최근 큰 관심을 받고 있는 딥러닝(Deep learning) 기술을 활용하여 단말간 통신에서 주변단말을 파악하는 방안에 대해서 제안한다. 제안 방안은 기존의 방안과 달리 무선채널의 공간적 연관성을 이용하여단말간의 신호 전송 없이 단말이 기지국으로 전송하는 상향링크 파일럿 신호를 기반으로 주변 단말을 찾고 따라서기존의 방식에 비해 신호전송 복잡도(signaling complexity)를 크게 줄일 수 있다. 또한 제안 방안에서는 떨어져 있는거리에 따라서 주변 단말을 분류 가능하여 기존 방안에 비해서 좀 더 세밀한 단말 탐색이 가능하다. 마지막으로 본 논문에서는 tensorflow를 이용한 컴퓨터 시뮬레이션을 통해 제안 방안의 성능을 검증하였다.Recently, the device-to-device (D2D) communication has been conceived as the key technology for the next-generationmobile communication systems. The neighbor discovery in which the nearby users are found, is essential for the properoperation of the D2D communication. In this paper, we propose new neighbor discovery scheme based on deep learningtechnology which has gained a lot of attention recently. In the proposed scheme, the neighboring users can be foundusing the uplink pilot transmission of users only, unlike conventional neighbor discovery schemes in which direct pilotcommunication among users is required, such that the signaling overhead can be greatly reduced in our proposed scheme.
Moreover, the neighbors with different proximity can also be classified accordingly which enables more accurate neighbordiscovery compared to the conventional schemes. The performance of our proposed scheme is verified through thetensorflow-based computer simulations.
키워드 : 딥러닝, 주변단말 탐색, 단말-대-단말 통신Deep learning, Neighbor discovery, Device-to-device communication, Wireless channel correlation"
911,신뢰성있는 딥러닝 기반 분석 모델을 참조하기 위한 딥러닝 기술 언어,문종혁 /Jong Hyeok Mun,2021,한국정보처리학회,"최근 딥러닝은 하드웨어 성능이 향상됨에 따라 자연어 처리, 영상 인식 등의 다양한 기술에 접목되어 활용되고 있다. 이러한 기술들을 활용해 지능형교통 시스템(ITS), 스마트홈, 헬스케어 등의 산업분야에서 데이터를 분석하여 고속도로 속도위반 차량 검출, 에너지 사용량 제어, 응급상황 등과 같은고품질의 서비스를 제공하며, 고품질의 서비스를 제공하기 위해서는 정확도가 향상된 딥러닝 모델이 적용되어야 한다. 이를 위해 서비스 환경의 데이터를분석하기 위한 딥러닝 모델을 개발할 때, 개발자는 신뢰성이 검증된 최신의 딥러닝 모델을 적용할 수 있어야 한다. 이는 개발자가 참조하는 딥러닝 모델에적용된 학습 데이터셋의 정확도를 측정하여 검증할 수 있다. 이러한 검증을 위해서 개발자는 학습 데이터셋, 딥러닝의 계층구조 및 개발 환경 등과 같은내용을 포함하는 딥러닝 모델을 문서화하여 적용하기 위한 구조적인 정보가 필요하다. 본 논문에서는 신뢰성있는 딥러닝 기반 데이터 분석 모델을 참조하기위한 딥러닝 기술 언어를 제안한다. 제안하는 기술 언어는 신뢰성 있는 딥러닝 모델을 개발하는데 필요한 학습데이터셋, 개발 환경 및 설정 등의 정보와더불어 딥러닝 모델의 계층구조를 표현할 수 있다. 제안하는 딥러닝 기술 언어를 이용하여 개발자는 지능형 교통 시스템에서 참조하는 분석 모델의정확도를 검증할 수 있다. 실험에서는 제안하는 언어의 유효성을 검증하기 위해, 번호판 인식 모델을 중심으로 딥러닝 기술 문서의 적용과정을 보인다With the recent advancements of deep learning, companies such as smart home, healthcare, and intelligent transportation systems areutilizing its functionality to provide high-quality services for vehicle detection, emergency situation detection, and controlling energyconsumption. To provide reliable services in such sensitive systems, deep learning models are required to have high accuracy. In orderto develop a deep learning model for analyzing previously mentioned services, developers should utilize the state of the art deep learningmodels that have already been verified for higher accuracy. The developers can verify the accuracy of the referenced model by validatingthe model on the dataset. For this validation, the developer needs structural information to document and apply deep learning models,including metadata such as learning dataset, network architecture, and development environments. In this paper, we propose a descriptionlanguage that represents the network architecture of the deep learning model along with its metadata that are necessary to develop adeep learning model. Through the proposed description language, developers can easily verify the accuracy of the referenced deep learningmodel. Our experiments demonstrate the application scenario of a deep learning description document that focuses on the license platerecognition for the detection of illegally parked vehicles.Trusted Deep Learning, Model Reference, Deep Learning Description Language, Traffic Situation Analysis Model"
912,딥러닝 분석을 이용한 중국 역내·외 위안화 변동성 예측,이우식 /Lee Woosik,2016,한국데이터정보과학회,"2008년 글로벌 금융위기 이후 중국은 위안화 국제화의 점진적 추진을 진행하면서 중국상하이 외환시장과 중국홍콩 외환시장에서 거래되는 통화인 역내위안화와 역외위안화를 형성시켰다. 본 연구는 위안화 국제화와 점진적인 중국 자본계정 개방에 따라 급변하는 외환시장상황의 변동성을 정확하게 파악하기 위해서 GARCH모형 (일반화된 자기회귀 조건부이분산성모형)에 다단계인공신경망을 결합한 MLP-GARCH 모형과 GARCH모형과 기계학습의 일종인 딥러닝 (deep learning)을 통합한 DL-GARCH을 가지고 위안화 변동성예측을 비교 실험과 분석을 하였다. 비교분석 결과 DLGARCH 모형은 MLP-GARCH보다 모형 위안화 역내·외 환율변동성 예측 면에서 더욱 더 개선된예측값을 제공하였다. 그래서 이분산시계열모형을 딥러닝과 결합한 DL-GARCH 모형은 시계열의환율 변동성 예측 문제에 딥러닝을 응용할 수 있음을 확인하였다. 향후 이분산시계열과 결합된 딥러닝 모형은 다른 금융시계열 데이터에 응용하여 그 일반화 가능성을 높일 수 있을 것이다.The People’s Republic of China has vigorously been pursuing the internationalization of the Chinese Yuan or Renminbi after the financial crisis of 2008. In this view, an abrupt increase of use of the Chinese Yuan in the onshore and offshore markets are important milestones to be one of important currencies. One of the most frequently used methods to forecast volatility is GARCH model. Since a prediction error of the GARCH model has been reported quite high, a lot of efforts have been made to improve forecasting capability of the GARCH model. In this paper, we have proposed MLPGARCH and a DL-GARCH by employing Artificial Neural Network to the GARCH. In an application to forecasting Chinese Yuan volatility, we have successfully shown their overall outperformance in forecasting over the GARCH.Deep learning, DL-GARCH, GARCH, The Chinese Yuan, volatility."
913,딥러닝 기법을 이용한 내일강수 예측,하지훈,2016,한국지능시스템학회,"정확한 강수예측을 위해서는 예측인자 선정과 예측방법에 대한 선택이 매우 중요하다. 최근에는 강수예측 방법으로기계학습 기법이 많이 사용되고 있으며, 그 중에서도 특히 인공신경망을 사용한 강수예측 방법은 좋은 성능을 보였다. 본논문에서는 딥러닝 기법 중 하나인 DBN(deep belief network)를 이용한 새로운 강수예측 방법을 제안한다. DBN는 비지도사전 학습을 통해 초기 가중치를 설정하여 기존 인공신경망의 문제점을 보완한다. 예측인자로는 기온, 전일-전주 강수일, 태양과 달 궤도 관련 자료를 선정하였다. 기온과 전일-전주 강수일은 서울에서의 1974년부터 2013년까지 총 40년간의AWS(automatic weather system) 관측 자료를 사용하였고, 태양과 달의 궤도 관련 자료는 서울을 중심으로 계산한 결과를사용하였다. 전체 기간에서 일부는 학습 자료로 사용하여 예측모델을 생성하였고, 나머지를 생성한 모델의 검증 자료로사용하였다. 모델 검증 결과로 나온 예측값들은 확률값을 가지며 임계치를 이용하여 강수유무를 판별하였다. 강수 정확도의척도로 양분예보기법 중 CSI(critical successive index)와 Bias(frequency bias)를 계산하였다. 이를 통해 DBN와 MLP(multilayer perceptron)의 성능을 비교한 결과 DBN의 강수 예측 정확도가 높았고, 수행속도 또한 2배 이상 빨랐다.For accurate precipitation forecasts the choice of weather faoctrs and prediction method is very important. Recently, machinele arning has been widely used for forecasting precipitation, and artificial neural network, one of machine learning techniques, showed good performance. In this paper, we suggest a new method for forecasting precipitation using DBN, one of deep learning techniques.
DBN has an advantage that initial weights are set by unsupervised learning, so this compensates for the defects of artificial neural networks. We used past precipitation, temperature, and the parameters of the sun and moon's motion as features for forecasting precipitation. The dataset consists of observation data which had been measured for 40 years from AWS in Seoul. Experiments were based on 8-fold cross validation. As a result of estimation, we got probabilities of test dataset, so threshold was used for the decision of precipitation. CSI and Bias were used for indicating the precision of precipitation. Our experimental results showed that DBN performed better than MLP.Deep learning, Deep belief network, Precipitation, Forecast"
914,딥러닝 신경망모형을 이용한 실시간 교통정보수집 연구,김대현 /KIM DAE HYON,2016,사단법인 인문사회과학기술융합학회,"비디오 기반의 실시간 교통정보수집 시스템은 자율주행을 포함한 지능형교통체계의 주요 서브시스템중 하나이며, 우수한 예측(추정)력 때문에 핵심 알고리즘으로 인공신경망 모형이 주로 사용되고 있다. 특히, 최근 과학 학술지 네이처에 구글 딥마인드의 알파고가 공개되면서 딥러닝과 인공지능에 대한 전 세계적인 관심이 집중되고 있다. 최근까지 가장 널리 사용된 인공신경망모형은 역전파 모형이었으나 최근 컴퓨터 하드웨어 및 알고리즘의 새로운 개발과 더불어 딥러닝 알고리즘을 이용한 딥인공신경망 모형에 대한 관심이 높아지고 있다. 그러나 아직 딥러닝 인공신경망에 대한 연구는 초기단계에 있는 실정이며 적용분야도 극히 한정되어 있다. 본 연구에서는 실시간 도로교통정보 수집을 위한 영상처리분야에서 딥러닝 알고리즘을 활용하고 실험결과를 통하여 향후 교통정보수집‧분석 분야에서의 딥러닝 인공신경망 모형의 적용가능성을 살펴보고자 한다. 특히, 본 연구에서 도출한 딥러닝 인공신경망모형의 실험결과를 기존 역전파 모형에 의한 실험결과와 비교하여 모형의 우수성을 비교‧평가하고자 한다.Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Normally, artificial neural networks have been used in real-time applications for collecting traffic information because of their remarkable performance in terms of predictive accuracy and real-time computing. Especially, the recent publication of a paper in the journal Nature and the success of Google DeepMind’s world-beating Go program AlphaGo caused significant increases of the public interest in artificial intelligence. Even though various types of learning models have been proposed, the Backpropagation has been the most popular neural network model. However, the Deep learning neural network model has attracted much interest recently. In this study, the Deep leaning neural network will be applied in image processing techniques for the real-time traffic information collection to investigate the performance of Deep learning algorithm in traffic engineering fields. In addition, the experimental results from Deep learing algorithm will be compared with the results from Backpropagation model to evaluate and propose better performing model.Intelligent Transportation Systems (ITS), artificial neural networks, Deep learning, traffic information, image processing"
915,딥러닝의 모형과 응용사례,안성만 /SungMahn Ahn,2016,한국지능정보시스템학회,"딥러닝은 인공신경망(neural network)이라는 인공지능분야의 모형이 발전된 형태로서, 계층구조로 이루어진 인공신경망의 내부계층(hidden layer)이 여러 단계로 이루어진 구조이다. 딥러닝에서의 주요 모형은 합성곱신경망(convolutional neural network), 순환신경망(recurrent neural network), 그리고 심층신뢰신경망(deep belief network)의 세가지라고 할 수있다. 그 중에서 현재 흥미로운 연구가 많이 발표되어서 관심이 집중되고 있는 모형은 지도학습(supervised learning)모형인 처음 두 개의 모형이다. 따라서 본 논문에서는 지도학습모형의 가중치를 최적화하는 기본적인 방법인 오류역전파 알고리즘을 살펴본 뒤에 합성곱신경망과 순환신경망의 구조와 응용사례 등을 살펴보고자 한다. 본문에서 다루지 않은 모형인 심층신뢰신경망은 아직까지는 합성곱신경망 이나 순환신경망보다는 상대적으로 주목을 덜 받고 있다. 그러나 심층신뢰신경망은 CNN이나 RNN과는 달리 비지도학습(unsupervised learning)모형이며, 사람이나 동물은 관찰을 통해서 스스로 학습한다는 점에서 궁극적으로는 비지도학습모형이 더 많이 연구되어야 할 주제가 될 것이다.Deep learning model is a kind of neural networks that allows multiple hidden layers. There are various deep learning architectures such as convolutional neural networks, deep belief networks and recurrent neural networks. Those have been applied to fields like computer vision, automatic speech recognition, natural language processing, audio recognition and bioinformatics where they have been shown to produce state-of-the-art results on various tasks. Among those architectures, convolutional neural networks and recurrent neural networks are classified as the supervised learning model. And in recent years, those supervised learning models have gained more popularity than unsupervised learning models such as deep belief networks, because supervised learning models have shown fashionable applications in such fields mentioned above.
Deep learning models can be trained with backpropagation algorithm. Backpropagation is an abbreviation for “backward propagation of errors” and a common method of training artificial neural networks used in conjunction with an optimization method such as gradient descent. The method calculates the gradient of an error function with respect to all the weights in the network. The gradient is fed to the optimization method which in turn uses it to update the weights, in an attempt to minimize the error function.
Convolutional neural networks use a special architecture which is particularly well-adapted to classify images. Using this architecture makes convolutional networks fast to train. This, in turn, helps us train deep, muti-layer networks, which are very good at classifying images. These days, deep convolutional networks are used in most neural networks for image recognition. Convolutional neural networks use three basic ideas: local receptive fields, shared weights, and pooling. By local receptive fields, we mean that each neuron in the first(or any) hidden layer will be connected to a small region of the input(or previous layer’s) neurons. Shared weights mean that we’re going to use the same weights and bias for each of the local receptive field. This means that all the neurons in the hidden layer detect exactly the same feature, just at different locations in the input image. In addition to the convolutional layers just described, convolutional neural networks also contain pooling layers. Pooling layers are usually used immediately after convolutional layers. What the pooling layers do is to simplify the information in the output from the convolutional layer.
Recent convolutional network architectures have 10 to 20 hidden layers and billions of connections between units. Training deep learning networks has taken weeks several years ago, but thanks to progress in GPU and algorithm enhancement, training time has reduced to several hours.
Neural networks with time-varying behavior are known as recurrent neural networks or RNNs. A recurrent neural network is a class of artificial neural network where connections between units form a directed cycle. This creates an internal state of the network which allows it to exhibit dynamic temporal behavior. Unlike feedforward neural networks, RNNs can use their internal memory to process arbitrary sequences of inputs. Early RNN models turned out to be very difficult to train, harder even than deep feedforward networks. The reason is the unstable gradient problem such as vanishing gradient and exploding gradient. The gradient can get smaller and smaller as it is propagated back through layers. This makes learning in early layers extremely slow. The problem actually gets worse in RNNs, since gradients aren’t just propagated backward through layers, they’re propagated backward through time. If the network runs for a long time, that can make the gradient extremely unstable and hard to learn from. It has been possible to incorporate an idea known as long short-term memory units (LSTMs) into RNNs. LSTMs make it much easier to get good results when training RNNs, and many recent papers make use of LSTMs or related ideas.Deep learning, Convolutional neural networks, Recurrent neural networks, Error backpropagation algorithm"
916,딥러닝분석과 기술적 분석 지표를 이용한 한국 코스피주가지수 방향성 예측,이우식 /Lee Woosik,2017,한국데이터정보과학회,"2016년 3월 구글 (Google)의 바둑인공지능 알파고 (AlphaGo)가 이세돌 9단과의 바둑대결에서승리한 이후 다양한 분야에서 인공지능 사용에 대한 관심이 높아지고 있는 가운데 금융투자 분야에서도 인공지능과 투자자문 전문가의 합성어인 로보어드바이저 (Robo-Advisor)에 대한 관심이 높아지고 있다. 인공지능 (artificial intelligence)기반의 의사결정은 비용 절감은 물론 효과적인 의사결정을가능하게 한다는 점에서 큰 장점이 있다. 본 연구에서는 기술적 분석 (technical analysis) 지표와 딥러닝 (deep learning) 모형을 결합하여 한국 코스피 지수를 예측하는 모형을 개발하고 제시한 모형들의 예측력을 비교, 분석한다. 분석 결과 기술적 분석 지표에 딥러닝 알고리즘을 결합한 모형이 주가지수 방향성 예측 문제에 응용될 수 있음을 확인하였다. 향후 본 연구에서 제안된 기술적 분석 지표와딥러닝모형을 결합한 기법은 로보어드바이저서비스에 응용할 수 있는 일반화 가능성을 보여준다Since Google’s AlphaGo defeated a world champion of Go players in 2016, there have been many interests in the deep learning. In the financial sector, a Robo-Advisor using deep learning gains a significant attention, which builds and manages portfolios of financial instruments for investors.In this paper, we have proposed the a deep learning algorithm geared toward identification and forecast of the KOSPI index direction,and we also have compared the accuracy of the prediction.In an application of forecasting the financial market index direction, we have shown that the Robo-Advisor using deep learning has a significant effect on finance industry. The Robo-Advisor collects a massive data such as earnings statements, news reports and regulatory filings, analyzes those and recommends investors how to view market trends and identify the best time to purchase financial assets. On the other hand, the Robo-Advisor allows businesses to learn more about their customers, develop better marketing strategies, increase sales and decrease costs.Artificial intelligence, deep learning, FinTech, Robo-Advisor, technical analysis."
917,딥러닝의 효율적인 학습을 위한 학습데이터 선별 기법에 관한 연구,주경돈 /Gyoung-don Joo,2017,한국정보과학회,"빅데이터의 증가와 병렬 하드웨어 처리, 학습 알고리즘의 개선은 딥러닝의 발전을 가져왔으며 인공지능이 적용 가능한 영역을 획기적으로 확장시키고 있다. 하지만 이러한 대규모의 학습 데이터에 기반한 딥러닝은 학습시간이 증가되고, 더 나아가 학습 데이터의 구축을 위한 레이블링(labeling) 비용이 데이터 수에 비례하여 증가하게 된다. 따라서 본 논문에서는, 딥러닝의 정확도를 최대한 유지하면서 필요한 학습 데이터의 수를 줄일수 있도록, 전체 데이터에서 제한된 수의 학습 데이터의 후보를 선별하는 기법에 대하여 연구한다. 특히 기존의 능동 학습에 기반한 기법들이 학습 데이터의 수가 매우 적어질 경우 딥러닝의 정확도가 급격히 떨어지는 문제를 해결할 수 있는 새로운 기법을 제안하였으며, 제안된 기법으로 선별된 학습 데이터는 그 수의 변화에 대해 딥러닝이 일관되게 경쟁력있는 정확도를 가짐을 다양한 실험을 통하여 검증하였다.The increase of big data, parallel hardware processing and enhanced learning algorithms have led to the development of deep learning and are dramatically expanding the domain of applications based on artificial intelligence. However, such large-scale training data extends the learning time, and furthermore, the labeling cost for building the training set increases in proportion to the volume of data. Therefore, in this paper, we study how to select a limited number of candidates for training set from given data so as to reduce the number of training set while maintaining the accuracy of deep learning. Especially, we found that, with the training set selected by the previous work, the accuracy of deep learning drastically decreases when the number of training set is very small and so proposed new methods to resolve this fall-off in accuracy. Through experiments, we showed that deep learning consistently has the competitive accuracy with the training set selected by the proposed method regardless of the size of training set.Deep Learning, Train Set Selection, Active Learning, Gamma distribution"
918,딥러닝을 이용한 범용적 스테그아날리시스,김현재 /Hyunjae Kim,2017,한국정보과학회,"스테그아날리시스(Steganalysis)란 이미지 등 일반적인 자료에 암호화된 정보를 은닉하는 스테가노그래피(Steganography)에 대한 검출 및 분석 방법으로, 기계학습 기반 방법론을 포함한다. 기존 기계학습 기반 스테그아날리시스는 영상(Image)의 특징(Feature) 추출 및 모델링에 기반하며, 최근 딥러닝(Deep Learning)의 적용으로 검출 정확도가 큰 폭으로 향상되었다. 하지만 현존하는 스테그아날리시스 모델은 단일 스테가노그래피 기법에 대해 국한되어 있어 학습에 사용되지 않은 스테고(Stego) 이미지의 경우 검출이 불가능한 결정적 한계를 가진다. 본 연구에서는 다양한 스테가노그래피 기법으로 생성된 스테고이미지에 딥러닝을 적용하여 스테그아날리시스를 학습하는 범용적 모델을 제안한다. 다양한 실험을 통해 제안 기법의 효용성 및 가능성을 확인하고, 범용적 스테그아날리시스 모델이 각각에 특화된 검출 기법과 유사한 정확도로 스테고 이미지를 검출할 수 있음을 보인다.Steganalysis is to detect information hidden by steganography inside general data such as images. There are stegoanalysis techniques that use machine learning (ML). Existing ML approaches to steganalysis are based on extracting features from stego images and modeling them.
Recently deep learning-based methodologies have shown significant improvements in detection accuracy. However, all the existing methods, including deep learning-based ones, have a critical limitation in that they can only detect stego images that are created by a specific steganography method. In this paper, we propose a generalized steganalysis method that can model multiple types of stego images using deep learning. Through various experiments, we confirm the effectiveness of our approach and envision directions for future research. In particular, we show that our method can detect each type of steganography with the same level of accuracy as that of a steganalysis method dedicated to that type of steganography, thereby demonstrating the general applicability of our approach to multiple types of stego images.steganalysis, steganography, deep learning, generalized steganalysis model"
919,"딥러닝 프레임워크의 비교: 티아노, 텐서플로, CNTK를 중심으로",정여진 /Yeojin Chung,2017,한국지능정보시스템학회,"딥러닝 프레임워크의 대표적인 기능으로는 ‘자동미분’과 ‘GPU의 활용’ 등을 들 수 있다. 본 논문은 파이썬의 라이브러리 형태로 사용 가능한 프레임워크 중에서 구글의 텐서플로와 마이크로소프트의 CNTK, 그리고 텐서 플로의 원조라고 할 수 있는 티아노를 비교하였다. 본문에서는 자동미분의 개념과 GPU의 활용형태를 간단히 설명하고, 그 다음에 logistic regression을 실행하는 예를 통하여 각 프레임워크의 문법을 알아본 뒤에, 마지막으로 대표적인 딥러닝 응용인 CNN의 예제를 실행시켜보고 코딩의 편의성과 실행속도 등을 확인해 보았다. 그 결과, 편의성의 관점에서 보면 티아노가 가장 코딩 하기가 어렵고, CNTK와 텐서플로는 많은 부분이 비슷하게 추상화 되어 있어서 코딩이 비슷하지만 가중치와 편향을 직접 정의하느냐의 여부에서 차이를 보였다. 그리고 각프레임워크의 실행속도에 대한 평가는 ‘큰 차이는 없다’는 것이다. 텐서플로는 티아노에 비하여 속도가 느리다는 평가가 있어왔는데, 본 연구의 실험에 의하면, 비록 CNN 모형에 국한되었지만, 텐서플로가 아주 조금이지만빠른 것으로 나타났다. CNTK의 경우에도, 비록 실험환경이 달랐지만, 실험환경의 차이에 의한 속도의 차이의 편차범위 이내에 있는 것으로 판단이 되었다. 본 연구에서는 세 종류의 딥러닝 프레임워크만을 살펴보았는데,위키피디아에 따르면 딥러닝 프레임워크의 종류는 12가지가 있으며, 각 프레임워크의 특징을 15가지 속성으로구분하여 차이를 특정하고 있다. 그 많은 속성 중에서 사용자의 입장에서 볼 때 중요한 속성은 어떤 언어(파이썬, C++, Java, 등)로 사용가능한지, 어떤 딥러닝 모형에 대한 라이브러리가 잘 구현되어 있는지 등일 것이다. 그리고 사용자가 대규모의 딥러닝 모형을 구축한다면, 다중 GPU 혹은 다중 서버를 지원하는지의 여부도 중요할 것이다. 또한 딥러닝 모형을 처음 학습하는 경우에는 사용설명서가 많은지 예제 프로그램이 많은지 여부도 중요한 기준이 될 것이다.The deep learning framework is software designed to help develop deep learning models. Some of its important functions include “automatic differentiation” and “utilization of GPU”. The list of popular deep learning framework includes Caffe (BVLC) and Theano (University of Montreal). And recently, Microsoft's deep learning framework, Microsoft Cognitive Toolkit, was released as open-source license, following Google’s Tensorflow a year earlier. The early deep learning frameworks have been developed mainly for research at universities. Beginning with the inception of Tensorflow, however, it seems that companies such as Microsoft and Facebook have started to join the competition of framework development. Given the trend, Google and other companies are expected to continue investing in the deep learning framework to bring forward the initiative in the artificial intelligence business. From this point of view, we think it is a good time to compare some of deep learning frameworks. So we compare three deep learning frameworks which can be used as a Python library. Those are Google's Tensorflow, Microsoft’s CNTK, and Theano which is sort of a predecessor of the preceding two.
The most common and important function of deep learning frameworks is the ability to perform automatic differentiation. Basically all the mathematical expressions of deep learning models can be represented as computational graphs, which consist of nodes and edges. Partial derivatives on each edge of a computational graph can then be obtained. With the partial derivatives, we can let software compute differentiation of any node with respect to any variable by utilizing chain rule of Calculus.
First of all, the convenience of coding is in the order of CNTK, Tensorflow, and Theano. The criterion is simply based on the lengths of the codes and the learning curve and the ease of coding are not the main concern. According to the criteria, Theano was the most difficult to implement with, and CNTK and Tensorflow were somewhat easier. With Tensorflow, we need to define weight variables and biases explicitly. The reason that CNTK and Tensorflow are easier to implement with is that those frameworks provide us with more abstraction than Theano. We, however, need to mention that low-level coding is not always bad. It gives us flexibility of coding. With the low-level coding such as in Theano, we can implement and test any new deep learning models or any new search methods that we can think of.
The assessment of the execution speed of each framework is that there is not meaningful difference. According to the experiment, execution speeds of Theano and Tensorflow are very similar, although the experiment was limited to a CNN model. In the case of CNTK, the experimental environment was not maintained as the same. The code written in CNTK has to be run in PC environment without GPU where codes execute as much as 50 times slower than with GPU. But we concluded that the difference of execution speed was within the range of variation caused by the different hardware setup.
In this study, we compared three types of deep learning framework: Theano, Tensorflow, and CNTK. According to Wikipedia, there are 12 available deep learning frameworks. And 15 different attributes differentiate each framework. Some of the important attributes would include interface language (Python, C ++, Java, etc.) and the availability of libraries on various deep learning models such as CNN, RNN, DBN, and etc. And if a user implements a large scale deep learning model, it will also be important to support multiple GPU or multiple servers. Also, if you are learning the deep learning model, it would also be important if there are enough examples and references.deep learning framework, Theano, TensorFlow, CNTK, computational graph, CIFAR-10"
920,딥러닝을 이용한 주변 무선단말 파악방안,이웅섭 /Lee， Woong-sup,2018,한국정보통신학회,"최근 단말-대-단말(Device-to-device, D2D) 통신기술이 차세대 무선통신시스템의 핵심기술로 큰 관심을 받고 있다. 이러한 단말간 통신에서는 자신의 주변에 어떠한 단말이 있는지 파악하는 주변단말 탐색(Neighbor discovery)이매우 중요하다. 본 논문에서는 최근 큰 관심을 받고 있는 딥러닝(Deep learning) 기술을 활용하여 단말간 통신에서 주변단말을 파악하는 방안에 대해서 제안한다. 제안 방안은 기존의 방안과 달리 무선채널의 공간적 연관성을 이용하여단말간의 신호 전송 없이 단말이 기지국으로 전송하는 상향링크 파일럿 신호를 기반으로 주변 단말을 찾고 따라서기존의 방식에 비해 신호전송 복잡도(signaling complexity)를 크게 줄일 수 있다. 또한 제안 방안에서는 떨어져 있는거리에 따라서 주변 단말을 분류 가능하여 기존 방안에 비해서 좀 더 세밀한 단말 탐색이 가능하다. 마지막으로 본 논문에서는 tensorflow를 이용한 컴퓨터 시뮬레이션을 통해 제안 방안의 성능을 검증하였다.Recently, the device-to-device (D2D) communication has been conceived as the key technology for the next-generationmobile communication systems. The neighbor discovery in which the nearby users are found, is essential for the properoperation of the D2D communication. In this paper, we propose new neighbor discovery scheme based on deep learningtechnology which has gained a lot of attention recently. In the proposed scheme, the neighboring users can be foundusing the uplink pilot transmission of users only, unlike conventional neighbor discovery schemes in which direct pilotcommunication among users is required, such that the signaling overhead can be greatly reduced in our proposed scheme.
Moreover, the neighbors with different proximity can also be classified accordingly which enables more accurate neighbordiscovery compared to the conventional schemes. The performance of our proposed scheme is verified through thetensorflow-based computer simulations.
키워드 : 딥러닝, 주변단말 탐색, 단말-대-단말 통신Deep learning, Neighbor discovery, Device-to-device communication, Wireless channel correlation"
921,신뢰성있는 딥러닝 기반 분석 모델을 참조하기 위한 딥러닝 기술 언어,문종혁 /Jong Hyeok Mun,2021,한국정보처리학회,"최근 딥러닝은 하드웨어 성능이 향상됨에 따라 자연어 처리, 영상 인식 등의 다양한 기술에 접목되어 활용되고 있다. 이러한 기술들을 활용해 지능형교통 시스템(ITS), 스마트홈, 헬스케어 등의 산업분야에서 데이터를 분석하여 고속도로 속도위반 차량 검출, 에너지 사용량 제어, 응급상황 등과 같은고품질의 서비스를 제공하며, 고품질의 서비스를 제공하기 위해서는 정확도가 향상된 딥러닝 모델이 적용되어야 한다. 이를 위해 서비스 환경의 데이터를분석하기 위한 딥러닝 모델을 개발할 때, 개발자는 신뢰성이 검증된 최신의 딥러닝 모델을 적용할 수 있어야 한다. 이는 개발자가 참조하는 딥러닝 모델에적용된 학습 데이터셋의 정확도를 측정하여 검증할 수 있다. 이러한 검증을 위해서 개발자는 학습 데이터셋, 딥러닝의 계층구조 및 개발 환경 등과 같은내용을 포함하는 딥러닝 모델을 문서화하여 적용하기 위한 구조적인 정보가 필요하다. 본 논문에서는 신뢰성있는 딥러닝 기반 데이터 분석 모델을 참조하기위한 딥러닝 기술 언어를 제안한다. 제안하는 기술 언어는 신뢰성 있는 딥러닝 모델을 개발하는데 필요한 학습데이터셋, 개발 환경 및 설정 등의 정보와더불어 딥러닝 모델의 계층구조를 표현할 수 있다. 제안하는 딥러닝 기술 언어를 이용하여 개발자는 지능형 교통 시스템에서 참조하는 분석 모델의정확도를 검증할 수 있다. 실험에서는 제안하는 언어의 유효성을 검증하기 위해, 번호판 인식 모델을 중심으로 딥러닝 기술 문서의 적용과정을 보인다With the recent advancements of deep learning, companies such as smart home, healthcare, and intelligent transportation systems areutilizing its functionality to provide high-quality services for vehicle detection, emergency situation detection, and controlling energyconsumption. To provide reliable services in such sensitive systems, deep learning models are required to have high accuracy. In orderto develop a deep learning model for analyzing previously mentioned services, developers should utilize the state of the art deep learningmodels that have already been verified for higher accuracy. The developers can verify the accuracy of the referenced model by validatingthe model on the dataset. For this validation, the developer needs structural information to document and apply deep learning models,including metadata such as learning dataset, network architecture, and development environments. In this paper, we propose a descriptionlanguage that represents the network architecture of the deep learning model along with its metadata that are necessary to develop adeep learning model. Through the proposed description language, developers can easily verify the accuracy of the referenced deep learningmodel. Our experiments demonstrate the application scenario of a deep learning description document that focuses on the license platerecognition for the detection of illegally parked vehicles.Trusted Deep Learning, Model Reference, Deep Learning Description Language, Traffic Situation Analysis Model"
922,파견과 도급의 구별기준에 관한 독일과 한국의 판결 비교,"박지순 /PARK, Ji Soon",2013,노동법이론실무학회,"Making a distinction between genuine (or legal) subcontract and so-called disguised subcontract is a highly complex matter. Considering that subcontracting is a traditional and legal form of work in the modern economy system where labor is specialized and that employers have right to subcontract, utilizing outside labor should not be interrupted. subcontracting usual tasks that were usually conducted in the workplace could be carried out by subcontract workers if the structure of command and share of risk are clearly formed in the way of subcontract. Therefore one should not illegalize all types of subcontract. Also current regulations do not normatively support the argument that dispatch employment of subcontract should not be utilized for usual tasks.
The strict requirements of the Temporary Work Act and the duty of making direct employment relationship between employer and employee in the case of violation of the act clearly limit the freedom of contract to a considerable extent. If the Temporary Work Act sets a broad scope of dispatch employment even when the parties to the subcontract wished to specialize the labor, it can leads to arbitrary judgement damaging legal stability. Thus, only construction in conformity with the Constitution on normative concept of dispatch employment can justify the legal effects of the Temporary Work Act.
In this context, this paper examined the standards that Korean courts could use to reasonably distinguish dispatch employment and subcontract on a basis of recent decisions of German Federal Labor Court(BAG). BAG does not limit in-house subcontracting in range and approves a contract as a legal subcontract as long as the employer command the employee and the aim of contract and the relationship of burden of risk are not out of the nature of subcontract contract. In contrast, Korean Supreme Court tends to distinguish between dispatch employment and subcontract based on the form of working, details of work, means of work putting too much importance on phenomenal facts. The paper marks this point as the difference between decisions of German courts and Korean courts. At the same time, an important question of how to protect workers put to subcontract remains to be dealt with in harmony as future challenges in making legislation policies.Inhouse-Outsourcing, Dispatch, Subcontracting Contract, disguised subcontract, command, BAG"
923,딥러닝 기법을 이용한 내일강수 예측,하지훈,2016,한국지능시스템학회,"정확한 강수예측을 위해서는 예측인자 선정과 예측방법에 대한 선택이 매우 중요하다. 최근에는 강수예측 방법으로기계학습 기법이 많이 사용되고 있으며, 그 중에서도 특히 인공신경망을 사용한 강수예측 방법은 좋은 성능을 보였다. 본논문에서는 딥러닝 기법 중 하나인 DBN(deep belief network)를 이용한 새로운 강수예측 방법을 제안한다. DBN는 비지도사전 학습을 통해 초기 가중치를 설정하여 기존 인공신경망의 문제점을 보완한다. 예측인자로는 기온, 전일-전주 강수일, 태양과 달 궤도 관련 자료를 선정하였다. 기온과 전일-전주 강수일은 서울에서의 1974년부터 2013년까지 총 40년간의AWS(automatic weather system) 관측 자료를 사용하였고, 태양과 달의 궤도 관련 자료는 서울을 중심으로 계산한 결과를사용하였다. 전체 기간에서 일부는 학습 자료로 사용하여 예측모델을 생성하였고, 나머지를 생성한 모델의 검증 자료로사용하였다. 모델 검증 결과로 나온 예측값들은 확률값을 가지며 임계치를 이용하여 강수유무를 판별하였다. 강수 정확도의척도로 양분예보기법 중 CSI(critical successive index)와 Bias(frequency bias)를 계산하였다. 이를 통해 DBN와 MLP(multilayer perceptron)의 성능을 비교한 결과 DBN의 강수 예측 정확도가 높았고, 수행속도 또한 2배 이상 빨랐다.For accurate precipitation forecasts the choice of weather faoctrs and prediction method is very important. Recently, machinele arning has been widely used for forecasting precipitation, and artificial neural network, one of machine learning techniques, showed good performance. In this paper, we suggest a new method for forecasting precipitation using DBN, one of deep learning techniques.
DBN has an advantage that initial weights are set by unsupervised learning, so this compensates for the defects of artificial neural networks. We used past precipitation, temperature, and the parameters of the sun and moon's motion as features for forecasting precipitation. The dataset consists of observation data which had been measured for 40 years from AWS in Seoul. Experiments were based on 8-fold cross validation. As a result of estimation, we got probabilities of test dataset, so threshold was used for the decision of precipitation. CSI and Bias were used for indicating the precision of precipitation. Our experimental results showed that DBN performed better than MLP.Deep learning, Deep belief network, Precipitation, Forecast"
924,딥러닝 신경망모형을 이용한 실시간 교통정보수집 연구,김대현 /KIM DAE HYON,2016,사단법인 인문사회과학기술융합학회,"비디오 기반의 실시간 교통정보수집 시스템은 자율주행을 포함한 지능형교통체계의 주요 서브시스템중 하나이며, 우수한 예측(추정)력 때문에 핵심 알고리즘으로 인공신경망 모형이 주로 사용되고 있다. 특히, 최근 과학 학술지 네이처에 구글 딥마인드의 알파고가 공개되면서 딥러닝과 인공지능에 대한 전 세계적인 관심이 집중되고 있다. 최근까지 가장 널리 사용된 인공신경망모형은 역전파 모형이었으나 최근 컴퓨터 하드웨어 및 알고리즘의 새로운 개발과 더불어 딥러닝 알고리즘을 이용한 딥인공신경망 모형에 대한 관심이 높아지고 있다. 그러나 아직 딥러닝 인공신경망에 대한 연구는 초기단계에 있는 실정이며 적용분야도 극히 한정되어 있다. 본 연구에서는 실시간 도로교통정보 수집을 위한 영상처리분야에서 딥러닝 알고리즘을 활용하고 실험결과를 통하여 향후 교통정보수집‧분석 분야에서의 딥러닝 인공신경망 모형의 적용가능성을 살펴보고자 한다. 특히, 본 연구에서 도출한 딥러닝 인공신경망모형의 실험결과를 기존 역전파 모형에 의한 실험결과와 비교하여 모형의 우수성을 비교‧평가하고자 한다.Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Normally, artificial neural networks have been used in real-time applications for collecting traffic information because of their remarkable performance in terms of predictive accuracy and real-time computing. Especially, the recent publication of a paper in the journal Nature and the success of Google DeepMind’s world-beating Go program AlphaGo caused significant increases of the public interest in artificial intelligence. Even though various types of learning models have been proposed, the Backpropagation has been the most popular neural network model. However, the Deep learning neural network model has attracted much interest recently. In this study, the Deep leaning neural network will be applied in image processing techniques for the real-time traffic information collection to investigate the performance of Deep learning algorithm in traffic engineering fields. In addition, the experimental results from Deep learing algorithm will be compared with the results from Backpropagation model to evaluate and propose better performing model.Intelligent Transportation Systems (ITS), artificial neural networks, Deep learning, traffic information, image processing"
925,딥러닝의 모형과 응용사례,안성만 /SungMahn Ahn,2016,한국지능정보시스템학회,"딥러닝은 인공신경망(neural network)이라는 인공지능분야의 모형이 발전된 형태로서, 계층구조로 이루어진 인공신경망의 내부계층(hidden layer)이 여러 단계로 이루어진 구조이다. 딥러닝에서의 주요 모형은 합성곱신경망(convolutional neural network), 순환신경망(recurrent neural network), 그리고 심층신뢰신경망(deep belief network)의 세가지라고 할 수있다. 그 중에서 현재 흥미로운 연구가 많이 발표되어서 관심이 집중되고 있는 모형은 지도학습(supervised learning)모형인 처음 두 개의 모형이다. 따라서 본 논문에서는 지도학습모형의 가중치를 최적화하는 기본적인 방법인 오류역전파 알고리즘을 살펴본 뒤에 합성곱신경망과 순환신경망의 구조와 응용사례 등을 살펴보고자 한다. 본문에서 다루지 않은 모형인 심층신뢰신경망은 아직까지는 합성곱신경망 이나 순환신경망보다는 상대적으로 주목을 덜 받고 있다. 그러나 심층신뢰신경망은 CNN이나 RNN과는 달리 비지도학습(unsupervised learning)모형이며, 사람이나 동물은 관찰을 통해서 스스로 학습한다는 점에서 궁극적으로는 비지도학습모형이 더 많이 연구되어야 할 주제가 될 것이다.Deep learning model is a kind of neural networks that allows multiple hidden layers. There are various deep learning architectures such as convolutional neural networks, deep belief networks and recurrent neural networks. Those have been applied to fields like computer vision, automatic speech recognition, natural language processing, audio recognition and bioinformatics where they have been shown to produce state-of-the-art results on various tasks. Among those architectures, convolutional neural networks and recurrent neural networks are classified as the supervised learning model. And in recent years, those supervised learning models have gained more popularity than unsupervised learning models such as deep belief networks, because supervised learning models have shown fashionable applications in such fields mentioned above.
Deep learning models can be trained with backpropagation algorithm. Backpropagation is an abbreviation for “backward propagation of errors” and a common method of training artificial neural networks used in conjunction with an optimization method such as gradient descent. The method calculates the gradient of an error function with respect to all the weights in the network. The gradient is fed to the optimization method which in turn uses it to update the weights, in an attempt to minimize the error function.
Convolutional neural networks use a special architecture which is particularly well-adapted to classify images. Using this architecture makes convolutional networks fast to train. This, in turn, helps us train deep, muti-layer networks, which are very good at classifying images. These days, deep convolutional networks are used in most neural networks for image recognition. Convolutional neural networks use three basic ideas: local receptive fields, shared weights, and pooling. By local receptive fields, we mean that each neuron in the first(or any) hidden layer will be connected to a small region of the input(or previous layer’s) neurons. Shared weights mean that we’re going to use the same weights and bias for each of the local receptive field. This means that all the neurons in the hidden layer detect exactly the same feature, just at different locations in the input image. In addition to the convolutional layers just described, convolutional neural networks also contain pooling layers. Pooling layers are usually used immediately after convolutional layers. What the pooling layers do is to simplify the information in the output from the convolutional layer.
Recent convolutional network architectures have 10 to 20 hidden layers and billions of connections between units. Training deep learning networks has taken weeks several years ago, but thanks to progress in GPU and algorithm enhancement, training time has reduced to several hours.
Neural networks with time-varying behavior are known as recurrent neural networks or RNNs. A recurrent neural network is a class of artificial neural network where connections between units form a directed cycle. This creates an internal state of the network which allows it to exhibit dynamic temporal behavior. Unlike feedforward neural networks, RNNs can use their internal memory to process arbitrary sequences of inputs. Early RNN models turned out to be very difficult to train, harder even than deep feedforward networks. The reason is the unstable gradient problem such as vanishing gradient and exploding gradient. The gradient can get smaller and smaller as it is propagated back through layers. This makes learning in early layers extremely slow. The problem actually gets worse in RNNs, since gradients aren’t just propagated backward through layers, they’re propagated backward through time. If the network runs for a long time, that can make the gradient extremely unstable and hard to learn from. It has been possible to incorporate an idea known as long short-term memory units (LSTMs) into RNNs. LSTMs make it much easier to get good results when training RNNs, and many recent papers make use of LSTMs or related ideas.Deep learning, Convolutional neural networks, Recurrent neural networks, Error backpropagation algorithm"
926,딥러닝분석과 기술적 분석 지표를 이용한 한국 코스피주가지수 방향성 예측,이우식 /Lee Woosik,2017,한국데이터정보과학회,"2016년 3월 구글 (Google)의 바둑인공지능 알파고 (AlphaGo)가 이세돌 9단과의 바둑대결에서승리한 이후 다양한 분야에서 인공지능 사용에 대한 관심이 높아지고 있는 가운데 금융투자 분야에서도 인공지능과 투자자문 전문가의 합성어인 로보어드바이저 (Robo-Advisor)에 대한 관심이 높아지고 있다. 인공지능 (artificial intelligence)기반의 의사결정은 비용 절감은 물론 효과적인 의사결정을가능하게 한다는 점에서 큰 장점이 있다. 본 연구에서는 기술적 분석 (technical analysis) 지표와 딥러닝 (deep learning) 모형을 결합하여 한국 코스피 지수를 예측하는 모형을 개발하고 제시한 모형들의 예측력을 비교, 분석한다. 분석 결과 기술적 분석 지표에 딥러닝 알고리즘을 결합한 모형이 주가지수 방향성 예측 문제에 응용될 수 있음을 확인하였다. 향후 본 연구에서 제안된 기술적 분석 지표와딥러닝모형을 결합한 기법은 로보어드바이저서비스에 응용할 수 있는 일반화 가능성을 보여준다Since Google’s AlphaGo defeated a world champion of Go players in 2016, there have been many interests in the deep learning. In the financial sector, a Robo-Advisor using deep learning gains a significant attention, which builds and manages portfolios of financial instruments for investors.In this paper, we have proposed the a deep learning algorithm geared toward identification and forecast of the KOSPI index direction,and we also have compared the accuracy of the prediction.In an application of forecasting the financial market index direction, we have shown that the Robo-Advisor using deep learning has a significant effect on finance industry. The Robo-Advisor collects a massive data such as earnings statements, news reports and regulatory filings, analyzes those and recommends investors how to view market trends and identify the best time to purchase financial assets. On the other hand, the Robo-Advisor allows businesses to learn more about their customers, develop better marketing strategies, increase sales and decrease costs.Artificial intelligence, deep learning, FinTech, Robo-Advisor, technical analysis."
927,딥러닝의 효율적인 학습을 위한 학습데이터 선별 기법에 관한 연구,주경돈 /Gyoung-don Joo,2017,한국정보과학회,"빅데이터의 증가와 병렬 하드웨어 처리, 학습 알고리즘의 개선은 딥러닝의 발전을 가져왔으며 인공지능이 적용 가능한 영역을 획기적으로 확장시키고 있다. 하지만 이러한 대규모의 학습 데이터에 기반한 딥러닝은 학습시간이 증가되고, 더 나아가 학습 데이터의 구축을 위한 레이블링(labeling) 비용이 데이터 수에 비례하여 증가하게 된다. 따라서 본 논문에서는, 딥러닝의 정확도를 최대한 유지하면서 필요한 학습 데이터의 수를 줄일수 있도록, 전체 데이터에서 제한된 수의 학습 데이터의 후보를 선별하는 기법에 대하여 연구한다. 특히 기존의 능동 학습에 기반한 기법들이 학습 데이터의 수가 매우 적어질 경우 딥러닝의 정확도가 급격히 떨어지는 문제를 해결할 수 있는 새로운 기법을 제안하였으며, 제안된 기법으로 선별된 학습 데이터는 그 수의 변화에 대해 딥러닝이 일관되게 경쟁력있는 정확도를 가짐을 다양한 실험을 통하여 검증하였다.The increase of big data, parallel hardware processing and enhanced learning algorithms have led to the development of deep learning and are dramatically expanding the domain of applications based on artificial intelligence. However, such large-scale training data extends the learning time, and furthermore, the labeling cost for building the training set increases in proportion to the volume of data. Therefore, in this paper, we study how to select a limited number of candidates for training set from given data so as to reduce the number of training set while maintaining the accuracy of deep learning. Especially, we found that, with the training set selected by the previous work, the accuracy of deep learning drastically decreases when the number of training set is very small and so proposed new methods to resolve this fall-off in accuracy. Through experiments, we showed that deep learning consistently has the competitive accuracy with the training set selected by the proposed method regardless of the size of training set.Deep Learning, Train Set Selection, Active Learning, Gamma distribution"
928,딥러닝을 이용한 범용적 스테그아날리시스,김현재 /Hyunjae Kim,2017,한국정보과학회,"스테그아날리시스(Steganalysis)란 이미지 등 일반적인 자료에 암호화된 정보를 은닉하는 스테가노그래피(Steganography)에 대한 검출 및 분석 방법으로, 기계학습 기반 방법론을 포함한다. 기존 기계학습 기반 스테그아날리시스는 영상(Image)의 특징(Feature) 추출 및 모델링에 기반하며, 최근 딥러닝(Deep Learning)의 적용으로 검출 정확도가 큰 폭으로 향상되었다. 하지만 현존하는 스테그아날리시스 모델은 단일 스테가노그래피 기법에 대해 국한되어 있어 학습에 사용되지 않은 스테고(Stego) 이미지의 경우 검출이 불가능한 결정적 한계를 가진다. 본 연구에서는 다양한 스테가노그래피 기법으로 생성된 스테고이미지에 딥러닝을 적용하여 스테그아날리시스를 학습하는 범용적 모델을 제안한다. 다양한 실험을 통해 제안 기법의 효용성 및 가능성을 확인하고, 범용적 스테그아날리시스 모델이 각각에 특화된 검출 기법과 유사한 정확도로 스테고 이미지를 검출할 수 있음을 보인다.Steganalysis is to detect information hidden by steganography inside general data such as images. There are stegoanalysis techniques that use machine learning (ML). Existing ML approaches to steganalysis are based on extracting features from stego images and modeling them.
Recently deep learning-based methodologies have shown significant improvements in detection accuracy. However, all the existing methods, including deep learning-based ones, have a critical limitation in that they can only detect stego images that are created by a specific steganography method. In this paper, we propose a generalized steganalysis method that can model multiple types of stego images using deep learning. Through various experiments, we confirm the effectiveness of our approach and envision directions for future research. In particular, we show that our method can detect each type of steganography with the same level of accuracy as that of a steganalysis method dedicated to that type of steganography, thereby demonstrating the general applicability of our approach to multiple types of stego images.steganalysis, steganography, deep learning, generalized steganalysis model"
929,"딥러닝 프레임워크의 비교: 티아노, 텐서플로, CNTK를 중심으로",정여진 /Yeojin Chung,2017,한국지능정보시스템학회,"딥러닝 프레임워크의 대표적인 기능으로는 ‘자동미분’과 ‘GPU의 활용’ 등을 들 수 있다. 본 논문은 파이썬의 라이브러리 형태로 사용 가능한 프레임워크 중에서 구글의 텐서플로와 마이크로소프트의 CNTK, 그리고 텐서 플로의 원조라고 할 수 있는 티아노를 비교하였다. 본문에서는 자동미분의 개념과 GPU의 활용형태를 간단히 설명하고, 그 다음에 logistic regression을 실행하는 예를 통하여 각 프레임워크의 문법을 알아본 뒤에, 마지막으로 대표적인 딥러닝 응용인 CNN의 예제를 실행시켜보고 코딩의 편의성과 실행속도 등을 확인해 보았다. 그 결과, 편의성의 관점에서 보면 티아노가 가장 코딩 하기가 어렵고, CNTK와 텐서플로는 많은 부분이 비슷하게 추상화 되어 있어서 코딩이 비슷하지만 가중치와 편향을 직접 정의하느냐의 여부에서 차이를 보였다. 그리고 각프레임워크의 실행속도에 대한 평가는 ‘큰 차이는 없다’는 것이다. 텐서플로는 티아노에 비하여 속도가 느리다는 평가가 있어왔는데, 본 연구의 실험에 의하면, 비록 CNN 모형에 국한되었지만, 텐서플로가 아주 조금이지만빠른 것으로 나타났다. CNTK의 경우에도, 비록 실험환경이 달랐지만, 실험환경의 차이에 의한 속도의 차이의 편차범위 이내에 있는 것으로 판단이 되었다. 본 연구에서는 세 종류의 딥러닝 프레임워크만을 살펴보았는데,위키피디아에 따르면 딥러닝 프레임워크의 종류는 12가지가 있으며, 각 프레임워크의 특징을 15가지 속성으로구분하여 차이를 특정하고 있다. 그 많은 속성 중에서 사용자의 입장에서 볼 때 중요한 속성은 어떤 언어(파이썬, C++, Java, 등)로 사용가능한지, 어떤 딥러닝 모형에 대한 라이브러리가 잘 구현되어 있는지 등일 것이다. 그리고 사용자가 대규모의 딥러닝 모형을 구축한다면, 다중 GPU 혹은 다중 서버를 지원하는지의 여부도 중요할 것이다. 또한 딥러닝 모형을 처음 학습하는 경우에는 사용설명서가 많은지 예제 프로그램이 많은지 여부도 중요한 기준이 될 것이다.The deep learning framework is software designed to help develop deep learning models. Some of its important functions include “automatic differentiation” and “utilization of GPU”. The list of popular deep learning framework includes Caffe (BVLC) and Theano (University of Montreal). And recently, Microsoft's deep learning framework, Microsoft Cognitive Toolkit, was released as open-source license, following Google’s Tensorflow a year earlier. The early deep learning frameworks have been developed mainly for research at universities. Beginning with the inception of Tensorflow, however, it seems that companies such as Microsoft and Facebook have started to join the competition of framework development. Given the trend, Google and other companies are expected to continue investing in the deep learning framework to bring forward the initiative in the artificial intelligence business. From this point of view, we think it is a good time to compare some of deep learning frameworks. So we compare three deep learning frameworks which can be used as a Python library. Those are Google's Tensorflow, Microsoft’s CNTK, and Theano which is sort of a predecessor of the preceding two.
The most common and important function of deep learning frameworks is the ability to perform automatic differentiation. Basically all the mathematical expressions of deep learning models can be represented as computational graphs, which consist of nodes and edges. Partial derivatives on each edge of a computational graph can then be obtained. With the partial derivatives, we can let software compute differentiation of any node with respect to any variable by utilizing chain rule of Calculus.
First of all, the convenience of coding is in the order of CNTK, Tensorflow, and Theano. The criterion is simply based on the lengths of the codes and the learning curve and the ease of coding are not the main concern. According to the criteria, Theano was the most difficult to implement with, and CNTK and Tensorflow were somewhat easier. With Tensorflow, we need to define weight variables and biases explicitly. The reason that CNTK and Tensorflow are easier to implement with is that those frameworks provide us with more abstraction than Theano. We, however, need to mention that low-level coding is not always bad. It gives us flexibility of coding. With the low-level coding such as in Theano, we can implement and test any new deep learning models or any new search methods that we can think of.
The assessment of the execution speed of each framework is that there is not meaningful difference. According to the experiment, execution speeds of Theano and Tensorflow are very similar, although the experiment was limited to a CNN model. In the case of CNTK, the experimental environment was not maintained as the same. The code written in CNTK has to be run in PC environment without GPU where codes execute as much as 50 times slower than with GPU. But we concluded that the difference of execution speed was within the range of variation caused by the different hardware setup.
In this study, we compared three types of deep learning framework: Theano, Tensorflow, and CNTK. According to Wikipedia, there are 12 available deep learning frameworks. And 15 different attributes differentiate each framework. Some of the important attributes would include interface language (Python, C ++, Java, etc.) and the availability of libraries on various deep learning models such as CNN, RNN, DBN, and etc. And if a user implements a large scale deep learning model, it will also be important to support multiple GPU or multiple servers. Also, if you are learning the deep learning model, it would also be important if there are enough examples and references.deep learning framework, Theano, TensorFlow, CNTK, computational graph, CIFAR-10"
930,딥러닝을 이용한 주변 무선단말 파악방안,이웅섭 /Lee， Woong-sup,2018,한국정보통신학회,"최근 단말-대-단말(Device-to-device, D2D) 통신기술이 차세대 무선통신시스템의 핵심기술로 큰 관심을 받고 있다. 이러한 단말간 통신에서는 자신의 주변에 어떠한 단말이 있는지 파악하는 주변단말 탐색(Neighbor discovery)이매우 중요하다. 본 논문에서는 최근 큰 관심을 받고 있는 딥러닝(Deep learning) 기술을 활용하여 단말간 통신에서 주변단말을 파악하는 방안에 대해서 제안한다. 제안 방안은 기존의 방안과 달리 무선채널의 공간적 연관성을 이용하여단말간의 신호 전송 없이 단말이 기지국으로 전송하는 상향링크 파일럿 신호를 기반으로 주변 단말을 찾고 따라서기존의 방식에 비해 신호전송 복잡도(signaling complexity)를 크게 줄일 수 있다. 또한 제안 방안에서는 떨어져 있는거리에 따라서 주변 단말을 분류 가능하여 기존 방안에 비해서 좀 더 세밀한 단말 탐색이 가능하다. 마지막으로 본 논문에서는 tensorflow를 이용한 컴퓨터 시뮬레이션을 통해 제안 방안의 성능을 검증하였다.Recently, the device-to-device (D2D) communication has been conceived as the key technology for the next-generationmobile communication systems. The neighbor discovery in which the nearby users are found, is essential for the properoperation of the D2D communication. In this paper, we propose new neighbor discovery scheme based on deep learningtechnology which has gained a lot of attention recently. In the proposed scheme, the neighboring users can be foundusing the uplink pilot transmission of users only, unlike conventional neighbor discovery schemes in which direct pilotcommunication among users is required, such that the signaling overhead can be greatly reduced in our proposed scheme.
Moreover, the neighbors with different proximity can also be classified accordingly which enables more accurate neighbordiscovery compared to the conventional schemes. The performance of our proposed scheme is verified through thetensorflow-based computer simulations.
키워드 : 딥러닝, 주변단말 탐색, 단말-대-단말 통신Deep learning, Neighbor discovery, Device-to-device communication, Wireless channel correlation"
931,신뢰성있는 딥러닝 기반 분석 모델을 참조하기 위한 딥러닝 기술 언어,문종혁 /Jong Hyeok Mun,2021,한국정보처리학회,"최근 딥러닝은 하드웨어 성능이 향상됨에 따라 자연어 처리, 영상 인식 등의 다양한 기술에 접목되어 활용되고 있다. 이러한 기술들을 활용해 지능형교통 시스템(ITS), 스마트홈, 헬스케어 등의 산업분야에서 데이터를 분석하여 고속도로 속도위반 차량 검출, 에너지 사용량 제어, 응급상황 등과 같은고품질의 서비스를 제공하며, 고품질의 서비스를 제공하기 위해서는 정확도가 향상된 딥러닝 모델이 적용되어야 한다. 이를 위해 서비스 환경의 데이터를분석하기 위한 딥러닝 모델을 개발할 때, 개발자는 신뢰성이 검증된 최신의 딥러닝 모델을 적용할 수 있어야 한다. 이는 개발자가 참조하는 딥러닝 모델에적용된 학습 데이터셋의 정확도를 측정하여 검증할 수 있다. 이러한 검증을 위해서 개발자는 학습 데이터셋, 딥러닝의 계층구조 및 개발 환경 등과 같은내용을 포함하는 딥러닝 모델을 문서화하여 적용하기 위한 구조적인 정보가 필요하다. 본 논문에서는 신뢰성있는 딥러닝 기반 데이터 분석 모델을 참조하기위한 딥러닝 기술 언어를 제안한다. 제안하는 기술 언어는 신뢰성 있는 딥러닝 모델을 개발하는데 필요한 학습데이터셋, 개발 환경 및 설정 등의 정보와더불어 딥러닝 모델의 계층구조를 표현할 수 있다. 제안하는 딥러닝 기술 언어를 이용하여 개발자는 지능형 교통 시스템에서 참조하는 분석 모델의정확도를 검증할 수 있다. 실험에서는 제안하는 언어의 유효성을 검증하기 위해, 번호판 인식 모델을 중심으로 딥러닝 기술 문서의 적용과정을 보인다With the recent advancements of deep learning, companies such as smart home, healthcare, and intelligent transportation systems areutilizing its functionality to provide high-quality services for vehicle detection, emergency situation detection, and controlling energyconsumption. To provide reliable services in such sensitive systems, deep learning models are required to have high accuracy. In orderto develop a deep learning model for analyzing previously mentioned services, developers should utilize the state of the art deep learningmodels that have already been verified for higher accuracy. The developers can verify the accuracy of the referenced model by validatingthe model on the dataset. For this validation, the developer needs structural information to document and apply deep learning models,including metadata such as learning dataset, network architecture, and development environments. In this paper, we propose a descriptionlanguage that represents the network architecture of the deep learning model along with its metadata that are necessary to develop adeep learning model. Through the proposed description language, developers can easily verify the accuracy of the referenced deep learningmodel. Our experiments demonstrate the application scenario of a deep learning description document that focuses on the license platerecognition for the detection of illegally parked vehicles.Trusted Deep Learning, Model Reference, Deep Learning Description Language, Traffic Situation Analysis Model"
932,딥러닝 분석을 이용한 중국 역내·외 위안화 변동성 예측,이우식 /Lee Woosik,2016,한국데이터정보과학회,"2008년 글로벌 금융위기 이후 중국은 위안화 국제화의 점진적 추진을 진행하면서 중국상하이 외환시장과 중국홍콩 외환시장에서 거래되는 통화인 역내위안화와 역외위안화를 형성시켰다. 본 연구는 위안화 국제화와 점진적인 중국 자본계정 개방에 따라 급변하는 외환시장상황의 변동성을 정확하게 파악하기 위해서 GARCH모형 (일반화된 자기회귀 조건부이분산성모형)에 다단계인공신경망을 결합한 MLP-GARCH 모형과 GARCH모형과 기계학습의 일종인 딥러닝 (deep learning)을 통합한 DL-GARCH을 가지고 위안화 변동성예측을 비교 실험과 분석을 하였다. 비교분석 결과 DLGARCH 모형은 MLP-GARCH보다 모형 위안화 역내·외 환율변동성 예측 면에서 더욱 더 개선된예측값을 제공하였다. 그래서 이분산시계열모형을 딥러닝과 결합한 DL-GARCH 모형은 시계열의환율 변동성 예측 문제에 딥러닝을 응용할 수 있음을 확인하였다. 향후 이분산시계열과 결합된 딥러닝 모형은 다른 금융시계열 데이터에 응용하여 그 일반화 가능성을 높일 수 있을 것이다.The People’s Republic of China has vigorously been pursuing the internationalization of the Chinese Yuan or Renminbi after the financial crisis of 2008. In this view, an abrupt increase of use of the Chinese Yuan in the onshore and offshore markets are important milestones to be one of important currencies. One of the most frequently used methods to forecast volatility is GARCH model. Since a prediction error of the GARCH model has been reported quite high, a lot of efforts have been made to improve forecasting capability of the GARCH model. In this paper, we have proposed MLPGARCH and a DL-GARCH by employing Artificial Neural Network to the GARCH. In an application to forecasting Chinese Yuan volatility, we have successfully shown their overall outperformance in forecasting over the GARCH.Deep learning, DL-GARCH, GARCH, The Chinese Yuan, volatility."
933,딥러닝 기법을 이용한 내일강수 예측,하지훈,2016,한국지능시스템학회,"정확한 강수예측을 위해서는 예측인자 선정과 예측방법에 대한 선택이 매우 중요하다. 최근에는 강수예측 방법으로기계학습 기법이 많이 사용되고 있으며, 그 중에서도 특히 인공신경망을 사용한 강수예측 방법은 좋은 성능을 보였다. 본논문에서는 딥러닝 기법 중 하나인 DBN(deep belief network)를 이용한 새로운 강수예측 방법을 제안한다. DBN는 비지도사전 학습을 통해 초기 가중치를 설정하여 기존 인공신경망의 문제점을 보완한다. 예측인자로는 기온, 전일-전주 강수일, 태양과 달 궤도 관련 자료를 선정하였다. 기온과 전일-전주 강수일은 서울에서의 1974년부터 2013년까지 총 40년간의AWS(automatic weather system) 관측 자료를 사용하였고, 태양과 달의 궤도 관련 자료는 서울을 중심으로 계산한 결과를사용하였다. 전체 기간에서 일부는 학습 자료로 사용하여 예측모델을 생성하였고, 나머지를 생성한 모델의 검증 자료로사용하였다. 모델 검증 결과로 나온 예측값들은 확률값을 가지며 임계치를 이용하여 강수유무를 판별하였다. 강수 정확도의척도로 양분예보기법 중 CSI(critical successive index)와 Bias(frequency bias)를 계산하였다. 이를 통해 DBN와 MLP(multilayer perceptron)의 성능을 비교한 결과 DBN의 강수 예측 정확도가 높았고, 수행속도 또한 2배 이상 빨랐다.For accurate precipitation forecasts the choice of weather faoctrs and prediction method is very important. Recently, machinele arning has been widely used for forecasting precipitation, and artificial neural network, one of machine learning techniques, showed good performance. In this paper, we suggest a new method for forecasting precipitation using DBN, one of deep learning techniques.
DBN has an advantage that initial weights are set by unsupervised learning, so this compensates for the defects of artificial neural networks. We used past precipitation, temperature, and the parameters of the sun and moon's motion as features for forecasting precipitation. The dataset consists of observation data which had been measured for 40 years from AWS in Seoul. Experiments were based on 8-fold cross validation. As a result of estimation, we got probabilities of test dataset, so threshold was used for the decision of precipitation. CSI and Bias were used for indicating the precision of precipitation. Our experimental results showed that DBN performed better than MLP.Deep learning, Deep belief network, Precipitation, Forecast"
934,딥러닝 신경망모형을 이용한 실시간 교통정보수집 연구,김대현 /KIM DAE HYON,2016,사단법인 인문사회과학기술융합학회,"비디오 기반의 실시간 교통정보수집 시스템은 자율주행을 포함한 지능형교통체계의 주요 서브시스템중 하나이며, 우수한 예측(추정)력 때문에 핵심 알고리즘으로 인공신경망 모형이 주로 사용되고 있다. 특히, 최근 과학 학술지 네이처에 구글 딥마인드의 알파고가 공개되면서 딥러닝과 인공지능에 대한 전 세계적인 관심이 집중되고 있다. 최근까지 가장 널리 사용된 인공신경망모형은 역전파 모형이었으나 최근 컴퓨터 하드웨어 및 알고리즘의 새로운 개발과 더불어 딥러닝 알고리즘을 이용한 딥인공신경망 모형에 대한 관심이 높아지고 있다. 그러나 아직 딥러닝 인공신경망에 대한 연구는 초기단계에 있는 실정이며 적용분야도 극히 한정되어 있다. 본 연구에서는 실시간 도로교통정보 수집을 위한 영상처리분야에서 딥러닝 알고리즘을 활용하고 실험결과를 통하여 향후 교통정보수집‧분석 분야에서의 딥러닝 인공신경망 모형의 적용가능성을 살펴보고자 한다. 특히, 본 연구에서 도출한 딥러닝 인공신경망모형의 실험결과를 기존 역전파 모형에 의한 실험결과와 비교하여 모형의 우수성을 비교‧평가하고자 한다.Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Normally, artificial neural networks have been used in real-time applications for collecting traffic information because of their remarkable performance in terms of predictive accuracy and real-time computing. Especially, the recent publication of a paper in the journal Nature and the success of Google DeepMind’s world-beating Go program AlphaGo caused significant increases of the public interest in artificial intelligence. Even though various types of learning models have been proposed, the Backpropagation has been the most popular neural network model. However, the Deep learning neural network model has attracted much interest recently. In this study, the Deep leaning neural network will be applied in image processing techniques for the real-time traffic information collection to investigate the performance of Deep learning algorithm in traffic engineering fields. In addition, the experimental results from Deep learing algorithm will be compared with the results from Backpropagation model to evaluate and propose better performing model.Intelligent Transportation Systems (ITS), artificial neural networks, Deep learning, traffic information, image processing"
935,딥러닝의 모형과 응용사례,안성만 /SungMahn Ahn,2016,한국지능정보시스템학회,"딥러닝은 인공신경망(neural network)이라는 인공지능분야의 모형이 발전된 형태로서, 계층구조로 이루어진 인공신경망의 내부계층(hidden layer)이 여러 단계로 이루어진 구조이다. 딥러닝에서의 주요 모형은 합성곱신경망(convolutional neural network), 순환신경망(recurrent neural network), 그리고 심층신뢰신경망(deep belief network)의 세가지라고 할 수있다. 그 중에서 현재 흥미로운 연구가 많이 발표되어서 관심이 집중되고 있는 모형은 지도학습(supervised learning)모형인 처음 두 개의 모형이다. 따라서 본 논문에서는 지도학습모형의 가중치를 최적화하는 기본적인 방법인 오류역전파 알고리즘을 살펴본 뒤에 합성곱신경망과 순환신경망의 구조와 응용사례 등을 살펴보고자 한다. 본문에서 다루지 않은 모형인 심층신뢰신경망은 아직까지는 합성곱신경망 이나 순환신경망보다는 상대적으로 주목을 덜 받고 있다. 그러나 심층신뢰신경망은 CNN이나 RNN과는 달리 비지도학습(unsupervised learning)모형이며, 사람이나 동물은 관찰을 통해서 스스로 학습한다는 점에서 궁극적으로는 비지도학습모형이 더 많이 연구되어야 할 주제가 될 것이다.Deep learning model is a kind of neural networks that allows multiple hidden layers. There are various deep learning architectures such as convolutional neural networks, deep belief networks and recurrent neural networks. Those have been applied to fields like computer vision, automatic speech recognition, natural language processing, audio recognition and bioinformatics where they have been shown to produce state-of-the-art results on various tasks. Among those architectures, convolutional neural networks and recurrent neural networks are classified as the supervised learning model. And in recent years, those supervised learning models have gained more popularity than unsupervised learning models such as deep belief networks, because supervised learning models have shown fashionable applications in such fields mentioned above.
Deep learning models can be trained with backpropagation algorithm. Backpropagation is an abbreviation for “backward propagation of errors” and a common method of training artificial neural networks used in conjunction with an optimization method such as gradient descent. The method calculates the gradient of an error function with respect to all the weights in the network. The gradient is fed to the optimization method which in turn uses it to update the weights, in an attempt to minimize the error function.
Convolutional neural networks use a special architecture which is particularly well-adapted to classify images. Using this architecture makes convolutional networks fast to train. This, in turn, helps us train deep, muti-layer networks, which are very good at classifying images. These days, deep convolutional networks are used in most neural networks for image recognition. Convolutional neural networks use three basic ideas: local receptive fields, shared weights, and pooling. By local receptive fields, we mean that each neuron in the first(or any) hidden layer will be connected to a small region of the input(or previous layer’s) neurons. Shared weights mean that we’re going to use the same weights and bias for each of the local receptive field. This means that all the neurons in the hidden layer detect exactly the same feature, just at different locations in the input image. In addition to the convolutional layers just described, convolutional neural networks also contain pooling layers. Pooling layers are usually used immediately after convolutional layers. What the pooling layers do is to simplify the information in the output from the convolutional layer.
Recent convolutional network architectures have 10 to 20 hidden layers and billions of connections between units. Training deep learning networks has taken weeks several years ago, but thanks to progress in GPU and algorithm enhancement, training time has reduced to several hours.
Neural networks with time-varying behavior are known as recurrent neural networks or RNNs. A recurrent neural network is a class of artificial neural network where connections between units form a directed cycle. This creates an internal state of the network which allows it to exhibit dynamic temporal behavior. Unlike feedforward neural networks, RNNs can use their internal memory to process arbitrary sequences of inputs. Early RNN models turned out to be very difficult to train, harder even than deep feedforward networks. The reason is the unstable gradient problem such as vanishing gradient and exploding gradient. The gradient can get smaller and smaller as it is propagated back through layers. This makes learning in early layers extremely slow. The problem actually gets worse in RNNs, since gradients aren’t just propagated backward through layers, they’re propagated backward through time. If the network runs for a long time, that can make the gradient extremely unstable and hard to learn from. It has been possible to incorporate an idea known as long short-term memory units (LSTMs) into RNNs. LSTMs make it much easier to get good results when training RNNs, and many recent papers make use of LSTMs or related ideas.Deep learning, Convolutional neural networks, Recurrent neural networks, Error backpropagation algorithm"
936,딥러닝분석과 기술적 분석 지표를 이용한 한국 코스피주가지수 방향성 예측,이우식 /Lee Woosik,2017,한국데이터정보과학회,"2016년 3월 구글 (Google)의 바둑인공지능 알파고 (AlphaGo)가 이세돌 9단과의 바둑대결에서승리한 이후 다양한 분야에서 인공지능 사용에 대한 관심이 높아지고 있는 가운데 금융투자 분야에서도 인공지능과 투자자문 전문가의 합성어인 로보어드바이저 (Robo-Advisor)에 대한 관심이 높아지고 있다. 인공지능 (artificial intelligence)기반의 의사결정은 비용 절감은 물론 효과적인 의사결정을가능하게 한다는 점에서 큰 장점이 있다. 본 연구에서는 기술적 분석 (technical analysis) 지표와 딥러닝 (deep learning) 모형을 결합하여 한국 코스피 지수를 예측하는 모형을 개발하고 제시한 모형들의 예측력을 비교, 분석한다. 분석 결과 기술적 분석 지표에 딥러닝 알고리즘을 결합한 모형이 주가지수 방향성 예측 문제에 응용될 수 있음을 확인하였다. 향후 본 연구에서 제안된 기술적 분석 지표와딥러닝모형을 결합한 기법은 로보어드바이저서비스에 응용할 수 있는 일반화 가능성을 보여준다Since Google’s AlphaGo defeated a world champion of Go players in 2016, there have been many interests in the deep learning. In the financial sector, a Robo-Advisor using deep learning gains a significant attention, which builds and manages portfolios of financial instruments for investors.In this paper, we have proposed the a deep learning algorithm geared toward identification and forecast of the KOSPI index direction,and we also have compared the accuracy of the prediction.In an application of forecasting the financial market index direction, we have shown that the Robo-Advisor using deep learning has a significant effect on finance industry. The Robo-Advisor collects a massive data such as earnings statements, news reports and regulatory filings, analyzes those and recommends investors how to view market trends and identify the best time to purchase financial assets. On the other hand, the Robo-Advisor allows businesses to learn more about their customers, develop better marketing strategies, increase sales and decrease costs.Artificial intelligence, deep learning, FinTech, Robo-Advisor, technical analysis."
937,딥러닝의 효율적인 학습을 위한 학습데이터 선별 기법에 관한 연구,주경돈 /Gyoung-don Joo,2017,한국정보과학회,"빅데이터의 증가와 병렬 하드웨어 처리, 학습 알고리즘의 개선은 딥러닝의 발전을 가져왔으며 인공지능이 적용 가능한 영역을 획기적으로 확장시키고 있다. 하지만 이러한 대규모의 학습 데이터에 기반한 딥러닝은 학습시간이 증가되고, 더 나아가 학습 데이터의 구축을 위한 레이블링(labeling) 비용이 데이터 수에 비례하여 증가하게 된다. 따라서 본 논문에서는, 딥러닝의 정확도를 최대한 유지하면서 필요한 학습 데이터의 수를 줄일수 있도록, 전체 데이터에서 제한된 수의 학습 데이터의 후보를 선별하는 기법에 대하여 연구한다. 특히 기존의 능동 학습에 기반한 기법들이 학습 데이터의 수가 매우 적어질 경우 딥러닝의 정확도가 급격히 떨어지는 문제를 해결할 수 있는 새로운 기법을 제안하였으며, 제안된 기법으로 선별된 학습 데이터는 그 수의 변화에 대해 딥러닝이 일관되게 경쟁력있는 정확도를 가짐을 다양한 실험을 통하여 검증하였다.The increase of big data, parallel hardware processing and enhanced learning algorithms have led to the development of deep learning and are dramatically expanding the domain of applications based on artificial intelligence. However, such large-scale training data extends the learning time, and furthermore, the labeling cost for building the training set increases in proportion to the volume of data. Therefore, in this paper, we study how to select a limited number of candidates for training set from given data so as to reduce the number of training set while maintaining the accuracy of deep learning. Especially, we found that, with the training set selected by the previous work, the accuracy of deep learning drastically decreases when the number of training set is very small and so proposed new methods to resolve this fall-off in accuracy. Through experiments, we showed that deep learning consistently has the competitive accuracy with the training set selected by the proposed method regardless of the size of training set.Deep Learning, Train Set Selection, Active Learning, Gamma distribution"
938,딥러닝을 이용한 범용적 스테그아날리시스,김현재 /Hyunjae Kim,2017,한국정보과학회,"스테그아날리시스(Steganalysis)란 이미지 등 일반적인 자료에 암호화된 정보를 은닉하는 스테가노그래피(Steganography)에 대한 검출 및 분석 방법으로, 기계학습 기반 방법론을 포함한다. 기존 기계학습 기반 스테그아날리시스는 영상(Image)의 특징(Feature) 추출 및 모델링에 기반하며, 최근 딥러닝(Deep Learning)의 적용으로 검출 정확도가 큰 폭으로 향상되었다. 하지만 현존하는 스테그아날리시스 모델은 단일 스테가노그래피 기법에 대해 국한되어 있어 학습에 사용되지 않은 스테고(Stego) 이미지의 경우 검출이 불가능한 결정적 한계를 가진다. 본 연구에서는 다양한 스테가노그래피 기법으로 생성된 스테고이미지에 딥러닝을 적용하여 스테그아날리시스를 학습하는 범용적 모델을 제안한다. 다양한 실험을 통해 제안 기법의 효용성 및 가능성을 확인하고, 범용적 스테그아날리시스 모델이 각각에 특화된 검출 기법과 유사한 정확도로 스테고 이미지를 검출할 수 있음을 보인다.Steganalysis is to detect information hidden by steganography inside general data such as images. There are stegoanalysis techniques that use machine learning (ML). Existing ML approaches to steganalysis are based on extracting features from stego images and modeling them.
Recently deep learning-based methodologies have shown significant improvements in detection accuracy. However, all the existing methods, including deep learning-based ones, have a critical limitation in that they can only detect stego images that are created by a specific steganography method. In this paper, we propose a generalized steganalysis method that can model multiple types of stego images using deep learning. Through various experiments, we confirm the effectiveness of our approach and envision directions for future research. In particular, we show that our method can detect each type of steganography with the same level of accuracy as that of a steganalysis method dedicated to that type of steganography, thereby demonstrating the general applicability of our approach to multiple types of stego images.steganalysis, steganography, deep learning, generalized steganalysis model"
939,"딥러닝 프레임워크의 비교: 티아노, 텐서플로, CNTK를 중심으로",정여진 /Yeojin Chung,2017,한국지능정보시스템학회,"딥러닝 프레임워크의 대표적인 기능으로는 ‘자동미분’과 ‘GPU의 활용’ 등을 들 수 있다. 본 논문은 파이썬의 라이브러리 형태로 사용 가능한 프레임워크 중에서 구글의 텐서플로와 마이크로소프트의 CNTK, 그리고 텐서 플로의 원조라고 할 수 있는 티아노를 비교하였다. 본문에서는 자동미분의 개념과 GPU의 활용형태를 간단히 설명하고, 그 다음에 logistic regression을 실행하는 예를 통하여 각 프레임워크의 문법을 알아본 뒤에, 마지막으로 대표적인 딥러닝 응용인 CNN의 예제를 실행시켜보고 코딩의 편의성과 실행속도 등을 확인해 보았다. 그 결과, 편의성의 관점에서 보면 티아노가 가장 코딩 하기가 어렵고, CNTK와 텐서플로는 많은 부분이 비슷하게 추상화 되어 있어서 코딩이 비슷하지만 가중치와 편향을 직접 정의하느냐의 여부에서 차이를 보였다. 그리고 각프레임워크의 실행속도에 대한 평가는 ‘큰 차이는 없다’는 것이다. 텐서플로는 티아노에 비하여 속도가 느리다는 평가가 있어왔는데, 본 연구의 실험에 의하면, 비록 CNN 모형에 국한되었지만, 텐서플로가 아주 조금이지만빠른 것으로 나타났다. CNTK의 경우에도, 비록 실험환경이 달랐지만, 실험환경의 차이에 의한 속도의 차이의 편차범위 이내에 있는 것으로 판단이 되었다. 본 연구에서는 세 종류의 딥러닝 프레임워크만을 살펴보았는데,위키피디아에 따르면 딥러닝 프레임워크의 종류는 12가지가 있으며, 각 프레임워크의 특징을 15가지 속성으로구분하여 차이를 특정하고 있다. 그 많은 속성 중에서 사용자의 입장에서 볼 때 중요한 속성은 어떤 언어(파이썬, C++, Java, 등)로 사용가능한지, 어떤 딥러닝 모형에 대한 라이브러리가 잘 구현되어 있는지 등일 것이다. 그리고 사용자가 대규모의 딥러닝 모형을 구축한다면, 다중 GPU 혹은 다중 서버를 지원하는지의 여부도 중요할 것이다. 또한 딥러닝 모형을 처음 학습하는 경우에는 사용설명서가 많은지 예제 프로그램이 많은지 여부도 중요한 기준이 될 것이다.The deep learning framework is software designed to help develop deep learning models. Some of its important functions include “automatic differentiation” and “utilization of GPU”. The list of popular deep learning framework includes Caffe (BVLC) and Theano (University of Montreal). And recently, Microsoft's deep learning framework, Microsoft Cognitive Toolkit, was released as open-source license, following Google’s Tensorflow a year earlier. The early deep learning frameworks have been developed mainly for research at universities. Beginning with the inception of Tensorflow, however, it seems that companies such as Microsoft and Facebook have started to join the competition of framework development. Given the trend, Google and other companies are expected to continue investing in the deep learning framework to bring forward the initiative in the artificial intelligence business. From this point of view, we think it is a good time to compare some of deep learning frameworks. So we compare three deep learning frameworks which can be used as a Python library. Those are Google's Tensorflow, Microsoft’s CNTK, and Theano which is sort of a predecessor of the preceding two.
The most common and important function of deep learning frameworks is the ability to perform automatic differentiation. Basically all the mathematical expressions of deep learning models can be represented as computational graphs, which consist of nodes and edges. Partial derivatives on each edge of a computational graph can then be obtained. With the partial derivatives, we can let software compute differentiation of any node with respect to any variable by utilizing chain rule of Calculus.
First of all, the convenience of coding is in the order of CNTK, Tensorflow, and Theano. The criterion is simply based on the lengths of the codes and the learning curve and the ease of coding are not the main concern. According to the criteria, Theano was the most difficult to implement with, and CNTK and Tensorflow were somewhat easier. With Tensorflow, we need to define weight variables and biases explicitly. The reason that CNTK and Tensorflow are easier to implement with is that those frameworks provide us with more abstraction than Theano. We, however, need to mention that low-level coding is not always bad. It gives us flexibility of coding. With the low-level coding such as in Theano, we can implement and test any new deep learning models or any new search methods that we can think of.
The assessment of the execution speed of each framework is that there is not meaningful difference. According to the experiment, execution speeds of Theano and Tensorflow are very similar, although the experiment was limited to a CNN model. In the case of CNTK, the experimental environment was not maintained as the same. The code written in CNTK has to be run in PC environment without GPU where codes execute as much as 50 times slower than with GPU. But we concluded that the difference of execution speed was within the range of variation caused by the different hardware setup.
In this study, we compared three types of deep learning framework: Theano, Tensorflow, and CNTK. According to Wikipedia, there are 12 available deep learning frameworks. And 15 different attributes differentiate each framework. Some of the important attributes would include interface language (Python, C ++, Java, etc.) and the availability of libraries on various deep learning models such as CNN, RNN, DBN, and etc. And if a user implements a large scale deep learning model, it will also be important to support multiple GPU or multiple servers. Also, if you are learning the deep learning model, it would also be important if there are enough examples and references.deep learning framework, Theano, TensorFlow, CNTK, computational graph, CIFAR-10"
940,딥러닝을 이용한 주변 무선단말 파악방안,이웅섭 /Lee， Woong-sup,2018,한국정보통신학회,"최근 단말-대-단말(Device-to-device, D2D) 통신기술이 차세대 무선통신시스템의 핵심기술로 큰 관심을 받고 있다. 이러한 단말간 통신에서는 자신의 주변에 어떠한 단말이 있는지 파악하는 주변단말 탐색(Neighbor discovery)이매우 중요하다. 본 논문에서는 최근 큰 관심을 받고 있는 딥러닝(Deep learning) 기술을 활용하여 단말간 통신에서 주변단말을 파악하는 방안에 대해서 제안한다. 제안 방안은 기존의 방안과 달리 무선채널의 공간적 연관성을 이용하여단말간의 신호 전송 없이 단말이 기지국으로 전송하는 상향링크 파일럿 신호를 기반으로 주변 단말을 찾고 따라서기존의 방식에 비해 신호전송 복잡도(signaling complexity)를 크게 줄일 수 있다. 또한 제안 방안에서는 떨어져 있는거리에 따라서 주변 단말을 분류 가능하여 기존 방안에 비해서 좀 더 세밀한 단말 탐색이 가능하다. 마지막으로 본 논문에서는 tensorflow를 이용한 컴퓨터 시뮬레이션을 통해 제안 방안의 성능을 검증하였다.Recently, the device-to-device (D2D) communication has been conceived as the key technology for the next-generationmobile communication systems. The neighbor discovery in which the nearby users are found, is essential for the properoperation of the D2D communication. In this paper, we propose new neighbor discovery scheme based on deep learningtechnology which has gained a lot of attention recently. In the proposed scheme, the neighboring users can be foundusing the uplink pilot transmission of users only, unlike conventional neighbor discovery schemes in which direct pilotcommunication among users is required, such that the signaling overhead can be greatly reduced in our proposed scheme.
Moreover, the neighbors with different proximity can also be classified accordingly which enables more accurate neighbordiscovery compared to the conventional schemes. The performance of our proposed scheme is verified through thetensorflow-based computer simulations.
키워드 : 딥러닝, 주변단말 탐색, 단말-대-단말 통신Deep learning, Neighbor discovery, Device-to-device communication, Wireless channel correlation"
941,신뢰성있는 딥러닝 기반 분석 모델을 참조하기 위한 딥러닝 기술 언어,문종혁 /Jong Hyeok Mun,2021,한국정보처리학회,"최근 딥러닝은 하드웨어 성능이 향상됨에 따라 자연어 처리, 영상 인식 등의 다양한 기술에 접목되어 활용되고 있다. 이러한 기술들을 활용해 지능형교통 시스템(ITS), 스마트홈, 헬스케어 등의 산업분야에서 데이터를 분석하여 고속도로 속도위반 차량 검출, 에너지 사용량 제어, 응급상황 등과 같은고품질의 서비스를 제공하며, 고품질의 서비스를 제공하기 위해서는 정확도가 향상된 딥러닝 모델이 적용되어야 한다. 이를 위해 서비스 환경의 데이터를분석하기 위한 딥러닝 모델을 개발할 때, 개발자는 신뢰성이 검증된 최신의 딥러닝 모델을 적용할 수 있어야 한다. 이는 개발자가 참조하는 딥러닝 모델에적용된 학습 데이터셋의 정확도를 측정하여 검증할 수 있다. 이러한 검증을 위해서 개발자는 학습 데이터셋, 딥러닝의 계층구조 및 개발 환경 등과 같은내용을 포함하는 딥러닝 모델을 문서화하여 적용하기 위한 구조적인 정보가 필요하다. 본 논문에서는 신뢰성있는 딥러닝 기반 데이터 분석 모델을 참조하기위한 딥러닝 기술 언어를 제안한다. 제안하는 기술 언어는 신뢰성 있는 딥러닝 모델을 개발하는데 필요한 학습데이터셋, 개발 환경 및 설정 등의 정보와더불어 딥러닝 모델의 계층구조를 표현할 수 있다. 제안하는 딥러닝 기술 언어를 이용하여 개발자는 지능형 교통 시스템에서 참조하는 분석 모델의정확도를 검증할 수 있다. 실험에서는 제안하는 언어의 유효성을 검증하기 위해, 번호판 인식 모델을 중심으로 딥러닝 기술 문서의 적용과정을 보인다With the recent advancements of deep learning, companies such as smart home, healthcare, and intelligent transportation systems areutilizing its functionality to provide high-quality services for vehicle detection, emergency situation detection, and controlling energyconsumption. To provide reliable services in such sensitive systems, deep learning models are required to have high accuracy. In orderto develop a deep learning model for analyzing previously mentioned services, developers should utilize the state of the art deep learningmodels that have already been verified for higher accuracy. The developers can verify the accuracy of the referenced model by validatingthe model on the dataset. For this validation, the developer needs structural information to document and apply deep learning models,including metadata such as learning dataset, network architecture, and development environments. In this paper, we propose a descriptionlanguage that represents the network architecture of the deep learning model along with its metadata that are necessary to develop adeep learning model. Through the proposed description language, developers can easily verify the accuracy of the referenced deep learningmodel. Our experiments demonstrate the application scenario of a deep learning description document that focuses on the license platerecognition for the detection of illegally parked vehicles.Trusted Deep Learning, Model Reference, Deep Learning Description Language, Traffic Situation Analysis Model"
942,딥러닝 분석을 이용한 중국 역내·외 위안화 변동성 예측,이우식 /Lee Woosik,2016,한국데이터정보과학회,"2008년 글로벌 금융위기 이후 중국은 위안화 국제화의 점진적 추진을 진행하면서 중국상하이 외환시장과 중국홍콩 외환시장에서 거래되는 통화인 역내위안화와 역외위안화를 형성시켰다. 본 연구는 위안화 국제화와 점진적인 중국 자본계정 개방에 따라 급변하는 외환시장상황의 변동성을 정확하게 파악하기 위해서 GARCH모형 (일반화된 자기회귀 조건부이분산성모형)에 다단계인공신경망을 결합한 MLP-GARCH 모형과 GARCH모형과 기계학습의 일종인 딥러닝 (deep learning)을 통합한 DL-GARCH을 가지고 위안화 변동성예측을 비교 실험과 분석을 하였다. 비교분석 결과 DLGARCH 모형은 MLP-GARCH보다 모형 위안화 역내·외 환율변동성 예측 면에서 더욱 더 개선된예측값을 제공하였다. 그래서 이분산시계열모형을 딥러닝과 결합한 DL-GARCH 모형은 시계열의환율 변동성 예측 문제에 딥러닝을 응용할 수 있음을 확인하였다. 향후 이분산시계열과 결합된 딥러닝 모형은 다른 금융시계열 데이터에 응용하여 그 일반화 가능성을 높일 수 있을 것이다.The People’s Republic of China has vigorously been pursuing the internationalization of the Chinese Yuan or Renminbi after the financial crisis of 2008. In this view, an abrupt increase of use of the Chinese Yuan in the onshore and offshore markets are important milestones to be one of important currencies. One of the most frequently used methods to forecast volatility is GARCH model. Since a prediction error of the GARCH model has been reported quite high, a lot of efforts have been made to improve forecasting capability of the GARCH model. In this paper, we have proposed MLPGARCH and a DL-GARCH by employing Artificial Neural Network to the GARCH. In an application to forecasting Chinese Yuan volatility, we have successfully shown their overall outperformance in forecasting over the GARCH.Deep learning, DL-GARCH, GARCH, The Chinese Yuan, volatility."
943,딥러닝 기법을 이용한 내일강수 예측,하지훈,2016,한국지능시스템학회,"정확한 강수예측을 위해서는 예측인자 선정과 예측방법에 대한 선택이 매우 중요하다. 최근에는 강수예측 방법으로기계학습 기법이 많이 사용되고 있으며, 그 중에서도 특히 인공신경망을 사용한 강수예측 방법은 좋은 성능을 보였다. 본논문에서는 딥러닝 기법 중 하나인 DBN(deep belief network)를 이용한 새로운 강수예측 방법을 제안한다. DBN는 비지도사전 학습을 통해 초기 가중치를 설정하여 기존 인공신경망의 문제점을 보완한다. 예측인자로는 기온, 전일-전주 강수일, 태양과 달 궤도 관련 자료를 선정하였다. 기온과 전일-전주 강수일은 서울에서의 1974년부터 2013년까지 총 40년간의AWS(automatic weather system) 관측 자료를 사용하였고, 태양과 달의 궤도 관련 자료는 서울을 중심으로 계산한 결과를사용하였다. 전체 기간에서 일부는 학습 자료로 사용하여 예측모델을 생성하였고, 나머지를 생성한 모델의 검증 자료로사용하였다. 모델 검증 결과로 나온 예측값들은 확률값을 가지며 임계치를 이용하여 강수유무를 판별하였다. 강수 정확도의척도로 양분예보기법 중 CSI(critical successive index)와 Bias(frequency bias)를 계산하였다. 이를 통해 DBN와 MLP(multilayer perceptron)의 성능을 비교한 결과 DBN의 강수 예측 정확도가 높았고, 수행속도 또한 2배 이상 빨랐다.For accurate precipitation forecasts the choice of weather faoctrs and prediction method is very important. Recently, machinele arning has been widely used for forecasting precipitation, and artificial neural network, one of machine learning techniques, showed good performance. In this paper, we suggest a new method for forecasting precipitation using DBN, one of deep learning techniques.
DBN has an advantage that initial weights are set by unsupervised learning, so this compensates for the defects of artificial neural networks. We used past precipitation, temperature, and the parameters of the sun and moon's motion as features for forecasting precipitation. The dataset consists of observation data which had been measured for 40 years from AWS in Seoul. Experiments were based on 8-fold cross validation. As a result of estimation, we got probabilities of test dataset, so threshold was used for the decision of precipitation. CSI and Bias were used for indicating the precision of precipitation. Our experimental results showed that DBN performed better than MLP.Deep learning, Deep belief network, Precipitation, Forecast"
944,딥러닝 신경망모형을 이용한 실시간 교통정보수집 연구,김대현 /KIM DAE HYON,2016,사단법인 인문사회과학기술융합학회,"비디오 기반의 실시간 교통정보수집 시스템은 자율주행을 포함한 지능형교통체계의 주요 서브시스템중 하나이며, 우수한 예측(추정)력 때문에 핵심 알고리즘으로 인공신경망 모형이 주로 사용되고 있다. 특히, 최근 과학 학술지 네이처에 구글 딥마인드의 알파고가 공개되면서 딥러닝과 인공지능에 대한 전 세계적인 관심이 집중되고 있다. 최근까지 가장 널리 사용된 인공신경망모형은 역전파 모형이었으나 최근 컴퓨터 하드웨어 및 알고리즘의 새로운 개발과 더불어 딥러닝 알고리즘을 이용한 딥인공신경망 모형에 대한 관심이 높아지고 있다. 그러나 아직 딥러닝 인공신경망에 대한 연구는 초기단계에 있는 실정이며 적용분야도 극히 한정되어 있다. 본 연구에서는 실시간 도로교통정보 수집을 위한 영상처리분야에서 딥러닝 알고리즘을 활용하고 실험결과를 통하여 향후 교통정보수집‧분석 분야에서의 딥러닝 인공신경망 모형의 적용가능성을 살펴보고자 한다. 특히, 본 연구에서 도출한 딥러닝 인공신경망모형의 실험결과를 기존 역전파 모형에 의한 실험결과와 비교하여 모형의 우수성을 비교‧평가하고자 한다.Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Normally, artificial neural networks have been used in real-time applications for collecting traffic information because of their remarkable performance in terms of predictive accuracy and real-time computing. Especially, the recent publication of a paper in the journal Nature and the success of Google DeepMind’s world-beating Go program AlphaGo caused significant increases of the public interest in artificial intelligence. Even though various types of learning models have been proposed, the Backpropagation has been the most popular neural network model. However, the Deep learning neural network model has attracted much interest recently. In this study, the Deep leaning neural network will be applied in image processing techniques for the real-time traffic information collection to investigate the performance of Deep learning algorithm in traffic engineering fields. In addition, the experimental results from Deep learing algorithm will be compared with the results from Backpropagation model to evaluate and propose better performing model.Intelligent Transportation Systems (ITS), artificial neural networks, Deep learning, traffic information, image processing"
945,딥러닝의 모형과 응용사례,안성만 /SungMahn Ahn,2016,한국지능정보시스템학회,"딥러닝은 인공신경망(neural network)이라는 인공지능분야의 모형이 발전된 형태로서, 계층구조로 이루어진 인공신경망의 내부계층(hidden layer)이 여러 단계로 이루어진 구조이다. 딥러닝에서의 주요 모형은 합성곱신경망(convolutional neural network), 순환신경망(recurrent neural network), 그리고 심층신뢰신경망(deep belief network)의 세가지라고 할 수있다. 그 중에서 현재 흥미로운 연구가 많이 발표되어서 관심이 집중되고 있는 모형은 지도학습(supervised learning)모형인 처음 두 개의 모형이다. 따라서 본 논문에서는 지도학습모형의 가중치를 최적화하는 기본적인 방법인 오류역전파 알고리즘을 살펴본 뒤에 합성곱신경망과 순환신경망의 구조와 응용사례 등을 살펴보고자 한다. 본문에서 다루지 않은 모형인 심층신뢰신경망은 아직까지는 합성곱신경망 이나 순환신경망보다는 상대적으로 주목을 덜 받고 있다. 그러나 심층신뢰신경망은 CNN이나 RNN과는 달리 비지도학습(unsupervised learning)모형이며, 사람이나 동물은 관찰을 통해서 스스로 학습한다는 점에서 궁극적으로는 비지도학습모형이 더 많이 연구되어야 할 주제가 될 것이다.Deep learning model is a kind of neural networks that allows multiple hidden layers. There are various deep learning architectures such as convolutional neural networks, deep belief networks and recurrent neural networks. Those have been applied to fields like computer vision, automatic speech recognition, natural language processing, audio recognition and bioinformatics where they have been shown to produce state-of-the-art results on various tasks. Among those architectures, convolutional neural networks and recurrent neural networks are classified as the supervised learning model. And in recent years, those supervised learning models have gained more popularity than unsupervised learning models such as deep belief networks, because supervised learning models have shown fashionable applications in such fields mentioned above.
Deep learning models can be trained with backpropagation algorithm. Backpropagation is an abbreviation for “backward propagation of errors” and a common method of training artificial neural networks used in conjunction with an optimization method such as gradient descent. The method calculates the gradient of an error function with respect to all the weights in the network. The gradient is fed to the optimization method which in turn uses it to update the weights, in an attempt to minimize the error function.
Convolutional neural networks use a special architecture which is particularly well-adapted to classify images. Using this architecture makes convolutional networks fast to train. This, in turn, helps us train deep, muti-layer networks, which are very good at classifying images. These days, deep convolutional networks are used in most neural networks for image recognition. Convolutional neural networks use three basic ideas: local receptive fields, shared weights, and pooling. By local receptive fields, we mean that each neuron in the first(or any) hidden layer will be connected to a small region of the input(or previous layer’s) neurons. Shared weights mean that we’re going to use the same weights and bias for each of the local receptive field. This means that all the neurons in the hidden layer detect exactly the same feature, just at different locations in the input image. In addition to the convolutional layers just described, convolutional neural networks also contain pooling layers. Pooling layers are usually used immediately after convolutional layers. What the pooling layers do is to simplify the information in the output from the convolutional layer.
Recent convolutional network architectures have 10 to 20 hidden layers and billions of connections between units. Training deep learning networks has taken weeks several years ago, but thanks to progress in GPU and algorithm enhancement, training time has reduced to several hours.
Neural networks with time-varying behavior are known as recurrent neural networks or RNNs. A recurrent neural network is a class of artificial neural network where connections between units form a directed cycle. This creates an internal state of the network which allows it to exhibit dynamic temporal behavior. Unlike feedforward neural networks, RNNs can use their internal memory to process arbitrary sequences of inputs. Early RNN models turned out to be very difficult to train, harder even than deep feedforward networks. The reason is the unstable gradient problem such as vanishing gradient and exploding gradient. The gradient can get smaller and smaller as it is propagated back through layers. This makes learning in early layers extremely slow. The problem actually gets worse in RNNs, since gradients aren’t just propagated backward through layers, they’re propagated backward through time. If the network runs for a long time, that can make the gradient extremely unstable and hard to learn from. It has been possible to incorporate an idea known as long short-term memory units (LSTMs) into RNNs. LSTMs make it much easier to get good results when training RNNs, and many recent papers make use of LSTMs or related ideas.Deep learning, Convolutional neural networks, Recurrent neural networks, Error backpropagation algorithm"
946,딥러닝분석과 기술적 분석 지표를 이용한 한국 코스피주가지수 방향성 예측,이우식 /Lee Woosik,2017,한국데이터정보과학회,"2016년 3월 구글 (Google)의 바둑인공지능 알파고 (AlphaGo)가 이세돌 9단과의 바둑대결에서승리한 이후 다양한 분야에서 인공지능 사용에 대한 관심이 높아지고 있는 가운데 금융투자 분야에서도 인공지능과 투자자문 전문가의 합성어인 로보어드바이저 (Robo-Advisor)에 대한 관심이 높아지고 있다. 인공지능 (artificial intelligence)기반의 의사결정은 비용 절감은 물론 효과적인 의사결정을가능하게 한다는 점에서 큰 장점이 있다. 본 연구에서는 기술적 분석 (technical analysis) 지표와 딥러닝 (deep learning) 모형을 결합하여 한국 코스피 지수를 예측하는 모형을 개발하고 제시한 모형들의 예측력을 비교, 분석한다. 분석 결과 기술적 분석 지표에 딥러닝 알고리즘을 결합한 모형이 주가지수 방향성 예측 문제에 응용될 수 있음을 확인하였다. 향후 본 연구에서 제안된 기술적 분석 지표와딥러닝모형을 결합한 기법은 로보어드바이저서비스에 응용할 수 있는 일반화 가능성을 보여준다Since Google’s AlphaGo defeated a world champion of Go players in 2016, there have been many interests in the deep learning. In the financial sector, a Robo-Advisor using deep learning gains a significant attention, which builds and manages portfolios of financial instruments for investors.In this paper, we have proposed the a deep learning algorithm geared toward identification and forecast of the KOSPI index direction,and we also have compared the accuracy of the prediction.In an application of forecasting the financial market index direction, we have shown that the Robo-Advisor using deep learning has a significant effect on finance industry. The Robo-Advisor collects a massive data such as earnings statements, news reports and regulatory filings, analyzes those and recommends investors how to view market trends and identify the best time to purchase financial assets. On the other hand, the Robo-Advisor allows businesses to learn more about their customers, develop better marketing strategies, increase sales and decrease costs.Artificial intelligence, deep learning, FinTech, Robo-Advisor, technical analysis."
947,딥러닝의 효율적인 학습을 위한 학습데이터 선별 기법에 관한 연구,주경돈 /Gyoung-don Joo,2017,한국정보과학회,"빅데이터의 증가와 병렬 하드웨어 처리, 학습 알고리즘의 개선은 딥러닝의 발전을 가져왔으며 인공지능이 적용 가능한 영역을 획기적으로 확장시키고 있다. 하지만 이러한 대규모의 학습 데이터에 기반한 딥러닝은 학습시간이 증가되고, 더 나아가 학습 데이터의 구축을 위한 레이블링(labeling) 비용이 데이터 수에 비례하여 증가하게 된다. 따라서 본 논문에서는, 딥러닝의 정확도를 최대한 유지하면서 필요한 학습 데이터의 수를 줄일수 있도록, 전체 데이터에서 제한된 수의 학습 데이터의 후보를 선별하는 기법에 대하여 연구한다. 특히 기존의 능동 학습에 기반한 기법들이 학습 데이터의 수가 매우 적어질 경우 딥러닝의 정확도가 급격히 떨어지는 문제를 해결할 수 있는 새로운 기법을 제안하였으며, 제안된 기법으로 선별된 학습 데이터는 그 수의 변화에 대해 딥러닝이 일관되게 경쟁력있는 정확도를 가짐을 다양한 실험을 통하여 검증하였다.The increase of big data, parallel hardware processing and enhanced learning algorithms have led to the development of deep learning and are dramatically expanding the domain of applications based on artificial intelligence. However, such large-scale training data extends the learning time, and furthermore, the labeling cost for building the training set increases in proportion to the volume of data. Therefore, in this paper, we study how to select a limited number of candidates for training set from given data so as to reduce the number of training set while maintaining the accuracy of deep learning. Especially, we found that, with the training set selected by the previous work, the accuracy of deep learning drastically decreases when the number of training set is very small and so proposed new methods to resolve this fall-off in accuracy. Through experiments, we showed that deep learning consistently has the competitive accuracy with the training set selected by the proposed method regardless of the size of training set.Deep Learning, Train Set Selection, Active Learning, Gamma distribution"
948,딥러닝을 이용한 범용적 스테그아날리시스,김현재 /Hyunjae Kim,2017,한국정보과학회,"스테그아날리시스(Steganalysis)란 이미지 등 일반적인 자료에 암호화된 정보를 은닉하는 스테가노그래피(Steganography)에 대한 검출 및 분석 방법으로, 기계학습 기반 방법론을 포함한다. 기존 기계학습 기반 스테그아날리시스는 영상(Image)의 특징(Feature) 추출 및 모델링에 기반하며, 최근 딥러닝(Deep Learning)의 적용으로 검출 정확도가 큰 폭으로 향상되었다. 하지만 현존하는 스테그아날리시스 모델은 단일 스테가노그래피 기법에 대해 국한되어 있어 학습에 사용되지 않은 스테고(Stego) 이미지의 경우 검출이 불가능한 결정적 한계를 가진다. 본 연구에서는 다양한 스테가노그래피 기법으로 생성된 스테고이미지에 딥러닝을 적용하여 스테그아날리시스를 학습하는 범용적 모델을 제안한다. 다양한 실험을 통해 제안 기법의 효용성 및 가능성을 확인하고, 범용적 스테그아날리시스 모델이 각각에 특화된 검출 기법과 유사한 정확도로 스테고 이미지를 검출할 수 있음을 보인다.Steganalysis is to detect information hidden by steganography inside general data such as images. There are stegoanalysis techniques that use machine learning (ML). Existing ML approaches to steganalysis are based on extracting features from stego images and modeling them.
Recently deep learning-based methodologies have shown significant improvements in detection accuracy. However, all the existing methods, including deep learning-based ones, have a critical limitation in that they can only detect stego images that are created by a specific steganography method. In this paper, we propose a generalized steganalysis method that can model multiple types of stego images using deep learning. Through various experiments, we confirm the effectiveness of our approach and envision directions for future research. In particular, we show that our method can detect each type of steganography with the same level of accuracy as that of a steganalysis method dedicated to that type of steganography, thereby demonstrating the general applicability of our approach to multiple types of stego images.steganalysis, steganography, deep learning, generalized steganalysis model"
949,"딥러닝 프레임워크의 비교: 티아노, 텐서플로, CNTK를 중심으로",정여진 /Yeojin Chung,2017,한국지능정보시스템학회,"딥러닝 프레임워크의 대표적인 기능으로는 ‘자동미분’과 ‘GPU의 활용’ 등을 들 수 있다. 본 논문은 파이썬의 라이브러리 형태로 사용 가능한 프레임워크 중에서 구글의 텐서플로와 마이크로소프트의 CNTK, 그리고 텐서 플로의 원조라고 할 수 있는 티아노를 비교하였다. 본문에서는 자동미분의 개념과 GPU의 활용형태를 간단히 설명하고, 그 다음에 logistic regression을 실행하는 예를 통하여 각 프레임워크의 문법을 알아본 뒤에, 마지막으로 대표적인 딥러닝 응용인 CNN의 예제를 실행시켜보고 코딩의 편의성과 실행속도 등을 확인해 보았다. 그 결과, 편의성의 관점에서 보면 티아노가 가장 코딩 하기가 어렵고, CNTK와 텐서플로는 많은 부분이 비슷하게 추상화 되어 있어서 코딩이 비슷하지만 가중치와 편향을 직접 정의하느냐의 여부에서 차이를 보였다. 그리고 각프레임워크의 실행속도에 대한 평가는 ‘큰 차이는 없다’는 것이다. 텐서플로는 티아노에 비하여 속도가 느리다는 평가가 있어왔는데, 본 연구의 실험에 의하면, 비록 CNN 모형에 국한되었지만, 텐서플로가 아주 조금이지만빠른 것으로 나타났다. CNTK의 경우에도, 비록 실험환경이 달랐지만, 실험환경의 차이에 의한 속도의 차이의 편차범위 이내에 있는 것으로 판단이 되었다. 본 연구에서는 세 종류의 딥러닝 프레임워크만을 살펴보았는데,위키피디아에 따르면 딥러닝 프레임워크의 종류는 12가지가 있으며, 각 프레임워크의 특징을 15가지 속성으로구분하여 차이를 특정하고 있다. 그 많은 속성 중에서 사용자의 입장에서 볼 때 중요한 속성은 어떤 언어(파이썬, C++, Java, 등)로 사용가능한지, 어떤 딥러닝 모형에 대한 라이브러리가 잘 구현되어 있는지 등일 것이다. 그리고 사용자가 대규모의 딥러닝 모형을 구축한다면, 다중 GPU 혹은 다중 서버를 지원하는지의 여부도 중요할 것이다. 또한 딥러닝 모형을 처음 학습하는 경우에는 사용설명서가 많은지 예제 프로그램이 많은지 여부도 중요한 기준이 될 것이다.The deep learning framework is software designed to help develop deep learning models. Some of its important functions include “automatic differentiation” and “utilization of GPU”. The list of popular deep learning framework includes Caffe (BVLC) and Theano (University of Montreal). And recently, Microsoft's deep learning framework, Microsoft Cognitive Toolkit, was released as open-source license, following Google’s Tensorflow a year earlier. The early deep learning frameworks have been developed mainly for research at universities. Beginning with the inception of Tensorflow, however, it seems that companies such as Microsoft and Facebook have started to join the competition of framework development. Given the trend, Google and other companies are expected to continue investing in the deep learning framework to bring forward the initiative in the artificial intelligence business. From this point of view, we think it is a good time to compare some of deep learning frameworks. So we compare three deep learning frameworks which can be used as a Python library. Those are Google's Tensorflow, Microsoft’s CNTK, and Theano which is sort of a predecessor of the preceding two.
The most common and important function of deep learning frameworks is the ability to perform automatic differentiation. Basically all the mathematical expressions of deep learning models can be represented as computational graphs, which consist of nodes and edges. Partial derivatives on each edge of a computational graph can then be obtained. With the partial derivatives, we can let software compute differentiation of any node with respect to any variable by utilizing chain rule of Calculus.
First of all, the convenience of coding is in the order of CNTK, Tensorflow, and Theano. The criterion is simply based on the lengths of the codes and the learning curve and the ease of coding are not the main concern. According to the criteria, Theano was the most difficult to implement with, and CNTK and Tensorflow were somewhat easier. With Tensorflow, we need to define weight variables and biases explicitly. The reason that CNTK and Tensorflow are easier to implement with is that those frameworks provide us with more abstraction than Theano. We, however, need to mention that low-level coding is not always bad. It gives us flexibility of coding. With the low-level coding such as in Theano, we can implement and test any new deep learning models or any new search methods that we can think of.
The assessment of the execution speed of each framework is that there is not meaningful difference. According to the experiment, execution speeds of Theano and Tensorflow are very similar, although the experiment was limited to a CNN model. In the case of CNTK, the experimental environment was not maintained as the same. The code written in CNTK has to be run in PC environment without GPU where codes execute as much as 50 times slower than with GPU. But we concluded that the difference of execution speed was within the range of variation caused by the different hardware setup.
In this study, we compared three types of deep learning framework: Theano, Tensorflow, and CNTK. According to Wikipedia, there are 12 available deep learning frameworks. And 15 different attributes differentiate each framework. Some of the important attributes would include interface language (Python, C ++, Java, etc.) and the availability of libraries on various deep learning models such as CNN, RNN, DBN, and etc. And if a user implements a large scale deep learning model, it will also be important to support multiple GPU or multiple servers. Also, if you are learning the deep learning model, it would also be important if there are enough examples and references.deep learning framework, Theano, TensorFlow, CNTK, computational graph, CIFAR-10"
950,딥러닝을 이용한 주변 무선단말 파악방안,이웅섭 /Lee， Woong-sup,2018,한국정보통신학회,"최근 단말-대-단말(Device-to-device, D2D) 통신기술이 차세대 무선통신시스템의 핵심기술로 큰 관심을 받고 있다. 이러한 단말간 통신에서는 자신의 주변에 어떠한 단말이 있는지 파악하는 주변단말 탐색(Neighbor discovery)이매우 중요하다. 본 논문에서는 최근 큰 관심을 받고 있는 딥러닝(Deep learning) 기술을 활용하여 단말간 통신에서 주변단말을 파악하는 방안에 대해서 제안한다. 제안 방안은 기존의 방안과 달리 무선채널의 공간적 연관성을 이용하여단말간의 신호 전송 없이 단말이 기지국으로 전송하는 상향링크 파일럿 신호를 기반으로 주변 단말을 찾고 따라서기존의 방식에 비해 신호전송 복잡도(signaling complexity)를 크게 줄일 수 있다. 또한 제안 방안에서는 떨어져 있는거리에 따라서 주변 단말을 분류 가능하여 기존 방안에 비해서 좀 더 세밀한 단말 탐색이 가능하다. 마지막으로 본 논문에서는 tensorflow를 이용한 컴퓨터 시뮬레이션을 통해 제안 방안의 성능을 검증하였다.Recently, the device-to-device (D2D) communication has been conceived as the key technology for the next-generationmobile communication systems. The neighbor discovery in which the nearby users are found, is essential for the properoperation of the D2D communication. In this paper, we propose new neighbor discovery scheme based on deep learningtechnology which has gained a lot of attention recently. In the proposed scheme, the neighboring users can be foundusing the uplink pilot transmission of users only, unlike conventional neighbor discovery schemes in which direct pilotcommunication among users is required, such that the signaling overhead can be greatly reduced in our proposed scheme.
Moreover, the neighbors with different proximity can also be classified accordingly which enables more accurate neighbordiscovery compared to the conventional schemes. The performance of our proposed scheme is verified through thetensorflow-based computer simulations.
키워드 : 딥러닝, 주변단말 탐색, 단말-대-단말 통신Deep learning, Neighbor discovery, Device-to-device communication, Wireless channel correlation"
951,신뢰성있는 딥러닝 기반 분석 모델을 참조하기 위한 딥러닝 기술 언어,문종혁 /Jong Hyeok Mun,2021,한국정보처리학회,"최근 딥러닝은 하드웨어 성능이 향상됨에 따라 자연어 처리, 영상 인식 등의 다양한 기술에 접목되어 활용되고 있다. 이러한 기술들을 활용해 지능형교통 시스템(ITS), 스마트홈, 헬스케어 등의 산업분야에서 데이터를 분석하여 고속도로 속도위반 차량 검출, 에너지 사용량 제어, 응급상황 등과 같은고품질의 서비스를 제공하며, 고품질의 서비스를 제공하기 위해서는 정확도가 향상된 딥러닝 모델이 적용되어야 한다. 이를 위해 서비스 환경의 데이터를분석하기 위한 딥러닝 모델을 개발할 때, 개발자는 신뢰성이 검증된 최신의 딥러닝 모델을 적용할 수 있어야 한다. 이는 개발자가 참조하는 딥러닝 모델에적용된 학습 데이터셋의 정확도를 측정하여 검증할 수 있다. 이러한 검증을 위해서 개발자는 학습 데이터셋, 딥러닝의 계층구조 및 개발 환경 등과 같은내용을 포함하는 딥러닝 모델을 문서화하여 적용하기 위한 구조적인 정보가 필요하다. 본 논문에서는 신뢰성있는 딥러닝 기반 데이터 분석 모델을 참조하기위한 딥러닝 기술 언어를 제안한다. 제안하는 기술 언어는 신뢰성 있는 딥러닝 모델을 개발하는데 필요한 학습데이터셋, 개발 환경 및 설정 등의 정보와더불어 딥러닝 모델의 계층구조를 표현할 수 있다. 제안하는 딥러닝 기술 언어를 이용하여 개발자는 지능형 교통 시스템에서 참조하는 분석 모델의정확도를 검증할 수 있다. 실험에서는 제안하는 언어의 유효성을 검증하기 위해, 번호판 인식 모델을 중심으로 딥러닝 기술 문서의 적용과정을 보인다With the recent advancements of deep learning, companies such as smart home, healthcare, and intelligent transportation systems areutilizing its functionality to provide high-quality services for vehicle detection, emergency situation detection, and controlling energyconsumption. To provide reliable services in such sensitive systems, deep learning models are required to have high accuracy. In orderto develop a deep learning model for analyzing previously mentioned services, developers should utilize the state of the art deep learningmodels that have already been verified for higher accuracy. The developers can verify the accuracy of the referenced model by validatingthe model on the dataset. For this validation, the developer needs structural information to document and apply deep learning models,including metadata such as learning dataset, network architecture, and development environments. In this paper, we propose a descriptionlanguage that represents the network architecture of the deep learning model along with its metadata that are necessary to develop adeep learning model. Through the proposed description language, developers can easily verify the accuracy of the referenced deep learningmodel. Our experiments demonstrate the application scenario of a deep learning description document that focuses on the license platerecognition for the detection of illegally parked vehicles.Trusted Deep Learning, Model Reference, Deep Learning Description Language, Traffic Situation Analysis Model"
952,딥러닝 분석을 이용한 중국 역내·외 위안화 변동성 예측,이우식 /Lee Woosik,2016,한국데이터정보과학회,"2008년 글로벌 금융위기 이후 중국은 위안화 국제화의 점진적 추진을 진행하면서 중국상하이 외환시장과 중국홍콩 외환시장에서 거래되는 통화인 역내위안화와 역외위안화를 형성시켰다. 본 연구는 위안화 국제화와 점진적인 중국 자본계정 개방에 따라 급변하는 외환시장상황의 변동성을 정확하게 파악하기 위해서 GARCH모형 (일반화된 자기회귀 조건부이분산성모형)에 다단계인공신경망을 결합한 MLP-GARCH 모형과 GARCH모형과 기계학습의 일종인 딥러닝 (deep learning)을 통합한 DL-GARCH을 가지고 위안화 변동성예측을 비교 실험과 분석을 하였다. 비교분석 결과 DLGARCH 모형은 MLP-GARCH보다 모형 위안화 역내·외 환율변동성 예측 면에서 더욱 더 개선된예측값을 제공하였다. 그래서 이분산시계열모형을 딥러닝과 결합한 DL-GARCH 모형은 시계열의환율 변동성 예측 문제에 딥러닝을 응용할 수 있음을 확인하였다. 향후 이분산시계열과 결합된 딥러닝 모형은 다른 금융시계열 데이터에 응용하여 그 일반화 가능성을 높일 수 있을 것이다.The People’s Republic of China has vigorously been pursuing the internationalization of the Chinese Yuan or Renminbi after the financial crisis of 2008. In this view, an abrupt increase of use of the Chinese Yuan in the onshore and offshore markets are important milestones to be one of important currencies. One of the most frequently used methods to forecast volatility is GARCH model. Since a prediction error of the GARCH model has been reported quite high, a lot of efforts have been made to improve forecasting capability of the GARCH model. In this paper, we have proposed MLPGARCH and a DL-GARCH by employing Artificial Neural Network to the GARCH. In an application to forecasting Chinese Yuan volatility, we have successfully shown their overall outperformance in forecasting over the GARCH.Deep learning, DL-GARCH, GARCH, The Chinese Yuan, volatility."
953,딥러닝 기법을 이용한 내일강수 예측,하지훈,2016,한국지능시스템학회,"정확한 강수예측을 위해서는 예측인자 선정과 예측방법에 대한 선택이 매우 중요하다. 최근에는 강수예측 방법으로기계학습 기법이 많이 사용되고 있으며, 그 중에서도 특히 인공신경망을 사용한 강수예측 방법은 좋은 성능을 보였다. 본논문에서는 딥러닝 기법 중 하나인 DBN(deep belief network)를 이용한 새로운 강수예측 방법을 제안한다. DBN는 비지도사전 학습을 통해 초기 가중치를 설정하여 기존 인공신경망의 문제점을 보완한다. 예측인자로는 기온, 전일-전주 강수일, 태양과 달 궤도 관련 자료를 선정하였다. 기온과 전일-전주 강수일은 서울에서의 1974년부터 2013년까지 총 40년간의AWS(automatic weather system) 관측 자료를 사용하였고, 태양과 달의 궤도 관련 자료는 서울을 중심으로 계산한 결과를사용하였다. 전체 기간에서 일부는 학습 자료로 사용하여 예측모델을 생성하였고, 나머지를 생성한 모델의 검증 자료로사용하였다. 모델 검증 결과로 나온 예측값들은 확률값을 가지며 임계치를 이용하여 강수유무를 판별하였다. 강수 정확도의척도로 양분예보기법 중 CSI(critical successive index)와 Bias(frequency bias)를 계산하였다. 이를 통해 DBN와 MLP(multilayer perceptron)의 성능을 비교한 결과 DBN의 강수 예측 정확도가 높았고, 수행속도 또한 2배 이상 빨랐다.For accurate precipitation forecasts the choice of weather faoctrs and prediction method is very important. Recently, machinele arning has been widely used for forecasting precipitation, and artificial neural network, one of machine learning techniques, showed good performance. In this paper, we suggest a new method for forecasting precipitation using DBN, one of deep learning techniques.
DBN has an advantage that initial weights are set by unsupervised learning, so this compensates for the defects of artificial neural networks. We used past precipitation, temperature, and the parameters of the sun and moon's motion as features for forecasting precipitation. The dataset consists of observation data which had been measured for 40 years from AWS in Seoul. Experiments were based on 8-fold cross validation. As a result of estimation, we got probabilities of test dataset, so threshold was used for the decision of precipitation. CSI and Bias were used for indicating the precision of precipitation. Our experimental results showed that DBN performed better than MLP.Deep learning, Deep belief network, Precipitation, Forecast"
954,딥러닝 신경망모형을 이용한 실시간 교통정보수집 연구,김대현 /KIM DAE HYON,2016,사단법인 인문사회과학기술융합학회,"비디오 기반의 실시간 교통정보수집 시스템은 자율주행을 포함한 지능형교통체계의 주요 서브시스템중 하나이며, 우수한 예측(추정)력 때문에 핵심 알고리즘으로 인공신경망 모형이 주로 사용되고 있다. 특히, 최근 과학 학술지 네이처에 구글 딥마인드의 알파고가 공개되면서 딥러닝과 인공지능에 대한 전 세계적인 관심이 집중되고 있다. 최근까지 가장 널리 사용된 인공신경망모형은 역전파 모형이었으나 최근 컴퓨터 하드웨어 및 알고리즘의 새로운 개발과 더불어 딥러닝 알고리즘을 이용한 딥인공신경망 모형에 대한 관심이 높아지고 있다. 그러나 아직 딥러닝 인공신경망에 대한 연구는 초기단계에 있는 실정이며 적용분야도 극히 한정되어 있다. 본 연구에서는 실시간 도로교통정보 수집을 위한 영상처리분야에서 딥러닝 알고리즘을 활용하고 실험결과를 통하여 향후 교통정보수집‧분석 분야에서의 딥러닝 인공신경망 모형의 적용가능성을 살펴보고자 한다. 특히, 본 연구에서 도출한 딥러닝 인공신경망모형의 실험결과를 기존 역전파 모형에 의한 실험결과와 비교하여 모형의 우수성을 비교‧평가하고자 한다.Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Normally, artificial neural networks have been used in real-time applications for collecting traffic information because of their remarkable performance in terms of predictive accuracy and real-time computing. Especially, the recent publication of a paper in the journal Nature and the success of Google DeepMind’s world-beating Go program AlphaGo caused significant increases of the public interest in artificial intelligence. Even though various types of learning models have been proposed, the Backpropagation has been the most popular neural network model. However, the Deep learning neural network model has attracted much interest recently. In this study, the Deep leaning neural network will be applied in image processing techniques for the real-time traffic information collection to investigate the performance of Deep learning algorithm in traffic engineering fields. In addition, the experimental results from Deep learing algorithm will be compared with the results from Backpropagation model to evaluate and propose better performing model.Intelligent Transportation Systems (ITS), artificial neural networks, Deep learning, traffic information, image processing"
955,딥러닝의 모형과 응용사례,안성만 /SungMahn Ahn,2016,한국지능정보시스템학회,"딥러닝은 인공신경망(neural network)이라는 인공지능분야의 모형이 발전된 형태로서, 계층구조로 이루어진 인공신경망의 내부계층(hidden layer)이 여러 단계로 이루어진 구조이다. 딥러닝에서의 주요 모형은 합성곱신경망(convolutional neural network), 순환신경망(recurrent neural network), 그리고 심층신뢰신경망(deep belief network)의 세가지라고 할 수있다. 그 중에서 현재 흥미로운 연구가 많이 발표되어서 관심이 집중되고 있는 모형은 지도학습(supervised learning)모형인 처음 두 개의 모형이다. 따라서 본 논문에서는 지도학습모형의 가중치를 최적화하는 기본적인 방법인 오류역전파 알고리즘을 살펴본 뒤에 합성곱신경망과 순환신경망의 구조와 응용사례 등을 살펴보고자 한다. 본문에서 다루지 않은 모형인 심층신뢰신경망은 아직까지는 합성곱신경망 이나 순환신경망보다는 상대적으로 주목을 덜 받고 있다. 그러나 심층신뢰신경망은 CNN이나 RNN과는 달리 비지도학습(unsupervised learning)모형이며, 사람이나 동물은 관찰을 통해서 스스로 학습한다는 점에서 궁극적으로는 비지도학습모형이 더 많이 연구되어야 할 주제가 될 것이다.Deep learning model is a kind of neural networks that allows multiple hidden layers. There are various deep learning architectures such as convolutional neural networks, deep belief networks and recurrent neural networks. Those have been applied to fields like computer vision, automatic speech recognition, natural language processing, audio recognition and bioinformatics where they have been shown to produce state-of-the-art results on various tasks. Among those architectures, convolutional neural networks and recurrent neural networks are classified as the supervised learning model. And in recent years, those supervised learning models have gained more popularity than unsupervised learning models such as deep belief networks, because supervised learning models have shown fashionable applications in such fields mentioned above.
Deep learning models can be trained with backpropagation algorithm. Backpropagation is an abbreviation for “backward propagation of errors” and a common method of training artificial neural networks used in conjunction with an optimization method such as gradient descent. The method calculates the gradient of an error function with respect to all the weights in the network. The gradient is fed to the optimization method which in turn uses it to update the weights, in an attempt to minimize the error function.
Convolutional neural networks use a special architecture which is particularly well-adapted to classify images. Using this architecture makes convolutional networks fast to train. This, in turn, helps us train deep, muti-layer networks, which are very good at classifying images. These days, deep convolutional networks are used in most neural networks for image recognition. Convolutional neural networks use three basic ideas: local receptive fields, shared weights, and pooling. By local receptive fields, we mean that each neuron in the first(or any) hidden layer will be connected to a small region of the input(or previous layer’s) neurons. Shared weights mean that we’re going to use the same weights and bias for each of the local receptive field. This means that all the neurons in the hidden layer detect exactly the same feature, just at different locations in the input image. In addition to the convolutional layers just described, convolutional neural networks also contain pooling layers. Pooling layers are usually used immediately after convolutional layers. What the pooling layers do is to simplify the information in the output from the convolutional layer.
Recent convolutional network architectures have 10 to 20 hidden layers and billions of connections between units. Training deep learning networks has taken weeks several years ago, but thanks to progress in GPU and algorithm enhancement, training time has reduced to several hours.
Neural networks with time-varying behavior are known as recurrent neural networks or RNNs. A recurrent neural network is a class of artificial neural network where connections between units form a directed cycle. This creates an internal state of the network which allows it to exhibit dynamic temporal behavior. Unlike feedforward neural networks, RNNs can use their internal memory to process arbitrary sequences of inputs. Early RNN models turned out to be very difficult to train, harder even than deep feedforward networks. The reason is the unstable gradient problem such as vanishing gradient and exploding gradient. The gradient can get smaller and smaller as it is propagated back through layers. This makes learning in early layers extremely slow. The problem actually gets worse in RNNs, since gradients aren’t just propagated backward through layers, they’re propagated backward through time. If the network runs for a long time, that can make the gradient extremely unstable and hard to learn from. It has been possible to incorporate an idea known as long short-term memory units (LSTMs) into RNNs. LSTMs make it much easier to get good results when training RNNs, and many recent papers make use of LSTMs or related ideas.Deep learning, Convolutional neural networks, Recurrent neural networks, Error backpropagation algorithm"
956,딥러닝분석과 기술적 분석 지표를 이용한 한국 코스피주가지수 방향성 예측,이우식 /Lee Woosik,2017,한국데이터정보과학회,"2016년 3월 구글 (Google)의 바둑인공지능 알파고 (AlphaGo)가 이세돌 9단과의 바둑대결에서승리한 이후 다양한 분야에서 인공지능 사용에 대한 관심이 높아지고 있는 가운데 금융투자 분야에서도 인공지능과 투자자문 전문가의 합성어인 로보어드바이저 (Robo-Advisor)에 대한 관심이 높아지고 있다. 인공지능 (artificial intelligence)기반의 의사결정은 비용 절감은 물론 효과적인 의사결정을가능하게 한다는 점에서 큰 장점이 있다. 본 연구에서는 기술적 분석 (technical analysis) 지표와 딥러닝 (deep learning) 모형을 결합하여 한국 코스피 지수를 예측하는 모형을 개발하고 제시한 모형들의 예측력을 비교, 분석한다. 분석 결과 기술적 분석 지표에 딥러닝 알고리즘을 결합한 모형이 주가지수 방향성 예측 문제에 응용될 수 있음을 확인하였다. 향후 본 연구에서 제안된 기술적 분석 지표와딥러닝모형을 결합한 기법은 로보어드바이저서비스에 응용할 수 있는 일반화 가능성을 보여준다Since Google’s AlphaGo defeated a world champion of Go players in 2016, there have been many interests in the deep learning. In the financial sector, a Robo-Advisor using deep learning gains a significant attention, which builds and manages portfolios of financial instruments for investors.In this paper, we have proposed the a deep learning algorithm geared toward identification and forecast of the KOSPI index direction,and we also have compared the accuracy of the prediction.In an application of forecasting the financial market index direction, we have shown that the Robo-Advisor using deep learning has a significant effect on finance industry. The Robo-Advisor collects a massive data such as earnings statements, news reports and regulatory filings, analyzes those and recommends investors how to view market trends and identify the best time to purchase financial assets. On the other hand, the Robo-Advisor allows businesses to learn more about their customers, develop better marketing strategies, increase sales and decrease costs.Artificial intelligence, deep learning, FinTech, Robo-Advisor, technical analysis."
957,딥러닝의 효율적인 학습을 위한 학습데이터 선별 기법에 관한 연구,주경돈 /Gyoung-don Joo,2017,한국정보과학회,"빅데이터의 증가와 병렬 하드웨어 처리, 학습 알고리즘의 개선은 딥러닝의 발전을 가져왔으며 인공지능이 적용 가능한 영역을 획기적으로 확장시키고 있다. 하지만 이러한 대규모의 학습 데이터에 기반한 딥러닝은 학습시간이 증가되고, 더 나아가 학습 데이터의 구축을 위한 레이블링(labeling) 비용이 데이터 수에 비례하여 증가하게 된다. 따라서 본 논문에서는, 딥러닝의 정확도를 최대한 유지하면서 필요한 학습 데이터의 수를 줄일수 있도록, 전체 데이터에서 제한된 수의 학습 데이터의 후보를 선별하는 기법에 대하여 연구한다. 특히 기존의 능동 학습에 기반한 기법들이 학습 데이터의 수가 매우 적어질 경우 딥러닝의 정확도가 급격히 떨어지는 문제를 해결할 수 있는 새로운 기법을 제안하였으며, 제안된 기법으로 선별된 학습 데이터는 그 수의 변화에 대해 딥러닝이 일관되게 경쟁력있는 정확도를 가짐을 다양한 실험을 통하여 검증하였다.The increase of big data, parallel hardware processing and enhanced learning algorithms have led to the development of deep learning and are dramatically expanding the domain of applications based on artificial intelligence. However, such large-scale training data extends the learning time, and furthermore, the labeling cost for building the training set increases in proportion to the volume of data. Therefore, in this paper, we study how to select a limited number of candidates for training set from given data so as to reduce the number of training set while maintaining the accuracy of deep learning. Especially, we found that, with the training set selected by the previous work, the accuracy of deep learning drastically decreases when the number of training set is very small and so proposed new methods to resolve this fall-off in accuracy. Through experiments, we showed that deep learning consistently has the competitive accuracy with the training set selected by the proposed method regardless of the size of training set.Deep Learning, Train Set Selection, Active Learning, Gamma distribution"
958,딥러닝을 이용한 범용적 스테그아날리시스,김현재 /Hyunjae Kim,2017,한국정보과학회,"스테그아날리시스(Steganalysis)란 이미지 등 일반적인 자료에 암호화된 정보를 은닉하는 스테가노그래피(Steganography)에 대한 검출 및 분석 방법으로, 기계학습 기반 방법론을 포함한다. 기존 기계학습 기반 스테그아날리시스는 영상(Image)의 특징(Feature) 추출 및 모델링에 기반하며, 최근 딥러닝(Deep Learning)의 적용으로 검출 정확도가 큰 폭으로 향상되었다. 하지만 현존하는 스테그아날리시스 모델은 단일 스테가노그래피 기법에 대해 국한되어 있어 학습에 사용되지 않은 스테고(Stego) 이미지의 경우 검출이 불가능한 결정적 한계를 가진다. 본 연구에서는 다양한 스테가노그래피 기법으로 생성된 스테고이미지에 딥러닝을 적용하여 스테그아날리시스를 학습하는 범용적 모델을 제안한다. 다양한 실험을 통해 제안 기법의 효용성 및 가능성을 확인하고, 범용적 스테그아날리시스 모델이 각각에 특화된 검출 기법과 유사한 정확도로 스테고 이미지를 검출할 수 있음을 보인다.Steganalysis is to detect information hidden by steganography inside general data such as images. There are stegoanalysis techniques that use machine learning (ML). Existing ML approaches to steganalysis are based on extracting features from stego images and modeling them.
Recently deep learning-based methodologies have shown significant improvements in detection accuracy. However, all the existing methods, including deep learning-based ones, have a critical limitation in that they can only detect stego images that are created by a specific steganography method. In this paper, we propose a generalized steganalysis method that can model multiple types of stego images using deep learning. Through various experiments, we confirm the effectiveness of our approach and envision directions for future research. In particular, we show that our method can detect each type of steganography with the same level of accuracy as that of a steganalysis method dedicated to that type of steganography, thereby demonstrating the general applicability of our approach to multiple types of stego images.steganalysis, steganography, deep learning, generalized steganalysis model"
959,"딥러닝 프레임워크의 비교: 티아노, 텐서플로, CNTK를 중심으로",정여진 /Yeojin Chung,2017,한국지능정보시스템학회,"딥러닝 프레임워크의 대표적인 기능으로는 ‘자동미분’과 ‘GPU의 활용’ 등을 들 수 있다. 본 논문은 파이썬의 라이브러리 형태로 사용 가능한 프레임워크 중에서 구글의 텐서플로와 마이크로소프트의 CNTK, 그리고 텐서 플로의 원조라고 할 수 있는 티아노를 비교하였다. 본문에서는 자동미분의 개념과 GPU의 활용형태를 간단히 설명하고, 그 다음에 logistic regression을 실행하는 예를 통하여 각 프레임워크의 문법을 알아본 뒤에, 마지막으로 대표적인 딥러닝 응용인 CNN의 예제를 실행시켜보고 코딩의 편의성과 실행속도 등을 확인해 보았다. 그 결과, 편의성의 관점에서 보면 티아노가 가장 코딩 하기가 어렵고, CNTK와 텐서플로는 많은 부분이 비슷하게 추상화 되어 있어서 코딩이 비슷하지만 가중치와 편향을 직접 정의하느냐의 여부에서 차이를 보였다. 그리고 각프레임워크의 실행속도에 대한 평가는 ‘큰 차이는 없다’는 것이다. 텐서플로는 티아노에 비하여 속도가 느리다는 평가가 있어왔는데, 본 연구의 실험에 의하면, 비록 CNN 모형에 국한되었지만, 텐서플로가 아주 조금이지만빠른 것으로 나타났다. CNTK의 경우에도, 비록 실험환경이 달랐지만, 실험환경의 차이에 의한 속도의 차이의 편차범위 이내에 있는 것으로 판단이 되었다. 본 연구에서는 세 종류의 딥러닝 프레임워크만을 살펴보았는데,위키피디아에 따르면 딥러닝 프레임워크의 종류는 12가지가 있으며, 각 프레임워크의 특징을 15가지 속성으로구분하여 차이를 특정하고 있다. 그 많은 속성 중에서 사용자의 입장에서 볼 때 중요한 속성은 어떤 언어(파이썬, C++, Java, 등)로 사용가능한지, 어떤 딥러닝 모형에 대한 라이브러리가 잘 구현되어 있는지 등일 것이다. 그리고 사용자가 대규모의 딥러닝 모형을 구축한다면, 다중 GPU 혹은 다중 서버를 지원하는지의 여부도 중요할 것이다. 또한 딥러닝 모형을 처음 학습하는 경우에는 사용설명서가 많은지 예제 프로그램이 많은지 여부도 중요한 기준이 될 것이다.The deep learning framework is software designed to help develop deep learning models. Some of its important functions include “automatic differentiation” and “utilization of GPU”. The list of popular deep learning framework includes Caffe (BVLC) and Theano (University of Montreal). And recently, Microsoft's deep learning framework, Microsoft Cognitive Toolkit, was released as open-source license, following Google’s Tensorflow a year earlier. The early deep learning frameworks have been developed mainly for research at universities. Beginning with the inception of Tensorflow, however, it seems that companies such as Microsoft and Facebook have started to join the competition of framework development. Given the trend, Google and other companies are expected to continue investing in the deep learning framework to bring forward the initiative in the artificial intelligence business. From this point of view, we think it is a good time to compare some of deep learning frameworks. So we compare three deep learning frameworks which can be used as a Python library. Those are Google's Tensorflow, Microsoft’s CNTK, and Theano which is sort of a predecessor of the preceding two.
The most common and important function of deep learning frameworks is the ability to perform automatic differentiation. Basically all the mathematical expressions of deep learning models can be represented as computational graphs, which consist of nodes and edges. Partial derivatives on each edge of a computational graph can then be obtained. With the partial derivatives, we can let software compute differentiation of any node with respect to any variable by utilizing chain rule of Calculus.
First of all, the convenience of coding is in the order of CNTK, Tensorflow, and Theano. The criterion is simply based on the lengths of the codes and the learning curve and the ease of coding are not the main concern. According to the criteria, Theano was the most difficult to implement with, and CNTK and Tensorflow were somewhat easier. With Tensorflow, we need to define weight variables and biases explicitly. The reason that CNTK and Tensorflow are easier to implement with is that those frameworks provide us with more abstraction than Theano. We, however, need to mention that low-level coding is not always bad. It gives us flexibility of coding. With the low-level coding such as in Theano, we can implement and test any new deep learning models or any new search methods that we can think of.
The assessment of the execution speed of each framework is that there is not meaningful difference. According to the experiment, execution speeds of Theano and Tensorflow are very similar, although the experiment was limited to a CNN model. In the case of CNTK, the experimental environment was not maintained as the same. The code written in CNTK has to be run in PC environment without GPU where codes execute as much as 50 times slower than with GPU. But we concluded that the difference of execution speed was within the range of variation caused by the different hardware setup.
In this study, we compared three types of deep learning framework: Theano, Tensorflow, and CNTK. According to Wikipedia, there are 12 available deep learning frameworks. And 15 different attributes differentiate each framework. Some of the important attributes would include interface language (Python, C ++, Java, etc.) and the availability of libraries on various deep learning models such as CNN, RNN, DBN, and etc. And if a user implements a large scale deep learning model, it will also be important to support multiple GPU or multiple servers. Also, if you are learning the deep learning model, it would also be important if there are enough examples and references.deep learning framework, Theano, TensorFlow, CNTK, computational graph, CIFAR-10"
960,딥러닝을 이용한 주변 무선단말 파악방안,이웅섭 /Lee， Woong-sup,2018,한국정보통신학회,"최근 단말-대-단말(Device-to-device, D2D) 통신기술이 차세대 무선통신시스템의 핵심기술로 큰 관심을 받고 있다. 이러한 단말간 통신에서는 자신의 주변에 어떠한 단말이 있는지 파악하는 주변단말 탐색(Neighbor discovery)이매우 중요하다. 본 논문에서는 최근 큰 관심을 받고 있는 딥러닝(Deep learning) 기술을 활용하여 단말간 통신에서 주변단말을 파악하는 방안에 대해서 제안한다. 제안 방안은 기존의 방안과 달리 무선채널의 공간적 연관성을 이용하여단말간의 신호 전송 없이 단말이 기지국으로 전송하는 상향링크 파일럿 신호를 기반으로 주변 단말을 찾고 따라서기존의 방식에 비해 신호전송 복잡도(signaling complexity)를 크게 줄일 수 있다. 또한 제안 방안에서는 떨어져 있는거리에 따라서 주변 단말을 분류 가능하여 기존 방안에 비해서 좀 더 세밀한 단말 탐색이 가능하다. 마지막으로 본 논문에서는 tensorflow를 이용한 컴퓨터 시뮬레이션을 통해 제안 방안의 성능을 검증하였다.Recently, the device-to-device (D2D) communication has been conceived as the key technology for the next-generationmobile communication systems. The neighbor discovery in which the nearby users are found, is essential for the properoperation of the D2D communication. In this paper, we propose new neighbor discovery scheme based on deep learningtechnology which has gained a lot of attention recently. In the proposed scheme, the neighboring users can be foundusing the uplink pilot transmission of users only, unlike conventional neighbor discovery schemes in which direct pilotcommunication among users is required, such that the signaling overhead can be greatly reduced in our proposed scheme.
Moreover, the neighbors with different proximity can also be classified accordingly which enables more accurate neighbordiscovery compared to the conventional schemes. The performance of our proposed scheme is verified through thetensorflow-based computer simulations.
키워드 : 딥러닝, 주변단말 탐색, 단말-대-단말 통신Deep learning, Neighbor discovery, Device-to-device communication, Wireless channel correlation"
961,신뢰성있는 딥러닝 기반 분석 모델을 참조하기 위한 딥러닝 기술 언어,문종혁 /Jong Hyeok Mun,2021,한국정보처리학회,"최근 딥러닝은 하드웨어 성능이 향상됨에 따라 자연어 처리, 영상 인식 등의 다양한 기술에 접목되어 활용되고 있다. 이러한 기술들을 활용해 지능형교통 시스템(ITS), 스마트홈, 헬스케어 등의 산업분야에서 데이터를 분석하여 고속도로 속도위반 차량 검출, 에너지 사용량 제어, 응급상황 등과 같은고품질의 서비스를 제공하며, 고품질의 서비스를 제공하기 위해서는 정확도가 향상된 딥러닝 모델이 적용되어야 한다. 이를 위해 서비스 환경의 데이터를분석하기 위한 딥러닝 모델을 개발할 때, 개발자는 신뢰성이 검증된 최신의 딥러닝 모델을 적용할 수 있어야 한다. 이는 개발자가 참조하는 딥러닝 모델에적용된 학습 데이터셋의 정확도를 측정하여 검증할 수 있다. 이러한 검증을 위해서 개발자는 학습 데이터셋, 딥러닝의 계층구조 및 개발 환경 등과 같은내용을 포함하는 딥러닝 모델을 문서화하여 적용하기 위한 구조적인 정보가 필요하다. 본 논문에서는 신뢰성있는 딥러닝 기반 데이터 분석 모델을 참조하기위한 딥러닝 기술 언어를 제안한다. 제안하는 기술 언어는 신뢰성 있는 딥러닝 모델을 개발하는데 필요한 학습데이터셋, 개발 환경 및 설정 등의 정보와더불어 딥러닝 모델의 계층구조를 표현할 수 있다. 제안하는 딥러닝 기술 언어를 이용하여 개발자는 지능형 교통 시스템에서 참조하는 분석 모델의정확도를 검증할 수 있다. 실험에서는 제안하는 언어의 유효성을 검증하기 위해, 번호판 인식 모델을 중심으로 딥러닝 기술 문서의 적용과정을 보인다With the recent advancements of deep learning, companies such as smart home, healthcare, and intelligent transportation systems areutilizing its functionality to provide high-quality services for vehicle detection, emergency situation detection, and controlling energyconsumption. To provide reliable services in such sensitive systems, deep learning models are required to have high accuracy. In orderto develop a deep learning model for analyzing previously mentioned services, developers should utilize the state of the art deep learningmodels that have already been verified for higher accuracy. The developers can verify the accuracy of the referenced model by validatingthe model on the dataset. For this validation, the developer needs structural information to document and apply deep learning models,including metadata such as learning dataset, network architecture, and development environments. In this paper, we propose a descriptionlanguage that represents the network architecture of the deep learning model along with its metadata that are necessary to develop adeep learning model. Through the proposed description language, developers can easily verify the accuracy of the referenced deep learningmodel. Our experiments demonstrate the application scenario of a deep learning description document that focuses on the license platerecognition for the detection of illegally parked vehicles.Trusted Deep Learning, Model Reference, Deep Learning Description Language, Traffic Situation Analysis Model"
962,딥러닝 분석을 이용한 중국 역내·외 위안화 변동성 예측,이우식 /Lee Woosik,2016,한국데이터정보과학회,"2008년 글로벌 금융위기 이후 중국은 위안화 국제화의 점진적 추진을 진행하면서 중국상하이 외환시장과 중국홍콩 외환시장에서 거래되는 통화인 역내위안화와 역외위안화를 형성시켰다. 본 연구는 위안화 국제화와 점진적인 중국 자본계정 개방에 따라 급변하는 외환시장상황의 변동성을 정확하게 파악하기 위해서 GARCH모형 (일반화된 자기회귀 조건부이분산성모형)에 다단계인공신경망을 결합한 MLP-GARCH 모형과 GARCH모형과 기계학습의 일종인 딥러닝 (deep learning)을 통합한 DL-GARCH을 가지고 위안화 변동성예측을 비교 실험과 분석을 하였다. 비교분석 결과 DLGARCH 모형은 MLP-GARCH보다 모형 위안화 역내·외 환율변동성 예측 면에서 더욱 더 개선된예측값을 제공하였다. 그래서 이분산시계열모형을 딥러닝과 결합한 DL-GARCH 모형은 시계열의환율 변동성 예측 문제에 딥러닝을 응용할 수 있음을 확인하였다. 향후 이분산시계열과 결합된 딥러닝 모형은 다른 금융시계열 데이터에 응용하여 그 일반화 가능성을 높일 수 있을 것이다.The People’s Republic of China has vigorously been pursuing the internationalization of the Chinese Yuan or Renminbi after the financial crisis of 2008. In this view, an abrupt increase of use of the Chinese Yuan in the onshore and offshore markets are important milestones to be one of important currencies. One of the most frequently used methods to forecast volatility is GARCH model. Since a prediction error of the GARCH model has been reported quite high, a lot of efforts have been made to improve forecasting capability of the GARCH model. In this paper, we have proposed MLPGARCH and a DL-GARCH by employing Artificial Neural Network to the GARCH. In an application to forecasting Chinese Yuan volatility, we have successfully shown their overall outperformance in forecasting over the GARCH.Deep learning, DL-GARCH, GARCH, The Chinese Yuan, volatility."
963,딥러닝 기법을 이용한 내일강수 예측,하지훈,2016,한국지능시스템학회,"정확한 강수예측을 위해서는 예측인자 선정과 예측방법에 대한 선택이 매우 중요하다. 최근에는 강수예측 방법으로기계학습 기법이 많이 사용되고 있으며, 그 중에서도 특히 인공신경망을 사용한 강수예측 방법은 좋은 성능을 보였다. 본논문에서는 딥러닝 기법 중 하나인 DBN(deep belief network)를 이용한 새로운 강수예측 방법을 제안한다. DBN는 비지도사전 학습을 통해 초기 가중치를 설정하여 기존 인공신경망의 문제점을 보완한다. 예측인자로는 기온, 전일-전주 강수일, 태양과 달 궤도 관련 자료를 선정하였다. 기온과 전일-전주 강수일은 서울에서의 1974년부터 2013년까지 총 40년간의AWS(automatic weather system) 관측 자료를 사용하였고, 태양과 달의 궤도 관련 자료는 서울을 중심으로 계산한 결과를사용하였다. 전체 기간에서 일부는 학습 자료로 사용하여 예측모델을 생성하였고, 나머지를 생성한 모델의 검증 자료로사용하였다. 모델 검증 결과로 나온 예측값들은 확률값을 가지며 임계치를 이용하여 강수유무를 판별하였다. 강수 정확도의척도로 양분예보기법 중 CSI(critical successive index)와 Bias(frequency bias)를 계산하였다. 이를 통해 DBN와 MLP(multilayer perceptron)의 성능을 비교한 결과 DBN의 강수 예측 정확도가 높았고, 수행속도 또한 2배 이상 빨랐다.For accurate precipitation forecasts the choice of weather faoctrs and prediction method is very important. Recently, machinele arning has been widely used for forecasting precipitation, and artificial neural network, one of machine learning techniques, showed good performance. In this paper, we suggest a new method for forecasting precipitation using DBN, one of deep learning techniques.
DBN has an advantage that initial weights are set by unsupervised learning, so this compensates for the defects of artificial neural networks. We used past precipitation, temperature, and the parameters of the sun and moon's motion as features for forecasting precipitation. The dataset consists of observation data which had been measured for 40 years from AWS in Seoul. Experiments were based on 8-fold cross validation. As a result of estimation, we got probabilities of test dataset, so threshold was used for the decision of precipitation. CSI and Bias were used for indicating the precision of precipitation. Our experimental results showed that DBN performed better than MLP.Deep learning, Deep belief network, Precipitation, Forecast"
964,딥러닝 신경망모형을 이용한 실시간 교통정보수집 연구,김대현 /KIM DAE HYON,2016,사단법인 인문사회과학기술융합학회,"비디오 기반의 실시간 교통정보수집 시스템은 자율주행을 포함한 지능형교통체계의 주요 서브시스템중 하나이며, 우수한 예측(추정)력 때문에 핵심 알고리즘으로 인공신경망 모형이 주로 사용되고 있다. 특히, 최근 과학 학술지 네이처에 구글 딥마인드의 알파고가 공개되면서 딥러닝과 인공지능에 대한 전 세계적인 관심이 집중되고 있다. 최근까지 가장 널리 사용된 인공신경망모형은 역전파 모형이었으나 최근 컴퓨터 하드웨어 및 알고리즘의 새로운 개발과 더불어 딥러닝 알고리즘을 이용한 딥인공신경망 모형에 대한 관심이 높아지고 있다. 그러나 아직 딥러닝 인공신경망에 대한 연구는 초기단계에 있는 실정이며 적용분야도 극히 한정되어 있다. 본 연구에서는 실시간 도로교통정보 수집을 위한 영상처리분야에서 딥러닝 알고리즘을 활용하고 실험결과를 통하여 향후 교통정보수집‧분석 분야에서의 딥러닝 인공신경망 모형의 적용가능성을 살펴보고자 한다. 특히, 본 연구에서 도출한 딥러닝 인공신경망모형의 실험결과를 기존 역전파 모형에 의한 실험결과와 비교하여 모형의 우수성을 비교‧평가하고자 한다.Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Normally, artificial neural networks have been used in real-time applications for collecting traffic information because of their remarkable performance in terms of predictive accuracy and real-time computing. Especially, the recent publication of a paper in the journal Nature and the success of Google DeepMind’s world-beating Go program AlphaGo caused significant increases of the public interest in artificial intelligence. Even though various types of learning models have been proposed, the Backpropagation has been the most popular neural network model. However, the Deep learning neural network model has attracted much interest recently. In this study, the Deep leaning neural network will be applied in image processing techniques for the real-time traffic information collection to investigate the performance of Deep learning algorithm in traffic engineering fields. In addition, the experimental results from Deep learing algorithm will be compared with the results from Backpropagation model to evaluate and propose better performing model.Intelligent Transportation Systems (ITS), artificial neural networks, Deep learning, traffic information, image processing"
965,딥러닝의 모형과 응용사례,안성만 /SungMahn Ahn,2016,한국지능정보시스템학회,"딥러닝은 인공신경망(neural network)이라는 인공지능분야의 모형이 발전된 형태로서, 계층구조로 이루어진 인공신경망의 내부계층(hidden layer)이 여러 단계로 이루어진 구조이다. 딥러닝에서의 주요 모형은 합성곱신경망(convolutional neural network), 순환신경망(recurrent neural network), 그리고 심층신뢰신경망(deep belief network)의 세가지라고 할 수있다. 그 중에서 현재 흥미로운 연구가 많이 발표되어서 관심이 집중되고 있는 모형은 지도학습(supervised learning)모형인 처음 두 개의 모형이다. 따라서 본 논문에서는 지도학습모형의 가중치를 최적화하는 기본적인 방법인 오류역전파 알고리즘을 살펴본 뒤에 합성곱신경망과 순환신경망의 구조와 응용사례 등을 살펴보고자 한다. 본문에서 다루지 않은 모형인 심층신뢰신경망은 아직까지는 합성곱신경망 이나 순환신경망보다는 상대적으로 주목을 덜 받고 있다. 그러나 심층신뢰신경망은 CNN이나 RNN과는 달리 비지도학습(unsupervised learning)모형이며, 사람이나 동물은 관찰을 통해서 스스로 학습한다는 점에서 궁극적으로는 비지도학습모형이 더 많이 연구되어야 할 주제가 될 것이다.Deep learning model is a kind of neural networks that allows multiple hidden layers. There are various deep learning architectures such as convolutional neural networks, deep belief networks and recurrent neural networks. Those have been applied to fields like computer vision, automatic speech recognition, natural language processing, audio recognition and bioinformatics where they have been shown to produce state-of-the-art results on various tasks. Among those architectures, convolutional neural networks and recurrent neural networks are classified as the supervised learning model. And in recent years, those supervised learning models have gained more popularity than unsupervised learning models such as deep belief networks, because supervised learning models have shown fashionable applications in such fields mentioned above.
Deep learning models can be trained with backpropagation algorithm. Backpropagation is an abbreviation for “backward propagation of errors” and a common method of training artificial neural networks used in conjunction with an optimization method such as gradient descent. The method calculates the gradient of an error function with respect to all the weights in the network. The gradient is fed to the optimization method which in turn uses it to update the weights, in an attempt to minimize the error function.
Convolutional neural networks use a special architecture which is particularly well-adapted to classify images. Using this architecture makes convolutional networks fast to train. This, in turn, helps us train deep, muti-layer networks, which are very good at classifying images. These days, deep convolutional networks are used in most neural networks for image recognition. Convolutional neural networks use three basic ideas: local receptive fields, shared weights, and pooling. By local receptive fields, we mean that each neuron in the first(or any) hidden layer will be connected to a small region of the input(or previous layer’s) neurons. Shared weights mean that we’re going to use the same weights and bias for each of the local receptive field. This means that all the neurons in the hidden layer detect exactly the same feature, just at different locations in the input image. In addition to the convolutional layers just described, convolutional neural networks also contain pooling layers. Pooling layers are usually used immediately after convolutional layers. What the pooling layers do is to simplify the information in the output from the convolutional layer.
Recent convolutional network architectures have 10 to 20 hidden layers and billions of connections between units. Training deep learning networks has taken weeks several years ago, but thanks to progress in GPU and algorithm enhancement, training time has reduced to several hours.
Neural networks with time-varying behavior are known as recurrent neural networks or RNNs. A recurrent neural network is a class of artificial neural network where connections between units form a directed cycle. This creates an internal state of the network which allows it to exhibit dynamic temporal behavior. Unlike feedforward neural networks, RNNs can use their internal memory to process arbitrary sequences of inputs. Early RNN models turned out to be very difficult to train, harder even than deep feedforward networks. The reason is the unstable gradient problem such as vanishing gradient and exploding gradient. The gradient can get smaller and smaller as it is propagated back through layers. This makes learning in early layers extremely slow. The problem actually gets worse in RNNs, since gradients aren’t just propagated backward through layers, they’re propagated backward through time. If the network runs for a long time, that can make the gradient extremely unstable and hard to learn from. It has been possible to incorporate an idea known as long short-term memory units (LSTMs) into RNNs. LSTMs make it much easier to get good results when training RNNs, and many recent papers make use of LSTMs or related ideas.Deep learning, Convolutional neural networks, Recurrent neural networks, Error backpropagation algorithm"
966,딥러닝분석과 기술적 분석 지표를 이용한 한국 코스피주가지수 방향성 예측,이우식 /Lee Woosik,2017,한국데이터정보과학회,"2016년 3월 구글 (Google)의 바둑인공지능 알파고 (AlphaGo)가 이세돌 9단과의 바둑대결에서승리한 이후 다양한 분야에서 인공지능 사용에 대한 관심이 높아지고 있는 가운데 금융투자 분야에서도 인공지능과 투자자문 전문가의 합성어인 로보어드바이저 (Robo-Advisor)에 대한 관심이 높아지고 있다. 인공지능 (artificial intelligence)기반의 의사결정은 비용 절감은 물론 효과적인 의사결정을가능하게 한다는 점에서 큰 장점이 있다. 본 연구에서는 기술적 분석 (technical analysis) 지표와 딥러닝 (deep learning) 모형을 결합하여 한국 코스피 지수를 예측하는 모형을 개발하고 제시한 모형들의 예측력을 비교, 분석한다. 분석 결과 기술적 분석 지표에 딥러닝 알고리즘을 결합한 모형이 주가지수 방향성 예측 문제에 응용될 수 있음을 확인하였다. 향후 본 연구에서 제안된 기술적 분석 지표와딥러닝모형을 결합한 기법은 로보어드바이저서비스에 응용할 수 있는 일반화 가능성을 보여준다Since Google’s AlphaGo defeated a world champion of Go players in 2016, there have been many interests in the deep learning. In the financial sector, a Robo-Advisor using deep learning gains a significant attention, which builds and manages portfolios of financial instruments for investors.In this paper, we have proposed the a deep learning algorithm geared toward identification and forecast of the KOSPI index direction,and we also have compared the accuracy of the prediction.In an application of forecasting the financial market index direction, we have shown that the Robo-Advisor using deep learning has a significant effect on finance industry. The Robo-Advisor collects a massive data such as earnings statements, news reports and regulatory filings, analyzes those and recommends investors how to view market trends and identify the best time to purchase financial assets. On the other hand, the Robo-Advisor allows businesses to learn more about their customers, develop better marketing strategies, increase sales and decrease costs.Artificial intelligence, deep learning, FinTech, Robo-Advisor, technical analysis."
967,딥러닝의 효율적인 학습을 위한 학습데이터 선별 기법에 관한 연구,주경돈 /Gyoung-don Joo,2017,한국정보과학회,"빅데이터의 증가와 병렬 하드웨어 처리, 학습 알고리즘의 개선은 딥러닝의 발전을 가져왔으며 인공지능이 적용 가능한 영역을 획기적으로 확장시키고 있다. 하지만 이러한 대규모의 학습 데이터에 기반한 딥러닝은 학습시간이 증가되고, 더 나아가 학습 데이터의 구축을 위한 레이블링(labeling) 비용이 데이터 수에 비례하여 증가하게 된다. 따라서 본 논문에서는, 딥러닝의 정확도를 최대한 유지하면서 필요한 학습 데이터의 수를 줄일수 있도록, 전체 데이터에서 제한된 수의 학습 데이터의 후보를 선별하는 기법에 대하여 연구한다. 특히 기존의 능동 학습에 기반한 기법들이 학습 데이터의 수가 매우 적어질 경우 딥러닝의 정확도가 급격히 떨어지는 문제를 해결할 수 있는 새로운 기법을 제안하였으며, 제안된 기법으로 선별된 학습 데이터는 그 수의 변화에 대해 딥러닝이 일관되게 경쟁력있는 정확도를 가짐을 다양한 실험을 통하여 검증하였다.The increase of big data, parallel hardware processing and enhanced learning algorithms have led to the development of deep learning and are dramatically expanding the domain of applications based on artificial intelligence. However, such large-scale training data extends the learning time, and furthermore, the labeling cost for building the training set increases in proportion to the volume of data. Therefore, in this paper, we study how to select a limited number of candidates for training set from given data so as to reduce the number of training set while maintaining the accuracy of deep learning. Especially, we found that, with the training set selected by the previous work, the accuracy of deep learning drastically decreases when the number of training set is very small and so proposed new methods to resolve this fall-off in accuracy. Through experiments, we showed that deep learning consistently has the competitive accuracy with the training set selected by the proposed method regardless of the size of training set.Deep Learning, Train Set Selection, Active Learning, Gamma distribution"
968,딥러닝을 이용한 범용적 스테그아날리시스,김현재 /Hyunjae Kim,2017,한국정보과학회,"스테그아날리시스(Steganalysis)란 이미지 등 일반적인 자료에 암호화된 정보를 은닉하는 스테가노그래피(Steganography)에 대한 검출 및 분석 방법으로, 기계학습 기반 방법론을 포함한다. 기존 기계학습 기반 스테그아날리시스는 영상(Image)의 특징(Feature) 추출 및 모델링에 기반하며, 최근 딥러닝(Deep Learning)의 적용으로 검출 정확도가 큰 폭으로 향상되었다. 하지만 현존하는 스테그아날리시스 모델은 단일 스테가노그래피 기법에 대해 국한되어 있어 학습에 사용되지 않은 스테고(Stego) 이미지의 경우 검출이 불가능한 결정적 한계를 가진다. 본 연구에서는 다양한 스테가노그래피 기법으로 생성된 스테고이미지에 딥러닝을 적용하여 스테그아날리시스를 학습하는 범용적 모델을 제안한다. 다양한 실험을 통해 제안 기법의 효용성 및 가능성을 확인하고, 범용적 스테그아날리시스 모델이 각각에 특화된 검출 기법과 유사한 정확도로 스테고 이미지를 검출할 수 있음을 보인다.Steganalysis is to detect information hidden by steganography inside general data such as images. There are stegoanalysis techniques that use machine learning (ML). Existing ML approaches to steganalysis are based on extracting features from stego images and modeling them.
Recently deep learning-based methodologies have shown significant improvements in detection accuracy. However, all the existing methods, including deep learning-based ones, have a critical limitation in that they can only detect stego images that are created by a specific steganography method. In this paper, we propose a generalized steganalysis method that can model multiple types of stego images using deep learning. Through various experiments, we confirm the effectiveness of our approach and envision directions for future research. In particular, we show that our method can detect each type of steganography with the same level of accuracy as that of a steganalysis method dedicated to that type of steganography, thereby demonstrating the general applicability of our approach to multiple types of stego images.steganalysis, steganography, deep learning, generalized steganalysis model"
969,"딥러닝 프레임워크의 비교: 티아노, 텐서플로, CNTK를 중심으로",정여진 /Yeojin Chung,2017,한국지능정보시스템학회,"딥러닝 프레임워크의 대표적인 기능으로는 ‘자동미분’과 ‘GPU의 활용’ 등을 들 수 있다. 본 논문은 파이썬의 라이브러리 형태로 사용 가능한 프레임워크 중에서 구글의 텐서플로와 마이크로소프트의 CNTK, 그리고 텐서 플로의 원조라고 할 수 있는 티아노를 비교하였다. 본문에서는 자동미분의 개념과 GPU의 활용형태를 간단히 설명하고, 그 다음에 logistic regression을 실행하는 예를 통하여 각 프레임워크의 문법을 알아본 뒤에, 마지막으로 대표적인 딥러닝 응용인 CNN의 예제를 실행시켜보고 코딩의 편의성과 실행속도 등을 확인해 보았다. 그 결과, 편의성의 관점에서 보면 티아노가 가장 코딩 하기가 어렵고, CNTK와 텐서플로는 많은 부분이 비슷하게 추상화 되어 있어서 코딩이 비슷하지만 가중치와 편향을 직접 정의하느냐의 여부에서 차이를 보였다. 그리고 각프레임워크의 실행속도에 대한 평가는 ‘큰 차이는 없다’는 것이다. 텐서플로는 티아노에 비하여 속도가 느리다는 평가가 있어왔는데, 본 연구의 실험에 의하면, 비록 CNN 모형에 국한되었지만, 텐서플로가 아주 조금이지만빠른 것으로 나타났다. CNTK의 경우에도, 비록 실험환경이 달랐지만, 실험환경의 차이에 의한 속도의 차이의 편차범위 이내에 있는 것으로 판단이 되었다. 본 연구에서는 세 종류의 딥러닝 프레임워크만을 살펴보았는데,위키피디아에 따르면 딥러닝 프레임워크의 종류는 12가지가 있으며, 각 프레임워크의 특징을 15가지 속성으로구분하여 차이를 특정하고 있다. 그 많은 속성 중에서 사용자의 입장에서 볼 때 중요한 속성은 어떤 언어(파이썬, C++, Java, 등)로 사용가능한지, 어떤 딥러닝 모형에 대한 라이브러리가 잘 구현되어 있는지 등일 것이다. 그리고 사용자가 대규모의 딥러닝 모형을 구축한다면, 다중 GPU 혹은 다중 서버를 지원하는지의 여부도 중요할 것이다. 또한 딥러닝 모형을 처음 학습하는 경우에는 사용설명서가 많은지 예제 프로그램이 많은지 여부도 중요한 기준이 될 것이다.The deep learning framework is software designed to help develop deep learning models. Some of its important functions include “automatic differentiation” and “utilization of GPU”. The list of popular deep learning framework includes Caffe (BVLC) and Theano (University of Montreal). And recently, Microsoft's deep learning framework, Microsoft Cognitive Toolkit, was released as open-source license, following Google’s Tensorflow a year earlier. The early deep learning frameworks have been developed mainly for research at universities. Beginning with the inception of Tensorflow, however, it seems that companies such as Microsoft and Facebook have started to join the competition of framework development. Given the trend, Google and other companies are expected to continue investing in the deep learning framework to bring forward the initiative in the artificial intelligence business. From this point of view, we think it is a good time to compare some of deep learning frameworks. So we compare three deep learning frameworks which can be used as a Python library. Those are Google's Tensorflow, Microsoft’s CNTK, and Theano which is sort of a predecessor of the preceding two.
The most common and important function of deep learning frameworks is the ability to perform automatic differentiation. Basically all the mathematical expressions of deep learning models can be represented as computational graphs, which consist of nodes and edges. Partial derivatives on each edge of a computational graph can then be obtained. With the partial derivatives, we can let software compute differentiation of any node with respect to any variable by utilizing chain rule of Calculus.
First of all, the convenience of coding is in the order of CNTK, Tensorflow, and Theano. The criterion is simply based on the lengths of the codes and the learning curve and the ease of coding are not the main concern. According to the criteria, Theano was the most difficult to implement with, and CNTK and Tensorflow were somewhat easier. With Tensorflow, we need to define weight variables and biases explicitly. The reason that CNTK and Tensorflow are easier to implement with is that those frameworks provide us with more abstraction than Theano. We, however, need to mention that low-level coding is not always bad. It gives us flexibility of coding. With the low-level coding such as in Theano, we can implement and test any new deep learning models or any new search methods that we can think of.
The assessment of the execution speed of each framework is that there is not meaningful difference. According to the experiment, execution speeds of Theano and Tensorflow are very similar, although the experiment was limited to a CNN model. In the case of CNTK, the experimental environment was not maintained as the same. The code written in CNTK has to be run in PC environment without GPU where codes execute as much as 50 times slower than with GPU. But we concluded that the difference of execution speed was within the range of variation caused by the different hardware setup.
In this study, we compared three types of deep learning framework: Theano, Tensorflow, and CNTK. According to Wikipedia, there are 12 available deep learning frameworks. And 15 different attributes differentiate each framework. Some of the important attributes would include interface language (Python, C ++, Java, etc.) and the availability of libraries on various deep learning models such as CNN, RNN, DBN, and etc. And if a user implements a large scale deep learning model, it will also be important to support multiple GPU or multiple servers. Also, if you are learning the deep learning model, it would also be important if there are enough examples and references.deep learning framework, Theano, TensorFlow, CNTK, computational graph, CIFAR-10"
970,딥러닝을 이용한 주변 무선단말 파악방안,이웅섭 /Lee， Woong-sup,2018,한국정보통신학회,"최근 단말-대-단말(Device-to-device, D2D) 통신기술이 차세대 무선통신시스템의 핵심기술로 큰 관심을 받고 있다. 이러한 단말간 통신에서는 자신의 주변에 어떠한 단말이 있는지 파악하는 주변단말 탐색(Neighbor discovery)이매우 중요하다. 본 논문에서는 최근 큰 관심을 받고 있는 딥러닝(Deep learning) 기술을 활용하여 단말간 통신에서 주변단말을 파악하는 방안에 대해서 제안한다. 제안 방안은 기존의 방안과 달리 무선채널의 공간적 연관성을 이용하여단말간의 신호 전송 없이 단말이 기지국으로 전송하는 상향링크 파일럿 신호를 기반으로 주변 단말을 찾고 따라서기존의 방식에 비해 신호전송 복잡도(signaling complexity)를 크게 줄일 수 있다. 또한 제안 방안에서는 떨어져 있는거리에 따라서 주변 단말을 분류 가능하여 기존 방안에 비해서 좀 더 세밀한 단말 탐색이 가능하다. 마지막으로 본 논문에서는 tensorflow를 이용한 컴퓨터 시뮬레이션을 통해 제안 방안의 성능을 검증하였다.Recently, the device-to-device (D2D) communication has been conceived as the key technology for the next-generationmobile communication systems. The neighbor discovery in which the nearby users are found, is essential for the properoperation of the D2D communication. In this paper, we propose new neighbor discovery scheme based on deep learningtechnology which has gained a lot of attention recently. In the proposed scheme, the neighboring users can be foundusing the uplink pilot transmission of users only, unlike conventional neighbor discovery schemes in which direct pilotcommunication among users is required, such that the signaling overhead can be greatly reduced in our proposed scheme.
Moreover, the neighbors with different proximity can also be classified accordingly which enables more accurate neighbordiscovery compared to the conventional schemes. The performance of our proposed scheme is verified through thetensorflow-based computer simulations.
키워드 : 딥러닝, 주변단말 탐색, 단말-대-단말 통신Deep learning, Neighbor discovery, Device-to-device communication, Wireless channel correlation"
971,신뢰성있는 딥러닝 기반 분석 모델을 참조하기 위한 딥러닝 기술 언어,문종혁 /Jong Hyeok Mun,2021,한국정보처리학회,"최근 딥러닝은 하드웨어 성능이 향상됨에 따라 자연어 처리, 영상 인식 등의 다양한 기술에 접목되어 활용되고 있다. 이러한 기술들을 활용해 지능형교통 시스템(ITS), 스마트홈, 헬스케어 등의 산업분야에서 데이터를 분석하여 고속도로 속도위반 차량 검출, 에너지 사용량 제어, 응급상황 등과 같은고품질의 서비스를 제공하며, 고품질의 서비스를 제공하기 위해서는 정확도가 향상된 딥러닝 모델이 적용되어야 한다. 이를 위해 서비스 환경의 데이터를분석하기 위한 딥러닝 모델을 개발할 때, 개발자는 신뢰성이 검증된 최신의 딥러닝 모델을 적용할 수 있어야 한다. 이는 개발자가 참조하는 딥러닝 모델에적용된 학습 데이터셋의 정확도를 측정하여 검증할 수 있다. 이러한 검증을 위해서 개발자는 학습 데이터셋, 딥러닝의 계층구조 및 개발 환경 등과 같은내용을 포함하는 딥러닝 모델을 문서화하여 적용하기 위한 구조적인 정보가 필요하다. 본 논문에서는 신뢰성있는 딥러닝 기반 데이터 분석 모델을 참조하기위한 딥러닝 기술 언어를 제안한다. 제안하는 기술 언어는 신뢰성 있는 딥러닝 모델을 개발하는데 필요한 학습데이터셋, 개발 환경 및 설정 등의 정보와더불어 딥러닝 모델의 계층구조를 표현할 수 있다. 제안하는 딥러닝 기술 언어를 이용하여 개발자는 지능형 교통 시스템에서 참조하는 분석 모델의정확도를 검증할 수 있다. 실험에서는 제안하는 언어의 유효성을 검증하기 위해, 번호판 인식 모델을 중심으로 딥러닝 기술 문서의 적용과정을 보인다With the recent advancements of deep learning, companies such as smart home, healthcare, and intelligent transportation systems areutilizing its functionality to provide high-quality services for vehicle detection, emergency situation detection, and controlling energyconsumption. To provide reliable services in such sensitive systems, deep learning models are required to have high accuracy. In orderto develop a deep learning model for analyzing previously mentioned services, developers should utilize the state of the art deep learningmodels that have already been verified for higher accuracy. The developers can verify the accuracy of the referenced model by validatingthe model on the dataset. For this validation, the developer needs structural information to document and apply deep learning models,including metadata such as learning dataset, network architecture, and development environments. In this paper, we propose a descriptionlanguage that represents the network architecture of the deep learning model along with its metadata that are necessary to develop adeep learning model. Through the proposed description language, developers can easily verify the accuracy of the referenced deep learningmodel. Our experiments demonstrate the application scenario of a deep learning description document that focuses on the license platerecognition for the detection of illegally parked vehicles.Trusted Deep Learning, Model Reference, Deep Learning Description Language, Traffic Situation Analysis Model"
972,딥러닝 분석을 이용한 중국 역내·외 위안화 변동성 예측,이우식 /Lee Woosik,2016,한국데이터정보과학회,"2008년 글로벌 금융위기 이후 중국은 위안화 국제화의 점진적 추진을 진행하면서 중국상하이 외환시장과 중국홍콩 외환시장에서 거래되는 통화인 역내위안화와 역외위안화를 형성시켰다. 본 연구는 위안화 국제화와 점진적인 중국 자본계정 개방에 따라 급변하는 외환시장상황의 변동성을 정확하게 파악하기 위해서 GARCH모형 (일반화된 자기회귀 조건부이분산성모형)에 다단계인공신경망을 결합한 MLP-GARCH 모형과 GARCH모형과 기계학습의 일종인 딥러닝 (deep learning)을 통합한 DL-GARCH을 가지고 위안화 변동성예측을 비교 실험과 분석을 하였다. 비교분석 결과 DLGARCH 모형은 MLP-GARCH보다 모형 위안화 역내·외 환율변동성 예측 면에서 더욱 더 개선된예측값을 제공하였다. 그래서 이분산시계열모형을 딥러닝과 결합한 DL-GARCH 모형은 시계열의환율 변동성 예측 문제에 딥러닝을 응용할 수 있음을 확인하였다. 향후 이분산시계열과 결합된 딥러닝 모형은 다른 금융시계열 데이터에 응용하여 그 일반화 가능성을 높일 수 있을 것이다.The People’s Republic of China has vigorously been pursuing the internationalization of the Chinese Yuan or Renminbi after the financial crisis of 2008. In this view, an abrupt increase of use of the Chinese Yuan in the onshore and offshore markets are important milestones to be one of important currencies. One of the most frequently used methods to forecast volatility is GARCH model. Since a prediction error of the GARCH model has been reported quite high, a lot of efforts have been made to improve forecasting capability of the GARCH model. In this paper, we have proposed MLPGARCH and a DL-GARCH by employing Artificial Neural Network to the GARCH. In an application to forecasting Chinese Yuan volatility, we have successfully shown their overall outperformance in forecasting over the GARCH.Deep learning, DL-GARCH, GARCH, The Chinese Yuan, volatility."
973,딥러닝 기법을 이용한 내일강수 예측,하지훈,2016,한국지능시스템학회,"정확한 강수예측을 위해서는 예측인자 선정과 예측방법에 대한 선택이 매우 중요하다. 최근에는 강수예측 방법으로기계학습 기법이 많이 사용되고 있으며, 그 중에서도 특히 인공신경망을 사용한 강수예측 방법은 좋은 성능을 보였다. 본논문에서는 딥러닝 기법 중 하나인 DBN(deep belief network)를 이용한 새로운 강수예측 방법을 제안한다. DBN는 비지도사전 학습을 통해 초기 가중치를 설정하여 기존 인공신경망의 문제점을 보완한다. 예측인자로는 기온, 전일-전주 강수일, 태양과 달 궤도 관련 자료를 선정하였다. 기온과 전일-전주 강수일은 서울에서의 1974년부터 2013년까지 총 40년간의AWS(automatic weather system) 관측 자료를 사용하였고, 태양과 달의 궤도 관련 자료는 서울을 중심으로 계산한 결과를사용하였다. 전체 기간에서 일부는 학습 자료로 사용하여 예측모델을 생성하였고, 나머지를 생성한 모델의 검증 자료로사용하였다. 모델 검증 결과로 나온 예측값들은 확률값을 가지며 임계치를 이용하여 강수유무를 판별하였다. 강수 정확도의척도로 양분예보기법 중 CSI(critical successive index)와 Bias(frequency bias)를 계산하였다. 이를 통해 DBN와 MLP(multilayer perceptron)의 성능을 비교한 결과 DBN의 강수 예측 정확도가 높았고, 수행속도 또한 2배 이상 빨랐다.For accurate precipitation forecasts the choice of weather faoctrs and prediction method is very important. Recently, machinele arning has been widely used for forecasting precipitation, and artificial neural network, one of machine learning techniques, showed good performance. In this paper, we suggest a new method for forecasting precipitation using DBN, one of deep learning techniques.
DBN has an advantage that initial weights are set by unsupervised learning, so this compensates for the defects of artificial neural networks. We used past precipitation, temperature, and the parameters of the sun and moon's motion as features for forecasting precipitation. The dataset consists of observation data which had been measured for 40 years from AWS in Seoul. Experiments were based on 8-fold cross validation. As a result of estimation, we got probabilities of test dataset, so threshold was used for the decision of precipitation. CSI and Bias were used for indicating the precision of precipitation. Our experimental results showed that DBN performed better than MLP.Deep learning, Deep belief network, Precipitation, Forecast"
974,딥러닝 신경망모형을 이용한 실시간 교통정보수집 연구,김대현 /KIM DAE HYON,2016,사단법인 인문사회과학기술융합학회,"비디오 기반의 실시간 교통정보수집 시스템은 자율주행을 포함한 지능형교통체계의 주요 서브시스템중 하나이며, 우수한 예측(추정)력 때문에 핵심 알고리즘으로 인공신경망 모형이 주로 사용되고 있다. 특히, 최근 과학 학술지 네이처에 구글 딥마인드의 알파고가 공개되면서 딥러닝과 인공지능에 대한 전 세계적인 관심이 집중되고 있다. 최근까지 가장 널리 사용된 인공신경망모형은 역전파 모형이었으나 최근 컴퓨터 하드웨어 및 알고리즘의 새로운 개발과 더불어 딥러닝 알고리즘을 이용한 딥인공신경망 모형에 대한 관심이 높아지고 있다. 그러나 아직 딥러닝 인공신경망에 대한 연구는 초기단계에 있는 실정이며 적용분야도 극히 한정되어 있다. 본 연구에서는 실시간 도로교통정보 수집을 위한 영상처리분야에서 딥러닝 알고리즘을 활용하고 실험결과를 통하여 향후 교통정보수집‧분석 분야에서의 딥러닝 인공신경망 모형의 적용가능성을 살펴보고자 한다. 특히, 본 연구에서 도출한 딥러닝 인공신경망모형의 실험결과를 기존 역전파 모형에 의한 실험결과와 비교하여 모형의 우수성을 비교‧평가하고자 한다.Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Normally, artificial neural networks have been used in real-time applications for collecting traffic information because of their remarkable performance in terms of predictive accuracy and real-time computing. Especially, the recent publication of a paper in the journal Nature and the success of Google DeepMind’s world-beating Go program AlphaGo caused significant increases of the public interest in artificial intelligence. Even though various types of learning models have been proposed, the Backpropagation has been the most popular neural network model. However, the Deep learning neural network model has attracted much interest recently. In this study, the Deep leaning neural network will be applied in image processing techniques for the real-time traffic information collection to investigate the performance of Deep learning algorithm in traffic engineering fields. In addition, the experimental results from Deep learing algorithm will be compared with the results from Backpropagation model to evaluate and propose better performing model.Intelligent Transportation Systems (ITS), artificial neural networks, Deep learning, traffic information, image processing"
975,딥러닝의 모형과 응용사례,안성만 /SungMahn Ahn,2016,한국지능정보시스템학회,"딥러닝은 인공신경망(neural network)이라는 인공지능분야의 모형이 발전된 형태로서, 계층구조로 이루어진 인공신경망의 내부계층(hidden layer)이 여러 단계로 이루어진 구조이다. 딥러닝에서의 주요 모형은 합성곱신경망(convolutional neural network), 순환신경망(recurrent neural network), 그리고 심층신뢰신경망(deep belief network)의 세가지라고 할 수있다. 그 중에서 현재 흥미로운 연구가 많이 발표되어서 관심이 집중되고 있는 모형은 지도학습(supervised learning)모형인 처음 두 개의 모형이다. 따라서 본 논문에서는 지도학습모형의 가중치를 최적화하는 기본적인 방법인 오류역전파 알고리즘을 살펴본 뒤에 합성곱신경망과 순환신경망의 구조와 응용사례 등을 살펴보고자 한다. 본문에서 다루지 않은 모형인 심층신뢰신경망은 아직까지는 합성곱신경망 이나 순환신경망보다는 상대적으로 주목을 덜 받고 있다. 그러나 심층신뢰신경망은 CNN이나 RNN과는 달리 비지도학습(unsupervised learning)모형이며, 사람이나 동물은 관찰을 통해서 스스로 학습한다는 점에서 궁극적으로는 비지도학습모형이 더 많이 연구되어야 할 주제가 될 것이다.Deep learning model is a kind of neural networks that allows multiple hidden layers. There are various deep learning architectures such as convolutional neural networks, deep belief networks and recurrent neural networks. Those have been applied to fields like computer vision, automatic speech recognition, natural language processing, audio recognition and bioinformatics where they have been shown to produce state-of-the-art results on various tasks. Among those architectures, convolutional neural networks and recurrent neural networks are classified as the supervised learning model. And in recent years, those supervised learning models have gained more popularity than unsupervised learning models such as deep belief networks, because supervised learning models have shown fashionable applications in such fields mentioned above.
Deep learning models can be trained with backpropagation algorithm. Backpropagation is an abbreviation for “backward propagation of errors” and a common method of training artificial neural networks used in conjunction with an optimization method such as gradient descent. The method calculates the gradient of an error function with respect to all the weights in the network. The gradient is fed to the optimization method which in turn uses it to update the weights, in an attempt to minimize the error function.
Convolutional neural networks use a special architecture which is particularly well-adapted to classify images. Using this architecture makes convolutional networks fast to train. This, in turn, helps us train deep, muti-layer networks, which are very good at classifying images. These days, deep convolutional networks are used in most neural networks for image recognition. Convolutional neural networks use three basic ideas: local receptive fields, shared weights, and pooling. By local receptive fields, we mean that each neuron in the first(or any) hidden layer will be connected to a small region of the input(or previous layer’s) neurons. Shared weights mean that we’re going to use the same weights and bias for each of the local receptive field. This means that all the neurons in the hidden layer detect exactly the same feature, just at different locations in the input image. In addition to the convolutional layers just described, convolutional neural networks also contain pooling layers. Pooling layers are usually used immediately after convolutional layers. What the pooling layers do is to simplify the information in the output from the convolutional layer.
Recent convolutional network architectures have 10 to 20 hidden layers and billions of connections between units. Training deep learning networks has taken weeks several years ago, but thanks to progress in GPU and algorithm enhancement, training time has reduced to several hours.
Neural networks with time-varying behavior are known as recurrent neural networks or RNNs. A recurrent neural network is a class of artificial neural network where connections between units form a directed cycle. This creates an internal state of the network which allows it to exhibit dynamic temporal behavior. Unlike feedforward neural networks, RNNs can use their internal memory to process arbitrary sequences of inputs. Early RNN models turned out to be very difficult to train, harder even than deep feedforward networks. The reason is the unstable gradient problem such as vanishing gradient and exploding gradient. The gradient can get smaller and smaller as it is propagated back through layers. This makes learning in early layers extremely slow. The problem actually gets worse in RNNs, since gradients aren’t just propagated backward through layers, they’re propagated backward through time. If the network runs for a long time, that can make the gradient extremely unstable and hard to learn from. It has been possible to incorporate an idea known as long short-term memory units (LSTMs) into RNNs. LSTMs make it much easier to get good results when training RNNs, and many recent papers make use of LSTMs or related ideas.Deep learning, Convolutional neural networks, Recurrent neural networks, Error backpropagation algorithm"
976,딥러닝분석과 기술적 분석 지표를 이용한 한국 코스피주가지수 방향성 예측,이우식 /Lee Woosik,2017,한국데이터정보과학회,"2016년 3월 구글 (Google)의 바둑인공지능 알파고 (AlphaGo)가 이세돌 9단과의 바둑대결에서승리한 이후 다양한 분야에서 인공지능 사용에 대한 관심이 높아지고 있는 가운데 금융투자 분야에서도 인공지능과 투자자문 전문가의 합성어인 로보어드바이저 (Robo-Advisor)에 대한 관심이 높아지고 있다. 인공지능 (artificial intelligence)기반의 의사결정은 비용 절감은 물론 효과적인 의사결정을가능하게 한다는 점에서 큰 장점이 있다. 본 연구에서는 기술적 분석 (technical analysis) 지표와 딥러닝 (deep learning) 모형을 결합하여 한국 코스피 지수를 예측하는 모형을 개발하고 제시한 모형들의 예측력을 비교, 분석한다. 분석 결과 기술적 분석 지표에 딥러닝 알고리즘을 결합한 모형이 주가지수 방향성 예측 문제에 응용될 수 있음을 확인하였다. 향후 본 연구에서 제안된 기술적 분석 지표와딥러닝모형을 결합한 기법은 로보어드바이저서비스에 응용할 수 있는 일반화 가능성을 보여준다Since Google’s AlphaGo defeated a world champion of Go players in 2016, there have been many interests in the deep learning. In the financial sector, a Robo-Advisor using deep learning gains a significant attention, which builds and manages portfolios of financial instruments for investors.In this paper, we have proposed the a deep learning algorithm geared toward identification and forecast of the KOSPI index direction,and we also have compared the accuracy of the prediction.In an application of forecasting the financial market index direction, we have shown that the Robo-Advisor using deep learning has a significant effect on finance industry. The Robo-Advisor collects a massive data such as earnings statements, news reports and regulatory filings, analyzes those and recommends investors how to view market trends and identify the best time to purchase financial assets. On the other hand, the Robo-Advisor allows businesses to learn more about their customers, develop better marketing strategies, increase sales and decrease costs.Artificial intelligence, deep learning, FinTech, Robo-Advisor, technical analysis."
977,딥러닝의 효율적인 학습을 위한 학습데이터 선별 기법에 관한 연구,주경돈 /Gyoung-don Joo,2017,한국정보과학회,"빅데이터의 증가와 병렬 하드웨어 처리, 학습 알고리즘의 개선은 딥러닝의 발전을 가져왔으며 인공지능이 적용 가능한 영역을 획기적으로 확장시키고 있다. 하지만 이러한 대규모의 학습 데이터에 기반한 딥러닝은 학습시간이 증가되고, 더 나아가 학습 데이터의 구축을 위한 레이블링(labeling) 비용이 데이터 수에 비례하여 증가하게 된다. 따라서 본 논문에서는, 딥러닝의 정확도를 최대한 유지하면서 필요한 학습 데이터의 수를 줄일수 있도록, 전체 데이터에서 제한된 수의 학습 데이터의 후보를 선별하는 기법에 대하여 연구한다. 특히 기존의 능동 학습에 기반한 기법들이 학습 데이터의 수가 매우 적어질 경우 딥러닝의 정확도가 급격히 떨어지는 문제를 해결할 수 있는 새로운 기법을 제안하였으며, 제안된 기법으로 선별된 학습 데이터는 그 수의 변화에 대해 딥러닝이 일관되게 경쟁력있는 정확도를 가짐을 다양한 실험을 통하여 검증하였다.The increase of big data, parallel hardware processing and enhanced learning algorithms have led to the development of deep learning and are dramatically expanding the domain of applications based on artificial intelligence. However, such large-scale training data extends the learning time, and furthermore, the labeling cost for building the training set increases in proportion to the volume of data. Therefore, in this paper, we study how to select a limited number of candidates for training set from given data so as to reduce the number of training set while maintaining the accuracy of deep learning. Especially, we found that, with the training set selected by the previous work, the accuracy of deep learning drastically decreases when the number of training set is very small and so proposed new methods to resolve this fall-off in accuracy. Through experiments, we showed that deep learning consistently has the competitive accuracy with the training set selected by the proposed method regardless of the size of training set.Deep Learning, Train Set Selection, Active Learning, Gamma distribution"
978,딥러닝을 이용한 범용적 스테그아날리시스,김현재 /Hyunjae Kim,2017,한국정보과학회,"스테그아날리시스(Steganalysis)란 이미지 등 일반적인 자료에 암호화된 정보를 은닉하는 스테가노그래피(Steganography)에 대한 검출 및 분석 방법으로, 기계학습 기반 방법론을 포함한다. 기존 기계학습 기반 스테그아날리시스는 영상(Image)의 특징(Feature) 추출 및 모델링에 기반하며, 최근 딥러닝(Deep Learning)의 적용으로 검출 정확도가 큰 폭으로 향상되었다. 하지만 현존하는 스테그아날리시스 모델은 단일 스테가노그래피 기법에 대해 국한되어 있어 학습에 사용되지 않은 스테고(Stego) 이미지의 경우 검출이 불가능한 결정적 한계를 가진다. 본 연구에서는 다양한 스테가노그래피 기법으로 생성된 스테고이미지에 딥러닝을 적용하여 스테그아날리시스를 학습하는 범용적 모델을 제안한다. 다양한 실험을 통해 제안 기법의 효용성 및 가능성을 확인하고, 범용적 스테그아날리시스 모델이 각각에 특화된 검출 기법과 유사한 정확도로 스테고 이미지를 검출할 수 있음을 보인다.Steganalysis is to detect information hidden by steganography inside general data such as images. There are stegoanalysis techniques that use machine learning (ML). Existing ML approaches to steganalysis are based on extracting features from stego images and modeling them.
Recently deep learning-based methodologies have shown significant improvements in detection accuracy. However, all the existing methods, including deep learning-based ones, have a critical limitation in that they can only detect stego images that are created by a specific steganography method. In this paper, we propose a generalized steganalysis method that can model multiple types of stego images using deep learning. Through various experiments, we confirm the effectiveness of our approach and envision directions for future research. In particular, we show that our method can detect each type of steganography with the same level of accuracy as that of a steganalysis method dedicated to that type of steganography, thereby demonstrating the general applicability of our approach to multiple types of stego images.steganalysis, steganography, deep learning, generalized steganalysis model"
979,"딥러닝 프레임워크의 비교: 티아노, 텐서플로, CNTK를 중심으로",정여진 /Yeojin Chung,2017,한국지능정보시스템학회,"딥러닝 프레임워크의 대표적인 기능으로는 ‘자동미분’과 ‘GPU의 활용’ 등을 들 수 있다. 본 논문은 파이썬의 라이브러리 형태로 사용 가능한 프레임워크 중에서 구글의 텐서플로와 마이크로소프트의 CNTK, 그리고 텐서 플로의 원조라고 할 수 있는 티아노를 비교하였다. 본문에서는 자동미분의 개념과 GPU의 활용형태를 간단히 설명하고, 그 다음에 logistic regression을 실행하는 예를 통하여 각 프레임워크의 문법을 알아본 뒤에, 마지막으로 대표적인 딥러닝 응용인 CNN의 예제를 실행시켜보고 코딩의 편의성과 실행속도 등을 확인해 보았다. 그 결과, 편의성의 관점에서 보면 티아노가 가장 코딩 하기가 어렵고, CNTK와 텐서플로는 많은 부분이 비슷하게 추상화 되어 있어서 코딩이 비슷하지만 가중치와 편향을 직접 정의하느냐의 여부에서 차이를 보였다. 그리고 각프레임워크의 실행속도에 대한 평가는 ‘큰 차이는 없다’는 것이다. 텐서플로는 티아노에 비하여 속도가 느리다는 평가가 있어왔는데, 본 연구의 실험에 의하면, 비록 CNN 모형에 국한되었지만, 텐서플로가 아주 조금이지만빠른 것으로 나타났다. CNTK의 경우에도, 비록 실험환경이 달랐지만, 실험환경의 차이에 의한 속도의 차이의 편차범위 이내에 있는 것으로 판단이 되었다. 본 연구에서는 세 종류의 딥러닝 프레임워크만을 살펴보았는데,위키피디아에 따르면 딥러닝 프레임워크의 종류는 12가지가 있으며, 각 프레임워크의 특징을 15가지 속성으로구분하여 차이를 특정하고 있다. 그 많은 속성 중에서 사용자의 입장에서 볼 때 중요한 속성은 어떤 언어(파이썬, C++, Java, 등)로 사용가능한지, 어떤 딥러닝 모형에 대한 라이브러리가 잘 구현되어 있는지 등일 것이다. 그리고 사용자가 대규모의 딥러닝 모형을 구축한다면, 다중 GPU 혹은 다중 서버를 지원하는지의 여부도 중요할 것이다. 또한 딥러닝 모형을 처음 학습하는 경우에는 사용설명서가 많은지 예제 프로그램이 많은지 여부도 중요한 기준이 될 것이다.The deep learning framework is software designed to help develop deep learning models. Some of its important functions include “automatic differentiation” and “utilization of GPU”. The list of popular deep learning framework includes Caffe (BVLC) and Theano (University of Montreal). And recently, Microsoft's deep learning framework, Microsoft Cognitive Toolkit, was released as open-source license, following Google’s Tensorflow a year earlier. The early deep learning frameworks have been developed mainly for research at universities. Beginning with the inception of Tensorflow, however, it seems that companies such as Microsoft and Facebook have started to join the competition of framework development. Given the trend, Google and other companies are expected to continue investing in the deep learning framework to bring forward the initiative in the artificial intelligence business. From this point of view, we think it is a good time to compare some of deep learning frameworks. So we compare three deep learning frameworks which can be used as a Python library. Those are Google's Tensorflow, Microsoft’s CNTK, and Theano which is sort of a predecessor of the preceding two.
The most common and important function of deep learning frameworks is the ability to perform automatic differentiation. Basically all the mathematical expressions of deep learning models can be represented as computational graphs, which consist of nodes and edges. Partial derivatives on each edge of a computational graph can then be obtained. With the partial derivatives, we can let software compute differentiation of any node with respect to any variable by utilizing chain rule of Calculus.
First of all, the convenience of coding is in the order of CNTK, Tensorflow, and Theano. The criterion is simply based on the lengths of the codes and the learning curve and the ease of coding are not the main concern. According to the criteria, Theano was the most difficult to implement with, and CNTK and Tensorflow were somewhat easier. With Tensorflow, we need to define weight variables and biases explicitly. The reason that CNTK and Tensorflow are easier to implement with is that those frameworks provide us with more abstraction than Theano. We, however, need to mention that low-level coding is not always bad. It gives us flexibility of coding. With the low-level coding such as in Theano, we can implement and test any new deep learning models or any new search methods that we can think of.
The assessment of the execution speed of each framework is that there is not meaningful difference. According to the experiment, execution speeds of Theano and Tensorflow are very similar, although the experiment was limited to a CNN model. In the case of CNTK, the experimental environment was not maintained as the same. The code written in CNTK has to be run in PC environment without GPU where codes execute as much as 50 times slower than with GPU. But we concluded that the difference of execution speed was within the range of variation caused by the different hardware setup.
In this study, we compared three types of deep learning framework: Theano, Tensorflow, and CNTK. According to Wikipedia, there are 12 available deep learning frameworks. And 15 different attributes differentiate each framework. Some of the important attributes would include interface language (Python, C ++, Java, etc.) and the availability of libraries on various deep learning models such as CNN, RNN, DBN, and etc. And if a user implements a large scale deep learning model, it will also be important to support multiple GPU or multiple servers. Also, if you are learning the deep learning model, it would also be important if there are enough examples and references.deep learning framework, Theano, TensorFlow, CNTK, computational graph, CIFAR-10"
980,딥러닝을 이용한 주변 무선단말 파악방안,이웅섭 /Lee， Woong-sup,2018,한국정보통신학회,"최근 단말-대-단말(Device-to-device, D2D) 통신기술이 차세대 무선통신시스템의 핵심기술로 큰 관심을 받고 있다. 이러한 단말간 통신에서는 자신의 주변에 어떠한 단말이 있는지 파악하는 주변단말 탐색(Neighbor discovery)이매우 중요하다. 본 논문에서는 최근 큰 관심을 받고 있는 딥러닝(Deep learning) 기술을 활용하여 단말간 통신에서 주변단말을 파악하는 방안에 대해서 제안한다. 제안 방안은 기존의 방안과 달리 무선채널의 공간적 연관성을 이용하여단말간의 신호 전송 없이 단말이 기지국으로 전송하는 상향링크 파일럿 신호를 기반으로 주변 단말을 찾고 따라서기존의 방식에 비해 신호전송 복잡도(signaling complexity)를 크게 줄일 수 있다. 또한 제안 방안에서는 떨어져 있는거리에 따라서 주변 단말을 분류 가능하여 기존 방안에 비해서 좀 더 세밀한 단말 탐색이 가능하다. 마지막으로 본 논문에서는 tensorflow를 이용한 컴퓨터 시뮬레이션을 통해 제안 방안의 성능을 검증하였다.Recently, the device-to-device (D2D) communication has been conceived as the key technology for the next-generationmobile communication systems. The neighbor discovery in which the nearby users are found, is essential for the properoperation of the D2D communication. In this paper, we propose new neighbor discovery scheme based on deep learningtechnology which has gained a lot of attention recently. In the proposed scheme, the neighboring users can be foundusing the uplink pilot transmission of users only, unlike conventional neighbor discovery schemes in which direct pilotcommunication among users is required, such that the signaling overhead can be greatly reduced in our proposed scheme.
Moreover, the neighbors with different proximity can also be classified accordingly which enables more accurate neighbordiscovery compared to the conventional schemes. The performance of our proposed scheme is verified through thetensorflow-based computer simulations.
키워드 : 딥러닝, 주변단말 탐색, 단말-대-단말 통신Deep learning, Neighbor discovery, Device-to-device communication, Wireless channel correlation"
981,신뢰성있는 딥러닝 기반 분석 모델을 참조하기 위한 딥러닝 기술 언어,문종혁 /Jong Hyeok Mun,2021,한국정보처리학회,"최근 딥러닝은 하드웨어 성능이 향상됨에 따라 자연어 처리, 영상 인식 등의 다양한 기술에 접목되어 활용되고 있다. 이러한 기술들을 활용해 지능형교통 시스템(ITS), 스마트홈, 헬스케어 등의 산업분야에서 데이터를 분석하여 고속도로 속도위반 차량 검출, 에너지 사용량 제어, 응급상황 등과 같은고품질의 서비스를 제공하며, 고품질의 서비스를 제공하기 위해서는 정확도가 향상된 딥러닝 모델이 적용되어야 한다. 이를 위해 서비스 환경의 데이터를분석하기 위한 딥러닝 모델을 개발할 때, 개발자는 신뢰성이 검증된 최신의 딥러닝 모델을 적용할 수 있어야 한다. 이는 개발자가 참조하는 딥러닝 모델에적용된 학습 데이터셋의 정확도를 측정하여 검증할 수 있다. 이러한 검증을 위해서 개발자는 학습 데이터셋, 딥러닝의 계층구조 및 개발 환경 등과 같은내용을 포함하는 딥러닝 모델을 문서화하여 적용하기 위한 구조적인 정보가 필요하다. 본 논문에서는 신뢰성있는 딥러닝 기반 데이터 분석 모델을 참조하기위한 딥러닝 기술 언어를 제안한다. 제안하는 기술 언어는 신뢰성 있는 딥러닝 모델을 개발하는데 필요한 학습데이터셋, 개발 환경 및 설정 등의 정보와더불어 딥러닝 모델의 계층구조를 표현할 수 있다. 제안하는 딥러닝 기술 언어를 이용하여 개발자는 지능형 교통 시스템에서 참조하는 분석 모델의정확도를 검증할 수 있다. 실험에서는 제안하는 언어의 유효성을 검증하기 위해, 번호판 인식 모델을 중심으로 딥러닝 기술 문서의 적용과정을 보인다With the recent advancements of deep learning, companies such as smart home, healthcare, and intelligent transportation systems areutilizing its functionality to provide high-quality services for vehicle detection, emergency situation detection, and controlling energyconsumption. To provide reliable services in such sensitive systems, deep learning models are required to have high accuracy. In orderto develop a deep learning model for analyzing previously mentioned services, developers should utilize the state of the art deep learningmodels that have already been verified for higher accuracy. The developers can verify the accuracy of the referenced model by validatingthe model on the dataset. For this validation, the developer needs structural information to document and apply deep learning models,including metadata such as learning dataset, network architecture, and development environments. In this paper, we propose a descriptionlanguage that represents the network architecture of the deep learning model along with its metadata that are necessary to develop adeep learning model. Through the proposed description language, developers can easily verify the accuracy of the referenced deep learningmodel. Our experiments demonstrate the application scenario of a deep learning description document that focuses on the license platerecognition for the detection of illegally parked vehicles.Trusted Deep Learning, Model Reference, Deep Learning Description Language, Traffic Situation Analysis Model"
982,딥러닝 분석을 이용한 중국 역내·외 위안화 변동성 예측,이우식 /Lee Woosik,2016,한국데이터정보과학회,"2008년 글로벌 금융위기 이후 중국은 위안화 국제화의 점진적 추진을 진행하면서 중국상하이 외환시장과 중국홍콩 외환시장에서 거래되는 통화인 역내위안화와 역외위안화를 형성시켰다. 본 연구는 위안화 국제화와 점진적인 중국 자본계정 개방에 따라 급변하는 외환시장상황의 변동성을 정확하게 파악하기 위해서 GARCH모형 (일반화된 자기회귀 조건부이분산성모형)에 다단계인공신경망을 결합한 MLP-GARCH 모형과 GARCH모형과 기계학습의 일종인 딥러닝 (deep learning)을 통합한 DL-GARCH을 가지고 위안화 변동성예측을 비교 실험과 분석을 하였다. 비교분석 결과 DLGARCH 모형은 MLP-GARCH보다 모형 위안화 역내·외 환율변동성 예측 면에서 더욱 더 개선된예측값을 제공하였다. 그래서 이분산시계열모형을 딥러닝과 결합한 DL-GARCH 모형은 시계열의환율 변동성 예측 문제에 딥러닝을 응용할 수 있음을 확인하였다. 향후 이분산시계열과 결합된 딥러닝 모형은 다른 금융시계열 데이터에 응용하여 그 일반화 가능성을 높일 수 있을 것이다.The People’s Republic of China has vigorously been pursuing the internationalization of the Chinese Yuan or Renminbi after the financial crisis of 2008. In this view, an abrupt increase of use of the Chinese Yuan in the onshore and offshore markets are important milestones to be one of important currencies. One of the most frequently used methods to forecast volatility is GARCH model. Since a prediction error of the GARCH model has been reported quite high, a lot of efforts have been made to improve forecasting capability of the GARCH model. In this paper, we have proposed MLPGARCH and a DL-GARCH by employing Artificial Neural Network to the GARCH. In an application to forecasting Chinese Yuan volatility, we have successfully shown their overall outperformance in forecasting over the GARCH.Deep learning, DL-GARCH, GARCH, The Chinese Yuan, volatility."
983,딥러닝 기법을 이용한 내일강수 예측,하지훈,2016,한국지능시스템학회,"정확한 강수예측을 위해서는 예측인자 선정과 예측방법에 대한 선택이 매우 중요하다. 최근에는 강수예측 방법으로기계학습 기법이 많이 사용되고 있으며, 그 중에서도 특히 인공신경망을 사용한 강수예측 방법은 좋은 성능을 보였다. 본논문에서는 딥러닝 기법 중 하나인 DBN(deep belief network)를 이용한 새로운 강수예측 방법을 제안한다. DBN는 비지도사전 학습을 통해 초기 가중치를 설정하여 기존 인공신경망의 문제점을 보완한다. 예측인자로는 기온, 전일-전주 강수일, 태양과 달 궤도 관련 자료를 선정하였다. 기온과 전일-전주 강수일은 서울에서의 1974년부터 2013년까지 총 40년간의AWS(automatic weather system) 관측 자료를 사용하였고, 태양과 달의 궤도 관련 자료는 서울을 중심으로 계산한 결과를사용하였다. 전체 기간에서 일부는 학습 자료로 사용하여 예측모델을 생성하였고, 나머지를 생성한 모델의 검증 자료로사용하였다. 모델 검증 결과로 나온 예측값들은 확률값을 가지며 임계치를 이용하여 강수유무를 판별하였다. 강수 정확도의척도로 양분예보기법 중 CSI(critical successive index)와 Bias(frequency bias)를 계산하였다. 이를 통해 DBN와 MLP(multilayer perceptron)의 성능을 비교한 결과 DBN의 강수 예측 정확도가 높았고, 수행속도 또한 2배 이상 빨랐다.For accurate precipitation forecasts the choice of weather faoctrs and prediction method is very important. Recently, machinele arning has been widely used for forecasting precipitation, and artificial neural network, one of machine learning techniques, showed good performance. In this paper, we suggest a new method for forecasting precipitation using DBN, one of deep learning techniques.
DBN has an advantage that initial weights are set by unsupervised learning, so this compensates for the defects of artificial neural networks. We used past precipitation, temperature, and the parameters of the sun and moon's motion as features for forecasting precipitation. The dataset consists of observation data which had been measured for 40 years from AWS in Seoul. Experiments were based on 8-fold cross validation. As a result of estimation, we got probabilities of test dataset, so threshold was used for the decision of precipitation. CSI and Bias were used for indicating the precision of precipitation. Our experimental results showed that DBN performed better than MLP.Deep learning, Deep belief network, Precipitation, Forecast"
984,딥러닝 신경망모형을 이용한 실시간 교통정보수집 연구,김대현 /KIM DAE HYON,2016,사단법인 인문사회과학기술융합학회,"비디오 기반의 실시간 교통정보수집 시스템은 자율주행을 포함한 지능형교통체계의 주요 서브시스템중 하나이며, 우수한 예측(추정)력 때문에 핵심 알고리즘으로 인공신경망 모형이 주로 사용되고 있다. 특히, 최근 과학 학술지 네이처에 구글 딥마인드의 알파고가 공개되면서 딥러닝과 인공지능에 대한 전 세계적인 관심이 집중되고 있다. 최근까지 가장 널리 사용된 인공신경망모형은 역전파 모형이었으나 최근 컴퓨터 하드웨어 및 알고리즘의 새로운 개발과 더불어 딥러닝 알고리즘을 이용한 딥인공신경망 모형에 대한 관심이 높아지고 있다. 그러나 아직 딥러닝 인공신경망에 대한 연구는 초기단계에 있는 실정이며 적용분야도 극히 한정되어 있다. 본 연구에서는 실시간 도로교통정보 수집을 위한 영상처리분야에서 딥러닝 알고리즘을 활용하고 실험결과를 통하여 향후 교통정보수집‧분석 분야에서의 딥러닝 인공신경망 모형의 적용가능성을 살펴보고자 한다. 특히, 본 연구에서 도출한 딥러닝 인공신경망모형의 실험결과를 기존 역전파 모형에 의한 실험결과와 비교하여 모형의 우수성을 비교‧평가하고자 한다.Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Normally, artificial neural networks have been used in real-time applications for collecting traffic information because of their remarkable performance in terms of predictive accuracy and real-time computing. Especially, the recent publication of a paper in the journal Nature and the success of Google DeepMind’s world-beating Go program AlphaGo caused significant increases of the public interest in artificial intelligence. Even though various types of learning models have been proposed, the Backpropagation has been the most popular neural network model. However, the Deep learning neural network model has attracted much interest recently. In this study, the Deep leaning neural network will be applied in image processing techniques for the real-time traffic information collection to investigate the performance of Deep learning algorithm in traffic engineering fields. In addition, the experimental results from Deep learing algorithm will be compared with the results from Backpropagation model to evaluate and propose better performing model.Intelligent Transportation Systems (ITS), artificial neural networks, Deep learning, traffic information, image processing"
985,딥러닝의 모형과 응용사례,안성만 /SungMahn Ahn,2016,한국지능정보시스템학회,"딥러닝은 인공신경망(neural network)이라는 인공지능분야의 모형이 발전된 형태로서, 계층구조로 이루어진 인공신경망의 내부계층(hidden layer)이 여러 단계로 이루어진 구조이다. 딥러닝에서의 주요 모형은 합성곱신경망(convolutional neural network), 순환신경망(recurrent neural network), 그리고 심층신뢰신경망(deep belief network)의 세가지라고 할 수있다. 그 중에서 현재 흥미로운 연구가 많이 발표되어서 관심이 집중되고 있는 모형은 지도학습(supervised learning)모형인 처음 두 개의 모형이다. 따라서 본 논문에서는 지도학습모형의 가중치를 최적화하는 기본적인 방법인 오류역전파 알고리즘을 살펴본 뒤에 합성곱신경망과 순환신경망의 구조와 응용사례 등을 살펴보고자 한다. 본문에서 다루지 않은 모형인 심층신뢰신경망은 아직까지는 합성곱신경망 이나 순환신경망보다는 상대적으로 주목을 덜 받고 있다. 그러나 심층신뢰신경망은 CNN이나 RNN과는 달리 비지도학습(unsupervised learning)모형이며, 사람이나 동물은 관찰을 통해서 스스로 학습한다는 점에서 궁극적으로는 비지도학습모형이 더 많이 연구되어야 할 주제가 될 것이다.Deep learning model is a kind of neural networks that allows multiple hidden layers. There are various deep learning architectures such as convolutional neural networks, deep belief networks and recurrent neural networks. Those have been applied to fields like computer vision, automatic speech recognition, natural language processing, audio recognition and bioinformatics where they have been shown to produce state-of-the-art results on various tasks. Among those architectures, convolutional neural networks and recurrent neural networks are classified as the supervised learning model. And in recent years, those supervised learning models have gained more popularity than unsupervised learning models such as deep belief networks, because supervised learning models have shown fashionable applications in such fields mentioned above.
Deep learning models can be trained with backpropagation algorithm. Backpropagation is an abbreviation for “backward propagation of errors” and a common method of training artificial neural networks used in conjunction with an optimization method such as gradient descent. The method calculates the gradient of an error function with respect to all the weights in the network. The gradient is fed to the optimization method which in turn uses it to update the weights, in an attempt to minimize the error function.
Convolutional neural networks use a special architecture which is particularly well-adapted to classify images. Using this architecture makes convolutional networks fast to train. This, in turn, helps us train deep, muti-layer networks, which are very good at classifying images. These days, deep convolutional networks are used in most neural networks for image recognition. Convolutional neural networks use three basic ideas: local receptive fields, shared weights, and pooling. By local receptive fields, we mean that each neuron in the first(or any) hidden layer will be connected to a small region of the input(or previous layer’s) neurons. Shared weights mean that we’re going to use the same weights and bias for each of the local receptive field. This means that all the neurons in the hidden layer detect exactly the same feature, just at different locations in the input image. In addition to the convolutional layers just described, convolutional neural networks also contain pooling layers. Pooling layers are usually used immediately after convolutional layers. What the pooling layers do is to simplify the information in the output from the convolutional layer.
Recent convolutional network architectures have 10 to 20 hidden layers and billions of connections between units. Training deep learning networks has taken weeks several years ago, but thanks to progress in GPU and algorithm enhancement, training time has reduced to several hours.
Neural networks with time-varying behavior are known as recurrent neural networks or RNNs. A recurrent neural network is a class of artificial neural network where connections between units form a directed cycle. This creates an internal state of the network which allows it to exhibit dynamic temporal behavior. Unlike feedforward neural networks, RNNs can use their internal memory to process arbitrary sequences of inputs. Early RNN models turned out to be very difficult to train, harder even than deep feedforward networks. The reason is the unstable gradient problem such as vanishing gradient and exploding gradient. The gradient can get smaller and smaller as it is propagated back through layers. This makes learning in early layers extremely slow. The problem actually gets worse in RNNs, since gradients aren’t just propagated backward through layers, they’re propagated backward through time. If the network runs for a long time, that can make the gradient extremely unstable and hard to learn from. It has been possible to incorporate an idea known as long short-term memory units (LSTMs) into RNNs. LSTMs make it much easier to get good results when training RNNs, and many recent papers make use of LSTMs or related ideas.Deep learning, Convolutional neural networks, Recurrent neural networks, Error backpropagation algorithm"
986,딥러닝분석과 기술적 분석 지표를 이용한 한국 코스피주가지수 방향성 예측,이우식 /Lee Woosik,2017,한국데이터정보과학회,"2016년 3월 구글 (Google)의 바둑인공지능 알파고 (AlphaGo)가 이세돌 9단과의 바둑대결에서승리한 이후 다양한 분야에서 인공지능 사용에 대한 관심이 높아지고 있는 가운데 금융투자 분야에서도 인공지능과 투자자문 전문가의 합성어인 로보어드바이저 (Robo-Advisor)에 대한 관심이 높아지고 있다. 인공지능 (artificial intelligence)기반의 의사결정은 비용 절감은 물론 효과적인 의사결정을가능하게 한다는 점에서 큰 장점이 있다. 본 연구에서는 기술적 분석 (technical analysis) 지표와 딥러닝 (deep learning) 모형을 결합하여 한국 코스피 지수를 예측하는 모형을 개발하고 제시한 모형들의 예측력을 비교, 분석한다. 분석 결과 기술적 분석 지표에 딥러닝 알고리즘을 결합한 모형이 주가지수 방향성 예측 문제에 응용될 수 있음을 확인하였다. 향후 본 연구에서 제안된 기술적 분석 지표와딥러닝모형을 결합한 기법은 로보어드바이저서비스에 응용할 수 있는 일반화 가능성을 보여준다Since Google’s AlphaGo defeated a world champion of Go players in 2016, there have been many interests in the deep learning. In the financial sector, a Robo-Advisor using deep learning gains a significant attention, which builds and manages portfolios of financial instruments for investors.In this paper, we have proposed the a deep learning algorithm geared toward identification and forecast of the KOSPI index direction,and we also have compared the accuracy of the prediction.In an application of forecasting the financial market index direction, we have shown that the Robo-Advisor using deep learning has a significant effect on finance industry. The Robo-Advisor collects a massive data such as earnings statements, news reports and regulatory filings, analyzes those and recommends investors how to view market trends and identify the best time to purchase financial assets. On the other hand, the Robo-Advisor allows businesses to learn more about their customers, develop better marketing strategies, increase sales and decrease costs.Artificial intelligence, deep learning, FinTech, Robo-Advisor, technical analysis."
987,딥러닝의 효율적인 학습을 위한 학습데이터 선별 기법에 관한 연구,주경돈 /Gyoung-don Joo,2017,한국정보과학회,"빅데이터의 증가와 병렬 하드웨어 처리, 학습 알고리즘의 개선은 딥러닝의 발전을 가져왔으며 인공지능이 적용 가능한 영역을 획기적으로 확장시키고 있다. 하지만 이러한 대규모의 학습 데이터에 기반한 딥러닝은 학습시간이 증가되고, 더 나아가 학습 데이터의 구축을 위한 레이블링(labeling) 비용이 데이터 수에 비례하여 증가하게 된다. 따라서 본 논문에서는, 딥러닝의 정확도를 최대한 유지하면서 필요한 학습 데이터의 수를 줄일수 있도록, 전체 데이터에서 제한된 수의 학습 데이터의 후보를 선별하는 기법에 대하여 연구한다. 특히 기존의 능동 학습에 기반한 기법들이 학습 데이터의 수가 매우 적어질 경우 딥러닝의 정확도가 급격히 떨어지는 문제를 해결할 수 있는 새로운 기법을 제안하였으며, 제안된 기법으로 선별된 학습 데이터는 그 수의 변화에 대해 딥러닝이 일관되게 경쟁력있는 정확도를 가짐을 다양한 실험을 통하여 검증하였다.The increase of big data, parallel hardware processing and enhanced learning algorithms have led to the development of deep learning and are dramatically expanding the domain of applications based on artificial intelligence. However, such large-scale training data extends the learning time, and furthermore, the labeling cost for building the training set increases in proportion to the volume of data. Therefore, in this paper, we study how to select a limited number of candidates for training set from given data so as to reduce the number of training set while maintaining the accuracy of deep learning. Especially, we found that, with the training set selected by the previous work, the accuracy of deep learning drastically decreases when the number of training set is very small and so proposed new methods to resolve this fall-off in accuracy. Through experiments, we showed that deep learning consistently has the competitive accuracy with the training set selected by the proposed method regardless of the size of training set.Deep Learning, Train Set Selection, Active Learning, Gamma distribution"
988,딥러닝을 이용한 범용적 스테그아날리시스,김현재 /Hyunjae Kim,2017,한국정보과학회,"스테그아날리시스(Steganalysis)란 이미지 등 일반적인 자료에 암호화된 정보를 은닉하는 스테가노그래피(Steganography)에 대한 검출 및 분석 방법으로, 기계학습 기반 방법론을 포함한다. 기존 기계학습 기반 스테그아날리시스는 영상(Image)의 특징(Feature) 추출 및 모델링에 기반하며, 최근 딥러닝(Deep Learning)의 적용으로 검출 정확도가 큰 폭으로 향상되었다. 하지만 현존하는 스테그아날리시스 모델은 단일 스테가노그래피 기법에 대해 국한되어 있어 학습에 사용되지 않은 스테고(Stego) 이미지의 경우 검출이 불가능한 결정적 한계를 가진다. 본 연구에서는 다양한 스테가노그래피 기법으로 생성된 스테고이미지에 딥러닝을 적용하여 스테그아날리시스를 학습하는 범용적 모델을 제안한다. 다양한 실험을 통해 제안 기법의 효용성 및 가능성을 확인하고, 범용적 스테그아날리시스 모델이 각각에 특화된 검출 기법과 유사한 정확도로 스테고 이미지를 검출할 수 있음을 보인다.Steganalysis is to detect information hidden by steganography inside general data such as images. There are stegoanalysis techniques that use machine learning (ML). Existing ML approaches to steganalysis are based on extracting features from stego images and modeling them.
Recently deep learning-based methodologies have shown significant improvements in detection accuracy. However, all the existing methods, including deep learning-based ones, have a critical limitation in that they can only detect stego images that are created by a specific steganography method. In this paper, we propose a generalized steganalysis method that can model multiple types of stego images using deep learning. Through various experiments, we confirm the effectiveness of our approach and envision directions for future research. In particular, we show that our method can detect each type of steganography with the same level of accuracy as that of a steganalysis method dedicated to that type of steganography, thereby demonstrating the general applicability of our approach to multiple types of stego images.steganalysis, steganography, deep learning, generalized steganalysis model"
989,"딥러닝 프레임워크의 비교: 티아노, 텐서플로, CNTK를 중심으로",정여진 /Yeojin Chung,2017,한국지능정보시스템학회,"딥러닝 프레임워크의 대표적인 기능으로는 ‘자동미분’과 ‘GPU의 활용’ 등을 들 수 있다. 본 논문은 파이썬의 라이브러리 형태로 사용 가능한 프레임워크 중에서 구글의 텐서플로와 마이크로소프트의 CNTK, 그리고 텐서 플로의 원조라고 할 수 있는 티아노를 비교하였다. 본문에서는 자동미분의 개념과 GPU의 활용형태를 간단히 설명하고, 그 다음에 logistic regression을 실행하는 예를 통하여 각 프레임워크의 문법을 알아본 뒤에, 마지막으로 대표적인 딥러닝 응용인 CNN의 예제를 실행시켜보고 코딩의 편의성과 실행속도 등을 확인해 보았다. 그 결과, 편의성의 관점에서 보면 티아노가 가장 코딩 하기가 어렵고, CNTK와 텐서플로는 많은 부분이 비슷하게 추상화 되어 있어서 코딩이 비슷하지만 가중치와 편향을 직접 정의하느냐의 여부에서 차이를 보였다. 그리고 각프레임워크의 실행속도에 대한 평가는 ‘큰 차이는 없다’는 것이다. 텐서플로는 티아노에 비하여 속도가 느리다는 평가가 있어왔는데, 본 연구의 실험에 의하면, 비록 CNN 모형에 국한되었지만, 텐서플로가 아주 조금이지만빠른 것으로 나타났다. CNTK의 경우에도, 비록 실험환경이 달랐지만, 실험환경의 차이에 의한 속도의 차이의 편차범위 이내에 있는 것으로 판단이 되었다. 본 연구에서는 세 종류의 딥러닝 프레임워크만을 살펴보았는데,위키피디아에 따르면 딥러닝 프레임워크의 종류는 12가지가 있으며, 각 프레임워크의 특징을 15가지 속성으로구분하여 차이를 특정하고 있다. 그 많은 속성 중에서 사용자의 입장에서 볼 때 중요한 속성은 어떤 언어(파이썬, C++, Java, 등)로 사용가능한지, 어떤 딥러닝 모형에 대한 라이브러리가 잘 구현되어 있는지 등일 것이다. 그리고 사용자가 대규모의 딥러닝 모형을 구축한다면, 다중 GPU 혹은 다중 서버를 지원하는지의 여부도 중요할 것이다. 또한 딥러닝 모형을 처음 학습하는 경우에는 사용설명서가 많은지 예제 프로그램이 많은지 여부도 중요한 기준이 될 것이다.The deep learning framework is software designed to help develop deep learning models. Some of its important functions include “automatic differentiation” and “utilization of GPU”. The list of popular deep learning framework includes Caffe (BVLC) and Theano (University of Montreal). And recently, Microsoft's deep learning framework, Microsoft Cognitive Toolkit, was released as open-source license, following Google’s Tensorflow a year earlier. The early deep learning frameworks have been developed mainly for research at universities. Beginning with the inception of Tensorflow, however, it seems that companies such as Microsoft and Facebook have started to join the competition of framework development. Given the trend, Google and other companies are expected to continue investing in the deep learning framework to bring forward the initiative in the artificial intelligence business. From this point of view, we think it is a good time to compare some of deep learning frameworks. So we compare three deep learning frameworks which can be used as a Python library. Those are Google's Tensorflow, Microsoft’s CNTK, and Theano which is sort of a predecessor of the preceding two.
The most common and important function of deep learning frameworks is the ability to perform automatic differentiation. Basically all the mathematical expressions of deep learning models can be represented as computational graphs, which consist of nodes and edges. Partial derivatives on each edge of a computational graph can then be obtained. With the partial derivatives, we can let software compute differentiation of any node with respect to any variable by utilizing chain rule of Calculus.
First of all, the convenience of coding is in the order of CNTK, Tensorflow, and Theano. The criterion is simply based on the lengths of the codes and the learning curve and the ease of coding are not the main concern. According to the criteria, Theano was the most difficult to implement with, and CNTK and Tensorflow were somewhat easier. With Tensorflow, we need to define weight variables and biases explicitly. The reason that CNTK and Tensorflow are easier to implement with is that those frameworks provide us with more abstraction than Theano. We, however, need to mention that low-level coding is not always bad. It gives us flexibility of coding. With the low-level coding such as in Theano, we can implement and test any new deep learning models or any new search methods that we can think of.
The assessment of the execution speed of each framework is that there is not meaningful difference. According to the experiment, execution speeds of Theano and Tensorflow are very similar, although the experiment was limited to a CNN model. In the case of CNTK, the experimental environment was not maintained as the same. The code written in CNTK has to be run in PC environment without GPU where codes execute as much as 50 times slower than with GPU. But we concluded that the difference of execution speed was within the range of variation caused by the different hardware setup.
In this study, we compared three types of deep learning framework: Theano, Tensorflow, and CNTK. According to Wikipedia, there are 12 available deep learning frameworks. And 15 different attributes differentiate each framework. Some of the important attributes would include interface language (Python, C ++, Java, etc.) and the availability of libraries on various deep learning models such as CNN, RNN, DBN, and etc. And if a user implements a large scale deep learning model, it will also be important to support multiple GPU or multiple servers. Also, if you are learning the deep learning model, it would also be important if there are enough examples and references.deep learning framework, Theano, TensorFlow, CNTK, computational graph, CIFAR-10"
990,딥러닝을 이용한 주변 무선단말 파악방안,이웅섭 /Lee， Woong-sup,2018,한국정보통신학회,"최근 단말-대-단말(Device-to-device, D2D) 통신기술이 차세대 무선통신시스템의 핵심기술로 큰 관심을 받고 있다. 이러한 단말간 통신에서는 자신의 주변에 어떠한 단말이 있는지 파악하는 주변단말 탐색(Neighbor discovery)이매우 중요하다. 본 논문에서는 최근 큰 관심을 받고 있는 딥러닝(Deep learning) 기술을 활용하여 단말간 통신에서 주변단말을 파악하는 방안에 대해서 제안한다. 제안 방안은 기존의 방안과 달리 무선채널의 공간적 연관성을 이용하여단말간의 신호 전송 없이 단말이 기지국으로 전송하는 상향링크 파일럿 신호를 기반으로 주변 단말을 찾고 따라서기존의 방식에 비해 신호전송 복잡도(signaling complexity)를 크게 줄일 수 있다. 또한 제안 방안에서는 떨어져 있는거리에 따라서 주변 단말을 분류 가능하여 기존 방안에 비해서 좀 더 세밀한 단말 탐색이 가능하다. 마지막으로 본 논문에서는 tensorflow를 이용한 컴퓨터 시뮬레이션을 통해 제안 방안의 성능을 검증하였다.Recently, the device-to-device (D2D) communication has been conceived as the key technology for the next-generationmobile communication systems. The neighbor discovery in which the nearby users are found, is essential for the properoperation of the D2D communication. In this paper, we propose new neighbor discovery scheme based on deep learningtechnology which has gained a lot of attention recently. In the proposed scheme, the neighboring users can be foundusing the uplink pilot transmission of users only, unlike conventional neighbor discovery schemes in which direct pilotcommunication among users is required, such that the signaling overhead can be greatly reduced in our proposed scheme.
Moreover, the neighbors with different proximity can also be classified accordingly which enables more accurate neighbordiscovery compared to the conventional schemes. The performance of our proposed scheme is verified through thetensorflow-based computer simulations.
키워드 : 딥러닝, 주변단말 탐색, 단말-대-단말 통신Deep learning, Neighbor discovery, Device-to-device communication, Wireless channel correlation"
991,신뢰성있는 딥러닝 기반 분석 모델을 참조하기 위한 딥러닝 기술 언어,문종혁 /Jong Hyeok Mun,2021,한국정보처리학회,"최근 딥러닝은 하드웨어 성능이 향상됨에 따라 자연어 처리, 영상 인식 등의 다양한 기술에 접목되어 활용되고 있다. 이러한 기술들을 활용해 지능형교통 시스템(ITS), 스마트홈, 헬스케어 등의 산업분야에서 데이터를 분석하여 고속도로 속도위반 차량 검출, 에너지 사용량 제어, 응급상황 등과 같은고품질의 서비스를 제공하며, 고품질의 서비스를 제공하기 위해서는 정확도가 향상된 딥러닝 모델이 적용되어야 한다. 이를 위해 서비스 환경의 데이터를분석하기 위한 딥러닝 모델을 개발할 때, 개발자는 신뢰성이 검증된 최신의 딥러닝 모델을 적용할 수 있어야 한다. 이는 개발자가 참조하는 딥러닝 모델에적용된 학습 데이터셋의 정확도를 측정하여 검증할 수 있다. 이러한 검증을 위해서 개발자는 학습 데이터셋, 딥러닝의 계층구조 및 개발 환경 등과 같은내용을 포함하는 딥러닝 모델을 문서화하여 적용하기 위한 구조적인 정보가 필요하다. 본 논문에서는 신뢰성있는 딥러닝 기반 데이터 분석 모델을 참조하기위한 딥러닝 기술 언어를 제안한다. 제안하는 기술 언어는 신뢰성 있는 딥러닝 모델을 개발하는데 필요한 학습데이터셋, 개발 환경 및 설정 등의 정보와더불어 딥러닝 모델의 계층구조를 표현할 수 있다. 제안하는 딥러닝 기술 언어를 이용하여 개발자는 지능형 교통 시스템에서 참조하는 분석 모델의정확도를 검증할 수 있다. 실험에서는 제안하는 언어의 유효성을 검증하기 위해, 번호판 인식 모델을 중심으로 딥러닝 기술 문서의 적용과정을 보인다With the recent advancements of deep learning, companies such as smart home, healthcare, and intelligent transportation systems areutilizing its functionality to provide high-quality services for vehicle detection, emergency situation detection, and controlling energyconsumption. To provide reliable services in such sensitive systems, deep learning models are required to have high accuracy. In orderto develop a deep learning model for analyzing previously mentioned services, developers should utilize the state of the art deep learningmodels that have already been verified for higher accuracy. The developers can verify the accuracy of the referenced model by validatingthe model on the dataset. For this validation, the developer needs structural information to document and apply deep learning models,including metadata such as learning dataset, network architecture, and development environments. In this paper, we propose a descriptionlanguage that represents the network architecture of the deep learning model along with its metadata that are necessary to develop adeep learning model. Through the proposed description language, developers can easily verify the accuracy of the referenced deep learningmodel. Our experiments demonstrate the application scenario of a deep learning description document that focuses on the license platerecognition for the detection of illegally parked vehicles.Trusted Deep Learning, Model Reference, Deep Learning Description Language, Traffic Situation Analysis Model"
992,딥러닝 분석을 이용한 중국 역내·외 위안화 변동성 예측,이우식 /Lee Woosik,2016,한국데이터정보과학회,"2008년 글로벌 금융위기 이후 중국은 위안화 국제화의 점진적 추진을 진행하면서 중국상하이 외환시장과 중국홍콩 외환시장에서 거래되는 통화인 역내위안화와 역외위안화를 형성시켰다. 본 연구는 위안화 국제화와 점진적인 중국 자본계정 개방에 따라 급변하는 외환시장상황의 변동성을 정확하게 파악하기 위해서 GARCH모형 (일반화된 자기회귀 조건부이분산성모형)에 다단계인공신경망을 결합한 MLP-GARCH 모형과 GARCH모형과 기계학습의 일종인 딥러닝 (deep learning)을 통합한 DL-GARCH을 가지고 위안화 변동성예측을 비교 실험과 분석을 하였다. 비교분석 결과 DLGARCH 모형은 MLP-GARCH보다 모형 위안화 역내·외 환율변동성 예측 면에서 더욱 더 개선된예측값을 제공하였다. 그래서 이분산시계열모형을 딥러닝과 결합한 DL-GARCH 모형은 시계열의환율 변동성 예측 문제에 딥러닝을 응용할 수 있음을 확인하였다. 향후 이분산시계열과 결합된 딥러닝 모형은 다른 금융시계열 데이터에 응용하여 그 일반화 가능성을 높일 수 있을 것이다.The People’s Republic of China has vigorously been pursuing the internationalization of the Chinese Yuan or Renminbi after the financial crisis of 2008. In this view, an abrupt increase of use of the Chinese Yuan in the onshore and offshore markets are important milestones to be one of important currencies. One of the most frequently used methods to forecast volatility is GARCH model. Since a prediction error of the GARCH model has been reported quite high, a lot of efforts have been made to improve forecasting capability of the GARCH model. In this paper, we have proposed MLPGARCH and a DL-GARCH by employing Artificial Neural Network to the GARCH. In an application to forecasting Chinese Yuan volatility, we have successfully shown their overall outperformance in forecasting over the GARCH.Deep learning, DL-GARCH, GARCH, The Chinese Yuan, volatility."
993,딥러닝 기법을 이용한 내일강수 예측,하지훈,2016,한국지능시스템학회,"정확한 강수예측을 위해서는 예측인자 선정과 예측방법에 대한 선택이 매우 중요하다. 최근에는 강수예측 방법으로기계학습 기법이 많이 사용되고 있으며, 그 중에서도 특히 인공신경망을 사용한 강수예측 방법은 좋은 성능을 보였다. 본논문에서는 딥러닝 기법 중 하나인 DBN(deep belief network)를 이용한 새로운 강수예측 방법을 제안한다. DBN는 비지도사전 학습을 통해 초기 가중치를 설정하여 기존 인공신경망의 문제점을 보완한다. 예측인자로는 기온, 전일-전주 강수일, 태양과 달 궤도 관련 자료를 선정하였다. 기온과 전일-전주 강수일은 서울에서의 1974년부터 2013년까지 총 40년간의AWS(automatic weather system) 관측 자료를 사용하였고, 태양과 달의 궤도 관련 자료는 서울을 중심으로 계산한 결과를사용하였다. 전체 기간에서 일부는 학습 자료로 사용하여 예측모델을 생성하였고, 나머지를 생성한 모델의 검증 자료로사용하였다. 모델 검증 결과로 나온 예측값들은 확률값을 가지며 임계치를 이용하여 강수유무를 판별하였다. 강수 정확도의척도로 양분예보기법 중 CSI(critical successive index)와 Bias(frequency bias)를 계산하였다. 이를 통해 DBN와 MLP(multilayer perceptron)의 성능을 비교한 결과 DBN의 강수 예측 정확도가 높았고, 수행속도 또한 2배 이상 빨랐다.For accurate precipitation forecasts the choice of weather faoctrs and prediction method is very important. Recently, machinele arning has been widely used for forecasting precipitation, and artificial neural network, one of machine learning techniques, showed good performance. In this paper, we suggest a new method for forecasting precipitation using DBN, one of deep learning techniques.
DBN has an advantage that initial weights are set by unsupervised learning, so this compensates for the defects of artificial neural networks. We used past precipitation, temperature, and the parameters of the sun and moon's motion as features for forecasting precipitation. The dataset consists of observation data which had been measured for 40 years from AWS in Seoul. Experiments were based on 8-fold cross validation. As a result of estimation, we got probabilities of test dataset, so threshold was used for the decision of precipitation. CSI and Bias were used for indicating the precision of precipitation. Our experimental results showed that DBN performed better than MLP.Deep learning, Deep belief network, Precipitation, Forecast"
994,딥러닝 신경망모형을 이용한 실시간 교통정보수집 연구,김대현 /KIM DAE HYON,2016,사단법인 인문사회과학기술융합학회,"비디오 기반의 실시간 교통정보수집 시스템은 자율주행을 포함한 지능형교통체계의 주요 서브시스템중 하나이며, 우수한 예측(추정)력 때문에 핵심 알고리즘으로 인공신경망 모형이 주로 사용되고 있다. 특히, 최근 과학 학술지 네이처에 구글 딥마인드의 알파고가 공개되면서 딥러닝과 인공지능에 대한 전 세계적인 관심이 집중되고 있다. 최근까지 가장 널리 사용된 인공신경망모형은 역전파 모형이었으나 최근 컴퓨터 하드웨어 및 알고리즘의 새로운 개발과 더불어 딥러닝 알고리즘을 이용한 딥인공신경망 모형에 대한 관심이 높아지고 있다. 그러나 아직 딥러닝 인공신경망에 대한 연구는 초기단계에 있는 실정이며 적용분야도 극히 한정되어 있다. 본 연구에서는 실시간 도로교통정보 수집을 위한 영상처리분야에서 딥러닝 알고리즘을 활용하고 실험결과를 통하여 향후 교통정보수집‧분석 분야에서의 딥러닝 인공신경망 모형의 적용가능성을 살펴보고자 한다. 특히, 본 연구에서 도출한 딥러닝 인공신경망모형의 실험결과를 기존 역전파 모형에 의한 실험결과와 비교하여 모형의 우수성을 비교‧평가하고자 한다.Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Normally, artificial neural networks have been used in real-time applications for collecting traffic information because of their remarkable performance in terms of predictive accuracy and real-time computing. Especially, the recent publication of a paper in the journal Nature and the success of Google DeepMind’s world-beating Go program AlphaGo caused significant increases of the public interest in artificial intelligence. Even though various types of learning models have been proposed, the Backpropagation has been the most popular neural network model. However, the Deep learning neural network model has attracted much interest recently. In this study, the Deep leaning neural network will be applied in image processing techniques for the real-time traffic information collection to investigate the performance of Deep learning algorithm in traffic engineering fields. In addition, the experimental results from Deep learing algorithm will be compared with the results from Backpropagation model to evaluate and propose better performing model.Intelligent Transportation Systems (ITS), artificial neural networks, Deep learning, traffic information, image processing"
995,딥러닝의 모형과 응용사례,안성만 /SungMahn Ahn,2016,한국지능정보시스템학회,"딥러닝은 인공신경망(neural network)이라는 인공지능분야의 모형이 발전된 형태로서, 계층구조로 이루어진 인공신경망의 내부계층(hidden layer)이 여러 단계로 이루어진 구조이다. 딥러닝에서의 주요 모형은 합성곱신경망(convolutional neural network), 순환신경망(recurrent neural network), 그리고 심층신뢰신경망(deep belief network)의 세가지라고 할 수있다. 그 중에서 현재 흥미로운 연구가 많이 발표되어서 관심이 집중되고 있는 모형은 지도학습(supervised learning)모형인 처음 두 개의 모형이다. 따라서 본 논문에서는 지도학습모형의 가중치를 최적화하는 기본적인 방법인 오류역전파 알고리즘을 살펴본 뒤에 합성곱신경망과 순환신경망의 구조와 응용사례 등을 살펴보고자 한다. 본문에서 다루지 않은 모형인 심층신뢰신경망은 아직까지는 합성곱신경망 이나 순환신경망보다는 상대적으로 주목을 덜 받고 있다. 그러나 심층신뢰신경망은 CNN이나 RNN과는 달리 비지도학습(unsupervised learning)모형이며, 사람이나 동물은 관찰을 통해서 스스로 학습한다는 점에서 궁극적으로는 비지도학습모형이 더 많이 연구되어야 할 주제가 될 것이다.Deep learning model is a kind of neural networks that allows multiple hidden layers. There are various deep learning architectures such as convolutional neural networks, deep belief networks and recurrent neural networks. Those have been applied to fields like computer vision, automatic speech recognition, natural language processing, audio recognition and bioinformatics where they have been shown to produce state-of-the-art results on various tasks. Among those architectures, convolutional neural networks and recurrent neural networks are classified as the supervised learning model. And in recent years, those supervised learning models have gained more popularity than unsupervised learning models such as deep belief networks, because supervised learning models have shown fashionable applications in such fields mentioned above.
Deep learning models can be trained with backpropagation algorithm. Backpropagation is an abbreviation for “backward propagation of errors” and a common method of training artificial neural networks used in conjunction with an optimization method such as gradient descent. The method calculates the gradient of an error function with respect to all the weights in the network. The gradient is fed to the optimization method which in turn uses it to update the weights, in an attempt to minimize the error function.
Convolutional neural networks use a special architecture which is particularly well-adapted to classify images. Using this architecture makes convolutional networks fast to train. This, in turn, helps us train deep, muti-layer networks, which are very good at classifying images. These days, deep convolutional networks are used in most neural networks for image recognition. Convolutional neural networks use three basic ideas: local receptive fields, shared weights, and pooling. By local receptive fields, we mean that each neuron in the first(or any) hidden layer will be connected to a small region of the input(or previous layer’s) neurons. Shared weights mean that we’re going to use the same weights and bias for each of the local receptive field. This means that all the neurons in the hidden layer detect exactly the same feature, just at different locations in the input image. In addition to the convolutional layers just described, convolutional neural networks also contain pooling layers. Pooling layers are usually used immediately after convolutional layers. What the pooling layers do is to simplify the information in the output from the convolutional layer.
Recent convolutional network architectures have 10 to 20 hidden layers and billions of connections between units. Training deep learning networks has taken weeks several years ago, but thanks to progress in GPU and algorithm enhancement, training time has reduced to several hours.
Neural networks with time-varying behavior are known as recurrent neural networks or RNNs. A recurrent neural network is a class of artificial neural network where connections between units form a directed cycle. This creates an internal state of the network which allows it to exhibit dynamic temporal behavior. Unlike feedforward neural networks, RNNs can use their internal memory to process arbitrary sequences of inputs. Early RNN models turned out to be very difficult to train, harder even than deep feedforward networks. The reason is the unstable gradient problem such as vanishing gradient and exploding gradient. The gradient can get smaller and smaller as it is propagated back through layers. This makes learning in early layers extremely slow. The problem actually gets worse in RNNs, since gradients aren’t just propagated backward through layers, they’re propagated backward through time. If the network runs for a long time, that can make the gradient extremely unstable and hard to learn from. It has been possible to incorporate an idea known as long short-term memory units (LSTMs) into RNNs. LSTMs make it much easier to get good results when training RNNs, and many recent papers make use of LSTMs or related ideas.Deep learning, Convolutional neural networks, Recurrent neural networks, Error backpropagation algorithm"
996,딥러닝분석과 기술적 분석 지표를 이용한 한국 코스피주가지수 방향성 예측,이우식 /Lee Woosik,2017,한국데이터정보과학회,"2016년 3월 구글 (Google)의 바둑인공지능 알파고 (AlphaGo)가 이세돌 9단과의 바둑대결에서승리한 이후 다양한 분야에서 인공지능 사용에 대한 관심이 높아지고 있는 가운데 금융투자 분야에서도 인공지능과 투자자문 전문가의 합성어인 로보어드바이저 (Robo-Advisor)에 대한 관심이 높아지고 있다. 인공지능 (artificial intelligence)기반의 의사결정은 비용 절감은 물론 효과적인 의사결정을가능하게 한다는 점에서 큰 장점이 있다. 본 연구에서는 기술적 분석 (technical analysis) 지표와 딥러닝 (deep learning) 모형을 결합하여 한국 코스피 지수를 예측하는 모형을 개발하고 제시한 모형들의 예측력을 비교, 분석한다. 분석 결과 기술적 분석 지표에 딥러닝 알고리즘을 결합한 모형이 주가지수 방향성 예측 문제에 응용될 수 있음을 확인하였다. 향후 본 연구에서 제안된 기술적 분석 지표와딥러닝모형을 결합한 기법은 로보어드바이저서비스에 응용할 수 있는 일반화 가능성을 보여준다Since Google’s AlphaGo defeated a world champion of Go players in 2016, there have been many interests in the deep learning. In the financial sector, a Robo-Advisor using deep learning gains a significant attention, which builds and manages portfolios of financial instruments for investors.In this paper, we have proposed the a deep learning algorithm geared toward identification and forecast of the KOSPI index direction,and we also have compared the accuracy of the prediction.In an application of forecasting the financial market index direction, we have shown that the Robo-Advisor using deep learning has a significant effect on finance industry. The Robo-Advisor collects a massive data such as earnings statements, news reports and regulatory filings, analyzes those and recommends investors how to view market trends and identify the best time to purchase financial assets. On the other hand, the Robo-Advisor allows businesses to learn more about their customers, develop better marketing strategies, increase sales and decrease costs.Artificial intelligence, deep learning, FinTech, Robo-Advisor, technical analysis."
997,딥러닝의 효율적인 학습을 위한 학습데이터 선별 기법에 관한 연구,주경돈 /Gyoung-don Joo,2017,한국정보과학회,"빅데이터의 증가와 병렬 하드웨어 처리, 학습 알고리즘의 개선은 딥러닝의 발전을 가져왔으며 인공지능이 적용 가능한 영역을 획기적으로 확장시키고 있다. 하지만 이러한 대규모의 학습 데이터에 기반한 딥러닝은 학습시간이 증가되고, 더 나아가 학습 데이터의 구축을 위한 레이블링(labeling) 비용이 데이터 수에 비례하여 증가하게 된다. 따라서 본 논문에서는, 딥러닝의 정확도를 최대한 유지하면서 필요한 학습 데이터의 수를 줄일수 있도록, 전체 데이터에서 제한된 수의 학습 데이터의 후보를 선별하는 기법에 대하여 연구한다. 특히 기존의 능동 학습에 기반한 기법들이 학습 데이터의 수가 매우 적어질 경우 딥러닝의 정확도가 급격히 떨어지는 문제를 해결할 수 있는 새로운 기법을 제안하였으며, 제안된 기법으로 선별된 학습 데이터는 그 수의 변화에 대해 딥러닝이 일관되게 경쟁력있는 정확도를 가짐을 다양한 실험을 통하여 검증하였다.The increase of big data, parallel hardware processing and enhanced learning algorithms have led to the development of deep learning and are dramatically expanding the domain of applications based on artificial intelligence. However, such large-scale training data extends the learning time, and furthermore, the labeling cost for building the training set increases in proportion to the volume of data. Therefore, in this paper, we study how to select a limited number of candidates for training set from given data so as to reduce the number of training set while maintaining the accuracy of deep learning. Especially, we found that, with the training set selected by the previous work, the accuracy of deep learning drastically decreases when the number of training set is very small and so proposed new methods to resolve this fall-off in accuracy. Through experiments, we showed that deep learning consistently has the competitive accuracy with the training set selected by the proposed method regardless of the size of training set.Deep Learning, Train Set Selection, Active Learning, Gamma distribution"
998,딥러닝을 이용한 범용적 스테그아날리시스,김현재 /Hyunjae Kim,2017,한국정보과학회,"스테그아날리시스(Steganalysis)란 이미지 등 일반적인 자료에 암호화된 정보를 은닉하는 스테가노그래피(Steganography)에 대한 검출 및 분석 방법으로, 기계학습 기반 방법론을 포함한다. 기존 기계학습 기반 스테그아날리시스는 영상(Image)의 특징(Feature) 추출 및 모델링에 기반하며, 최근 딥러닝(Deep Learning)의 적용으로 검출 정확도가 큰 폭으로 향상되었다. 하지만 현존하는 스테그아날리시스 모델은 단일 스테가노그래피 기법에 대해 국한되어 있어 학습에 사용되지 않은 스테고(Stego) 이미지의 경우 검출이 불가능한 결정적 한계를 가진다. 본 연구에서는 다양한 스테가노그래피 기법으로 생성된 스테고이미지에 딥러닝을 적용하여 스테그아날리시스를 학습하는 범용적 모델을 제안한다. 다양한 실험을 통해 제안 기법의 효용성 및 가능성을 확인하고, 범용적 스테그아날리시스 모델이 각각에 특화된 검출 기법과 유사한 정확도로 스테고 이미지를 검출할 수 있음을 보인다.Steganalysis is to detect information hidden by steganography inside general data such as images. There are stegoanalysis techniques that use machine learning (ML). Existing ML approaches to steganalysis are based on extracting features from stego images and modeling them.
Recently deep learning-based methodologies have shown significant improvements in detection accuracy. However, all the existing methods, including deep learning-based ones, have a critical limitation in that they can only detect stego images that are created by a specific steganography method. In this paper, we propose a generalized steganalysis method that can model multiple types of stego images using deep learning. Through various experiments, we confirm the effectiveness of our approach and envision directions for future research. In particular, we show that our method can detect each type of steganography with the same level of accuracy as that of a steganalysis method dedicated to that type of steganography, thereby demonstrating the general applicability of our approach to multiple types of stego images.steganalysis, steganography, deep learning, generalized steganalysis model"
999,"딥러닝 프레임워크의 비교: 티아노, 텐서플로, CNTK를 중심으로",정여진 /Yeojin Chung,2017,한국지능정보시스템학회,"딥러닝 프레임워크의 대표적인 기능으로는 ‘자동미분’과 ‘GPU의 활용’ 등을 들 수 있다. 본 논문은 파이썬의 라이브러리 형태로 사용 가능한 프레임워크 중에서 구글의 텐서플로와 마이크로소프트의 CNTK, 그리고 텐서 플로의 원조라고 할 수 있는 티아노를 비교하였다. 본문에서는 자동미분의 개념과 GPU의 활용형태를 간단히 설명하고, 그 다음에 logistic regression을 실행하는 예를 통하여 각 프레임워크의 문법을 알아본 뒤에, 마지막으로 대표적인 딥러닝 응용인 CNN의 예제를 실행시켜보고 코딩의 편의성과 실행속도 등을 확인해 보았다. 그 결과, 편의성의 관점에서 보면 티아노가 가장 코딩 하기가 어렵고, CNTK와 텐서플로는 많은 부분이 비슷하게 추상화 되어 있어서 코딩이 비슷하지만 가중치와 편향을 직접 정의하느냐의 여부에서 차이를 보였다. 그리고 각프레임워크의 실행속도에 대한 평가는 ‘큰 차이는 없다’는 것이다. 텐서플로는 티아노에 비하여 속도가 느리다는 평가가 있어왔는데, 본 연구의 실험에 의하면, 비록 CNN 모형에 국한되었지만, 텐서플로가 아주 조금이지만빠른 것으로 나타났다. CNTK의 경우에도, 비록 실험환경이 달랐지만, 실험환경의 차이에 의한 속도의 차이의 편차범위 이내에 있는 것으로 판단이 되었다. 본 연구에서는 세 종류의 딥러닝 프레임워크만을 살펴보았는데,위키피디아에 따르면 딥러닝 프레임워크의 종류는 12가지가 있으며, 각 프레임워크의 특징을 15가지 속성으로구분하여 차이를 특정하고 있다. 그 많은 속성 중에서 사용자의 입장에서 볼 때 중요한 속성은 어떤 언어(파이썬, C++, Java, 등)로 사용가능한지, 어떤 딥러닝 모형에 대한 라이브러리가 잘 구현되어 있는지 등일 것이다. 그리고 사용자가 대규모의 딥러닝 모형을 구축한다면, 다중 GPU 혹은 다중 서버를 지원하는지의 여부도 중요할 것이다. 또한 딥러닝 모형을 처음 학습하는 경우에는 사용설명서가 많은지 예제 프로그램이 많은지 여부도 중요한 기준이 될 것이다.The deep learning framework is software designed to help develop deep learning models. Some of its important functions include “automatic differentiation” and “utilization of GPU”. The list of popular deep learning framework includes Caffe (BVLC) and Theano (University of Montreal). And recently, Microsoft's deep learning framework, Microsoft Cognitive Toolkit, was released as open-source license, following Google’s Tensorflow a year earlier. The early deep learning frameworks have been developed mainly for research at universities. Beginning with the inception of Tensorflow, however, it seems that companies such as Microsoft and Facebook have started to join the competition of framework development. Given the trend, Google and other companies are expected to continue investing in the deep learning framework to bring forward the initiative in the artificial intelligence business. From this point of view, we think it is a good time to compare some of deep learning frameworks. So we compare three deep learning frameworks which can be used as a Python library. Those are Google's Tensorflow, Microsoft’s CNTK, and Theano which is sort of a predecessor of the preceding two.
The most common and important function of deep learning frameworks is the ability to perform automatic differentiation. Basically all the mathematical expressions of deep learning models can be represented as computational graphs, which consist of nodes and edges. Partial derivatives on each edge of a computational graph can then be obtained. With the partial derivatives, we can let software compute differentiation of any node with respect to any variable by utilizing chain rule of Calculus.
First of all, the convenience of coding is in the order of CNTK, Tensorflow, and Theano. The criterion is simply based on the lengths of the codes and the learning curve and the ease of coding are not the main concern. According to the criteria, Theano was the most difficult to implement with, and CNTK and Tensorflow were somewhat easier. With Tensorflow, we need to define weight variables and biases explicitly. The reason that CNTK and Tensorflow are easier to implement with is that those frameworks provide us with more abstraction than Theano. We, however, need to mention that low-level coding is not always bad. It gives us flexibility of coding. With the low-level coding such as in Theano, we can implement and test any new deep learning models or any new search methods that we can think of.
The assessment of the execution speed of each framework is that there is not meaningful difference. According to the experiment, execution speeds of Theano and Tensorflow are very similar, although the experiment was limited to a CNN model. In the case of CNTK, the experimental environment was not maintained as the same. The code written in CNTK has to be run in PC environment without GPU where codes execute as much as 50 times slower than with GPU. But we concluded that the difference of execution speed was within the range of variation caused by the different hardware setup.
In this study, we compared three types of deep learning framework: Theano, Tensorflow, and CNTK. According to Wikipedia, there are 12 available deep learning frameworks. And 15 different attributes differentiate each framework. Some of the important attributes would include interface language (Python, C ++, Java, etc.) and the availability of libraries on various deep learning models such as CNN, RNN, DBN, and etc. And if a user implements a large scale deep learning model, it will also be important to support multiple GPU or multiple servers. Also, if you are learning the deep learning model, it would also be important if there are enough examples and references.deep learning framework, Theano, TensorFlow, CNTK, computational graph, CIFAR-10"
1000,딥러닝을 이용한 주변 무선단말 파악방안,이웅섭 /Lee， Woong-sup,2018,한국정보통신학회,"최근 단말-대-단말(Device-to-device, D2D) 통신기술이 차세대 무선통신시스템의 핵심기술로 큰 관심을 받고 있다. 이러한 단말간 통신에서는 자신의 주변에 어떠한 단말이 있는지 파악하는 주변단말 탐색(Neighbor discovery)이매우 중요하다. 본 논문에서는 최근 큰 관심을 받고 있는 딥러닝(Deep learning) 기술을 활용하여 단말간 통신에서 주변단말을 파악하는 방안에 대해서 제안한다. 제안 방안은 기존의 방안과 달리 무선채널의 공간적 연관성을 이용하여단말간의 신호 전송 없이 단말이 기지국으로 전송하는 상향링크 파일럿 신호를 기반으로 주변 단말을 찾고 따라서기존의 방식에 비해 신호전송 복잡도(signaling complexity)를 크게 줄일 수 있다. 또한 제안 방안에서는 떨어져 있는거리에 따라서 주변 단말을 분류 가능하여 기존 방안에 비해서 좀 더 세밀한 단말 탐색이 가능하다. 마지막으로 본 논문에서는 tensorflow를 이용한 컴퓨터 시뮬레이션을 통해 제안 방안의 성능을 검증하였다.Recently, the device-to-device (D2D) communication has been conceived as the key technology for the next-generationmobile communication systems. The neighbor discovery in which the nearby users are found, is essential for the properoperation of the D2D communication. In this paper, we propose new neighbor discovery scheme based on deep learningtechnology which has gained a lot of attention recently. In the proposed scheme, the neighboring users can be foundusing the uplink pilot transmission of users only, unlike conventional neighbor discovery schemes in which direct pilotcommunication among users is required, such that the signaling overhead can be greatly reduced in our proposed scheme.
Moreover, the neighbors with different proximity can also be classified accordingly which enables more accurate neighbordiscovery compared to the conventional schemes. The performance of our proposed scheme is verified through thetensorflow-based computer simulations.
키워드 : 딥러닝, 주변단말 탐색, 단말-대-단말 통신Deep learning, Neighbor discovery, Device-to-device communication, Wireless channel correlation"
1001,신뢰성있는 딥러닝 기반 분석 모델을 참조하기 위한 딥러닝 기술 언어,문종혁 /Jong Hyeok Mun,2021,한국정보처리학회,"최근 딥러닝은 하드웨어 성능이 향상됨에 따라 자연어 처리, 영상 인식 등의 다양한 기술에 접목되어 활용되고 있다. 이러한 기술들을 활용해 지능형교통 시스템(ITS), 스마트홈, 헬스케어 등의 산업분야에서 데이터를 분석하여 고속도로 속도위반 차량 검출, 에너지 사용량 제어, 응급상황 등과 같은고품질의 서비스를 제공하며, 고품질의 서비스를 제공하기 위해서는 정확도가 향상된 딥러닝 모델이 적용되어야 한다. 이를 위해 서비스 환경의 데이터를분석하기 위한 딥러닝 모델을 개발할 때, 개발자는 신뢰성이 검증된 최신의 딥러닝 모델을 적용할 수 있어야 한다. 이는 개발자가 참조하는 딥러닝 모델에적용된 학습 데이터셋의 정확도를 측정하여 검증할 수 있다. 이러한 검증을 위해서 개발자는 학습 데이터셋, 딥러닝의 계층구조 및 개발 환경 등과 같은내용을 포함하는 딥러닝 모델을 문서화하여 적용하기 위한 구조적인 정보가 필요하다. 본 논문에서는 신뢰성있는 딥러닝 기반 데이터 분석 모델을 참조하기위한 딥러닝 기술 언어를 제안한다. 제안하는 기술 언어는 신뢰성 있는 딥러닝 모델을 개발하는데 필요한 학습데이터셋, 개발 환경 및 설정 등의 정보와더불어 딥러닝 모델의 계층구조를 표현할 수 있다. 제안하는 딥러닝 기술 언어를 이용하여 개발자는 지능형 교통 시스템에서 참조하는 분석 모델의정확도를 검증할 수 있다. 실험에서는 제안하는 언어의 유효성을 검증하기 위해, 번호판 인식 모델을 중심으로 딥러닝 기술 문서의 적용과정을 보인다With the recent advancements of deep learning, companies such as smart home, healthcare, and intelligent transportation systems areutilizing its functionality to provide high-quality services for vehicle detection, emergency situation detection, and controlling energyconsumption. To provide reliable services in such sensitive systems, deep learning models are required to have high accuracy. In orderto develop a deep learning model for analyzing previously mentioned services, developers should utilize the state of the art deep learningmodels that have already been verified for higher accuracy. The developers can verify the accuracy of the referenced model by validatingthe model on the dataset. For this validation, the developer needs structural information to document and apply deep learning models,including metadata such as learning dataset, network architecture, and development environments. In this paper, we propose a descriptionlanguage that represents the network architecture of the deep learning model along with its metadata that are necessary to develop adeep learning model. Through the proposed description language, developers can easily verify the accuracy of the referenced deep learningmodel. Our experiments demonstrate the application scenario of a deep learning description document that focuses on the license platerecognition for the detection of illegally parked vehicles.Trusted Deep Learning, Model Reference, Deep Learning Description Language, Traffic Situation Analysis Model"
1002,딥러닝 분석을 이용한 중국 역내·외 위안화 변동성 예측,이우식 /Lee Woosik,2016,한국데이터정보과학회,"2008년 글로벌 금융위기 이후 중국은 위안화 국제화의 점진적 추진을 진행하면서 중국상하이 외환시장과 중국홍콩 외환시장에서 거래되는 통화인 역내위안화와 역외위안화를 형성시켰다. 본 연구는 위안화 국제화와 점진적인 중국 자본계정 개방에 따라 급변하는 외환시장상황의 변동성을 정확하게 파악하기 위해서 GARCH모형 (일반화된 자기회귀 조건부이분산성모형)에 다단계인공신경망을 결합한 MLP-GARCH 모형과 GARCH모형과 기계학습의 일종인 딥러닝 (deep learning)을 통합한 DL-GARCH을 가지고 위안화 변동성예측을 비교 실험과 분석을 하였다. 비교분석 결과 DLGARCH 모형은 MLP-GARCH보다 모형 위안화 역내·외 환율변동성 예측 면에서 더욱 더 개선된예측값을 제공하였다. 그래서 이분산시계열모형을 딥러닝과 결합한 DL-GARCH 모형은 시계열의환율 변동성 예측 문제에 딥러닝을 응용할 수 있음을 확인하였다. 향후 이분산시계열과 결합된 딥러닝 모형은 다른 금융시계열 데이터에 응용하여 그 일반화 가능성을 높일 수 있을 것이다.The People’s Republic of China has vigorously been pursuing the internationalization of the Chinese Yuan or Renminbi after the financial crisis of 2008. In this view, an abrupt increase of use of the Chinese Yuan in the onshore and offshore markets are important milestones to be one of important currencies. One of the most frequently used methods to forecast volatility is GARCH model. Since a prediction error of the GARCH model has been reported quite high, a lot of efforts have been made to improve forecasting capability of the GARCH model. In this paper, we have proposed MLPGARCH and a DL-GARCH by employing Artificial Neural Network to the GARCH. In an application to forecasting Chinese Yuan volatility, we have successfully shown their overall outperformance in forecasting over the GARCH.Deep learning, DL-GARCH, GARCH, The Chinese Yuan, volatility."
1003,딥러닝 기법을 이용한 내일강수 예측,하지훈,2016,한국지능시스템학회,"정확한 강수예측을 위해서는 예측인자 선정과 예측방법에 대한 선택이 매우 중요하다. 최근에는 강수예측 방법으로기계학습 기법이 많이 사용되고 있으며, 그 중에서도 특히 인공신경망을 사용한 강수예측 방법은 좋은 성능을 보였다. 본논문에서는 딥러닝 기법 중 하나인 DBN(deep belief network)를 이용한 새로운 강수예측 방법을 제안한다. DBN는 비지도사전 학습을 통해 초기 가중치를 설정하여 기존 인공신경망의 문제점을 보완한다. 예측인자로는 기온, 전일-전주 강수일, 태양과 달 궤도 관련 자료를 선정하였다. 기온과 전일-전주 강수일은 서울에서의 1974년부터 2013년까지 총 40년간의AWS(automatic weather system) 관측 자료를 사용하였고, 태양과 달의 궤도 관련 자료는 서울을 중심으로 계산한 결과를사용하였다. 전체 기간에서 일부는 학습 자료로 사용하여 예측모델을 생성하였고, 나머지를 생성한 모델의 검증 자료로사용하였다. 모델 검증 결과로 나온 예측값들은 확률값을 가지며 임계치를 이용하여 강수유무를 판별하였다. 강수 정확도의척도로 양분예보기법 중 CSI(critical successive index)와 Bias(frequency bias)를 계산하였다. 이를 통해 DBN와 MLP(multilayer perceptron)의 성능을 비교한 결과 DBN의 강수 예측 정확도가 높았고, 수행속도 또한 2배 이상 빨랐다.For accurate precipitation forecasts the choice of weather faoctrs and prediction method is very important. Recently, machinele arning has been widely used for forecasting precipitation, and artificial neural network, one of machine learning techniques, showed good performance. In this paper, we suggest a new method for forecasting precipitation using DBN, one of deep learning techniques.
DBN has an advantage that initial weights are set by unsupervised learning, so this compensates for the defects of artificial neural networks. We used past precipitation, temperature, and the parameters of the sun and moon's motion as features for forecasting precipitation. The dataset consists of observation data which had been measured for 40 years from AWS in Seoul. Experiments were based on 8-fold cross validation. As a result of estimation, we got probabilities of test dataset, so threshold was used for the decision of precipitation. CSI and Bias were used for indicating the precision of precipitation. Our experimental results showed that DBN performed better than MLP.Deep learning, Deep belief network, Precipitation, Forecast"
1004,딥러닝 신경망모형을 이용한 실시간 교통정보수집 연구,김대현 /KIM DAE HYON,2016,사단법인 인문사회과학기술융합학회,"비디오 기반의 실시간 교통정보수집 시스템은 자율주행을 포함한 지능형교통체계의 주요 서브시스템중 하나이며, 우수한 예측(추정)력 때문에 핵심 알고리즘으로 인공신경망 모형이 주로 사용되고 있다. 특히, 최근 과학 학술지 네이처에 구글 딥마인드의 알파고가 공개되면서 딥러닝과 인공지능에 대한 전 세계적인 관심이 집중되고 있다. 최근까지 가장 널리 사용된 인공신경망모형은 역전파 모형이었으나 최근 컴퓨터 하드웨어 및 알고리즘의 새로운 개발과 더불어 딥러닝 알고리즘을 이용한 딥인공신경망 모형에 대한 관심이 높아지고 있다. 그러나 아직 딥러닝 인공신경망에 대한 연구는 초기단계에 있는 실정이며 적용분야도 극히 한정되어 있다. 본 연구에서는 실시간 도로교통정보 수집을 위한 영상처리분야에서 딥러닝 알고리즘을 활용하고 실험결과를 통하여 향후 교통정보수집‧분석 분야에서의 딥러닝 인공신경망 모형의 적용가능성을 살펴보고자 한다. 특히, 본 연구에서 도출한 딥러닝 인공신경망모형의 실험결과를 기존 역전파 모형에 의한 실험결과와 비교하여 모형의 우수성을 비교‧평가하고자 한다.Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Real-time automatic video-based traffic surveillance system is one of the main subsystems of Intelligent Transportation Systems (ITS) which are advanced applications in order to provide innovative services relating to traffic management and enable various users to be better informed and make safer. Normally, artificial neural networks have been used in real-time applications for collecting traffic information because of their remarkable performance in terms of predictive accuracy and real-time computing. Especially, the recent publication of a paper in the journal Nature and the success of Google DeepMind’s world-beating Go program AlphaGo caused significant increases of the public interest in artificial intelligence. Even though various types of learning models have been proposed, the Backpropagation has been the most popular neural network model. However, the Deep learning neural network model has attracted much interest recently. In this study, the Deep leaning neural network will be applied in image processing techniques for the real-time traffic information collection to investigate the performance of Deep learning algorithm in traffic engineering fields. In addition, the experimental results from Deep learing algorithm will be compared with the results from Backpropagation model to evaluate and propose better performing model.Intelligent Transportation Systems (ITS), artificial neural networks, Deep learning, traffic information, image processing"
1005,딥러닝의 모형과 응용사례,안성만 /SungMahn Ahn,2016,한국지능정보시스템학회,"딥러닝은 인공신경망(neural network)이라는 인공지능분야의 모형이 발전된 형태로서, 계층구조로 이루어진 인공신경망의 내부계층(hidden layer)이 여러 단계로 이루어진 구조이다. 딥러닝에서의 주요 모형은 합성곱신경망(convolutional neural network), 순환신경망(recurrent neural network), 그리고 심층신뢰신경망(deep belief network)의 세가지라고 할 수있다. 그 중에서 현재 흥미로운 연구가 많이 발표되어서 관심이 집중되고 있는 모형은 지도학습(supervised learning)모형인 처음 두 개의 모형이다. 따라서 본 논문에서는 지도학습모형의 가중치를 최적화하는 기본적인 방법인 오류역전파 알고리즘을 살펴본 뒤에 합성곱신경망과 순환신경망의 구조와 응용사례 등을 살펴보고자 한다. 본문에서 다루지 않은 모형인 심층신뢰신경망은 아직까지는 합성곱신경망 이나 순환신경망보다는 상대적으로 주목을 덜 받고 있다. 그러나 심층신뢰신경망은 CNN이나 RNN과는 달리 비지도학습(unsupervised learning)모형이며, 사람이나 동물은 관찰을 통해서 스스로 학습한다는 점에서 궁극적으로는 비지도학습모형이 더 많이 연구되어야 할 주제가 될 것이다.Deep learning model is a kind of neural networks that allows multiple hidden layers. There are various deep learning architectures such as convolutional neural networks, deep belief networks and recurrent neural networks. Those have been applied to fields like computer vision, automatic speech recognition, natural language processing, audio recognition and bioinformatics where they have been shown to produce state-of-the-art results on various tasks. Among those architectures, convolutional neural networks and recurrent neural networks are classified as the supervised learning model. And in recent years, those supervised learning models have gained more popularity than unsupervised learning models such as deep belief networks, because supervised learning models have shown fashionable applications in such fields mentioned above.
Deep learning models can be trained with backpropagation algorithm. Backpropagation is an abbreviation for “backward propagation of errors” and a common method of training artificial neural networks used in conjunction with an optimization method such as gradient descent. The method calculates the gradient of an error function with respect to all the weights in the network. The gradient is fed to the optimization method which in turn uses it to update the weights, in an attempt to minimize the error function.
Convolutional neural networks use a special architecture which is particularly well-adapted to classify images. Using this architecture makes convolutional networks fast to train. This, in turn, helps us train deep, muti-layer networks, which are very good at classifying images. These days, deep convolutional networks are used in most neural networks for image recognition. Convolutional neural networks use three basic ideas: local receptive fields, shared weights, and pooling. By local receptive fields, we mean that each neuron in the first(or any) hidden layer will be connected to a small region of the input(or previous layer’s) neurons. Shared weights mean that we’re going to use the same weights and bias for each of the local receptive field. This means that all the neurons in the hidden layer detect exactly the same feature, just at different locations in the input image. In addition to the convolutional layers just described, convolutional neural networks also contain pooling layers. Pooling layers are usually used immediately after convolutional layers. What the pooling layers do is to simplify the information in the output from the convolutional layer.
Recent convolutional network architectures have 10 to 20 hidden layers and billions of connections between units. Training deep learning networks has taken weeks several years ago, but thanks to progress in GPU and algorithm enhancement, training time has reduced to several hours.
Neural networks with time-varying behavior are known as recurrent neural networks or RNNs. A recurrent neural network is a class of artificial neural network where connections between units form a directed cycle. This creates an internal state of the network which allows it to exhibit dynamic temporal behavior. Unlike feedforward neural networks, RNNs can use their internal memory to process arbitrary sequences of inputs. Early RNN models turned out to be very difficult to train, harder even than deep feedforward networks. The reason is the unstable gradient problem such as vanishing gradient and exploding gradient. The gradient can get smaller and smaller as it is propagated back through layers. This makes learning in early layers extremely slow. The problem actually gets worse in RNNs, since gradients aren’t just propagated backward through layers, they’re propagated backward through time. If the network runs for a long time, that can make the gradient extremely unstable and hard to learn from. It has been possible to incorporate an idea known as long short-term memory units (LSTMs) into RNNs. LSTMs make it much easier to get good results when training RNNs, and many recent papers make use of LSTMs or related ideas.Deep learning, Convolutional neural networks, Recurrent neural networks, Error backpropagation algorithm"
1006,딥러닝분석과 기술적 분석 지표를 이용한 한국 코스피주가지수 방향성 예측,이우식 /Lee Woosik,2017,한국데이터정보과학회,"2016년 3월 구글 (Google)의 바둑인공지능 알파고 (AlphaGo)가 이세돌 9단과의 바둑대결에서승리한 이후 다양한 분야에서 인공지능 사용에 대한 관심이 높아지고 있는 가운데 금융투자 분야에서도 인공지능과 투자자문 전문가의 합성어인 로보어드바이저 (Robo-Advisor)에 대한 관심이 높아지고 있다. 인공지능 (artificial intelligence)기반의 의사결정은 비용 절감은 물론 효과적인 의사결정을가능하게 한다는 점에서 큰 장점이 있다. 본 연구에서는 기술적 분석 (technical analysis) 지표와 딥러닝 (deep learning) 모형을 결합하여 한국 코스피 지수를 예측하는 모형을 개발하고 제시한 모형들의 예측력을 비교, 분석한다. 분석 결과 기술적 분석 지표에 딥러닝 알고리즘을 결합한 모형이 주가지수 방향성 예측 문제에 응용될 수 있음을 확인하였다. 향후 본 연구에서 제안된 기술적 분석 지표와딥러닝모형을 결합한 기법은 로보어드바이저서비스에 응용할 수 있는 일반화 가능성을 보여준다Since Google’s AlphaGo defeated a world champion of Go players in 2016, there have been many interests in the deep learning. In the financial sector, a Robo-Advisor using deep learning gains a significant attention, which builds and manages portfolios of financial instruments for investors.In this paper, we have proposed the a deep learning algorithm geared toward identification and forecast of the KOSPI index direction,and we also have compared the accuracy of the prediction.In an application of forecasting the financial market index direction, we have shown that the Robo-Advisor using deep learning has a significant effect on finance industry. The Robo-Advisor collects a massive data such as earnings statements, news reports and regulatory filings, analyzes those and recommends investors how to view market trends and identify the best time to purchase financial assets. On the other hand, the Robo-Advisor allows businesses to learn more about their customers, develop better marketing strategies, increase sales and decrease costs.Artificial intelligence, deep learning, FinTech, Robo-Advisor, technical analysis."
1007,딥러닝의 효율적인 학습을 위한 학습데이터 선별 기법에 관한 연구,주경돈 /Gyoung-don Joo,2017,한국정보과학회,"빅데이터의 증가와 병렬 하드웨어 처리, 학습 알고리즘의 개선은 딥러닝의 발전을 가져왔으며 인공지능이 적용 가능한 영역을 획기적으로 확장시키고 있다. 하지만 이러한 대규모의 학습 데이터에 기반한 딥러닝은 학습시간이 증가되고, 더 나아가 학습 데이터의 구축을 위한 레이블링(labeling) 비용이 데이터 수에 비례하여 증가하게 된다. 따라서 본 논문에서는, 딥러닝의 정확도를 최대한 유지하면서 필요한 학습 데이터의 수를 줄일수 있도록, 전체 데이터에서 제한된 수의 학습 데이터의 후보를 선별하는 기법에 대하여 연구한다. 특히 기존의 능동 학습에 기반한 기법들이 학습 데이터의 수가 매우 적어질 경우 딥러닝의 정확도가 급격히 떨어지는 문제를 해결할 수 있는 새로운 기법을 제안하였으며, 제안된 기법으로 선별된 학습 데이터는 그 수의 변화에 대해 딥러닝이 일관되게 경쟁력있는 정확도를 가짐을 다양한 실험을 통하여 검증하였다.The increase of big data, parallel hardware processing and enhanced learning algorithms have led to the development of deep learning and are dramatically expanding the domain of applications based on artificial intelligence. However, such large-scale training data extends the learning time, and furthermore, the labeling cost for building the training set increases in proportion to the volume of data. Therefore, in this paper, we study how to select a limited number of candidates for training set from given data so as to reduce the number of training set while maintaining the accuracy of deep learning. Especially, we found that, with the training set selected by the previous work, the accuracy of deep learning drastically decreases when the number of training set is very small and so proposed new methods to resolve this fall-off in accuracy. Through experiments, we showed that deep learning consistently has the competitive accuracy with the training set selected by the proposed method regardless of the size of training set.Deep Learning, Train Set Selection, Active Learning, Gamma distribution"
1008,딥러닝을 이용한 범용적 스테그아날리시스,김현재 /Hyunjae Kim,2017,한국정보과학회,"스테그아날리시스(Steganalysis)란 이미지 등 일반적인 자료에 암호화된 정보를 은닉하는 스테가노그래피(Steganography)에 대한 검출 및 분석 방법으로, 기계학습 기반 방법론을 포함한다. 기존 기계학습 기반 스테그아날리시스는 영상(Image)의 특징(Feature) 추출 및 모델링에 기반하며, 최근 딥러닝(Deep Learning)의 적용으로 검출 정확도가 큰 폭으로 향상되었다. 하지만 현존하는 스테그아날리시스 모델은 단일 스테가노그래피 기법에 대해 국한되어 있어 학습에 사용되지 않은 스테고(Stego) 이미지의 경우 검출이 불가능한 결정적 한계를 가진다. 본 연구에서는 다양한 스테가노그래피 기법으로 생성된 스테고이미지에 딥러닝을 적용하여 스테그아날리시스를 학습하는 범용적 모델을 제안한다. 다양한 실험을 통해 제안 기법의 효용성 및 가능성을 확인하고, 범용적 스테그아날리시스 모델이 각각에 특화된 검출 기법과 유사한 정확도로 스테고 이미지를 검출할 수 있음을 보인다.Steganalysis is to detect information hidden by steganography inside general data such as images. There are stegoanalysis techniques that use machine learning (ML). Existing ML approaches to steganalysis are based on extracting features from stego images and modeling them.
Recently deep learning-based methodologies have shown significant improvements in detection accuracy. However, all the existing methods, including deep learning-based ones, have a critical limitation in that they can only detect stego images that are created by a specific steganography method. In this paper, we propose a generalized steganalysis method that can model multiple types of stego images using deep learning. Through various experiments, we confirm the effectiveness of our approach and envision directions for future research. In particular, we show that our method can detect each type of steganography with the same level of accuracy as that of a steganalysis method dedicated to that type of steganography, thereby demonstrating the general applicability of our approach to multiple types of stego images.steganalysis, steganography, deep learning, generalized steganalysis model"
1009,"딥러닝 프레임워크의 비교: 티아노, 텐서플로, CNTK를 중심으로",정여진 /Yeojin Chung,2017,한국지능정보시스템학회,"딥러닝 프레임워크의 대표적인 기능으로는 ‘자동미분’과 ‘GPU의 활용’ 등을 들 수 있다. 본 논문은 파이썬의 라이브러리 형태로 사용 가능한 프레임워크 중에서 구글의 텐서플로와 마이크로소프트의 CNTK, 그리고 텐서 플로의 원조라고 할 수 있는 티아노를 비교하였다. 본문에서는 자동미분의 개념과 GPU의 활용형태를 간단히 설명하고, 그 다음에 logistic regression을 실행하는 예를 통하여 각 프레임워크의 문법을 알아본 뒤에, 마지막으로 대표적인 딥러닝 응용인 CNN의 예제를 실행시켜보고 코딩의 편의성과 실행속도 등을 확인해 보았다. 그 결과, 편의성의 관점에서 보면 티아노가 가장 코딩 하기가 어렵고, CNTK와 텐서플로는 많은 부분이 비슷하게 추상화 되어 있어서 코딩이 비슷하지만 가중치와 편향을 직접 정의하느냐의 여부에서 차이를 보였다. 그리고 각프레임워크의 실행속도에 대한 평가는 ‘큰 차이는 없다’는 것이다. 텐서플로는 티아노에 비하여 속도가 느리다는 평가가 있어왔는데, 본 연구의 실험에 의하면, 비록 CNN 모형에 국한되었지만, 텐서플로가 아주 조금이지만빠른 것으로 나타났다. CNTK의 경우에도, 비록 실험환경이 달랐지만, 실험환경의 차이에 의한 속도의 차이의 편차범위 이내에 있는 것으로 판단이 되었다. 본 연구에서는 세 종류의 딥러닝 프레임워크만을 살펴보았는데,위키피디아에 따르면 딥러닝 프레임워크의 종류는 12가지가 있으며, 각 프레임워크의 특징을 15가지 속성으로구분하여 차이를 특정하고 있다. 그 많은 속성 중에서 사용자의 입장에서 볼 때 중요한 속성은 어떤 언어(파이썬, C++, Java, 등)로 사용가능한지, 어떤 딥러닝 모형에 대한 라이브러리가 잘 구현되어 있는지 등일 것이다. 그리고 사용자가 대규모의 딥러닝 모형을 구축한다면, 다중 GPU 혹은 다중 서버를 지원하는지의 여부도 중요할 것이다. 또한 딥러닝 모형을 처음 학습하는 경우에는 사용설명서가 많은지 예제 프로그램이 많은지 여부도 중요한 기준이 될 것이다.The deep learning framework is software designed to help develop deep learning models. Some of its important functions include “automatic differentiation” and “utilization of GPU”. The list of popular deep learning framework includes Caffe (BVLC) and Theano (University of Montreal). And recently, Microsoft's deep learning framework, Microsoft Cognitive Toolkit, was released as open-source license, following Google’s Tensorflow a year earlier. The early deep learning frameworks have been developed mainly for research at universities. Beginning with the inception of Tensorflow, however, it seems that companies such as Microsoft and Facebook have started to join the competition of framework development. Given the trend, Google and other companies are expected to continue investing in the deep learning framework to bring forward the initiative in the artificial intelligence business. From this point of view, we think it is a good time to compare some of deep learning frameworks. So we compare three deep learning frameworks which can be used as a Python library. Those are Google's Tensorflow, Microsoft’s CNTK, and Theano which is sort of a predecessor of the preceding two.
The most common and important function of deep learning frameworks is the ability to perform automatic differentiation. Basically all the mathematical expressions of deep learning models can be represented as computational graphs, which consist of nodes and edges. Partial derivatives on each edge of a computational graph can then be obtained. With the partial derivatives, we can let software compute differentiation of any node with respect to any variable by utilizing chain rule of Calculus.
First of all, the convenience of coding is in the order of CNTK, Tensorflow, and Theano. The criterion is simply based on the lengths of the codes and the learning curve and the ease of coding are not the main concern. According to the criteria, Theano was the most difficult to implement with, and CNTK and Tensorflow were somewhat easier. With Tensorflow, we need to define weight variables and biases explicitly. The reason that CNTK and Tensorflow are easier to implement with is that those frameworks provide us with more abstraction than Theano. We, however, need to mention that low-level coding is not always bad. It gives us flexibility of coding. With the low-level coding such as in Theano, we can implement and test any new deep learning models or any new search methods that we can think of.
The assessment of the execution speed of each framework is that there is not meaningful difference. According to the experiment, execution speeds of Theano and Tensorflow are very similar, although the experiment was limited to a CNN model. In the case of CNTK, the experimental environment was not maintained as the same. The code written in CNTK has to be run in PC environment without GPU where codes execute as much as 50 times slower than with GPU. But we concluded that the difference of execution speed was within the range of variation caused by the different hardware setup.
In this study, we compared three types of deep learning framework: Theano, Tensorflow, and CNTK. According to Wikipedia, there are 12 available deep learning frameworks. And 15 different attributes differentiate each framework. Some of the important attributes would include interface language (Python, C ++, Java, etc.) and the availability of libraries on various deep learning models such as CNN, RNN, DBN, and etc. And if a user implements a large scale deep learning model, it will also be important to support multiple GPU or multiple servers. Also, if you are learning the deep learning model, it would also be important if there are enough examples and references.deep learning framework, Theano, TensorFlow, CNTK, computational graph, CIFAR-10"
